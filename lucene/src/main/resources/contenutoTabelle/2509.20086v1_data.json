{
    "S4.T1": {
        "source_file": "OLaPh: Optimal Language Phonemizer",
        "caption": "Table 1: Word-level comparison of OLaPh, eSpeakNG, and Gruut, showing exact, partial, and mismatched phonemizations.",
        "body": "Language\n\n\n\n\n\nDE\n\n\n\n\nEN\n\n\n\n\n\n\n\n\nMatch between all systems\n\n\n\n\n1417\n\n\n\n\n2575\n\n\n\n\n\n\nMatch between OLaPh and eSpeakNG\n\n\n\n\n1506\n\n\n\n\n578\n\n\n\n\n\n\nMatch between OLaPh and Gruut\n\n\n\n\n2,334\n\n\n\n\n659\n\n\n\n\n\n\nMismatch OLaPh and Gruut or eSpeakNG\n\n\n\n\n10,337\n\n\n\n\n7,140\n\n\n\n\n\n\nTotal\n\n\n\n\n15,594\n\n\n\n\n10,952",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">&#8195;&#8195;Language</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">DE</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">EN</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">Match between all systems</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">1417</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">2575</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">Match between OLaPh and eSpeakNG</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">1506</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">578</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">Match between OLaPh and Gruut</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">2,334</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">659</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\">Mismatch OLaPh and Gruut or eSpeakNG</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">10,337</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">7,140</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Total</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text ltx_align_right ltx_font_bold\">15,594</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text ltx_align_right ltx_font_bold ltx_align_center\">10,952</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "language",
            "match",
            "gruut",
            "systems",
            "all",
            "wordlevel",
            "espeakng",
            "partial",
            "comparison",
            "mismatch",
            "showing",
            "mismatched",
            "olaph",
            "total",
            "exact",
            "between",
            "phonemizations"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.20086v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Evaluation of OLaPh &#8227; 4 Evaluations &#8227; OLaPh: Optimal Language Phonemizer\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> summarizes word-level phonemization comparisons across OLaPh, eSpeakNG, and Gruut, showing full matches, partial matches, and full mismatches where OLaPh differed from both baselines. In English, there were 7,140 mismatches (10,337 in German). A random sample of 500 mismatched words was manually reviewed and categorized as: alternative variants or differing encodings (not counted as errors); correct by OLaPh but incorrect by at least one baseline; incorrect by OLaPh while at least one baseline was correct; all systems incorrect; or ambiguous/unclear phonemization. This evaluation enabled direct comparison, identified linguistic challenges, and guided improvements. For comparability, verbose symbols (e.g., emphasis marks, doubly articulated consonants) were removed, as OLaPh outputs them more frequently than eSpeakNG and Gruut.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Phonemization, the conversion of text into phonemes, is a key step in text-to-speech. Traditional approaches use rule-based transformations and lexicon lookups, while more advanced methods apply preprocessing techniques or neural networks for improved accuracy on out-of-domain vocabulary. However, all systems struggle with names, loanwords, abbreviations, and homographs. This work presents OLaPh (Optimal Language Phonemizer), a framework that combines large lexica, multiple NLP techniques, and compound resolution with a probabilistic scoring function. Evaluations in German and English show improved accuracy over previous approaches, including on a challenging dataset. To further address unresolved cases, we train a large language model on OLaPh-generated data, which achieves even stronger generalization and performance. Together, the framework and LLM improve phonemization consistency and provide a freely available resource for future research.</p>\n\n",
                "matched_terms": [
                    "language",
                    "olaph",
                    "systems",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces OLaPh (Optimal Language Phonemizer), a framework that enhances previous approaches by leveraging large phonological resources, improving compound word handling with probabilistic scoring, and integrating NLP techniques such as named entity recognition (NER), POS tagging, and language detection. OLaPh provides accurate, transparent phoneme representations, particularly for morphologically rich languages like German, including dialectal variants. Manual evaluation in German and English compares OLaPh with baselines, while a challenge test set highlights its strengths on edge cases. Finally, a large language model (LLM) trained on OLaPh-generated data shows superior generalization compared to both the framework itself and other LLM-based phonemization models.</p>\n\n",
                "matched_terms": [
                    "language",
                    "olaph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on phonemization showed that simple grapheme-to-phoneme\nalignment is insufficient due to the many exceptions in pronunciation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib7\" title=\"\">7</a>, pp.&#160;105-106]</cite>. Two main strategies emerged: dictionary-based methods with rules for unknown words, and rule-based methods with exception lists <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib8\" title=\"\">8</a>]</cite>.\nModern systems such as eSpeakNG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib9\" title=\"\">9</a>]</cite> extend these approaches with curated rules and large dictionaries, but still fail on homographs and foreign words.\nGruut <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib10\" title=\"\">10</a>]</cite> instead combines lexica with grapheme-to-phoneme (G2P) models based on conditional random fields, supported by parsers and POS tagging. This allows it to disambiguate homographs (e.g., <span class=\"ltx_text ltx_font_italic\">wound</span> /wu<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textlengthmark</span>nd/ as a noun vs. <span class=\"ltx_text ltx_font_italic\">wound</span> /wa<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textupsilon</span>nd/ as a verb), but often reduces consistency overall.\nMore recent G2P approaches employ neural networks, such as the transformer-based T5G2P <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib11\" title=\"\">11</a>]</cite> for English and Czech or multilingual models based on ByT5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib13\" title=\"\">13</a>]</cite>. While these achieve strong results, they remain limited by the quality of training data and continue to struggle with context-dependent or low-resource phonemization. LLMs have also been proposed for phonemization <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib14\" title=\"\">14</a>]</cite>, though direct comparisons are difficult due to differing datasets and metrics <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib15\" title=\"\">15</a>]</cite>. These LLM-based approaches highlight a growing shift toward models that integrate broader contextual understanding, but they also raise questions of efficiency and adaptability across languages.</p>\n\n",
                "matched_terms": [
                    "espeakng",
                    "gruut",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many phonemization systems struggle with numerical expressions, mathematical symbols, and special characters. By normalizing numbers (e.g., dates, times, decimals, ordinals) and symbols into their spoken equivalents before phonemization, inconsistencies in synthesized speech can be reduced. For the English language, the library num2words <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib17\" title=\"\">17</a>]</cite> is used, which is also utilized in the Gruut framework. For German, a dedicated normalization algorithm was implemented, as its complex case system required a more refined solution.</p>\n\n",
                "matched_terms": [
                    "language",
                    "gruut",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When a word is not found in the extensive lexica, a probabilistic compound-splitting algorithm is applied. This algorithm segments the word into plausible subword candidates and scores each combination based on their frequency, length, and likelihood in a large reference corpus of over 15 million sentences from a recent Wikipedia dump, adapted to each supported language. In languages like German, where long compounds and interleaved loanwords are common, simple left-to-right or right-to-left lexicon lookups are insufficient. Instead, the algorithm evaluates all possible segmentations and assigns scores according to the following principle:</p>\n\n",
                "matched_terms": [
                    "language",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.20086v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 Framework overview &#8227; 3 Approach &#8227; OLaPh: Optimal Language Phonemizer\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, the workflow begins with an input text and its specified or automatically detected language. The text is split into sentences using language-specific spaCy models, which are also used for NER and POS tagging. Capitalized words are checked against an abbreviation lexicon (e.g., &#8220;NATO&#8221; &#8594; <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>ne<span class=\"ltx_ERROR undefined\">\\textsci</span>.to<span class=\"ltx_ERROR undefined\">\\textupsilon</span>/), and numbers and symbols are normalized. NER with language detection ensures foreign names are handled correctly, after which the sentence is phonemized through lexicon lookups with backup mechanisms. If a word is not found in the selected language&#8217;s lexicon, cross-language detection and global lookup are attempted. As a last resort, the compound splitting component generates all possible subword combinations, scores them, and produces the final phonemization, preserving punctuation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the rule-based framework, OLaPh also serves as a data generator for training phonemization models. To explore this, we trained a GemmaX-based LLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib20\" title=\"\">20</a>]</cite> on grapheme-phoneme pairs produced by OLaPh from the FineWeb corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib22\" title=\"\">22</a>]</cite>. For each of the four target languages (English, French, German, Spanish), approximately 2.5M pairs were used, yielding 10M training examples in total. The tokenizer was extended with 1024 phoneme-derived tokens, a small augmentation compared to the training set size, to test generalization from limited phoneme-specific vocabulary. Unlike modular NLP components such as SpaCy, the LLM learns end-to-end phonemization directly, without requiring explicit preprocessing.</p>\n\n",
                "matched_terms": [
                    "olaph",
                    "total"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate OLaPh, we implemented it in Python and compared its word-level performance against eSpeakNG and Gruut. For English and German, 5,000 sentences were sampled from FineWeb <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib22\" title=\"\">22</a>]</cite> and phonemized with all three systems. Phonemized outputs were aligned, and words with differing results were selected for analysis. A pre-trained ByT5 model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib13\" title=\"\">13</a>]</cite> was also considered, but excluded after preliminary tests showed it failed to disambiguate homographs and frequently misapplied phonemic transformations to loanwords. All evaluations were conducted by a researcher with expertise in applied speech synthesis and phonemization.</p>\n\n",
                "matched_terms": [
                    "gruut",
                    "systems",
                    "all",
                    "wordlevel",
                    "espeakng",
                    "olaph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.20086v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Evaluation of OLaPh &#8227; 4 Evaluations &#8227; OLaPh: Optimal Language Phonemizer\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, most discrepancies between the phonemization algorithms stem from alternative pronunciations and differing IPA encodings, suggesting that all frameworks already achieve high accuracy, particularly in English. However, OLaPh outperformed the others in several cases on the German test data. In nearly all of these instances, OLaPh excelled with foreign words, primarily English, which were correctly identified by the language detection module and phonemized accordingly. Some sentences also included English clauses that were flawlessly detected. Additionally, a few German words were correctly phonemized only by OLaPh. For example, Kriegsspiel (&#8220;war game&#8221;) was phonemized as <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/k<span class=\"ltx_ERROR undefined\">\\textinvscr</span>iks<span class=\"ltx_ERROR undefined\">\\textesh</span>pil/, while the other frameworks produced <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/k<span class=\"ltx_ERROR undefined\">\\textinvscr</span>ikzspil/ and <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/k<span class=\"ltx_ERROR undefined\">\\textfishhookr</span>ikspil/, omitting the meaningful phone [<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textesh</span>]. In statistically ambiguous cases, the built-in language detection occasionally misidentified the origin language, leading to errors. For example, the German word <span class=\"ltx_text ltx_font_italic\">Stapler</span> (forklift) was incorrectly recognized as the English <span class=\"ltx_text ltx_font_italic\">stapler</span> and consequently misphonemized. In a few cases, all phonemization outputs were incorrect, or the pronunciation was inherently ambiguous or depends on user preference. This includes words like Montr&#233;al, where pronunciation depends on the language context, such as choosing between the French /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>m\\&#771;textopeno.<span class=\"ltx_ERROR undefined\">\\textinvscr</span>eal / and the English /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textsecstress</span>m<span class=\"ltx_ERROR undefined\">\\textturnv</span>nt<span class=\"ltx_ERROR undefined\">\\textturnr</span>i<span class=\"ltx_ERROR undefined\">\\textlengthmark</span><span class=\"ltx_ERROR undefined\">\\textprimstress</span><span class=\"ltx_ERROR undefined\">\\textturnscripta</span>l/.\nTo better assess edge cases, a small test dataset was created with challenging phonemization examples. It includes 100 English sentences containing homographs, along with French, German, and Spanish names. All three phonemization systems processed the sentences, and their outputs were manually compared at the word level.</p>\n\n",
                "matched_terms": [
                    "language",
                    "systems",
                    "all",
                    "olaph",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">OLaPh made significantly fewer errors than the other systems on more complex sentences, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.20086v1#S4.T3\" title=\"Table 3 &#8227; 4.1 Evaluation of OLaPh &#8227; 4 Evaluations &#8227; OLaPh: Optimal Language Phonemizer\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. For example, names from other languages were phonemized more accurately, and homographs were less often incorrectly distinguished. However, there were a few instances where the model used for POS tagging made incorrect predictions or detected the incorrect origin language. Lastly, some homographs required contextual understanding for accurate phonemization. An example is the word <span class=\"ltx_text ltx_font_italic\">bass</span>, which can refer both to a fish (/b&#230;s/) and a musical instrument (/be<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textsci</span>s/). Both interpretations are nouns that must be phonemized based on context surrounding them. All systems made errors with this type of homograph.</p>\n\n",
                "matched_terms": [
                    "language",
                    "olaph",
                    "systems",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate OLaPh as a data generator, we sampled 100 additional sentences per language from the FineWeb corpus and compared outputs from the framework and the GemmaX-based LLM. For each word, we recorded whether the two systems produced identical phonemizations and manually classified mismatches as correct or incorrect.</p>\n\n",
                "matched_terms": [
                    "language",
                    "olaph",
                    "phonemizations",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English, 1,822 words matched exactly, with only 7 mismatches; in 5 of these cases the LLM produced the correct phonemization. In German, 1,286 words matched exactly, with 6 mismatches. In all of these cases, both outputs were plausible, such as the pronunciation of <span class=\"ltx_text ltx_font_italic\">Xbox</span> as the English /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textepsilon</span>ksb<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textscripta</span>ks/ (LLM) versus the germanized /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textsci</span>ksb<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textopeno</span>ks/ (framework), both of which are commonly used. Most LLM improvements arose from more reliable recognition of loanwords and from cases where OLaPh produced suboptimal compound splitting in English. Overall, the LLM closely matched the framework on standard vocabulary while showing greater robustness in cross-lingual and structurally complex cases.</p>\n\n",
                "matched_terms": [
                    "showing",
                    "olaph",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluations show that OLaPh performs on par with existing frameworks in baseline settings but surpasses them on complex sentences and German phonemization. The challenge dataset also revealed systematic errors. In particular, the language detection module sometimes misclassified named entities due to insufficient context, while NER and POS tagging occasionally produced incorrect predictions. These issues stem from model limitations and could be reduced by integrating more advanced NLP components. The framework has so far been evaluated only for German and English, where compound splitting proved effective; its performance in other morphologically rich languages remains to be verified.</p>\n\n",
                "matched_terms": [
                    "language",
                    "olaph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Certain words and phrases were misphonemized by all systems. Many of these involve context-dependent terms or user preferences. Addressing such ambiguities may require integrating language models to exploit broader sentence context, disambiguate homographs, or adapt phoneme predictions to domain vocabulary. Our follow-up evaluation with a GemmaX-based LLM trained on OLaPh data supports this: the LLM correctly handled most loanwords that OLaPh misdetected, showing improved robustness</p>\n\n",
                "matched_terms": [
                    "language",
                    "systems",
                    "all",
                    "showing",
                    "olaph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented OLaPh, a phonemization framework for English and German that extends previous approaches with NER, POS tagging, language detection, probabilistic compound handling, and stepwise backup mechanisms. Manual evaluation showed high accuracy and clear improvements on challenging phrases, while also revealing linguistic issues that remain open problems for phonemization. In follow-up work, we trained a GemmaX-based LLM on OLaPh-generated data, which further improved robustness in cross-lingual cases. To support future research, the framework, the LLM, and the challenge dataset are freely available.</p>\n\n",
                "matched_terms": [
                    "language",
                    "olaph"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "OLaPh: Optimal Language Phonemizer",
        "caption": "Table 2: Manual categorization of phonemization differences for English and German",
        "body": "Language\n\n\nDE\n\n\n\n\nEN\n\n\n\n\n\n\nDifferent but correct\n\n\n341 (68.2%)\n\n\n\n\n492 (98.4%)\n\n\n\n\nIncorrect eSpeakNG\n\n\n73 (14.6%)\n\n\n\n\n2 (0.4%)\n\n\n\n\nIncorrect Gruut\n\n\n118 (23.6%)\n\n\n\n\n3 (0.6%)\n\n\n\n\nIncorrect OLaPh\n\n\n16 (3.2%)\n\n\n\n\n1 (0.2%)\n\n\n\n\nIncorrect All\n\n\n3 (0.6%)\n\n\n\n\n0\n\n\n\n\nUnclear\n\n\n9 (1.8%)\n\n\n\n\n3 (0.6%)",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Language</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">DE</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">EN</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Different but correct</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">341 (68.2%)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">492 (98.4%)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Incorrect eSpeakNG</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">73 (14.6%)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">2 (0.4%)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Incorrect Gruut</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">118 (23.6%)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3 (0.6%)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Incorrect OLaPh</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">16 (3.2%)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">1 (0.2%)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Incorrect All</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3 (0.6%)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Unclear</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">9 (1.8%)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3 (0.6%)</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "unclear",
            "language",
            "english",
            "differences",
            "phonemization",
            "gruut",
            "all",
            "manual",
            "espeakng",
            "incorrect",
            "german",
            "different",
            "olaph",
            "correct",
            "categorization"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.20086v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Evaluation of OLaPh &#8227; 4 Evaluations &#8227; OLaPh: Optimal Language Phonemizer\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, most discrepancies between the phonemization algorithms stem from alternative pronunciations and differing IPA encodings, suggesting that all frameworks already achieve high accuracy, particularly in English. However, OLaPh outperformed the others in several cases on the German test data. In nearly all of these instances, OLaPh excelled with foreign words, primarily English, which were correctly identified by the language detection module and phonemized accordingly. Some sentences also included English clauses that were flawlessly detected. Additionally, a few German words were correctly phonemized only by OLaPh. For example, Kriegsspiel (&#8220;war game&#8221;) was phonemized as <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/k<span class=\"ltx_ERROR undefined\">\\textinvscr</span>iks<span class=\"ltx_ERROR undefined\">\\textesh</span>pil/, while the other frameworks produced <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/k<span class=\"ltx_ERROR undefined\">\\textinvscr</span>ikzspil/ and <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/k<span class=\"ltx_ERROR undefined\">\\textfishhookr</span>ikspil/, omitting the meaningful phone [<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textesh</span>]. In statistically ambiguous cases, the built-in language detection occasionally misidentified the origin language, leading to errors. For example, the German word <span class=\"ltx_text ltx_font_italic\">Stapler</span> (forklift) was incorrectly recognized as the English <span class=\"ltx_text ltx_font_italic\">stapler</span> and consequently misphonemized. In a few cases, all phonemization outputs were incorrect, or the pronunciation was inherently ambiguous or depends on user preference. This includes words like Montr&#233;al, where pronunciation depends on the language context, such as choosing between the French /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>m\\&#771;textopeno.<span class=\"ltx_ERROR undefined\">\\textinvscr</span>eal / and the English /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textsecstress</span>m<span class=\"ltx_ERROR undefined\">\\textturnv</span>nt<span class=\"ltx_ERROR undefined\">\\textturnr</span>i<span class=\"ltx_ERROR undefined\">\\textlengthmark</span><span class=\"ltx_ERROR undefined\">\\textprimstress</span><span class=\"ltx_ERROR undefined\">\\textturnscripta</span>l/.\nTo better assess edge cases, a small test dataset was created with challenging phonemization examples. It includes 100 English sentences containing homographs, along with French, German, and Spanish names. All three phonemization systems processed the sentences, and their outputs were manually compared at the word level.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Phonemization, the conversion of text into phonemes, is a key step in text-to-speech. Traditional approaches use rule-based transformations and lexicon lookups, while more advanced methods apply preprocessing techniques or neural networks for improved accuracy on out-of-domain vocabulary. However, all systems struggle with names, loanwords, abbreviations, and homographs. This work presents OLaPh (Optimal Language Phonemizer), a framework that combines large lexica, multiple NLP techniques, and compound resolution with a probabilistic scoring function. Evaluations in German and English show improved accuracy over previous approaches, including on a challenging dataset. To further address unresolved cases, we train a large language model on OLaPh-generated data, which achieves even stronger generalization and performance. Together, the framework and LLM improve phonemization consistency and provide a freely available resource for future research.</p>\n\n",
                "matched_terms": [
                    "language",
                    "english",
                    "phonemization",
                    "all",
                    "german",
                    "olaph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\nPhonemization, Natural Language Processing</p>\n\n",
                "matched_terms": [
                    "language",
                    "phonemization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Phonemization, the conversion of text (graphemes) into phonemes, is a core component of text-to-speech (TTS) systems, ensuring correct pronunciation, prosody, and intelligibility. Some modern TTS models rely solely on deep neural networks to infer pronunciation from raw text <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib2\" title=\"\">2</a>]</cite>, but this sacrifices control and adaptability. Users cannot manually correct rare words, loanwords, or domain-specific terms, and improving performance often requires resource-intensive retraining <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib3\" title=\"\">3</a>]</cite>, which is impractical in many applications.</p>\n\n",
                "matched_terms": [
                    "correct",
                    "phonemization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces OLaPh (Optimal Language Phonemizer), a framework that enhances previous approaches by leveraging large phonological resources, improving compound word handling with probabilistic scoring, and integrating NLP techniques such as named entity recognition (NER), POS tagging, and language detection. OLaPh provides accurate, transparent phoneme representations, particularly for morphologically rich languages like German, including dialectal variants. Manual evaluation in German and English compares OLaPh with baselines, while a challenge test set highlights its strengths on edge cases. Finally, a large language model (LLM) trained on OLaPh-generated data shows superior generalization compared to both the framework itself and other LLM-based phonemization models.</p>\n\n",
                "matched_terms": [
                    "language",
                    "english",
                    "phonemization",
                    "manual",
                    "german",
                    "olaph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on phonemization showed that simple grapheme-to-phoneme\nalignment is insufficient due to the many exceptions in pronunciation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib7\" title=\"\">7</a>, pp.&#160;105-106]</cite>. Two main strategies emerged: dictionary-based methods with rules for unknown words, and rule-based methods with exception lists <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib8\" title=\"\">8</a>]</cite>.\nModern systems such as eSpeakNG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib9\" title=\"\">9</a>]</cite> extend these approaches with curated rules and large dictionaries, but still fail on homographs and foreign words.\nGruut <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib10\" title=\"\">10</a>]</cite> instead combines lexica with grapheme-to-phoneme (G2P) models based on conditional random fields, supported by parsers and POS tagging. This allows it to disambiguate homographs (e.g., <span class=\"ltx_text ltx_font_italic\">wound</span> /wu<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textlengthmark</span>nd/ as a noun vs. <span class=\"ltx_text ltx_font_italic\">wound</span> /wa<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textupsilon</span>nd/ as a verb), but often reduces consistency overall.\nMore recent G2P approaches employ neural networks, such as the transformer-based T5G2P <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib11\" title=\"\">11</a>]</cite> for English and Czech or multilingual models based on ByT5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib13\" title=\"\">13</a>]</cite>. While these achieve strong results, they remain limited by the quality of training data and continue to struggle with context-dependent or low-resource phonemization. LLMs have also been proposed for phonemization <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib14\" title=\"\">14</a>]</cite>, though direct comparisons are difficult due to differing datasets and metrics <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib15\" title=\"\">15</a>]</cite>. These LLM-based approaches highlight a growing shift toward models that integrate broader contextual understanding, but they also raise questions of efficiency and adaptability across languages.</p>\n\n",
                "matched_terms": [
                    "espeakng",
                    "english",
                    "phonemization",
                    "gruut"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">OLaPh builds on Gruut&#8217;s lexicon-based approach, using recent Wiktionary dumps to extract IPA transcriptions in four languages. While the system currently supports English and German, lexica for French and Spanish were also extracted due to frequent loanwords. The English lexicon contains <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>147k entries, and the German lexicon <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>842k (including inflections). Additional lexica were created for character mapping, abbreviations, and symbols. As Wiktionary updates, the lexica can be easily extended, improving accuracy over time.</p>\n\n",
                "matched_terms": [
                    "olaph",
                    "english",
                    "german"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Proper names and organizations often follow non-standard or foreign pronunciation. Detecting entities prior to phonemization allows applying language-specific rules (e.g., &#8220;FBI&#8221; in German remains English). SpaCy models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib16\" title=\"\">16</a>]</cite> are used for NER in each language. Detected entities are phonemized via lexicon lookup using the identified source language when available.</p>\n\n",
                "matched_terms": [
                    "language",
                    "english",
                    "german",
                    "phonemization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many phonemization systems struggle with numerical expressions, mathematical symbols, and special characters. By normalizing numbers (e.g., dates, times, decimals, ordinals) and symbols into their spoken equivalents before phonemization, inconsistencies in synthesized speech can be reduced. For the English language, the library num2words <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib17\" title=\"\">17</a>]</cite> is used, which is also utilized in the Gruut framework. For German, a dedicated normalization algorithm was implemented, as its complex case system required a more refined solution.</p>\n\n",
                "matched_terms": [
                    "language",
                    "english",
                    "gruut",
                    "phonemization",
                    "german"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Homographs such as the English word read (present tense /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textturnr</span>i<span class=\"ltx_ERROR undefined\">\\textlengthmark</span>d/ versus past tense /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textturnr</span><span class=\"ltx_ERROR undefined\">\\textepsilon</span>d/), require context for accurate phonemization. The previously created lexica contain POS tags for homographs to help resolve such ambiguities by ensuring that pronunciation aligns with the correct usage. For POS tagging, the same spaCy models are used as for NER. This method also supports dialect phonemization if the lexicon contains corresponding word variants.</p>\n\n",
                "matched_terms": [
                    "correct",
                    "english",
                    "phonemization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Loanwords and code-switching <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib18\" title=\"\">18</a>]</cite> present challenges for phonemization, as different languages follow distinct pronunciation rules. By identifying the language of individual words or phrases (including named entities), language-specific lookups and operations can be performed, rather than enforcing a single-language phonemization, which would induce errors in such cases. For this task, the Lingua language detection framework <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib19\" title=\"\">19</a>]</cite> was utilized as it supports a large number of languages and overall achieved good detection accuracies in experiments.</p>\n\n",
                "matched_terms": [
                    "language",
                    "phonemization",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When a word is not found in the extensive lexica, a probabilistic compound-splitting algorithm is applied. This algorithm segments the word into plausible subword candidates and scores each combination based on their frequency, length, and likelihood in a large reference corpus of over 15 million sentences from a recent Wikipedia dump, adapted to each supported language. In languages like German, where long compounds and interleaved loanwords are common, simple left-to-right or right-to-left lexicon lookups are insufficient. Instead, the algorithm evaluates all possible segmentations and assigns scores according to the following principle:</p>\n\n",
                "matched_terms": [
                    "language",
                    "german",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using recursive word segmentation, probabilistic scoring, and subword language detection, the algorithm identifies the most probable segmentation without relying on predefined rules or language-specific heuristics. This approach enables the phonemization framework to decompose complex compounds, improving accuracy on structurally complex words.</p>\n\n",
                "matched_terms": [
                    "language",
                    "phonemization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.20086v1#S3.F1\" title=\"Figure 1 &#8227; 3.2 Framework overview &#8227; 3 Approach &#8227; OLaPh: Optimal Language Phonemizer\"><span class=\"ltx_text ltx_ref_tag\">Figure&#160;1</span></a>, the workflow begins with an input text and its specified or automatically detected language. The text is split into sentences using language-specific spaCy models, which are also used for NER and POS tagging. Capitalized words are checked against an abbreviation lexicon (e.g., &#8220;NATO&#8221; &#8594; <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/<span class=\"ltx_ERROR undefined\">\\textprimstress</span>ne<span class=\"ltx_ERROR undefined\">\\textsci</span>.to<span class=\"ltx_ERROR undefined\">\\textupsilon</span>/), and numbers and symbols are normalized. NER with language detection ensures foreign names are handled correctly, after which the sentence is phonemized through lexicon lookups with backup mechanisms. If a word is not found in the selected language&#8217;s lexicon, cross-language detection and global lookup are attempted. As a last resort, the compound splitting component generates all possible subword combinations, scores them, and produces the final phonemization, preserving punctuation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "phonemization",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the rule-based framework, OLaPh also serves as a data generator for training phonemization models. To explore this, we trained a GemmaX-based LLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib20\" title=\"\">20</a>]</cite> on grapheme-phoneme pairs produced by OLaPh from the FineWeb corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib22\" title=\"\">22</a>]</cite>. For each of the four target languages (English, French, German, Spanish), approximately 2.5M pairs were used, yielding 10M training examples in total. The tokenizer was extended with 1024 phoneme-derived tokens, a small augmentation compared to the training set size, to test generalization from limited phoneme-specific vocabulary. Unlike modular NLP components such as SpaCy, the LLM learns end-to-end phonemization directly, without requiring explicit preprocessing.</p>\n\n",
                "matched_terms": [
                    "olaph",
                    "english",
                    "german",
                    "phonemization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate OLaPh, we implemented it in Python and compared its word-level performance against eSpeakNG and Gruut. For English and German, 5,000 sentences were sampled from FineWeb <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib22\" title=\"\">22</a>]</cite> and phonemized with all three systems. Phonemized outputs were aligned, and words with differing results were selected for analysis. A pre-trained ByT5 model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib13\" title=\"\">13</a>]</cite> was also considered, but excluded after preliminary tests showed it failed to disambiguate homographs and frequently misapplied phonemic transformations to loanwords. All evaluations were conducted by a researcher with expertise in applied speech synthesis and phonemization.</p>\n\n",
                "matched_terms": [
                    "english",
                    "gruut",
                    "phonemization",
                    "all",
                    "espeakng",
                    "german",
                    "olaph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.20086v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Evaluation of OLaPh &#8227; 4 Evaluations &#8227; OLaPh: Optimal Language Phonemizer\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> summarizes word-level phonemization comparisons across OLaPh, eSpeakNG, and Gruut, showing full matches, partial matches, and full mismatches where OLaPh differed from both baselines. In English, there were 7,140 mismatches (10,337 in German). A random sample of 500 mismatched words was manually reviewed and categorized as: alternative variants or differing encodings (not counted as errors); correct by OLaPh but incorrect by at least one baseline; incorrect by OLaPh while at least one baseline was correct; all systems incorrect; or ambiguous/unclear phonemization. This evaluation enabled direct comparison, identified linguistic challenges, and guided improvements. For comparability, verbose symbols (e.g., emphasis marks, doubly articulated consonants) were removed, as OLaPh outputs them more frequently than eSpeakNG and Gruut.</p>\n\n",
                "matched_terms": [
                    "english",
                    "gruut",
                    "phonemization",
                    "all",
                    "espeakng",
                    "incorrect",
                    "german",
                    "olaph",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">OLaPh made significantly fewer errors than the other systems on more complex sentences, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.20086v1#S4.T3\" title=\"Table 3 &#8227; 4.1 Evaluation of OLaPh &#8227; 4 Evaluations &#8227; OLaPh: Optimal Language Phonemizer\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. For example, names from other languages were phonemized more accurately, and homographs were less often incorrectly distinguished. However, there were a few instances where the model used for POS tagging made incorrect predictions or detected the incorrect origin language. Lastly, some homographs required contextual understanding for accurate phonemization. An example is the word <span class=\"ltx_text ltx_font_italic\">bass</span>, which can refer both to a fish (/b&#230;s/) and a musical instrument (/be<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textsci</span>s/). Both interpretations are nouns that must be phonemized based on context surrounding them. All systems made errors with this type of homograph.</p>\n\n",
                "matched_terms": [
                    "language",
                    "phonemization",
                    "all",
                    "incorrect",
                    "olaph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate OLaPh as a data generator, we sampled 100 additional sentences per language from the FineWeb corpus and compared outputs from the framework and the GemmaX-based LLM. For each word, we recorded whether the two systems produced identical phonemizations and manually classified mismatches as correct or incorrect.</p>\n\n",
                "matched_terms": [
                    "language",
                    "olaph",
                    "incorrect",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English, 1,822 words matched exactly, with only 7 mismatches; in 5 of these cases the LLM produced the correct phonemization. In German, 1,286 words matched exactly, with 6 mismatches. In all of these cases, both outputs were plausible, such as the pronunciation of <span class=\"ltx_text ltx_font_italic\">Xbox</span> as the English /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textepsilon</span>ksb<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textscripta</span>ks/ (LLM) versus the germanized /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textsci</span>ksb<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textopeno</span>ks/ (framework), both of which are commonly used. Most LLM improvements arose from more reliable recognition of loanwords and from cases where OLaPh produced suboptimal compound splitting in English. Overall, the LLM closely matched the framework on standard vocabulary while showing greater robustness in cross-lingual and structurally complex cases.</p>\n\n",
                "matched_terms": [
                    "english",
                    "phonemization",
                    "all",
                    "german",
                    "olaph",
                    "correct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluations show that OLaPh performs on par with existing frameworks in baseline settings but surpasses them on complex sentences and German phonemization. The challenge dataset also revealed systematic errors. In particular, the language detection module sometimes misclassified named entities due to insufficient context, while NER and POS tagging occasionally produced incorrect predictions. These issues stem from model limitations and could be reduced by integrating more advanced NLP components. The framework has so far been evaluated only for German and English, where compound splitting proved effective; its performance in other morphologically rich languages remains to be verified.</p>\n\n",
                "matched_terms": [
                    "language",
                    "english",
                    "phonemization",
                    "incorrect",
                    "german",
                    "olaph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Certain words and phrases were misphonemized by all systems. Many of these involve context-dependent terms or user preferences. Addressing such ambiguities may require integrating language models to exploit broader sentence context, disambiguate homographs, or adapt phoneme predictions to domain vocabulary. Our follow-up evaluation with a GemmaX-based LLM trained on OLaPh data supports this: the LLM correctly handled most loanwords that OLaPh misdetected, showing improved robustness</p>\n\n",
                "matched_terms": [
                    "language",
                    "olaph",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented OLaPh, a phonemization framework for English and German that extends previous approaches with NER, POS tagging, language detection, probabilistic compound handling, and stepwise backup mechanisms. Manual evaluation showed high accuracy and clear improvements on challenging phrases, while also revealing linguistic issues that remain open problems for phonemization. In follow-up work, we trained a GemmaX-based LLM on OLaPh-generated data, which further improved robustness in cross-lingual cases. To support future research, the framework, the LLM, and the challenge dataset are freely available.</p>\n\n",
                "matched_terms": [
                    "language",
                    "english",
                    "phonemization",
                    "manual",
                    "german",
                    "olaph"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "OLaPh: Optimal Language Phonemizer",
        "caption": "Table 3: Word-level error identification of all evaluated systems on the hard test dataset",
        "body": "System\n\n\n\n\n# of Errors\n\n\n\n\n\n\n\n\neSpeakNG\n\n\n\n\n39\n\n\n\n\n\n\nGruut\n\n\n\n\n31\n\n\n\n\n\n\nOLaPh\n\n\n\n\n9",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">System</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"># of Errors</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">eSpeakNG</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">39</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">Gruut</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">31</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">OLaPh</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">9</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "system",
            "gruut",
            "systems",
            "all",
            "error",
            "wordlevel",
            "espeakng",
            "evaluated",
            "hard",
            "errors",
            "olaph",
            "dataset",
            "identification",
            "test"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">OLaPh made significantly fewer errors than the other systems on more complex sentences, as shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.20086v1#S4.T3\" title=\"Table 3 &#8227; 4.1 Evaluation of OLaPh &#8227; 4 Evaluations &#8227; OLaPh: Optimal Language Phonemizer\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;3</span></a>. For example, names from other languages were phonemized more accurately, and homographs were less often incorrectly distinguished. However, there were a few instances where the model used for POS tagging made incorrect predictions or detected the incorrect origin language. Lastly, some homographs required contextual understanding for accurate phonemization. An example is the word <span class=\"ltx_text ltx_font_italic\">bass</span>, which can refer both to a fish (/b&#230;s/) and a musical instrument (/be<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textsci</span>s/). Both interpretations are nouns that must be phonemized based on context surrounding them. All systems made errors with this type of homograph.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Phonemization, the conversion of text into phonemes, is a key step in text-to-speech. Traditional approaches use rule-based transformations and lexicon lookups, while more advanced methods apply preprocessing techniques or neural networks for improved accuracy on out-of-domain vocabulary. However, all systems struggle with names, loanwords, abbreviations, and homographs. This work presents OLaPh (Optimal Language Phonemizer), a framework that combines large lexica, multiple NLP techniques, and compound resolution with a probabilistic scoring function. Evaluations in German and English show improved accuracy over previous approaches, including on a challenging dataset. To further address unresolved cases, we train a large language model on OLaPh-generated data, which achieves even stronger generalization and performance. Together, the framework and LLM improve phonemization consistency and provide a freely available resource for future research.</p>\n\n",
                "matched_terms": [
                    "olaph",
                    "systems",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To retain flexibility and explainability, many TTS systems incorporate phonemization as an intermediate preprocessing step <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib4\" title=\"\">4</a>]</cite>. Widely used approaches rely on rule-based transformations and lexicon lookups, which struggle with morphological complexity <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib5\" title=\"\">5</a>]</cite>, number normalization, named entities, and foreign words <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib6\" title=\"\">6</a>]</cite>. Even more advanced frameworks that add preprocessing steps such as part-of-speech (POS) tagging and neural models for unseen words still produce systematic errors, preventing flawless results and limiting synthesized speech quality in real-world applications.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "errors"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces OLaPh (Optimal Language Phonemizer), a framework that enhances previous approaches by leveraging large phonological resources, improving compound word handling with probabilistic scoring, and integrating NLP techniques such as named entity recognition (NER), POS tagging, and language detection. OLaPh provides accurate, transparent phoneme representations, particularly for morphologically rich languages like German, including dialectal variants. Manual evaluation in German and English compares OLaPh with baselines, while a challenge test set highlights its strengths on edge cases. Finally, a large language model (LLM) trained on OLaPh-generated data shows superior generalization compared to both the framework itself and other LLM-based phonemization models.</p>\n\n",
                "matched_terms": [
                    "olaph",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on phonemization showed that simple grapheme-to-phoneme\nalignment is insufficient due to the many exceptions in pronunciation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib7\" title=\"\">7</a>, pp.&#160;105-106]</cite>. Two main strategies emerged: dictionary-based methods with rules for unknown words, and rule-based methods with exception lists <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib8\" title=\"\">8</a>]</cite>.\nModern systems such as eSpeakNG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib9\" title=\"\">9</a>]</cite> extend these approaches with curated rules and large dictionaries, but still fail on homographs and foreign words.\nGruut <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib10\" title=\"\">10</a>]</cite> instead combines lexica with grapheme-to-phoneme (G2P) models based on conditional random fields, supported by parsers and POS tagging. This allows it to disambiguate homographs (e.g., <span class=\"ltx_text ltx_font_italic\">wound</span> /wu<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textlengthmark</span>nd/ as a noun vs. <span class=\"ltx_text ltx_font_italic\">wound</span> /wa<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textupsilon</span>nd/ as a verb), but often reduces consistency overall.\nMore recent G2P approaches employ neural networks, such as the transformer-based T5G2P <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib11\" title=\"\">11</a>]</cite> for English and Czech or multilingual models based on ByT5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib13\" title=\"\">13</a>]</cite>. While these achieve strong results, they remain limited by the quality of training data and continue to struggle with context-dependent or low-resource phonemization. LLMs have also been proposed for phonemization <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib14\" title=\"\">14</a>]</cite>, though direct comparisons are difficult due to differing datasets and metrics <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib15\" title=\"\">15</a>]</cite>. These LLM-based approaches highlight a growing shift toward models that integrate broader contextual understanding, but they also raise questions of efficiency and adaptability across languages.</p>\n\n",
                "matched_terms": [
                    "espeakng",
                    "gruut",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">OLaPh builds on Gruut&#8217;s lexicon-based approach, using recent Wiktionary dumps to extract IPA transcriptions in four languages. While the system currently supports English and German, lexica for French and Spanish were also extracted due to frequent loanwords. The English lexicon contains <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>147k entries, and the German lexicon <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS1.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>842k (including inflections). Additional lexica were created for character mapping, abbreviations, and symbols. As Wiktionary updates, the lexica can be easily extended, improving accuracy over time.</p>\n\n",
                "matched_terms": [
                    "system",
                    "olaph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Many phonemization systems struggle with numerical expressions, mathematical symbols, and special characters. By normalizing numbers (e.g., dates, times, decimals, ordinals) and symbols into their spoken equivalents before phonemization, inconsistencies in synthesized speech can be reduced. For the English language, the library num2words <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib17\" title=\"\">17</a>]</cite> is used, which is also utilized in the Gruut framework. For German, a dedicated normalization algorithm was implemented, as its complex case system required a more refined solution.</p>\n\n",
                "matched_terms": [
                    "system",
                    "gruut",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond the rule-based framework, OLaPh also serves as a data generator for training phonemization models. To explore this, we trained a GemmaX-based LLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib20\" title=\"\">20</a>]</cite> on grapheme-phoneme pairs produced by OLaPh from the FineWeb corpus <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib22\" title=\"\">22</a>]</cite>. For each of the four target languages (English, French, German, Spanish), approximately 2.5M pairs were used, yielding 10M training examples in total. The tokenizer was extended with 1024 phoneme-derived tokens, a small augmentation compared to the training set size, to test generalization from limited phoneme-specific vocabulary. Unlike modular NLP components such as SpaCy, the LLM learns end-to-end phonemization directly, without requiring explicit preprocessing.</p>\n\n",
                "matched_terms": [
                    "olaph",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate OLaPh, we implemented it in Python and compared its word-level performance against eSpeakNG and Gruut. For English and German, 5,000 sentences were sampled from FineWeb <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib22\" title=\"\">22</a>]</cite> and phonemized with all three systems. Phonemized outputs were aligned, and words with differing results were selected for analysis. A pre-trained ByT5 model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20086v1#bib.bib13\" title=\"\">13</a>]</cite> was also considered, but excluded after preliminary tests showed it failed to disambiguate homographs and frequently misapplied phonemic transformations to loanwords. All evaluations were conducted by a researcher with expertise in applied speech synthesis and phonemization.</p>\n\n",
                "matched_terms": [
                    "gruut",
                    "systems",
                    "all",
                    "wordlevel",
                    "espeakng",
                    "olaph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.20086v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Evaluation of OLaPh &#8227; 4 Evaluations &#8227; OLaPh: Optimal Language Phonemizer\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;1</span></a> summarizes word-level phonemization comparisons across OLaPh, eSpeakNG, and Gruut, showing full matches, partial matches, and full mismatches where OLaPh differed from both baselines. In English, there were 7,140 mismatches (10,337 in German). A random sample of 500 mismatched words was manually reviewed and categorized as: alternative variants or differing encodings (not counted as errors); correct by OLaPh but incorrect by at least one baseline; incorrect by OLaPh while at least one baseline was correct; all systems incorrect; or ambiguous/unclear phonemization. This evaluation enabled direct comparison, identified linguistic challenges, and guided improvements. For comparability, verbose symbols (e.g., emphasis marks, doubly articulated consonants) were removed, as OLaPh outputs them more frequently than eSpeakNG and Gruut.</p>\n\n",
                "matched_terms": [
                    "gruut",
                    "systems",
                    "all",
                    "wordlevel",
                    "espeakng",
                    "errors",
                    "olaph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in <a class=\"ltx_ref ltx_refmacro_autoref\" href=\"https://arxiv.org/html/2509.20086v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Evaluation of OLaPh &#8227; 4 Evaluations &#8227; OLaPh: Optimal Language Phonemizer\"><span class=\"ltx_text ltx_ref_tag\">Table&#160;2</span></a>, most discrepancies between the phonemization algorithms stem from alternative pronunciations and differing IPA encodings, suggesting that all frameworks already achieve high accuracy, particularly in English. However, OLaPh outperformed the others in several cases on the German test data. In nearly all of these instances, OLaPh excelled with foreign words, primarily English, which were correctly identified by the language detection module and phonemized accordingly. Some sentences also included English clauses that were flawlessly detected. Additionally, a few German words were correctly phonemized only by OLaPh. For example, Kriegsspiel (&#8220;war game&#8221;) was phonemized as <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/k<span class=\"ltx_ERROR undefined\">\\textinvscr</span>iks<span class=\"ltx_ERROR undefined\">\\textesh</span>pil/, while the other frameworks produced <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/k<span class=\"ltx_ERROR undefined\">\\textinvscr</span>ikzspil/ and <span class=\"ltx_ERROR undefined\">\\tipaencoding</span>/k<span class=\"ltx_ERROR undefined\">\\textfishhookr</span>ikspil/, omitting the meaningful phone [<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textesh</span>]. In statistically ambiguous cases, the built-in language detection occasionally misidentified the origin language, leading to errors. For example, the German word <span class=\"ltx_text ltx_font_italic\">Stapler</span> (forklift) was incorrectly recognized as the English <span class=\"ltx_text ltx_font_italic\">stapler</span> and consequently misphonemized. In a few cases, all phonemization outputs were incorrect, or the pronunciation was inherently ambiguous or depends on user preference. This includes words like Montr&#233;al, where pronunciation depends on the language context, such as choosing between the French /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span>m\\&#771;textopeno.<span class=\"ltx_ERROR undefined\">\\textinvscr</span>eal / and the English /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textsecstress</span>m<span class=\"ltx_ERROR undefined\">\\textturnv</span>nt<span class=\"ltx_ERROR undefined\">\\textturnr</span>i<span class=\"ltx_ERROR undefined\">\\textlengthmark</span><span class=\"ltx_ERROR undefined\">\\textprimstress</span><span class=\"ltx_ERROR undefined\">\\textturnscripta</span>l/.\nTo better assess edge cases, a small test dataset was created with challenging phonemization examples. It includes 100 English sentences containing homographs, along with French, German, and Spanish names. All three phonemization systems processed the sentences, and their outputs were manually compared at the word level.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "all",
                    "errors",
                    "olaph",
                    "test",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate OLaPh as a data generator, we sampled 100 additional sentences per language from the FineWeb corpus and compared outputs from the framework and the GemmaX-based LLM. For each word, we recorded whether the two systems produced identical phonemizations and manually classified mismatches as correct or incorrect.</p>\n\n",
                "matched_terms": [
                    "olaph",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In English, 1,822 words matched exactly, with only 7 mismatches; in 5 of these cases the LLM produced the correct phonemization. In German, 1,286 words matched exactly, with 6 mismatches. In all of these cases, both outputs were plausible, such as the pronunciation of <span class=\"ltx_text ltx_font_italic\">Xbox</span> as the English /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textepsilon</span>ksb<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textscripta</span>ks/ (LLM) versus the germanized /<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textsci</span>ksb<span class=\"ltx_ERROR undefined\">\\tipaencoding</span><span class=\"ltx_ERROR undefined\">\\textopeno</span>ks/ (framework), both of which are commonly used. Most LLM improvements arose from more reliable recognition of loanwords and from cases where OLaPh produced suboptimal compound splitting in English. Overall, the LLM closely matched the framework on standard vocabulary while showing greater robustness in cross-lingual and structurally complex cases.</p>\n\n",
                "matched_terms": [
                    "olaph",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluations show that OLaPh performs on par with existing frameworks in baseline settings but surpasses them on complex sentences and German phonemization. The challenge dataset also revealed systematic errors. In particular, the language detection module sometimes misclassified named entities due to insufficient context, while NER and POS tagging occasionally produced incorrect predictions. These issues stem from model limitations and could be reduced by integrating more advanced NLP components. The framework has so far been evaluated only for German and English, where compound splitting proved effective; its performance in other morphologically rich languages remains to be verified.</p>\n\n",
                "matched_terms": [
                    "olaph",
                    "evaluated",
                    "dataset",
                    "errors"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Certain words and phrases were misphonemized by all systems. Many of these involve context-dependent terms or user preferences. Addressing such ambiguities may require integrating language models to exploit broader sentence context, disambiguate homographs, or adapt phoneme predictions to domain vocabulary. Our follow-up evaluation with a GemmaX-based LLM trained on OLaPh data supports this: the LLM correctly handled most loanwords that OLaPh misdetected, showing improved robustness</p>\n\n",
                "matched_terms": [
                    "olaph",
                    "systems",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented OLaPh, a phonemization framework for English and German that extends previous approaches with NER, POS tagging, language detection, probabilistic compound handling, and stepwise backup mechanisms. Manual evaluation showed high accuracy and clear improvements on challenging phrases, while also revealing linguistic issues that remain open problems for phonemization. In follow-up work, we trained a GemmaX-based LLM on OLaPh-generated data, which further improved robustness in cross-lingual cases. To support future research, the framework, the LLM, and the challenge dataset are freely available.</p>\n\n",
                "matched_terms": [
                    "olaph",
                    "dataset"
                ]
            }
        ]
    }
}