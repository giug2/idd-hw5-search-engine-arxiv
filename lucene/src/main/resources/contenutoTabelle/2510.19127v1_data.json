{
    "S4.T1": {
        "source_file": "Steering Autoregressive Music Generation with Recursive Feature Machines",
        "caption": "Table 1: Classification results for base SynTheory FFN (in Wei et al. (2024)), simple linear probes, RFMs trained on last-token activations, and MusicRFM (ours). We report R2 score on the tempos dataset and accuracy on the others. We don’t record performance on logistic probes as some fail to converge. Bold indicates best performing model per category.",
        "body": "Syntheory FFN",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Syntheory FFN</td>\n</tr>\n</table>\n",
        "informative_terms_identified": [
            "converge",
            "wei",
            "don’t",
            "lasttoken",
            "base",
            "fail",
            "classification",
            "best",
            "tempos",
            "accuracy",
            "results",
            "logistic",
            "linear",
            "bold",
            "indicates",
            "category",
            "score",
            "record",
            "probes",
            "performance",
            "performing",
            "some",
            "syntheory",
            "trained",
            "musicrfm",
            "ffn",
            "others",
            "dataset",
            "rfms",
            "ours",
            "activations",
            "report",
            "simple",
            "model"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model&#8217;s internal gradients to produce interpretable &#8220;concept directions&#8221;, or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/astradzhao/music-rfm\" title=\"\">https://github.com/astradzhao/music-rfm</a></span></span></span> to encourage further exploration on RFMs in the music domain.</p>\n\n",
                "matched_terms": [
                    "activations",
                    "probes",
                    "accuracy",
                    "musicrfm",
                    "model",
                    "rfms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We argue that a more direct and principled path to controllability lies in activation-space intervention. If we can identify directions within a model&#8217;s hidden states that reliably correspond to human-interpretable music-theoretic concepts, such as specific pitches, chord qualities, or tempo, we can then steer the generation along these axes, guiding the creative process without retraining the base model or altering its decoding procedure. The critical question then becomes how to discover these semantic directions in a robust and interpretable manner.</p>\n\n",
                "matched_terms": [
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recursive Feature Machines (RFMs) provide a powerful answer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>; Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">2025a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">b</a>)</cite>. By forming an Average Gradient Outer Product (AGOP) from lightweight task probes, RFMs yield a set of orthogonal, eigenvalue-ranked directions that capture the most salient axes of variation for a given concept within a model&#8217;s representation space. These directions directly represent the model&#8217;s principal axes of sensitivity to specific features.</p>\n\n",
                "matched_terms": [
                    "rfms",
                    "probes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce MusicRFM, a framework that adapts RFMs to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-Large model directly in its activation space. Our approach is twofold: first, we train extremely lightweight, layer-wise RFM probes on the <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> dataset &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> to extract these potent, concept-aligned directions. Then, at inference time, we inject them into the model&#8217;s residual stream via forward hooks, enabling real-time, fine-grained control over the generated output. To ensure that audio quality and fidelity is not sacrificed for steering controllability, we introduce layer-based methods that apply steering selectively across <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s 48 decoder blocks, using top-K selection or an exponential weighting scheme based on each layer&#8217;s probe performance. For dynamic control, we implement time-based schedules that modulate steering strength throughout the generation with functions like linear fades, sinusoidal patterns, and sparse, stochastic application. MusicRFM further supports multi-direction steering, allowing for simultaneous or staggered enforcement of multiple attributes, such as jointly controlling notes and tempos. This comprehensive approach to control proves highly effective: our primary analysis shows that steering can increase the classification accuracy of a target note from 0.23 to over 0.82, while CLAP score for text alignment remains within <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.02 of the unsteered baseline.</p>\n\n",
                "matched_terms": [
                    "score",
                    "performance",
                    "probes",
                    "wei",
                    "tempos",
                    "accuracy",
                    "syntheory",
                    "rfms",
                    "classification",
                    "linear",
                    "musicrfm",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In brief, MusicRFM establishes a general and efficient framework for fine-grained, interpretable control in text-to-music generation, requiring only the lightweight training of small RFM probes, with no model finetuning or costly optimization at inference time. Its layer- and time-aware mechanisms, coupled with support for multi-direction steering, enable flexible and controllable modulation of attributes while preserving high audio fidelity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "musicrfm",
                    "probes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Research on controllable generation spans several communities, from activation-level steering in large language models to decoding-time control methods and controllable music generation. Our work, MusicRFM, builds on and unifies these threads by adapting RFMs to the domain of music while adding new temporal and architectural control mechanisms.</p>\n\n",
                "matched_terms": [
                    "rfms",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We focus particularly on text-to-music (TTM) generation that relies on neural audio codecs and autoregressive sequence models in architectures like <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>, <span class=\"ltx_text ltx_font_smallcaps\">MusicLM</span>, and <span class=\"ltx_text ltx_font_smallcaps\">Jukebox</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib4\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib6\" title=\"\">2022</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib1\" title=\"\">2023</a>; Dhariwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib5\" title=\"\">2020</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib23\" title=\"\">2025</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib19\" title=\"\">2025</a>)</cite>. Additionally, a number of controllable TTM systems exist in the parallel diffusion domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Novack et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib16\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib15\" title=\"\">a</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib22\" title=\"\">2024</a>; Nistal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib13\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib14\" title=\"\">b</a>)</cite>. Most existing controllable methods for AR focus on multi-modal controls (e.g.&#160;video) <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib8\" title=\"\">2025</a>)</cite> or common musical controls like piano rolls <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib11\" title=\"\">2024</a>)</cite>. These approaches, while generally performant, still require reasonably compute-heavy finetuning runs and thus necessitate changing the base model, potentially breaking its core generative capabilities if the finetuning data is ill-chosen.</p>\n\n",
                "matched_terms": [
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recursive Feature Machines (RFMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> were introduced as probing methods that iteratively recondition features via AGOP matrices to uncover task-sensitive subspaces. More recently, RFM-derived directions have been re-injected into activations for <em class=\"ltx_emph ltx_font_italic\">steering</em> in LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. We extend this paradigm to autoregressive music generation with three innovations: (i) <em class=\"ltx_emph ltx_font_italic\">layer-based control</em> through top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> and exponential weighting across 48 layers, (ii) <em class=\"ltx_emph ltx_font_italic\">time-based control</em> using dynamic schedules, and (iii) <em class=\"ltx_emph ltx_font_italic\">multi-direction control</em> via simultaneous or staggered application of concept directions.</p>\n\n",
                "matched_terms": [
                    "activations",
                    "rfms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our overall goal is to enable fine-grained, interpretable control in AR music generation, steering towards concepts like specific notes, chord types, or slow/fast tempo. To do this, we train lightweight RFM probes to extract concept-aligned directions and re-inject them into <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span> activations at inference time. This framework allows us to generate music samples that still follow text conditioning with high accuracy, while also reflecting controlled variations in targeted musical attributes.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "activations",
                    "probes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first provide some more background on Recursive Feature Machines before describing our application to music generation. RFMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> were originally proposed as a probing method, iteratively reconditioning features with Average Gradient Outer Product (AGOP) matrices to identify task-sensitive subspaces.\nGiven training data <math alttext=\"\\{(x_{i},y_{i})\\}_{i=1}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(x_{i},y_{i})\\}_{i=1}^{n}</annotation></semantics></math> and predictor <math alttext=\"f:\\mathbb{R}^{d}\\!\\to\\!\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi>&#8477;</mi><mi>d</mi></msup><mo rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">f:\\mathbb{R}^{d}\\!\\to\\!\\mathbb{R}</annotation></semantics></math>, define per-sample gradients <math alttext=\"g_{i}=\\nabla_{x}f(x_{i})\\in\\mathbb{R}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><msub><mo rspace=\"0.167em\">&#8711;</mo><mi>x</mi></msub><mi>f</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>d</mi></msup></mrow><annotation encoding=\"application/x-tex\">g_{i}=\\nabla_{x}f(x_{i})\\in\\mathbb{R}^{d}</annotation></semantics></math> and the AGOP</p>\n\n",
                "matched_terms": [
                    "some",
                    "rfms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work has extended RFMs to <em class=\"ltx_emph ltx_font_italic\">steering</em>: injecting a concept direction <math alttext=\"q_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">q_{j}</annotation></semantics></math> back into hidden activations biases a frozen model toward that attribute during inference <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. In practice, steering is implemented by registering hooks on a subset of layers <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> and adding a broadcast control vector to each residual stream:</p>\n\n",
                "matched_terms": [
                    "model",
                    "activations",
                    "rfms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adapt RFMs to steer <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large (<math alttext=\"L{=}48\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">L{=}48</annotation></semantics></math> decoder blocks), a Transformer over EnCodec tokens conditioned on text <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib4\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib6\" title=\"\">2022</a>)</cite>. Our pipeline has three stages: (i) audio <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> <span class=\"ltx_text ltx_font_smallcaps\">EnCodec</span> codes, (ii) layerwise RFM probes that yield AGOP eigendirections, and (iii) steering applied at inference as described above.</p>\n\n",
                "matched_terms": [
                    "rfms",
                    "probes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> is a recently designed synthetic dataset made to study interpretable representations of music theory concepts in large models, divided into 7 categories: tempo, notes, chord progressions, chord types, scales, intervals, and time signatures. Compared to prior music datasets, <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> offers clean, fine-grained supervision of musical properties, enabling controlled experiments on model interpretability and controllability. This dataset is particularly well-suited for probing approaches, as its labeled attributes align directly with theoretical concepts that can be mapped onto latent representations. In our setting, <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> allows us to train lightweight RFM probes on layerwise activations of MusicGen, yielding gradient-based directions that correspond to human-interpretable musical attributes.</p>\n\n",
                "matched_terms": [
                    "activations",
                    "probes",
                    "wei",
                    "syntheory",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio clips are resampled to 32&#8201;kHz, encoded with <span class=\"ltx_text ltx_font_smallcaps\">EnCodec</span>, and passed through <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>. For clip <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> and layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we mean-pool over tokens,\n<math alttext=\"x_{i,\\ell}=\\tfrac{1}{T}\\sum_{t=1}^{T}h_{t,\\ell}^{(i)}\\in\\mathbb{R}^{d_{\\ell}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant=\"normal\">&#8467;</mi></mrow></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><msubsup><mi>h</mi><mrow><mi>t</mi><mo>,</mo><mi mathvariant=\"normal\">&#8467;</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mi mathvariant=\"normal\">&#8467;</mi></msub></msup></mrow><annotation encoding=\"application/x-tex\">x_{i,\\ell}=\\tfrac{1}{T}\\sum_{t=1}^{T}h_{t,\\ell}^{(i)}\\in\\mathbb{R}^{d_{\\ell}}</annotation></semantics></math>,\nyielding clip-level vectors. Unlike last-token pooling used in text-based RFMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">a</a>)</cite>, mean pooling better captures temporal structure and improves probe performance.</p>\n\n",
                "matched_terms": [
                    "rfms",
                    "lasttoken",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each concept <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> and layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we train RFM probes for 15 iterations (fit predictor, compute AGOP, apply PSD map), keeping the probe with best validation metric (AUC for classification, MSE for regression). Binary concepts use <math alttext=\"\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,1\\}</annotation></semantics></math> labels and regression targets are z-normalized. The resulting eigendirections <math alttext=\"q_{\\ell,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">q_{\\ell,j}</annotation></semantics></math> form interpretable axes used for steering at inference. Steering is performed by the same process described in Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.E3\" title=\"In 3.1 Background on Recursive Feature Machines &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. For classification tasks, we additionally train multiclass RFMs that simply replace binary labels with one-hot-encoded target vectors, predicting through softmaxing final outputs.</p>\n\n",
                "matched_terms": [
                    "best",
                    "rfms",
                    "classification",
                    "probes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With MusicRFM, we additionally train separate multiclass probes (different from the binary probes used to steer) to compare RFM clasification against the original probing methods used in <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>. We see that RFMs have better or comparable performance to the 2-layer FFN probes used in the original <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> paper across all categories. We highlight that RFMs beat the baseline probes in accuracy on scales, progressions, and intervals and in R2 score on the tempo dataset, resulting in a higher overall average score. We justify the mean pooling strategy by ablating RFMs over only classifying on last-token activations, showing much better performance.</p>\n\n",
                "matched_terms": [
                    "score",
                    "performance",
                    "probes",
                    "activations",
                    "accuracy",
                    "syntheory",
                    "lasttoken",
                    "rfms",
                    "musicrfm",
                    "ffn",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, we argue that FFNs do not naturally yield orthogonal, eigenvalue-ranked directions suitable for steering. In contrast, RFMs produce a PSD AGOP matrix whose eigenvectors correspond to stable, interpretable axes of sensitivity. These axes can be directly injected into the model at inference, making RFMs uniquely suited for controlled generation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rfms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report results on how well binary directions trained using MusicRFM are able to steer generations towards interpretable concepts, exploring both quantitative and subjective metrics.</p>\n\n",
                "matched_terms": [
                    "results",
                    "report",
                    "trained",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first quantify audio similarity and distributional shift of generations steered along a <em class=\"ltx_emph ltx_font_italic\">single</em> concept direction using three standard metrics as a function of the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: (i) <span class=\"ltx_text ltx_font_bold\">Fr&#233;chet Distance (FD)</span> (lower is better), and (ii) <span class=\"ltx_text ltx_font_bold\">Maximum Mean Discrepancy (MMD)</span> (lower is better), (iii) <span class=\"ltx_text ltx_font_bold\">CLAP</span> alignment (higher is better). For the <span class=\"ltx_text ltx_font_bold\">tempos</span> category, the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> is averaged among the absolute value of the coefficient (e.g. the results from -0.15 and 0.15 are averaged into the 0.15 column). We show these results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. All results are reported on generations steered with RFM probes using stochastic application <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math> with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>; these are settings we found to be most optimal when creating high-quality, conceptually accurate generations.</p>\n\n",
                "matched_terms": [
                    "results",
                    "tempos",
                    "category",
                    "probes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate whether MusicRFM steering actually enforces the intended musical concepts, we classify generated samples using the same multiclass RFM probes described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S4\" title=\"4 Classification Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. For each attribute, we varied the control coefficient <math alttext=\"\\eta_{0}\\in\\{0.15,0.30,0.45,0.60\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0.15</mn><mo>,</mo><mn>0.30</mn><mo>,</mo><mn>0.45</mn><mo>,</mo><mn>0.60</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}\\in\\{0.15,0.30,0.45,0.60\\}</annotation></semantics></math> and measured probe accuracy on all generations. The last column in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes results across six categories.</p>\n\n",
                "matched_terms": [
                    "results",
                    "accuracy",
                    "musicrfm",
                    "probes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe that classification accuracy is generally low for all attributes except <span class=\"ltx_text ltx_font_bold\">notes</span>, where probe accuracy climbs sharply from 0.23 at <math alttext=\"\\eta_{0}{=}0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.15</annotation></semantics></math> to 0.82 at <math alttext=\"\\eta_{0}{=}0.60\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.60</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.60</annotation></semantics></math>, reflecting the inherent difficulty of enforcing more abstract or temporal musical properties. Nevertheless, the key observation is that accuracy <em class=\"ltx_emph ltx_font_italic\">monotonically increases with the control coefficient</em> in every category. For example, chords rise from 0.27 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.34, intervals from 0.12 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.22, and time signatures from 0.17 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.25. Even when overall values remain low, the consistent positive slope indicates that higher steering strength pushes generations in the expected direction of the controlled attribute.</p>\n\n",
                "matched_terms": [
                    "indicates",
                    "accuracy",
                    "category",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results highlight both the promise and the limitations of probe-based evaluation. On the one hand, the monotonic response to <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> validates that MusicRFM manipulates representations in directions aligned with the intended probes. On the other hand, the <em class=\"ltx_emph ltx_font_italic\">absolute values of accuracy should not be over-interpreted</em>: the RFM probes were trained exclusively on <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>, a highly synthetic dataset with simplified musical attributes. When applied to naturalistic generations, these probes may (i) fail to generalize, or (ii) misclassify samples that do in fact satisfy the target property. Thus, the reported accuracies should be understood primarily as <em class=\"ltx_emph ltx_font_italic\">relative trends</em> across control coefficients, not as reliable ground-truth measures of musical validity.</p>\n\n",
                "matched_terms": [
                    "probes",
                    "accuracy",
                    "syntheory",
                    "results",
                    "trained",
                    "fail",
                    "musicrfm",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide results of a listening test, where we asked 12 participants to score 3 different audio samples for 4 control types, where they judge based on audio quality and adherence of the audio to the specified control. The 3 clips were randomly chosen base model generations (without control), na&#239;ve RFM generations, and optimal RFM generations (steering with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>). We show mean and STD of each type of steering in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T3\" title=\"Table 3 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Overall, the results indicate that both na&#239;ve and MusicRFM steering substantially improve perceived control compared to the base model, with MusicRFM consistently achieving the highest ratings across all attributes. In particular, chord and interval control benefit most from our optimizations, while tempo control shows the largest relative gain over the no-steering baseline.</p>\n\n",
                "matched_terms": [
                    "score",
                    "base",
                    "results",
                    "musicrfm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test transfer beyond synthetic data, we evaluate RFM probes on <span class=\"ltx_text ltx_font_smallcaps\">MusicBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Melechovsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib12\" title=\"\">2024</a>)</cite>, a real-music corpus with ground-truth tempo, notes, and keys. Using the same pipeline as in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS2\" title=\"3.2 MusicRFM: RFM Steering for Music Generation &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we mean-pool <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large hidden states and fit layerwise RFMs (train/val/test split 70/15/15). For tempo we report normalized MSE, for classification overall accuracy. RFM probes reach <math alttext=\"75.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>75.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">75.3\\%</annotation></semantics></math> accuracy on notes and <math alttext=\"67.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>67.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">67.5\\%</annotation></semantics></math> on keys, while tempo regression proves difficult (MSE <math alttext=\"0.862\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><mn>0.862</mn><annotation encoding=\"application/x-tex\">0.862</annotation></semantics></math>). Steering experiments (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Evaluation on MusicBench (Real Music) &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mirror <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>: higher <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases FD/MMD and reduces CLAP, showing that moderate control preserves text adherence but aggressive coefficients destabilize generations. Overall, MusicBench confirms that real-music attributes can be steered, though sensitivity varies by concept difficulty.</p>\n\n",
                "matched_terms": [
                    "probes",
                    "report",
                    "accuracy",
                    "syntheory",
                    "classification",
                    "rfms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate MusicRFM when (i) <em class=\"ltx_emph ltx_font_italic\">multiple</em> concept directions are injected simultaneously and (ii) when steering strength varies <em class=\"ltx_emph ltx_font_italic\">over time</em>. We report the same quantitative metrics used in the single-direction setting and for (ii) introduce temporal analyses based on RFM probe softmax scores.</p>\n\n",
                "matched_terms": [
                    "report",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test whether MusicRFM can jointly enforce <em class=\"ltx_emph ltx_font_italic\">multiple musical attributes</em>, we examine all pairwise combinations among {<span class=\"ltx_text ltx_font_bold\">notes</span>, <span class=\"ltx_text ltx_font_bold\">chords</span>, <span class=\"ltx_text ltx_font_bold\">intervals</span>}. For each pair <math alttext=\"(a,b)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a,b)</annotation></semantics></math>, we sample a random target class from category <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m2\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> (e.g., note <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>) and a random class from category <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m4\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> (e.g., major chord), then generate music conditioned on both controls simultaneously.</p>\n\n",
                "matched_terms": [
                    "category",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At inference, we inject two steering directions per selected layer, one for each concept, following Sec&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS3.SSS3\" title=\"3.3.3 Multi-Direction and Staggered Control &#8227; 3.3 Improving Robustness in Audio-Domain Steering &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.3.3</span></a>. Each direction is scaled by an independent global coefficient <math alttext=\"\\eta_{0}\\in\\{0.3,0.6\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0.3</mn><mo>,</mo><mn>0.6</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}\\in\\{0.3,0.6\\}</annotation></semantics></math>. We evaluate all four cross-combinations {[0.3,0.3], [0.3,0.6], [0.6,0.3], [0.6,0.6]}, where the first value corresponds to category <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m2\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and the second to category <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m3\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. For each pair, we generate <math alttext=\"N=100\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding=\"application/x-tex\">N=100</annotation></semantics></math> samples per configuration, yielding <math alttext=\"3\\times 4\\times N=1200\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow><mo>=</mo><mn>1200</mn></mrow><annotation encoding=\"application/x-tex\">3\\times 4\\times N=1200</annotation></semantics></math> total generations. To report results concisely, we reorganize outputs by attribute rather than by pair. For instance, all samples where <em class=\"ltx_emph ltx_font_italic\">notes</em> were steered with coefficient <math alttext=\"0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m6\" intent=\":literal\"><semantics><mn>0.3</mn><annotation encoding=\"application/x-tex\">0.3</annotation></semantics></math>&#8212;regardless of whether they were paired with chords or intervals&#8212;are averaged together. This gives us per-category summaries across all pairings, shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.T5\" title=\"Table 5 &#8227; 6.1 Multi-Direction Steering: Pairwise Cross-Category Control &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "report",
                    "category"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe several trends:\n(i) <span class=\"ltx_text ltx_font_bold\">Probe accuracy still rises with stronger coefficients.</span> For notes in particular, accuracy increases from 0.770 at <math alttext=\"\\eta_{0}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.3</annotation></semantics></math> to 0.920 at <math alttext=\"\\eta_{0}=0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.6</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.6</annotation></semantics></math>, indicating that control strength directly improves enforcement even in multi-direction cases.\n(ii) <span class=\"ltx_text ltx_font_bold\">Distributional metrics and CLAP scores degrade at higher strengths.</span> Both FD and MMD grow substantially as <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, consistent with the single-direction case, where aggressive steering pushes samples away from the reference distribution. CLAP alignment also degrades significantly. (iii) <span class=\"ltx_text ltx_font_bold\">Accuracy in multi-direction steering exceeds single-direction.</span> We actually observe higher probe accuracy in the multi-direction setting, which we hypothesize arises because stronger aggregate constraints reduce adherence to the text prompt (lower CLAP) and, in turn, compress the generative manifold. This yields less stylistic variance in the music, making classes easier for probes to detect.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "probes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To study temporal schedules in isolation, we analyze the <span class=\"ltx_text ltx_font_bold\">notes</span> dataset with per-step steering strength <math alttext=\"\\eta_{\\ell}(t)=\\eta_{0}\\,\\rho_{\\ell}\\,\\phi(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#961;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{\\ell}(t)=\\eta_{0}\\,\\rho_{\\ell}\\,\\phi(t)</annotation></semantics></math> and track the <em class=\"ltx_emph ltx_font_italic\">softmax score of the ground-truth note class</em> under the RFM probe as a function of time (generation steps). For the experiments in this section, we only analyze on notes because are they are the highest quality in terms of following control, and also can give us a measurable accuracy when evaluating with RFM probes.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "score",
                    "dataset",
                    "probes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use per-direction coefficients <math alttext=\"\\eta_{0,m}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\eta_{0,m}</annotation></semantics></math> and schedules <math alttext=\"\\phi_{m}(t)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi_{m}(t)\\in[0,1]</annotation></semantics></math>, so <math alttext=\"\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)</annotation></semantics></math>. The schedules we ablate are exponential decay, linear decay &amp; increase, logistic increase, and sine wave. We put formulas used in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A5\" title=\"Appendix E Control schedules used for time control ablations on note classification &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, and record FD, MMD, and CLAP scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.T6\" title=\"Table 6 &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "logistic",
                    "record",
                    "linear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each schedule we plot the probe softmax of the correct note over time in Figure &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.F1.sf1\" title=\"In Figure 1 &#8227; Crossfading Between Concepts. &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>. We see that the distribution over time follows exactly what we would expect from each of the smooth scheduling functions - exponential &amp; linear decay look like decay functions, sine is very similar to a sine wave, and logistic &amp; linear increase show an increase in predicted probability.</p>\n\n",
                "matched_terms": [
                    "logistic",
                    "linear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our probes rely on mean-pooled features, which discard temporal ordering. This limits performance on concepts with strong sequential dependencies, such as scales, chord progressions, and time signatures, where the temporal dynamics are essential for accurate classification and control. As a result, RFM probes underperform on these attributes compared to temporally local concepts like notes or chords. Future work should explore temporally aware pooling strategies (e.g., attention pooling, recurrent aggregation, convolutional pooling) or sequence-level RFMs that directly model time-evolving representations. Similarly, extending beyond the top eigenvector to incorporate multiple components could capture richer subspaces of variation, but we have not yet performed variance analyses to quantify how much information higher-order components retain.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "probes",
                    "classification",
                    "model",
                    "rfms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our experiments target <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large, but other large audio models open complementary directions for RFM steering. OpenAI&#8217;s <span class=\"ltx_text ltx_font_smallcaps\">Jukebox</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Dhariwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib5\" title=\"\">2020</a>)</cite> uses multi-scale VQ-VAE codes and hierarchical AR decoders, while Google&#8217;s recent <span class=\"ltx_text ltx_font_smallcaps\">Magenta-RT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib19\" title=\"\">2025</a>)</cite> framework supports <em class=\"ltx_emph ltx_font_italic\">real-time</em> audio generation. Applying RFMs in these contexts would require adapting probe extraction to multi-level codebooks (for Jukebox) and to low-latency streaming architectures (for Magenta-RT). In particular, real-time models highlight the possibility of <span class=\"ltx_text ltx_font_bold\">real-time steering</span>: dynamically injecting directions during live playback, enabling interactive control (i.e. live DJ-ing). Extending MusicRFM into these setups could bridge interpretability with performance-critical generative applications such as interactive music tools and live performance systems.</p>\n\n",
                "matched_terms": [
                    "musicrfm",
                    "rfms",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_italic\">MusicRFM</span>, a framework that leverages RFM-derived, eigenvalue-ranked directions to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large model directly in activation space. By combining concept-aligned directions with layer-aware weighting and time-dependent schedules, MusicRFM enables fine-grained, interpretable control over attributes such as notes, chords, and tempo without modifying the base model or relying on per-step optimization.</p>\n\n",
                "matched_terms": [
                    "base",
                    "model",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Kernel ridge regression (KRR) is the base model with which we apply the RFM procedure for iterative feature learning via the AGOP. We briefly explain the KRR model. Let <math alttext=\"X\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math> denote training samples with <math alttext=\"{x^{(i)}}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mmultiscripts><mi>x</mi><mrow/><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mrow/><mi>T</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{x^{(i)}}^{T}</annotation></semantics></math> denoting the sample in the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math><sup class=\"ltx_sup\">th</sup> row of <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> for <math alttext=\"i\\in[n]\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">i\\in[n]</annotation></semantics></math> and <math alttext=\"y\\in\\mathbb{R}^{n\\times c}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>y</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>c</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">y\\in\\mathbb{R}^{n\\times c}</annotation></semantics></math> denote training labels, where <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m7\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is the number of output channels (e.g. one-hot encoded classes for <math alttext=\"c&gt;2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&gt;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">c&gt;2</annotation></semantics></math> classes). Let <math alttext=\"K:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m9\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><msup><mi>&#8477;</mi><mi>d</mi></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>&#8477;</mi><mi>d</mi></msup></mrow><mo stretchy=\"false\">&#8594;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">K:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}</annotation></semantics></math> denote a kernel function (a positive semi-definite, symmetric function), such as the Gaussian/RBF kernel (<math alttext=\"K(x,z)=\\exp(-\\|x-z\\|_{2}^{2})/L^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m10\" intent=\":literal\"><semantics><mrow><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><msubsup><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>x</mi><mo>&#8722;</mo><mi>z</mi></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><msup><mi>L</mi><mn>2</mn></msup></mrow></mrow><annotation encoding=\"application/x-tex\">K(x,z)=\\exp(-\\|x-z\\|_{2}^{2})/L^{2}</annotation></semantics></math>), or the Laplace kernel (<math alttext=\"K(x,z)=\\exp(-\\|x-z\\|_{2})/L\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m11\" intent=\":literal\"><semantics><mrow><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><msub><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>x</mi><mo>&#8722;</mo><mi>z</mi></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mi>L</mi></mrow></mrow><annotation encoding=\"application/x-tex\">K(x,z)=\\exp(-\\|x-z\\|_{2})/L</annotation></semantics></math>) used in this work. Given a ridge regularization parameter <math alttext=\"\\lambda\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m12\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda\\geq 0</annotation></semantics></math>, KRR solved on the data <math alttext=\"(X,y)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m13\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(X,y)</annotation></semantics></math> gives a predictor, <math alttext=\"\\hat{f}:\\mathbb{R}^{d}\\to\\mathbb{R}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m14\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>^</mo></mover><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi>&#8477;</mi><mi>d</mi></msup><mo stretchy=\"false\">&#8594;</mo><msup><mi>&#8477;</mi><mi>c</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{f}:\\mathbb{R}^{d}\\to\\mathbb{R}^{c}</annotation></semantics></math>, of the form:</p>\n\n",
                "matched_terms": [
                    "base",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use 70/15/15 train/valid/test split on RFM training, 15 RFM iterations, and mean pooling over all timesteps. For multiclass training of simple progressions, we use 700 examples per class (there are &#160;1100 per class in dataset, but we cannot fit them given our A6000 GPU memory size. However, we note that even without all training data, we still get significantly better accuracy than baseline in this category). For all other classes, we use the entire dataset for our training &amp; validation. We use 100 random choices of hyperparameters listed below for layer-wise probes and 300 for aggregation. We maximize on AUC for layer-wise probes and accuracy for aggregation.</p>\n\n",
                "matched_terms": [
                    "category",
                    "probes",
                    "accuracy",
                    "simple",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When tuning the number of components calculated with our RFM probes, we tried a lower number of components (2-10) for categories with less data points and less perceived complexity (e.g. notes, time signatures). For categories with larger dataset size and higher perceived complexity (e.g. simple progressions, scales), we choose number of components ranging from 8 to 24.</p>\n\n",
                "matched_terms": [
                    "simple",
                    "dataset",
                    "probes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note we tune over a more general class of kernels <math alttext=\"K_{p,q}(x,x^{\\prime})=\\exp(-\\|x-x^{\\prime}\\|_{p}^{q}/L^{q})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>K</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><mrow><msubsup><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>x</mi><mo>&#8722;</mo><msup><mi>x</mi><mo>&#8242;</mo></msup></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mi>p</mi><mi>q</mi></msubsup><mo>/</mo><msup><mi>L</mi><mi>q</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">K_{p,q}(x,x^{\\prime})=\\exp(-\\|x-x^{\\prime}\\|_{p}^{q}/L^{q})</annotation></semantics></math> (indicated by the kernel type hyperparameter) for the aggregation model, which has been shown to improve the performance of RFM on tabular datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">2025a</a>)</cite>. We also tune over whether to center the gradients in each iteration of RFM, which can help de-noise the gradients in high-dimensional settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. Gradient centering modifies the AGOP computation to give the following centered M matrix in the RFM iteration, where <math alttext=\"\\bar{g}=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p3.m2\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>g</mi><mo>&#175;</mo></mover><mo>=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>g</mi><mi>i</mi></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\bar{g}=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We study three strategies that control how many (and how strongly) layers contribute to steering: (i) <em class=\"ltx_emph ltx_font_italic\">exponential</em> score-weighted steering, (ii) a simple <em class=\"ltx_emph ltx_font_italic\">linear</em> score-weighted scheme, and (iii) hard <em class=\"ltx_emph ltx_font_italic\">top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></em> selection. We show results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T9\" title=\"Table 9 &#8227; Discrete selection (Top-&#119870;). &#8227; C.2 Ablating Layer Pruning &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T8\" title=\"Table 8 &#8227; Discrete selection (Top-&#119870;). &#8227; C.2 Ablating Layer Pruning &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "simple",
                    "linear"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "Steering Autoregressive Music Generation with Recursive Feature Machines",
        "caption": "Table 2: Single-direction steering metrics. Steering uses RFM probes with ψp​(t)\\psi_{p}(t), p=0.3p=0.3 and exponential layer weighting with w0=1w_{0}=1 and κ=0.95\\kappa=0.95. Lower is better for FD/MMD, higher better for CLAP and Probe Accuracy (mean per-class). Ground-truth MusicGen-Large has CLAP 0.3320.332. Probe accuracy is not defined for tempos (regression). Accuracies should be interpreted only as relative trends due to probe training on synthetic SynTheory data rather than real music. Bold indicates best control coefficient per category.",
        "body": "FD ↓\\downarrow\n\nMMD ↓\\downarrow\n\nCLAP ↑\\uparrow\n\nProbe Acc. ↑\\uparrow\n\n\n\nCategory\nControl coefficient η0\\eta_{0}\n\n\n\n\n0.15\n0.30\n0.45\n0.60\n0.15\n0.30\n0.45\n0.60\n0.15\n0.30\n0.45\n0.60\n0.15\n0.30\n0.45\n0.60\n\n\nChords\n0.116\n0.114\n0.110\n0.119\n0.063\n0.086\n0.040\n0.095\n0.324\n0.326\n0.319\n0.326\n0.271\n0.288\n0.320\n0.344\n\n\nIntervals\n0.110\n0.128\n0.169\n0.232\n0.078\n0.119\n0.400\n0.817\n0.315\n0.324\n0.311\n0.307\n0.121\n0.156\n0.187\n0.223\n\n\nNotes\n0.113\n0.130\n0.138\n0.180\n0.052\n0.127\n0.217\n0.476\n0.315\n0.311\n0.318\n0.303\n0.231\n0.461\n0.684\n0.824\n\n\nScales\n0.114\n0.115\n0.114\n0.119\n0.052\n0.075\n0.061\n0.081\n0.318\n0.328\n0.322\n0.324\n0.154\n0.157\n0.161\n0.176\n\n\nProgressions\n0.131\n0.142\n0.173\n0.207\n0.157\n0.233\n0.443\n0.650\n0.315\n0.309\n0.296\n0.297\n0.070\n0.079\n0.096\n0.114\n\n\nTempos\n0.122\n0.150\n0.206\n0.377\n0.112\n0.324\n0.717\n1.880\n0.328\n0.325\n0.307\n0.280\n—\n\n\nTime signatures\n0.162\n0.264\n0.402\n0.492\n0.356\n1.046\n1.980\n2.647\n0.320\n0.317\n0.278\n0.264\n0.172\n0.204\n0.238\n0.245",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" style=\"padding:-0.5pt 2.8pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\" style=\"padding:-0.5pt 2.8pt;\">FD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\" style=\"padding:-0.5pt 2.8pt;\">MMD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\" style=\"padding:-0.5pt 2.8pt;\">CLAP <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding:-0.5pt 2.8pt;\">Probe Acc. <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-bottom:1.99997pt;padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">Category</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"16\" style=\"padding-bottom:1.99997pt;padding:-0.5pt 2.8pt;\">Control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m5\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.45</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\">Chords</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\">0.116</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\">0.114</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.110</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\">0.119</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\">0.063</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\">0.086</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.040</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\">0.095</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\">0.324</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.326</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\">0.319</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.326</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\">0.271</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\">0.288</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\">0.320</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.344</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">Intervals</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.110</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.128</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.169</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.232</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.078</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.119</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.400</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.817</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.315</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.324</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.311</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.307</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.121</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.156</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.187</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.223</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">Notes</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.113</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.130</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.138</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.180</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.052</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.127</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.217</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.476</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.315</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.311</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.318</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.303</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.231</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.461</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.684</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.824</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">Scales</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.114</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.115</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.114</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.119</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.052</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.075</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.061</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.081</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.318</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.328</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.322</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.324</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.154</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.157</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.161</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.176</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">Progressions</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.131</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.142</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.173</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.207</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.157</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.233</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.443</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.650</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.315</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.309</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.296</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.297</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.070</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.079</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.096</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.114</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">Tempos</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.122</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.150</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.206</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.377</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.112</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.324</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.717</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">1.880</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.328</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.325</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 2.8pt;\">0.307</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.280</td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"4\" style=\"padding:-0.5pt 2.8pt;\">&#8212;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">Time signatures</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.162</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\">0.264</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\">0.402</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.492</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.356</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\">1.046</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\">1.980</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">2.647</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.320</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\">0.317</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\">0.278</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:-0.5pt 2.8pt;\">0.264</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\">0.172</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\">0.204</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\">0.238</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\">0.245</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "exponential",
            "control",
            "progressions",
            "has",
            "data",
            "lower",
            "notes",
            "groundtruth",
            "time",
            "↑uparrow",
            "interpreted",
            "than",
            "intervals",
            "rather",
            "steering",
            "real",
            "best",
            "only",
            "accuracy",
            "tempos",
            "ψp​tpsipt",
            "defined",
            "trends",
            "w01w01",
            "p03p03",
            "weighting",
            "bold",
            "metrics",
            "indicates",
            "music",
            "scales",
            "layer",
            "category",
            "coefficient",
            "probes",
            "relative",
            "↓downarrow",
            "syntheory",
            "singledirection",
            "chords",
            "acc",
            "training",
            "higher",
            "perclass",
            "fdmmd",
            "clap",
            "mmd",
            "κ095kappa095",
            "η0eta0",
            "better",
            "uses",
            "probe",
            "musicgenlarge",
            "due",
            "accuracies",
            "rfm",
            "signatures",
            "synthetic",
            "regression",
            "mean",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We first quantify audio similarity and distributional shift of generations steered along a <em class=\"ltx_emph ltx_font_italic\">single</em> concept direction using three standard metrics as a function of the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: (i) <span class=\"ltx_text ltx_font_bold\">Fr&#233;chet Distance (FD)</span> (lower is better), and (ii) <span class=\"ltx_text ltx_font_bold\">Maximum Mean Discrepancy (MMD)</span> (lower is better), (iii) <span class=\"ltx_text ltx_font_bold\">CLAP</span> alignment (higher is better). For the <span class=\"ltx_text ltx_font_bold\">tempos</span> category, the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> is averaged among the absolute value of the coefficient (e.g. the results from -0.15 and 0.15 are averaged into the 0.15 column). We show these results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. All results are reported on generations steered with RFM probes using stochastic application <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math> with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>; these are settings we found to be most optimal when creating high-quality, conceptually accurate generations.</p>\n\n",
            "<p class=\"ltx_p\">To evaluate whether MusicRFM steering actually enforces the intended musical concepts, we classify generated samples using the same multiclass RFM probes described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S4\" title=\"4 Classification Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. For each attribute, we varied the control coefficient <math alttext=\"\\eta_{0}\\in\\{0.15,0.30,0.45,0.60\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0.15</mn><mo>,</mo><mn>0.30</mn><mo>,</mo><mn>0.45</mn><mo>,</mo><mn>0.60</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}\\in\\{0.15,0.30,0.45,0.60\\}</annotation></semantics></math> and measured probe accuracy on all generations. The last column in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes results across six categories.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model&#8217;s internal gradients to produce interpretable &#8220;concept directions&#8221;, or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/astradzhao/music-rfm\" title=\"\">https://github.com/astradzhao/music-rfm</a></span></span></span> to encourage further exploration on RFMs in the music domain.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "control",
                    "probes",
                    "accuracy",
                    "chords",
                    "notes",
                    "rfm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large autoregressive (AR) models, powered by neural audio codecs, have made remarkable strides in text-to-music (TTM) generation, producing audio with impressive fidelity and coherence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib4\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib23\" title=\"\">2025</a>)</cite>. Despite a growing body of work in conditioning TTM AR models on time-varying controls <cite class=\"ltx_cite ltx_citemacro_citep\">(Novack et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib16\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib15\" title=\"\">a</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib22\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib10\" title=\"\">2023</a>; Koo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib9\" title=\"\">2025</a>)</cite>, achieving precise control over fine-grained <em class=\"ltx_emph ltx_font_italic\">music-theoretic</em> (e.g. pitch classes and chord qualities) content across time in generations remains challenging. Current approaches often focus on broad temporal controls like dynamics or polyphonic melody, and may either require intense finetuning runs or costly\nper-step optimization during inference to avoid large-scale training.</p>\n\n",
                "matched_terms": [
                    "time",
                    "control",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce MusicRFM, a framework that adapts RFMs to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-Large model directly in its activation space. Our approach is twofold: first, we train extremely lightweight, layer-wise RFM probes on the <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> dataset &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> to extract these potent, concept-aligned directions. Then, at inference time, we inject them into the model&#8217;s residual stream via forward hooks, enabling real-time, fine-grained control over the generated output. To ensure that audio quality and fidelity is not sacrificed for steering controllability, we introduce layer-based methods that apply steering selectively across <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s 48 decoder blocks, using top-K selection or an exponential weighting scheme based on each layer&#8217;s probe performance. For dynamic control, we implement time-based schedules that modulate steering strength throughout the generation with functions like linear fades, sinusoidal patterns, and sparse, stochastic application. MusicRFM further supports multi-direction steering, allowing for simultaneous or staggered enforcement of multiple attributes, such as jointly controlling notes and tempos. This comprehensive approach to control proves highly effective: our primary analysis shows that steering can increase the classification accuracy of a target note from 0.23 to over 0.82, while CLAP score for text alignment remains within <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.02 of the unsteered baseline.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "clap",
                    "exponential",
                    "control",
                    "weighting",
                    "probes",
                    "accuracy",
                    "tempos",
                    "syntheory",
                    "probe",
                    "musicgenlarge",
                    "notes",
                    "rfm",
                    "time",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In brief, MusicRFM establishes a general and efficient framework for fine-grained, interpretable control in text-to-music generation, requiring only the lightweight training of small RFM probes, with no model finetuning or costly optimization at inference time. Its layer- and time-aware mechanisms, coupled with support for multi-direction steering, enable flexible and controllable modulation of attributes while preserving high audio fidelity.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "steering",
                    "control",
                    "probes",
                    "only",
                    "training",
                    "rfm",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Research on controllable generation spans several communities, from activation-level steering in large language models to decoding-time control methods and controllable music generation. Our work, MusicRFM, builds on and unifies these threads by adapting RFMs to the domain of music while adding new temporal and architectural control mechanisms.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "control",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond music, a growing body of work investigates <em class=\"ltx_emph ltx_font_italic\">activation-level steering</em> in language models.\nActivation Addition (<span class=\"ltx_text ltx_font_smallcaps\">ActAdd</span>) constructs steering vectors from paired prompts and injects them into hidden states for sentiment or style shifts, without retraining or optimization <cite class=\"ltx_cite ltx_citemacro_citep\">(Turner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib20\" title=\"\">2024</a>)</cite>.\nContrastive Activation Addition (CAA) extends this idea by contrasting positive/negative contexts to obtain more targeted steering directions in Llama-style models <cite class=\"ltx_cite ltx_citemacro_citep\">(Panickssery et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nThese methods illustrate a broader trend: interpretable steering can often be achieved by modifying internal activations, rather than logits or decoding heuristics. Within music, existing approaches either focus solely on binary controls &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Facchiano et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib7\" title=\"\">2025</a>)</cite> or broad concepts like instrument presense <cite class=\"ltx_cite ltx_citemacro_citep\">(Koo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib9\" title=\"\">2025</a>)</cite>, and thus it remains to be seen whether such approaches can extended to time-varying, music-theoretic control.</p>\n\n",
                "matched_terms": [
                    "rather",
                    "steering",
                    "control",
                    "than",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recursive Feature Machines (RFMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> were introduced as probing methods that iteratively recondition features via AGOP matrices to uncover task-sensitive subspaces. More recently, RFM-derived directions have been re-injected into activations for <em class=\"ltx_emph ltx_font_italic\">steering</em> in LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. We extend this paradigm to autoregressive music generation with three innovations: (i) <em class=\"ltx_emph ltx_font_italic\">layer-based control</em> through top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> and exponential weighting across 48 layers, (ii) <em class=\"ltx_emph ltx_font_italic\">time-based control</em> using dynamic schedules, and (iii) <em class=\"ltx_emph ltx_font_italic\">multi-direction control</em> via simultaneous or staggered application of concept directions.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "exponential",
                    "control",
                    "weighting",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our overall goal is to enable fine-grained, interpretable control in AR music generation, steering towards concepts like specific notes, chord types, or slow/fast tempo. To do this, we train lightweight RFM probes to extract concept-aligned directions and re-inject them into <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span> activations at inference time. This framework allows us to generate music samples that still follow text conditioning with high accuracy, while also reflecting controlled variations in targeted musical attributes.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "control",
                    "probes",
                    "accuracy",
                    "notes",
                    "rfm",
                    "time",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first provide some more background on Recursive Feature Machines before describing our application to music generation. RFMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> were originally proposed as a probing method, iteratively reconditioning features with Average Gradient Outer Product (AGOP) matrices to identify task-sensitive subspaces.\nGiven training data <math alttext=\"\\{(x_{i},y_{i})\\}_{i=1}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding=\"application/x-tex\">\\{(x_{i},y_{i})\\}_{i=1}^{n}</annotation></semantics></math> and predictor <math alttext=\"f:\\mathbb{R}^{d}\\!\\to\\!\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>f</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi>&#8477;</mi><mi>d</mi></msup><mo rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">f:\\mathbb{R}^{d}\\!\\to\\!\\mathbb{R}</annotation></semantics></math>, define per-sample gradients <math alttext=\"g_{i}=\\nabla_{x}f(x_{i})\\in\\mathbb{R}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo>=</mo><mrow><mrow><msub><mo rspace=\"0.167em\">&#8711;</mo><mi>x</mi></msub><mi>f</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>d</mi></msup></mrow><annotation encoding=\"application/x-tex\">g_{i}=\\nabla_{x}f(x_{i})\\in\\mathbb{R}^{d}</annotation></semantics></math> and the AGOP</p>\n\n",
                "matched_terms": [
                    "training",
                    "data",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RFM implements <em class=\"ltx_emph ltx_font_italic\">feature learning</em> by iterating: (i) train a base learner (kernel ridge regression as described in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A1\" title=\"Appendix A Overview of kernel ridge regression &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>) on features <math alttext=\"x^{(t)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">x^{(t)}</annotation></semantics></math> to obtain <math alttext=\"f^{(t)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><msup><mi>f</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">f^{(t)}</annotation></semantics></math>, (ii) compute <math alttext=\"M^{(t)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><msup><mi>M</mi><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">M^{(t)}</annotation></semantics></math> via equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.E1\" title=\"In 3.1 Background on Recursive Feature Machines &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, and (iii) update features with</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "regression"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work has extended RFMs to <em class=\"ltx_emph ltx_font_italic\">steering</em>: injecting a concept direction <math alttext=\"q_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">q_{j}</annotation></semantics></math> back into hidden activations biases a frozen model toward that attribute during inference <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. In practice, steering is implemented by registering hooks on a subset of layers <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> and adding a broadcast control vector to each residual stream:</p>\n\n",
                "matched_terms": [
                    "steering",
                    "has",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"q_{\\ell,j^{\\star}}\\!\\in\\!\\mathbb{R}^{d_{\\ell}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><msup><mi>j</mi><mo>&#8902;</mo></msup></mrow></msub><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mi mathvariant=\"normal\">&#8467;</mi></msub></msup></mrow><annotation encoding=\"application/x-tex\">q_{\\ell,j^{\\star}}\\!\\in\\!\\mathbb{R}^{d_{\\ell}}</annotation></semantics></math> is reshaped to <math alttext=\"(1,1,d_{\\ell})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo>,</mo><msub><mi>d</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(1,1,d_{\\ell})</annotation></semantics></math>. Steering only uses the <em class=\"ltx_emph ltx_font_italic\">top component</em> per direction.</p>\n\n",
                "matched_terms": [
                    "uses",
                    "steering",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adapt RFMs to steer <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large (<math alttext=\"L{=}48\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">L{=}48</annotation></semantics></math> decoder blocks), a Transformer over EnCodec tokens conditioned on text <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib4\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib6\" title=\"\">2022</a>)</cite>. Our pipeline has three stages: (i) audio <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> <span class=\"ltx_text ltx_font_smallcaps\">EnCodec</span> codes, (ii) layerwise RFM probes that yield AGOP eigendirections, and (iii) steering applied at inference as described above.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "probes",
                    "has",
                    "musicgenlarge",
                    "rfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> is a recently designed synthetic dataset made to study interpretable representations of music theory concepts in large models, divided into 7 categories: tempo, notes, chord progressions, chord types, scales, intervals, and time signatures. Compared to prior music datasets, <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> offers clean, fine-grained supervision of musical properties, enabling controlled experiments on model interpretability and controllability. This dataset is particularly well-suited for probing approaches, as its labeled attributes align directly with theoretical concepts that can be mapped onto latent representations. In our setting, <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> allows us to train lightweight RFM probes on layerwise activations of MusicGen, yielding gradient-based directions that correspond to human-interpretable musical attributes.</p>\n\n",
                "matched_terms": [
                    "progressions",
                    "probes",
                    "syntheory",
                    "synthetic",
                    "scales",
                    "notes",
                    "intervals",
                    "rfm",
                    "signatures",
                    "time",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio clips are resampled to 32&#8201;kHz, encoded with <span class=\"ltx_text ltx_font_smallcaps\">EnCodec</span>, and passed through <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>. For clip <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> and layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we mean-pool over tokens,\n<math alttext=\"x_{i,\\ell}=\\tfrac{1}{T}\\sum_{t=1}^{T}h_{t,\\ell}^{(i)}\\in\\mathbb{R}^{d_{\\ell}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant=\"normal\">&#8467;</mi></mrow></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><msubsup><mi>h</mi><mrow><mi>t</mi><mo>,</mo><mi mathvariant=\"normal\">&#8467;</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mi mathvariant=\"normal\">&#8467;</mi></msub></msup></mrow><annotation encoding=\"application/x-tex\">x_{i,\\ell}=\\tfrac{1}{T}\\sum_{t=1}^{T}h_{t,\\ell}^{(i)}\\in\\mathbb{R}^{d_{\\ell}}</annotation></semantics></math>,\nyielding clip-level vectors. Unlike last-token pooling used in text-based RFMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">a</a>)</cite>, mean pooling better captures temporal structure and improves probe performance.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "mean",
                    "probe",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each concept <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> and layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we train RFM probes for 15 iterations (fit predictor, compute AGOP, apply PSD map), keeping the probe with best validation metric (AUC for classification, MSE for regression). Binary concepts use <math alttext=\"\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,1\\}</annotation></semantics></math> labels and regression targets are z-normalized. The resulting eigendirections <math alttext=\"q_{\\ell,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">q_{\\ell,j}</annotation></semantics></math> form interpretable axes used for steering at inference. Steering is performed by the same process described in Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.E3\" title=\"In 3.1 Background on Recursive Feature Machines &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. For classification tasks, we additionally train multiclass RFMs that simply replace binary labels with one-hot-encoded target vectors, predicting through softmaxing final outputs.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "steering",
                    "probes",
                    "best",
                    "probe",
                    "rfm",
                    "regression"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As we extend the existing <em class=\"ltx_emph ltx_font_italic\">text</em>-steering framework of RFMs provided by <cite class=\"ltx_cite ltx_citemacro_cite\">Beaglehole et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite> to audio domain music, we introduce additional modifications to help reduce out-of-distribution behavior and improve control. In particular, given the difference between the discrete, variable-sampling rate nature of text and the continuous, fixed-sampling rate nature of audio-domain music, we found that many of the algorithmic choices made by <cite class=\"ltx_cite ltx_citemacro_citet\">Beaglehole et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite> were ill-suited for TTM generation. All modifications are <span class=\"ltx_text ltx_font_bold\">only applied during inference time</span>.</p>\n\n",
                "matched_terms": [
                    "only",
                    "time",
                    "control",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Na&#239;ve steering, where we inject RFM directions uniformly across all <math alttext=\"L{=}48\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">L{=}48</annotation></semantics></math> layers at every step as is done in the original RFM paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">2025a</a>)</cite>, leads to noticeable degradation in audio quality and weaker alignment to text prompts. To address this, we introduce <em class=\"ltx_emph ltx_font_italic\">layer pruning</em> strategies at inference time that prioritize informative layers and downweight noisy ones, thereby improving both perceptual fidelity and controllability (see App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3\" title=\"Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for full results).</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "layer",
                    "steering",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> selection.</span> We rank each layer <math alttext=\"\\ell\\in\\{1,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\ell\\in\\{1,\\dots,L\\}</annotation></semantics></math> by its validation probe performance <math alttext=\"\\mathrm{AUC}_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>AUC</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">\\mathrm{AUC}_{\\ell}</annotation></semantics></math>, then restrict steering to the top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> layers.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "steering",
                    "probe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exponential weighting.</span> Instead of hard pruning, we also apply continuous weighting across layers. For each layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we normalize its probe score <math alttext=\"s_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">s_{\\ell}</annotation></semantics></math> into <math alttext=\"\\hat{s}_{\\ell}\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{s}_{\\ell}\\in[0,1]</annotation></semantics></math>, and define <math alttext=\"w_{\\ell}=w_{0}\\cdot\\hat{s}_{\\ell}^{1/\\kappa}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>=</mo><mrow><msub><mi>w</mi><mn>0</mn></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msubsup><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>1</mn><mo>/</mo><mi>&#954;</mi></mrow></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">w_{\\ell}=w_{0}\\cdot\\hat{s}_{\\ell}^{1/\\kappa}</annotation></semantics></math> with <math alttext=\"\\kappa\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m5\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa\\in(0,1)</annotation></semantics></math>. This concentrates steering strength on high-performing layers, reducing unwanted artifacts and incorrect directions produced by the lower-scoring ones.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "steering",
                    "exponential",
                    "probe",
                    "weighting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We modulate steering strength over time as <math alttext=\"\\eta_{\\ell}(t)=\\eta_{0}\\,w_{\\ell}\\,\\phi(t)\\,\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{\\ell}(t)=\\eta_{0}\\,w_{\\ell}\\,\\phi(t)\\,\\psi_{p}(t)</annotation></semantics></math>, where <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> is a global coefficient, <math alttext=\"w_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">w_{\\ell}</annotation></semantics></math> a layer weight, <math alttext=\"\\phi(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(t)</annotation></semantics></math> a deterministic schedule, and <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math> an optional stochastic gate.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "steering",
                    "coefficient",
                    "η0eta0",
                    "ψp​tpsipt",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stochastic application <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math>.</span> At each step, apply control with probability <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p3.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> (Bernoulli gating). Similarly to layer pruning, this method reduces over-steering and cumulative artifacts while preserving the expected bias toward the target. Ablations are in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3\" title=\"Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "control",
                    "ψp​tpsipt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further extend MusicRFM to support <em class=\"ltx_emph ltx_font_italic\">multi-direction steering</em>, combining multiple concept vectors <math alttext=\"\\{q_{\\ell,j_{m}}\\}_{m=1}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><msub><mi>j</mi><mi>m</mi></msub></mrow></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><annotation encoding=\"application/x-tex\">\\{q_{\\ell,j_{m}}\\}_{m=1}^{M}</annotation></semantics></math> in parallel. At each step we inject\n<math alttext=\"h^{\\prime}_{t,\\ell}=h_{t,\\ell}+\\sum_{m=1}^{M}\\big[\\eta_{0,m}\\,w_{\\ell}\\,\\phi_{m}(t)\\,\\psi_{p}(t)\\big]\\,q_{\\ell,j_{m}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>h</mi><mrow><mi>t</mi><mo>,</mo><mi mathvariant=\"normal\">&#8467;</mi></mrow><mo>&#8242;</mo></msubsup><mo>=</mo><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>,</mo><mi mathvariant=\"normal\">&#8467;</mi></mrow></msub><mo rspace=\"0.055em\">+</mo><mrow><msubsup><mo rspace=\"0em\">&#8721;</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mrow><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">[</mo><mrow><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">]</mo></mrow><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><msub><mi>j</mi><mi>m</mi></msub></mrow></msub></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">h^{\\prime}_{t,\\ell}=h_{t,\\ell}+\\sum_{m=1}^{M}\\big[\\eta_{0,m}\\,w_{\\ell}\\,\\phi_{m}(t)\\,\\psi_{p}(t)\\big]\\,q_{\\ell,j_{m}}</annotation></semantics></math>,\nwhere each direction <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> has its own coefficient <math alttext=\"\\eta_{0,m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\eta_{0,m}</annotation></semantics></math> and schedule <math alttext=\"\\phi_{m}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi_{m}(t)</annotation></semantics></math>. This enables both (i) <em class=\"ltx_emph ltx_font_italic\">simultaneous</em> enforcement of multiple attributes and (ii) <em class=\"ltx_emph ltx_font_italic\">staggered</em> control where different concepts are activated at different times. For example, one schedule may enforce tempo strongly during the opening segment, while another gradually ramps in harmonic structure later.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "has",
                    "control",
                    "coefficient"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With MusicRFM, we additionally train separate multiclass probes (different from the binary probes used to steer) to compare RFM clasification against the original probing methods used in <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>. We see that RFMs have better or comparable performance to the 2-layer FFN probes used in the original <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> paper across all categories. We highlight that RFMs beat the baseline probes in accuracy on scales, progressions, and intervals and in R2 score on the tempo dataset, resulting in a higher overall average score. We justify the mean pooling strategy by ablating RFMs over only classifying on last-token activations, showing much better performance.</p>\n\n",
                "matched_terms": [
                    "higher",
                    "progressions",
                    "probes",
                    "only",
                    "accuracy",
                    "syntheory",
                    "better",
                    "intervals",
                    "rfm",
                    "mean",
                    "scales"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, we argue that FFNs do not naturally yield orthogonal, eigenvalue-ranked directions suitable for steering. In contrast, RFMs produce a PSD AGOP matrix whose eigenvectors correspond to stable, interpretable axes of sensitivity. These axes can be directly injected into the model at inference, making RFMs uniquely suited for controlled generation.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The trends observed are as expected. Distributional metrics (FD and MMD) are consistently lower at smaller control coefficients, since weak steering leaves generations closer to the reference distribution. As <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, stronger injections deviate more from ground truth and raise FD/MMD. By contrast, CLAP alignment&#8212;measuring similarity to the conditioning text prompt&#8212;remains essentially flat across control strengths, indicating that textual conditioning is preserved regardless of steering intensity, only with slight degradation in some categories as control coefficient increases. Thus, moderate values of <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> can balance concept control with distributional fidelity while maintaining prompt adherence. We provide additional visual graphs for the reader in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A4\" title=\"Appendix D Single Direction Metric Graphs &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "fdmmd",
                    "clap",
                    "control",
                    "coefficient",
                    "mmd",
                    "only",
                    "η0eta0",
                    "lower",
                    "trends",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe that classification accuracy is generally low for all attributes except <span class=\"ltx_text ltx_font_bold\">notes</span>, where probe accuracy climbs sharply from 0.23 at <math alttext=\"\\eta_{0}{=}0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.15</annotation></semantics></math> to 0.82 at <math alttext=\"\\eta_{0}{=}0.60\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.60</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.60</annotation></semantics></math>, reflecting the inherent difficulty of enforcing more abstract or temporal musical properties. Nevertheless, the key observation is that accuracy <em class=\"ltx_emph ltx_font_italic\">monotonically increases with the control coefficient</em> in every category. For example, chords rise from 0.27 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.34, intervals from 0.12 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.22, and time signatures from 0.17 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.25. Even when overall values remain low, the consistent positive slope indicates that higher steering strength pushes generations in the expected direction of the controlled attribute.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "category",
                    "higher",
                    "control",
                    "coefficient",
                    "accuracy",
                    "chords",
                    "probe",
                    "notes",
                    "signatures",
                    "time",
                    "indicates",
                    "intervals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results highlight both the promise and the limitations of probe-based evaluation. On the one hand, the monotonic response to <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> validates that MusicRFM manipulates representations in directions aligned with the intended probes. On the other hand, the <em class=\"ltx_emph ltx_font_italic\">absolute values of accuracy should not be over-interpreted</em>: the RFM probes were trained exclusively on <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>, a highly synthetic dataset with simplified musical attributes. When applied to naturalistic generations, these probes may (i) fail to generalize, or (ii) misclassify samples that do in fact satisfy the target property. Thus, the reported accuracies should be understood primarily as <em class=\"ltx_emph ltx_font_italic\">relative trends</em> across control coefficients, not as reliable ground-truth measures of musical validity.</p>\n\n",
                "matched_terms": [
                    "control",
                    "probes",
                    "relative",
                    "accuracy",
                    "syntheory",
                    "η0eta0",
                    "trends",
                    "accuracies",
                    "groundtruth",
                    "rfm",
                    "synthetic",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide results of a listening test, where we asked 12 participants to score 3 different audio samples for 4 control types, where they judge based on audio quality and adherence of the audio to the specified control. The 3 clips were randomly chosen base model generations (without control), na&#239;ve RFM generations, and optimal RFM generations (steering with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>). We show mean and STD of each type of steering in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T3\" title=\"Table 3 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Overall, the results indicate that both na&#239;ve and MusicRFM steering substantially improve perceived control compared to the base model, with MusicRFM consistently achieving the highest ratings across all attributes. In particular, chord and interval control benefit most from our optimizations, while tempo control shows the largest relative gain over the no-steering baseline.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "steering",
                    "exponential",
                    "control",
                    "relative",
                    "κ095kappa095",
                    "w01w01",
                    "rfm",
                    "p03p03",
                    "weighting",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the reader, we also provide representative audio samples from the listening test. Each clip is paired with its text prompt and steering metadata (<math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>, schedule), where all clips are steered with the &#8220;optimal&#8221; parameters listed above. An interactive demo of some of the clips used in our listening test is available at the project page.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://astradzhao.github.io/MusicRFMPage/\" title=\"\">https://astradzhao.github.io/MusicRFMPage/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "steering",
                    "η0eta0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test transfer beyond synthetic data, we evaluate RFM probes on <span class=\"ltx_text ltx_font_smallcaps\">MusicBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Melechovsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib12\" title=\"\">2024</a>)</cite>, a real-music corpus with ground-truth tempo, notes, and keys. Using the same pipeline as in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS2\" title=\"3.2 MusicRFM: RFM Steering for Music Generation &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we mean-pool <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large hidden states and fit layerwise RFMs (train/val/test split 70/15/15). For tempo we report normalized MSE, for classification overall accuracy. RFM probes reach <math alttext=\"75.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>75.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">75.3\\%</annotation></semantics></math> accuracy on notes and <math alttext=\"67.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>67.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">67.5\\%</annotation></semantics></math> on keys, while tempo regression proves difficult (MSE <math alttext=\"0.862\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><mn>0.862</mn><annotation encoding=\"application/x-tex\">0.862</annotation></semantics></math>). Steering experiments (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Evaluation on MusicBench (Real Music) &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mirror <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>: higher <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases FD/MMD and reduces CLAP, showing that moderate control preserves text adherence but aggressive coefficients destabilize generations. Overall, MusicBench confirms that real-music attributes can be steered, though sensitivity varies by concept difficulty.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "fdmmd",
                    "clap",
                    "control",
                    "probes",
                    "accuracy",
                    "syntheory",
                    "η0eta0",
                    "data",
                    "musicgenlarge",
                    "notes",
                    "groundtruth",
                    "rfm",
                    "synthetic",
                    "regression",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate MusicRFM when (i) <em class=\"ltx_emph ltx_font_italic\">multiple</em> concept directions are injected simultaneously and (ii) when steering strength varies <em class=\"ltx_emph ltx_font_italic\">over time</em>. We report the same quantitative metrics used in the single-direction setting and for (ii) introduce temporal analyses based on RFM probe softmax scores.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "singledirection",
                    "probe",
                    "rfm",
                    "time",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test whether MusicRFM can jointly enforce <em class=\"ltx_emph ltx_font_italic\">multiple musical attributes</em>, we examine all pairwise combinations among {<span class=\"ltx_text ltx_font_bold\">notes</span>, <span class=\"ltx_text ltx_font_bold\">chords</span>, <span class=\"ltx_text ltx_font_bold\">intervals</span>}. For each pair <math alttext=\"(a,b)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a,b)</annotation></semantics></math>, we sample a random target class from category <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m2\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> (e.g., note <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>) and a random class from category <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m4\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> (e.g., major chord), then generate music conditioned on both controls simultaneously.</p>\n\n",
                "matched_terms": [
                    "category",
                    "chords",
                    "notes",
                    "intervals",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At inference, we inject two steering directions per selected layer, one for each concept, following Sec&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS3.SSS3\" title=\"3.3.3 Multi-Direction and Staggered Control &#8227; 3.3 Improving Robustness in Audio-Domain Steering &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.3.3</span></a>. Each direction is scaled by an independent global coefficient <math alttext=\"\\eta_{0}\\in\\{0.3,0.6\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0.3</mn><mo>,</mo><mn>0.6</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}\\in\\{0.3,0.6\\}</annotation></semantics></math>. We evaluate all four cross-combinations {[0.3,0.3], [0.3,0.6], [0.6,0.3], [0.6,0.6]}, where the first value corresponds to category <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m2\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and the second to category <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m3\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. For each pair, we generate <math alttext=\"N=100\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding=\"application/x-tex\">N=100</annotation></semantics></math> samples per configuration, yielding <math alttext=\"3\\times 4\\times N=1200\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow><mo>=</mo><mn>1200</mn></mrow><annotation encoding=\"application/x-tex\">3\\times 4\\times N=1200</annotation></semantics></math> total generations. To report results concisely, we reorganize outputs by attribute rather than by pair. For instance, all samples where <em class=\"ltx_emph ltx_font_italic\">notes</em> were steered with coefficient <math alttext=\"0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m6\" intent=\":literal\"><semantics><mn>0.3</mn><annotation encoding=\"application/x-tex\">0.3</annotation></semantics></math>&#8212;regardless of whether they were paired with chords or intervals&#8212;are averaged together. This gives us per-category summaries across all pairings, shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.T5\" title=\"Table 5 &#8227; 6.1 Multi-Direction Steering: Pairwise Cross-Category Control &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "rather",
                    "layer",
                    "steering",
                    "category",
                    "coefficient",
                    "chords",
                    "notes",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe several trends:\n(i) <span class=\"ltx_text ltx_font_bold\">Probe accuracy still rises with stronger coefficients.</span> For notes in particular, accuracy increases from 0.770 at <math alttext=\"\\eta_{0}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.3</annotation></semantics></math> to 0.920 at <math alttext=\"\\eta_{0}=0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.6</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.6</annotation></semantics></math>, indicating that control strength directly improves enforcement even in multi-direction cases.\n(ii) <span class=\"ltx_text ltx_font_bold\">Distributional metrics and CLAP scores degrade at higher strengths.</span> Both FD and MMD grow substantially as <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, consistent with the single-direction case, where aggressive steering pushes samples away from the reference distribution. CLAP alignment also degrades significantly. (iii) <span class=\"ltx_text ltx_font_bold\">Accuracy in multi-direction steering exceeds single-direction.</span> We actually observe higher probe accuracy in the multi-direction setting, which we hypothesize arises because stronger aggregate constraints reduce adherence to the text prompt (lower CLAP) and, in turn, compress the generative manifold. This yields less stylistic variance in the music, making classes easier for probes to detect.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "higher",
                    "clap",
                    "control",
                    "mmd",
                    "probes",
                    "accuracy",
                    "singledirection",
                    "η0eta0",
                    "lower",
                    "trends",
                    "notes",
                    "probe",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results highlight that multi-direction steering can indeed enforce multiple concepts, but doing so amplifies distributional drift and weakens prompt adherence. Notably, <em class=\"ltx_emph ltx_font_italic\">notes</em> remain most controllable (large probe gains with modest <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>), while more abstract concepts like intervals yield smaller improvements. This suggests that balancing coefficients across attributes or staggering them temporally (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS3.SSS3\" title=\"3.3.3 Multi-Direction and Staggered Control &#8227; 3.3 Improving Robustness in Audio-Domain Steering &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.3.3</span></a>), may be necessary for high-quality joint control.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "control",
                    "η0eta0",
                    "probe",
                    "notes",
                    "intervals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To study temporal schedules in isolation, we analyze the <span class=\"ltx_text ltx_font_bold\">notes</span> dataset with per-step steering strength <math alttext=\"\\eta_{\\ell}(t)=\\eta_{0}\\,\\rho_{\\ell}\\,\\phi(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#961;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{\\ell}(t)=\\eta_{0}\\,\\rho_{\\ell}\\,\\phi(t)</annotation></semantics></math> and track the <em class=\"ltx_emph ltx_font_italic\">softmax score of the ground-truth note class</em> under the RFM probe as a function of time (generation steps). For the experiments in this section, we only analyze on notes because are they are the highest quality in terms of following control, and also can give us a measurable accuracy when evaluating with RFM probes.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "control",
                    "probes",
                    "only",
                    "accuracy",
                    "probe",
                    "notes",
                    "groundtruth",
                    "rfm",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use per-direction coefficients <math alttext=\"\\eta_{0,m}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\eta_{0,m}</annotation></semantics></math> and schedules <math alttext=\"\\phi_{m}(t)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi_{m}(t)\\in[0,1]</annotation></semantics></math>, so <math alttext=\"\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)</annotation></semantics></math>. The schedules we ablate are exponential decay, linear decay &amp; increase, logistic increase, and sine wave. We put formulas used in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A5\" title=\"Appendix E Control schedules used for time control ablations on note classification &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, and record FD, MMD, and CLAP scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.T6\" title=\"Table 6 &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd",
                    "exponential"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each schedule we plot the probe softmax of the correct note over time in Figure &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.F1.sf1\" title=\"In Figure 1 &#8227; Crossfading Between Concepts. &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>. We see that the distribution over time follows exactly what we would expect from each of the smooth scheduling functions - exponential &amp; linear decay look like decay functions, sine is very similar to a sine wave, and logistic &amp; linear increase show an increase in predicted probability.</p>\n\n",
                "matched_terms": [
                    "probe",
                    "time",
                    "exponential"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then log and display the RFM-probe softmax scores for both <math alttext=\"n_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">n_{1}</annotation></semantics></math> and <math alttext=\"n_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">n_{2}</annotation></semantics></math> at each timestep in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.F1.sf2\" title=\"In Figure 1 &#8227; Crossfading Between Concepts. &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>. As expected, the first note falls in probability while the second note rises. On average over 500 randomly sampled note pairs, crossfaded generations achieve FD of <math alttext=\"0.350\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mn>0.350</mn><annotation encoding=\"application/x-tex\">0.350</annotation></semantics></math>, MMD of <math alttext=\"1.922\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mn>1.922</mn><annotation encoding=\"application/x-tex\">1.922</annotation></semantics></math>, and CLAP alignment of <math alttext=\"0.250\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mn>0.250</mn><annotation encoding=\"application/x-tex\">0.250</annotation></semantics></math>, indicating modest distributional drift but stable prompt adherence.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our probes rely on mean-pooled features, which discard temporal ordering. This limits performance on concepts with strong sequential dependencies, such as scales, chord progressions, and time signatures, where the temporal dynamics are essential for accurate classification and control. As a result, RFM probes underperform on these attributes compared to temporally local concepts like notes or chords. Future work should explore temporally aware pooling strategies (e.g., attention pooling, recurrent aggregation, convolutional pooling) or sequence-level RFMs that directly model time-evolving representations. Similarly, extending beyond the top eigenvector to incorporate multiple components could capture richer subspaces of variation, but we have not yet performed variance analyses to quantify how much information higher-order components retain.</p>\n\n",
                "matched_terms": [
                    "control",
                    "progressions",
                    "probes",
                    "not",
                    "chords",
                    "notes",
                    "rfm",
                    "signatures",
                    "time",
                    "scales"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, our experiments so far are limited to <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>-based, symbolic music-theoretic concepts such as notes, chords, and tempo. Future work could extend MusicRFM to attributes more directly tied to perceptual or production-level qualities, including instrument identity, timbre, or articulation style. While we perform preliminary analysis on <span class=\"ltx_text ltx_font_smallcaps\">MusicBench</span>, extended RFM training and steering on real-music-based datasets remains an open direction. These studies would connect RFM steering more directly to interpretability in real-world generation tasks.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "chords",
                    "notes",
                    "training",
                    "rfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our experiments target <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large, but other large audio models open complementary directions for RFM steering. OpenAI&#8217;s <span class=\"ltx_text ltx_font_smallcaps\">Jukebox</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Dhariwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib5\" title=\"\">2020</a>)</cite> uses multi-scale VQ-VAE codes and hierarchical AR decoders, while Google&#8217;s recent <span class=\"ltx_text ltx_font_smallcaps\">Magenta-RT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib19\" title=\"\">2025</a>)</cite> framework supports <em class=\"ltx_emph ltx_font_italic\">real-time</em> audio generation. Applying RFMs in these contexts would require adapting probe extraction to multi-level codebooks (for Jukebox) and to low-latency streaming architectures (for Magenta-RT). In particular, real-time models highlight the possibility of <span class=\"ltx_text ltx_font_bold\">real-time steering</span>: dynamically injecting directions during live playback, enabling interactive control (i.e. live DJ-ing). Extending MusicRFM into these setups could bridge interpretability with performance-critical generative applications such as interactive music tools and live performance systems.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "control",
                    "uses",
                    "probe",
                    "musicgenlarge",
                    "rfm",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_italic\">MusicRFM</span>, a framework that leverages RFM-derived, eigenvalue-ranked directions to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large model directly in activation space. By combining concept-aligned directions with layer-aware weighting and time-dependent schedules, MusicRFM enables fine-grained, interpretable control over attributes such as notes, chords, and tempo without modifying the base model or relying on per-step optimization.</p>\n\n",
                "matched_terms": [
                    "control",
                    "chords",
                    "musicgenlarge",
                    "notes",
                    "weighting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across synthetic and real-music settings, we observed consistent trade-offs governed by the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S8.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: moderate steering improves alignment to targeted concepts with limited distributional drift (FD/MMD) and minimal degradation in prompt adherence (CLAP), while aggressive steering yields stronger control at the cost of artifacts. Notes are the most reliably controllable, multi-direction steering is feasible but amplifies drift, and simple schedules (e.g., decay/rise) support intuitive manipulations like crossfades. Time-based control is accurate and true-to-schedule in terms of evaluating on softmax probability of classes. Layer pruning and stochastic (Bernoulli) application help stabilize generations by limiting cumulative bias.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "steering",
                    "fdmmd",
                    "clap",
                    "control",
                    "coefficient",
                    "η0eta0",
                    "notes",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Kernel ridge regression (KRR) is the base model with which we apply the RFM procedure for iterative feature learning via the AGOP. We briefly explain the KRR model. Let <math alttext=\"X\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math> denote training samples with <math alttext=\"{x^{(i)}}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mmultiscripts><mi>x</mi><mrow/><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mrow/><mi>T</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{x^{(i)}}^{T}</annotation></semantics></math> denoting the sample in the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math><sup class=\"ltx_sup\">th</sup> row of <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> for <math alttext=\"i\\in[n]\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">i\\in[n]</annotation></semantics></math> and <math alttext=\"y\\in\\mathbb{R}^{n\\times c}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>y</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>c</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">y\\in\\mathbb{R}^{n\\times c}</annotation></semantics></math> denote training labels, where <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m7\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is the number of output channels (e.g. one-hot encoded classes for <math alttext=\"c&gt;2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&gt;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">c&gt;2</annotation></semantics></math> classes). Let <math alttext=\"K:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m9\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><msup><mi>&#8477;</mi><mi>d</mi></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>&#8477;</mi><mi>d</mi></msup></mrow><mo stretchy=\"false\">&#8594;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">K:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}</annotation></semantics></math> denote a kernel function (a positive semi-definite, symmetric function), such as the Gaussian/RBF kernel (<math alttext=\"K(x,z)=\\exp(-\\|x-z\\|_{2}^{2})/L^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m10\" intent=\":literal\"><semantics><mrow><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><msubsup><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>x</mi><mo>&#8722;</mo><mi>z</mi></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><msup><mi>L</mi><mn>2</mn></msup></mrow></mrow><annotation encoding=\"application/x-tex\">K(x,z)=\\exp(-\\|x-z\\|_{2}^{2})/L^{2}</annotation></semantics></math>), or the Laplace kernel (<math alttext=\"K(x,z)=\\exp(-\\|x-z\\|_{2})/L\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m11\" intent=\":literal\"><semantics><mrow><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><msub><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>x</mi><mo>&#8722;</mo><mi>z</mi></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mi>L</mi></mrow></mrow><annotation encoding=\"application/x-tex\">K(x,z)=\\exp(-\\|x-z\\|_{2})/L</annotation></semantics></math>) used in this work. Given a ridge regularization parameter <math alttext=\"\\lambda\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m12\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda\\geq 0</annotation></semantics></math>, KRR solved on the data <math alttext=\"(X,y)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m13\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(X,y)</annotation></semantics></math> gives a predictor, <math alttext=\"\\hat{f}:\\mathbb{R}^{d}\\to\\mathbb{R}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m14\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>^</mo></mover><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi>&#8477;</mi><mi>d</mi></msup><mo stretchy=\"false\">&#8594;</mo><msup><mi>&#8477;</mi><mi>c</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{f}:\\mathbb{R}^{d}\\to\\mathbb{R}^{c}</annotation></semantics></math>, of the form:</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "regression",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here the notation <math alttext=\"K(x,X)\\in\\mathbb{R}^{1\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m16\" intent=\":literal\"><semantics><mrow><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">K(x,X)\\in\\mathbb{R}^{1\\times n}</annotation></semantics></math> is the <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m17\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-dimensional row vector with <math alttext=\"K(x,X)_{i}=K(x,x^{(i)})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m18\" intent=\":literal\"><semantics><mrow><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mi>i</mi></msub></mrow><mo>=</mo><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">K(x,X)_{i}=K(x,x^{(i)})</annotation></semantics></math> and <math alttext=\"K(X,X)\\in\\mathbb{R}^{n\\times n}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m19\" intent=\":literal\"><semantics><mrow><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>n</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">K(X,X)\\in\\mathbb{R}^{n\\times n}</annotation></semantics></math> denotes the kernel matrix of pair-wise kernel evaluations <math alttext=\"K(X,X)_{ij}=K(x^{(i)},x^{(j)})\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m20\" intent=\":literal\"><semantics><mrow><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>X</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>j</mi></mrow></msub></mrow><mo>=</mo><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>,</mo><msup><mi>x</mi><mrow><mo stretchy=\"false\">(</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">K(X,X)_{ij}=K(x^{(i)},x^{(j)})</annotation></semantics></math>. The advantage of kernel functions in the context of this work is that the predictor admits a closed form solution, which can be robustly computed and generally has fast training times for datasets under <math alttext=\"70\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m21\" intent=\":literal\"><semantics><mn>70</mn><annotation encoding=\"application/x-tex\">70</annotation></semantics></math>k samples.</p>\n\n",
                "matched_terms": [
                    "has",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use 70/15/15 train/valid/test split on RFM training, 15 RFM iterations, and mean pooling over all timesteps. For multiclass training of simple progressions, we use 700 examples per class (there are &#160;1100 per class in dataset, but we cannot fit them given our A6000 GPU memory size. However, we note that even without all training data, we still get significantly better accuracy than baseline in this category). For all other classes, we use the entire dataset for our training &amp; validation. We use 100 random choices of hyperparameters listed below for layer-wise probes and 300 for aggregation. We maximize on AUC for layer-wise probes and accuracy for aggregation.</p>\n\n",
                "matched_terms": [
                    "category",
                    "progressions",
                    "probes",
                    "accuracy",
                    "data",
                    "better",
                    "training",
                    "rfm",
                    "mean",
                    "than"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When tuning the number of components calculated with our RFM probes, we tried a lower number of components (2-10) for categories with less data points and less perceived complexity (e.g. notes, time signatures). For categories with larger dataset size and higher perceived complexity (e.g. simple progressions, scales), we choose number of components ranging from 8 to 24.</p>\n\n",
                "matched_terms": [
                    "higher",
                    "progressions",
                    "probes",
                    "data",
                    "lower",
                    "notes",
                    "rfm",
                    "signatures",
                    "time",
                    "scales"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note we tune over a more general class of kernels <math alttext=\"K_{p,q}(x,x^{\\prime})=\\exp(-\\|x-x^{\\prime}\\|_{p}^{q}/L^{q})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>K</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><mrow><msubsup><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>x</mi><mo>&#8722;</mo><msup><mi>x</mi><mo>&#8242;</mo></msup></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mi>p</mi><mi>q</mi></msubsup><mo>/</mo><msup><mi>L</mi><mi>q</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">K_{p,q}(x,x^{\\prime})=\\exp(-\\|x-x^{\\prime}\\|_{p}^{q}/L^{q})</annotation></semantics></math> (indicated by the kernel type hyperparameter) for the aggregation model, which has been shown to improve the performance of RFM on tabular datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">2025a</a>)</cite>. We also tune over whether to center the gradients in each iteration of RFM, which can help de-noise the gradients in high-dimensional settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. Gradient centering modifies the AGOP computation to give the following centered M matrix in the RFM iteration, where <math alttext=\"\\bar{g}=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p3.m2\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>g</mi><mo>&#175;</mo></mover><mo>=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>g</mi><mi>i</mi></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\bar{g}=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generation, we ablate two steering knobs that most strongly impact generation quality and concept alignment: (i) the <em class=\"ltx_emph ltx_font_italic\">effective number of layers</em> contributing control via both a flat top-<span class=\"ltx_text ltx_font_italic\">k</span> value and an exponential, score-weighted layer scheme (&#8220;layer pruning&#8221;), and (ii) a <em class=\"ltx_emph ltx_font_italic\">per-timestep injection probability</em> <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> that sparsifies when control is applied.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "exponential",
                    "steering",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where for ablations we set <math alttext=\"\\phi(t)\\equiv 1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8801;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\phi(t)\\equiv 1</annotation></semantics></math> and vary layer weighting and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "weighting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We study three strategies that control how many (and how strongly) layers contribute to steering: (i) <em class=\"ltx_emph ltx_font_italic\">exponential</em> score-weighted steering, (ii) a simple <em class=\"ltx_emph ltx_font_italic\">linear</em> score-weighted scheme, and (iii) hard <em class=\"ltx_emph ltx_font_italic\">top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></em> selection. We show results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T9\" title=\"Table 9 &#8227; Discrete selection (Top-&#119870;). &#8227; C.2 Ablating Layer Pruning &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T8\" title=\"Table 8 &#8227; Discrete selection (Top-&#119870;). &#8227; C.2 Ablating Layer Pruning &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "exponential",
                    "steering",
                    "control"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "Steering Autoregressive Music Generation with Recursive Feature Machines",
        "caption": "Table 3: Listening test results (mean ±\\pm standard deviation) across musical attributes.",
        "body": "Steering Type\nChords\nIntervals\nNotes\nTempo\n\n\nNo Steering\n59.71±6.0159.71\\pm 6.01\n54.75±5.5254.75\\pm 5.52\n57.08±6.3757.08\\pm 6.37\n55.75±7.0855.75\\pm 7.08\n\n\nNaïve RFM (ours)\n69.21±5.2569.21\\pm 5.25\n62.58±5.8462.58\\pm 5.84\n68.13±5.9768.13\\pm 5.97\n73.33±4.3573.33\\pm 4.35\n\n\nMusicRFM (ours, optimal)\n73.46±4.1873.46\\pm 4.18\n70.33±4.0270.33\\pm 4.02\n72.88±5.6772.88\\pm 5.67\n73.38±4.7573.38\\pm 4.75",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Steering Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Chords</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Intervals</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Notes</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Tempo</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">No Steering</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"59.71\\pm 6.01\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mrow><mn>59.71</mn><mo>&#177;</mo><mn>6.01</mn></mrow><annotation encoding=\"application/x-tex\">59.71\\pm 6.01</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"54.75\\pm 5.52\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mrow><mn>54.75</mn><mo>&#177;</mo><mn>5.52</mn></mrow><annotation encoding=\"application/x-tex\">54.75\\pm 5.52</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"57.08\\pm 6.37\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mrow><mn>57.08</mn><mo>&#177;</mo><mn>6.37</mn></mrow><annotation encoding=\"application/x-tex\">57.08\\pm 6.37</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"55.75\\pm 7.08\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mrow><mn>55.75</mn><mo>&#177;</mo><mn>7.08</mn></mrow><annotation encoding=\"application/x-tex\">55.75\\pm 7.08</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Na&#239;ve RFM (ours)</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"69.21\\pm 5.25\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m5\" intent=\":literal\"><semantics><mrow><mn>69.21</mn><mo>&#177;</mo><mn>5.25</mn></mrow><annotation encoding=\"application/x-tex\">69.21\\pm 5.25</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"62.58\\pm 5.84\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m6\" intent=\":literal\"><semantics><mrow><mn>62.58</mn><mo>&#177;</mo><mn>5.84</mn></mrow><annotation encoding=\"application/x-tex\">62.58\\pm 5.84</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"68.13\\pm 5.97\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m7\" intent=\":literal\"><semantics><mrow><mn>68.13</mn><mo>&#177;</mo><mn>5.97</mn></mrow><annotation encoding=\"application/x-tex\">68.13\\pm 5.97</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"73.33\\pm 4.35\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m8\" intent=\":literal\"><semantics><mrow><mn>73.33</mn><mo>&#177;</mo><mn>4.35</mn></mrow><annotation encoding=\"application/x-tex\">73.33\\pm 4.35</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">MusicRFM (ours, optimal)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"73.46\\pm 4.18\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m9\" intent=\":literal\"><semantics><mrow><mn>73.46</mn><mo>&#177;</mo><mn>4.18</mn></mrow><annotation encoding=\"application/x-tex\">73.46\\pm 4.18</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"70.33\\pm 4.02\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m10\" intent=\":literal\"><semantics><mrow><mn>70.33</mn><mo>&#177;</mo><mn>4.02</mn></mrow><annotation encoding=\"application/x-tex\">70.33\\pm 4.02</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"72.88\\pm 5.67\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m11\" intent=\":literal\"><semantics><mrow><mn>72.88</mn><mo>&#177;</mo><mn>5.67</mn></mrow><annotation encoding=\"application/x-tex\">72.88\\pm 5.67</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"73.38\\pm 4.75\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m12\" intent=\":literal\"><semantics><mrow><mn>73.38</mn><mo>&#177;</mo><mn>4.75</mn></mrow><annotation encoding=\"application/x-tex\">73.38\\pm 4.75</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "6258±5846258pm",
            "7033±4027033pm",
            "±pm",
            "notes",
            "attributes",
            "mean",
            "7338±4757338pm",
            "intervals",
            "7288±5677288pm",
            "steering",
            "7346±4187346pm",
            "tempo",
            "5971±6015971pm",
            "6921±5256921pm",
            "6813±5976813pm",
            "results",
            "optimal",
            "deviation",
            "chords",
            "5575±7085575pm",
            "5708±6375708pm",
            "musicrfm",
            "naïve",
            "listening",
            "standard",
            "test",
            "ours",
            "across",
            "7333±4357333pm",
            "5475±5525475pm",
            "rfm",
            "musical",
            "type"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We provide results of a listening test, where we asked 12 participants to score 3 different audio samples for 4 control types, where they judge based on audio quality and adherence of the audio to the specified control. The 3 clips were randomly chosen base model generations (without control), na&#239;ve RFM generations, and optimal RFM generations (steering with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>). We show mean and STD of each type of steering in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T3\" title=\"Table 3 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Overall, the results indicate that both na&#239;ve and MusicRFM steering substantially improve perceived control compared to the base model, with MusicRFM consistently achieving the highest ratings across all attributes. In particular, chord and interval control benefit most from our optimizations, while tempo control shows the largest relative gain over the no-steering baseline.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model&#8217;s internal gradients to produce interpretable &#8220;concept directions&#8221;, or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/astradzhao/music-rfm\" title=\"\">https://github.com/astradzhao/music-rfm</a></span></span></span> to encourage further exploration on RFMs in the music domain.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "chords",
                    "notes",
                    "musicrfm",
                    "rfm",
                    "attributes",
                    "musical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce MusicRFM, a framework that adapts RFMs to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-Large model directly in its activation space. Our approach is twofold: first, we train extremely lightweight, layer-wise RFM probes on the <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> dataset &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> to extract these potent, concept-aligned directions. Then, at inference time, we inject them into the model&#8217;s residual stream via forward hooks, enabling real-time, fine-grained control over the generated output. To ensure that audio quality and fidelity is not sacrificed for steering controllability, we introduce layer-based methods that apply steering selectively across <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s 48 decoder blocks, using top-K selection or an exponential weighting scheme based on each layer&#8217;s probe performance. For dynamic control, we implement time-based schedules that modulate steering strength throughout the generation with functions like linear fades, sinusoidal patterns, and sparse, stochastic application. MusicRFM further supports multi-direction steering, allowing for simultaneous or staggered enforcement of multiple attributes, such as jointly controlling notes and tempos. This comprehensive approach to control proves highly effective: our primary analysis shows that steering can increase the classification accuracy of a target note from 0.23 to over 0.82, while CLAP score for text alignment remains within <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.02 of the unsteered baseline.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "across",
                    "notes",
                    "musicrfm",
                    "rfm",
                    "attributes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In brief, MusicRFM establishes a general and efficient framework for fine-grained, interpretable control in text-to-music generation, requiring only the lightweight training of small RFM probes, with no model finetuning or costly optimization at inference time. Its layer- and time-aware mechanisms, coupled with support for multi-direction steering, enable flexible and controllable modulation of attributes while preserving high audio fidelity.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "attributes",
                    "steering",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Research on controllable generation spans several communities, from activation-level steering in large language models to decoding-time control methods and controllable music generation. Our work, MusicRFM, builds on and unifies these threads by adapting RFMs to the domain of music while adding new temporal and architectural control mechanisms.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recursive Feature Machines (RFMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> were introduced as probing methods that iteratively recondition features via AGOP matrices to uncover task-sensitive subspaces. More recently, RFM-derived directions have been re-injected into activations for <em class=\"ltx_emph ltx_font_italic\">steering</em> in LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. We extend this paradigm to autoregressive music generation with three innovations: (i) <em class=\"ltx_emph ltx_font_italic\">layer-based control</em> through top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> and exponential weighting across 48 layers, (ii) <em class=\"ltx_emph ltx_font_italic\">time-based control</em> using dynamic schedules, and (iii) <em class=\"ltx_emph ltx_font_italic\">multi-direction control</em> via simultaneous or staggered application of concept directions.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our overall goal is to enable fine-grained, interpretable control in AR music generation, steering towards concepts like specific notes, chord types, or slow/fast tempo. To do this, we train lightweight RFM probes to extract concept-aligned directions and re-inject them into <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span> activations at inference time. This framework allows us to generate music samples that still follow text conditioning with high accuracy, while also reflecting controlled variations in targeted musical attributes.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "tempo",
                    "notes",
                    "rfm",
                    "attributes",
                    "musical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adapt RFMs to steer <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large (<math alttext=\"L{=}48\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">L{=}48</annotation></semantics></math> decoder blocks), a Transformer over EnCodec tokens conditioned on text <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib4\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib6\" title=\"\">2022</a>)</cite>. Our pipeline has three stages: (i) audio <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> <span class=\"ltx_text ltx_font_smallcaps\">EnCodec</span> codes, (ii) layerwise RFM probes that yield AGOP eigendirections, and (iii) steering applied at inference as described above.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> is a recently designed synthetic dataset made to study interpretable representations of music theory concepts in large models, divided into 7 categories: tempo, notes, chord progressions, chord types, scales, intervals, and time signatures. Compared to prior music datasets, <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> offers clean, fine-grained supervision of musical properties, enabling controlled experiments on model interpretability and controllability. This dataset is particularly well-suited for probing approaches, as its labeled attributes align directly with theoretical concepts that can be mapped onto latent representations. In our setting, <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> allows us to train lightweight RFM probes on layerwise activations of MusicGen, yielding gradient-based directions that correspond to human-interpretable musical attributes.</p>\n\n",
                "matched_terms": [
                    "tempo",
                    "notes",
                    "rfm",
                    "attributes",
                    "musical",
                    "intervals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each concept <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> and layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we train RFM probes for 15 iterations (fit predictor, compute AGOP, apply PSD map), keeping the probe with best validation metric (AUC for classification, MSE for regression). Binary concepts use <math alttext=\"\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,1\\}</annotation></semantics></math> labels and regression targets are z-normalized. The resulting eigendirections <math alttext=\"q_{\\ell,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">q_{\\ell,j}</annotation></semantics></math> form interpretable axes used for steering at inference. Steering is performed by the same process described in Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.E3\" title=\"In 3.1 Background on Recursive Feature Machines &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. For classification tasks, we additionally train multiclass RFMs that simply replace binary labels with one-hot-encoded target vectors, predicting through softmaxing final outputs.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Na&#239;ve steering, where we inject RFM directions uniformly across all <math alttext=\"L{=}48\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">L{=}48</annotation></semantics></math> layers at every step as is done in the original RFM paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">2025a</a>)</cite>, leads to noticeable degradation in audio quality and weaker alignment to text prompts. To address this, we introduce <em class=\"ltx_emph ltx_font_italic\">layer pruning</em> strategies at inference time that prioritize informative layers and downweight noisy ones, thereby improving both perceptual fidelity and controllability (see App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3\" title=\"Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for full results).</p>\n\n",
                "matched_terms": [
                    "steering",
                    "across",
                    "results",
                    "naïve",
                    "rfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exponential weighting.</span> Instead of hard pruning, we also apply continuous weighting across layers. For each layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we normalize its probe score <math alttext=\"s_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">s_{\\ell}</annotation></semantics></math> into <math alttext=\"\\hat{s}_{\\ell}\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{s}_{\\ell}\\in[0,1]</annotation></semantics></math>, and define <math alttext=\"w_{\\ell}=w_{0}\\cdot\\hat{s}_{\\ell}^{1/\\kappa}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>=</mo><mrow><msub><mi>w</mi><mn>0</mn></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msubsup><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>1</mn><mo>/</mo><mi>&#954;</mi></mrow></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">w_{\\ell}=w_{0}\\cdot\\hat{s}_{\\ell}^{1/\\kappa}</annotation></semantics></math> with <math alttext=\"\\kappa\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m5\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa\\in(0,1)</annotation></semantics></math>. This concentrates steering strength on high-performing layers, reducing unwanted artifacts and incorrect directions produced by the lower-scoring ones.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further extend MusicRFM to support <em class=\"ltx_emph ltx_font_italic\">multi-direction steering</em>, combining multiple concept vectors <math alttext=\"\\{q_{\\ell,j_{m}}\\}_{m=1}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><msub><mi>j</mi><mi>m</mi></msub></mrow></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><annotation encoding=\"application/x-tex\">\\{q_{\\ell,j_{m}}\\}_{m=1}^{M}</annotation></semantics></math> in parallel. At each step we inject\n<math alttext=\"h^{\\prime}_{t,\\ell}=h_{t,\\ell}+\\sum_{m=1}^{M}\\big[\\eta_{0,m}\\,w_{\\ell}\\,\\phi_{m}(t)\\,\\psi_{p}(t)\\big]\\,q_{\\ell,j_{m}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>h</mi><mrow><mi>t</mi><mo>,</mo><mi mathvariant=\"normal\">&#8467;</mi></mrow><mo>&#8242;</mo></msubsup><mo>=</mo><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>,</mo><mi mathvariant=\"normal\">&#8467;</mi></mrow></msub><mo rspace=\"0.055em\">+</mo><mrow><msubsup><mo rspace=\"0em\">&#8721;</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mrow><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">[</mo><mrow><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">]</mo></mrow><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><msub><mi>j</mi><mi>m</mi></msub></mrow></msub></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">h^{\\prime}_{t,\\ell}=h_{t,\\ell}+\\sum_{m=1}^{M}\\big[\\eta_{0,m}\\,w_{\\ell}\\,\\phi_{m}(t)\\,\\psi_{p}(t)\\big]\\,q_{\\ell,j_{m}}</annotation></semantics></math>,\nwhere each direction <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> has its own coefficient <math alttext=\"\\eta_{0,m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\eta_{0,m}</annotation></semantics></math> and schedule <math alttext=\"\\phi_{m}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi_{m}(t)</annotation></semantics></math>. This enables both (i) <em class=\"ltx_emph ltx_font_italic\">simultaneous</em> enforcement of multiple attributes and (ii) <em class=\"ltx_emph ltx_font_italic\">staggered</em> control where different concepts are activated at different times. For example, one schedule may enforce tempo strongly during the opening segment, while another gradually ramps in harmonic structure later.</p>\n\n",
                "matched_terms": [
                    "attributes",
                    "steering",
                    "tempo",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With MusicRFM, we additionally train separate multiclass probes (different from the binary probes used to steer) to compare RFM clasification against the original probing methods used in <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>. We see that RFMs have better or comparable performance to the 2-layer FFN probes used in the original <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> paper across all categories. We highlight that RFMs beat the baseline probes in accuracy on scales, progressions, and intervals and in R2 score on the tempo dataset, resulting in a higher overall average score. We justify the mean pooling strategy by ablating RFMs over only classifying on last-token activations, showing much better performance.</p>\n\n",
                "matched_terms": [
                    "across",
                    "tempo",
                    "musicrfm",
                    "rfm",
                    "mean",
                    "intervals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report results on how well binary directions trained using MusicRFM are able to steer generations towards interpretable concepts, exploring both quantitative and subjective metrics.</p>\n\n",
                "matched_terms": [
                    "results",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first quantify audio similarity and distributional shift of generations steered along a <em class=\"ltx_emph ltx_font_italic\">single</em> concept direction using three standard metrics as a function of the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: (i) <span class=\"ltx_text ltx_font_bold\">Fr&#233;chet Distance (FD)</span> (lower is better), and (ii) <span class=\"ltx_text ltx_font_bold\">Maximum Mean Discrepancy (MMD)</span> (lower is better), (iii) <span class=\"ltx_text ltx_font_bold\">CLAP</span> alignment (higher is better). For the <span class=\"ltx_text ltx_font_bold\">tempos</span> category, the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> is averaged among the absolute value of the coefficient (e.g. the results from -0.15 and 0.15 are averaged into the 0.15 column). We show these results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. All results are reported on generations steered with RFM probes using stochastic application <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math> with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>; these are settings we found to be most optimal when creating high-quality, conceptually accurate generations.</p>\n\n",
                "matched_terms": [
                    "optimal",
                    "results",
                    "rfm",
                    "standard",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The trends observed are as expected. Distributional metrics (FD and MMD) are consistently lower at smaller control coefficients, since weak steering leaves generations closer to the reference distribution. As <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, stronger injections deviate more from ground truth and raise FD/MMD. By contrast, CLAP alignment&#8212;measuring similarity to the conditioning text prompt&#8212;remains essentially flat across control strengths, indicating that textual conditioning is preserved regardless of steering intensity, only with slight degradation in some categories as control coefficient increases. Thus, moderate values of <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> can balance concept control with distributional fidelity while maintaining prompt adherence. We provide additional visual graphs for the reader in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A4\" title=\"Appendix D Single Direction Metric Graphs &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate whether MusicRFM steering actually enforces the intended musical concepts, we classify generated samples using the same multiclass RFM probes described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S4\" title=\"4 Classification Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. For each attribute, we varied the control coefficient <math alttext=\"\\eta_{0}\\in\\{0.15,0.30,0.45,0.60\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0.15</mn><mo>,</mo><mn>0.30</mn><mo>,</mo><mn>0.45</mn><mo>,</mo><mn>0.60</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}\\in\\{0.15,0.30,0.45,0.60\\}</annotation></semantics></math> and measured probe accuracy on all generations. The last column in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes results across six categories.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "across",
                    "results",
                    "musicrfm",
                    "rfm",
                    "musical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe that classification accuracy is generally low for all attributes except <span class=\"ltx_text ltx_font_bold\">notes</span>, where probe accuracy climbs sharply from 0.23 at <math alttext=\"\\eta_{0}{=}0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.15</annotation></semantics></math> to 0.82 at <math alttext=\"\\eta_{0}{=}0.60\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.60</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.60</annotation></semantics></math>, reflecting the inherent difficulty of enforcing more abstract or temporal musical properties. Nevertheless, the key observation is that accuracy <em class=\"ltx_emph ltx_font_italic\">monotonically increases with the control coefficient</em> in every category. For example, chords rise from 0.27 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.34, intervals from 0.12 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.22, and time signatures from 0.17 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.25. Even when overall values remain low, the consistent positive slope indicates that higher steering strength pushes generations in the expected direction of the controlled attribute.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "chords",
                    "notes",
                    "attributes",
                    "musical",
                    "intervals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results highlight both the promise and the limitations of probe-based evaluation. On the one hand, the monotonic response to <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> validates that MusicRFM manipulates representations in directions aligned with the intended probes. On the other hand, the <em class=\"ltx_emph ltx_font_italic\">absolute values of accuracy should not be over-interpreted</em>: the RFM probes were trained exclusively on <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>, a highly synthetic dataset with simplified musical attributes. When applied to naturalistic generations, these probes may (i) fail to generalize, or (ii) misclassify samples that do in fact satisfy the target property. Thus, the reported accuracies should be understood primarily as <em class=\"ltx_emph ltx_font_italic\">relative trends</em> across control coefficients, not as reliable ground-truth measures of musical validity.</p>\n\n",
                "matched_terms": [
                    "across",
                    "results",
                    "musicrfm",
                    "rfm",
                    "attributes",
                    "musical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the reader, we also provide representative audio samples from the listening test. Each clip is paired with its text prompt and steering metadata (<math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>, schedule), where all clips are steered with the &#8220;optimal&#8221; parameters listed above. An interactive demo of some of the clips used in our listening test is available at the project page.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://astradzhao.github.io/MusicRFMPage/\" title=\"\">https://astradzhao.github.io/MusicRFMPage/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "listening",
                    "steering",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test transfer beyond synthetic data, we evaluate RFM probes on <span class=\"ltx_text ltx_font_smallcaps\">MusicBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Melechovsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib12\" title=\"\">2024</a>)</cite>, a real-music corpus with ground-truth tempo, notes, and keys. Using the same pipeline as in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS2\" title=\"3.2 MusicRFM: RFM Steering for Music Generation &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we mean-pool <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large hidden states and fit layerwise RFMs (train/val/test split 70/15/15). For tempo we report normalized MSE, for classification overall accuracy. RFM probes reach <math alttext=\"75.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>75.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">75.3\\%</annotation></semantics></math> accuracy on notes and <math alttext=\"67.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>67.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">67.5\\%</annotation></semantics></math> on keys, while tempo regression proves difficult (MSE <math alttext=\"0.862\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><mn>0.862</mn><annotation encoding=\"application/x-tex\">0.862</annotation></semantics></math>). Steering experiments (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Evaluation on MusicBench (Real Music) &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mirror <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>: higher <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases FD/MMD and reduces CLAP, showing that moderate control preserves text adherence but aggressive coefficients destabilize generations. Overall, MusicBench confirms that real-music attributes can be steered, though sensitivity varies by concept difficulty.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "tempo",
                    "notes",
                    "rfm",
                    "attributes",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate MusicRFM when (i) <em class=\"ltx_emph ltx_font_italic\">multiple</em> concept directions are injected simultaneously and (ii) when steering strength varies <em class=\"ltx_emph ltx_font_italic\">over time</em>. We report the same quantitative metrics used in the single-direction setting and for (ii) introduce temporal analyses based on RFM probe softmax scores.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test whether MusicRFM can jointly enforce <em class=\"ltx_emph ltx_font_italic\">multiple musical attributes</em>, we examine all pairwise combinations among {<span class=\"ltx_text ltx_font_bold\">notes</span>, <span class=\"ltx_text ltx_font_bold\">chords</span>, <span class=\"ltx_text ltx_font_bold\">intervals</span>}. For each pair <math alttext=\"(a,b)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a,b)</annotation></semantics></math>, we sample a random target class from category <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m2\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> (e.g., note <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>) and a random class from category <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m4\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> (e.g., major chord), then generate music conditioned on both controls simultaneously.</p>\n\n",
                "matched_terms": [
                    "chords",
                    "notes",
                    "musicrfm",
                    "attributes",
                    "musical",
                    "test",
                    "intervals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At inference, we inject two steering directions per selected layer, one for each concept, following Sec&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS3.SSS3\" title=\"3.3.3 Multi-Direction and Staggered Control &#8227; 3.3 Improving Robustness in Audio-Domain Steering &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.3.3</span></a>. Each direction is scaled by an independent global coefficient <math alttext=\"\\eta_{0}\\in\\{0.3,0.6\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0.3</mn><mo>,</mo><mn>0.6</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}\\in\\{0.3,0.6\\}</annotation></semantics></math>. We evaluate all four cross-combinations {[0.3,0.3], [0.3,0.6], [0.6,0.3], [0.6,0.6]}, where the first value corresponds to category <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m2\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and the second to category <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m3\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. For each pair, we generate <math alttext=\"N=100\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding=\"application/x-tex\">N=100</annotation></semantics></math> samples per configuration, yielding <math alttext=\"3\\times 4\\times N=1200\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow><mo>=</mo><mn>1200</mn></mrow><annotation encoding=\"application/x-tex\">3\\times 4\\times N=1200</annotation></semantics></math> total generations. To report results concisely, we reorganize outputs by attribute rather than by pair. For instance, all samples where <em class=\"ltx_emph ltx_font_italic\">notes</em> were steered with coefficient <math alttext=\"0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m6\" intent=\":literal\"><semantics><mn>0.3</mn><annotation encoding=\"application/x-tex\">0.3</annotation></semantics></math>&#8212;regardless of whether they were paired with chords or intervals&#8212;are averaged together. This gives us per-category summaries across all pairings, shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.T5\" title=\"Table 5 &#8227; 6.1 Multi-Direction Steering: Pairwise Cross-Category Control &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "across",
                    "results",
                    "chords",
                    "notes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe several trends:\n(i) <span class=\"ltx_text ltx_font_bold\">Probe accuracy still rises with stronger coefficients.</span> For notes in particular, accuracy increases from 0.770 at <math alttext=\"\\eta_{0}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.3</annotation></semantics></math> to 0.920 at <math alttext=\"\\eta_{0}=0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.6</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.6</annotation></semantics></math>, indicating that control strength directly improves enforcement even in multi-direction cases.\n(ii) <span class=\"ltx_text ltx_font_bold\">Distributional metrics and CLAP scores degrade at higher strengths.</span> Both FD and MMD grow substantially as <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, consistent with the single-direction case, where aggressive steering pushes samples away from the reference distribution. CLAP alignment also degrades significantly. (iii) <span class=\"ltx_text ltx_font_bold\">Accuracy in multi-direction steering exceeds single-direction.</span> We actually observe higher probe accuracy in the multi-direction setting, which we hypothesize arises because stronger aggregate constraints reduce adherence to the text prompt (lower CLAP) and, in turn, compress the generative manifold. This yields less stylistic variance in the music, making classes easier for probes to detect.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "notes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results highlight that multi-direction steering can indeed enforce multiple concepts, but doing so amplifies distributional drift and weakens prompt adherence. Notably, <em class=\"ltx_emph ltx_font_italic\">notes</em> remain most controllable (large probe gains with modest <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>), while more abstract concepts like intervals yield smaller improvements. This suggests that balancing coefficients across attributes or staggering them temporally (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS3.SSS3\" title=\"3.3.3 Multi-Direction and Staggered Control &#8227; 3.3 Improving Robustness in Audio-Domain Steering &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.3.3</span></a>), may be necessary for high-quality joint control.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "across",
                    "results",
                    "notes",
                    "attributes",
                    "intervals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To study temporal schedules in isolation, we analyze the <span class=\"ltx_text ltx_font_bold\">notes</span> dataset with per-step steering strength <math alttext=\"\\eta_{\\ell}(t)=\\eta_{0}\\,\\rho_{\\ell}\\,\\phi(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#961;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{\\ell}(t)=\\eta_{0}\\,\\rho_{\\ell}\\,\\phi(t)</annotation></semantics></math> and track the <em class=\"ltx_emph ltx_font_italic\">softmax score of the ground-truth note class</em> under the RFM probe as a function of time (generation steps). For the experiments in this section, we only analyze on notes because are they are the highest quality in terms of following control, and also can give us a measurable accuracy when evaluating with RFM probes.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering",
                    "notes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our probes rely on mean-pooled features, which discard temporal ordering. This limits performance on concepts with strong sequential dependencies, such as scales, chord progressions, and time signatures, where the temporal dynamics are essential for accurate classification and control. As a result, RFM probes underperform on these attributes compared to temporally local concepts like notes or chords. Future work should explore temporally aware pooling strategies (e.g., attention pooling, recurrent aggregation, convolutional pooling) or sequence-level RFMs that directly model time-evolving representations. Similarly, extending beyond the top eigenvector to incorporate multiple components could capture richer subspaces of variation, but we have not yet performed variance analyses to quantify how much information higher-order components retain.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "attributes",
                    "notes",
                    "chords"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, our experiments so far are limited to <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>-based, symbolic music-theoretic concepts such as notes, chords, and tempo. Future work could extend MusicRFM to attributes more directly tied to perceptual or production-level qualities, including instrument identity, timbre, or articulation style. While we perform preliminary analysis on <span class=\"ltx_text ltx_font_smallcaps\">MusicBench</span>, extended RFM training and steering on real-music-based datasets remains an open direction. These studies would connect RFM steering more directly to interpretability in real-world generation tasks.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "tempo",
                    "chords",
                    "notes",
                    "musicrfm",
                    "rfm",
                    "attributes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our experiments target <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large, but other large audio models open complementary directions for RFM steering. OpenAI&#8217;s <span class=\"ltx_text ltx_font_smallcaps\">Jukebox</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Dhariwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib5\" title=\"\">2020</a>)</cite> uses multi-scale VQ-VAE codes and hierarchical AR decoders, while Google&#8217;s recent <span class=\"ltx_text ltx_font_smallcaps\">Magenta-RT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib19\" title=\"\">2025</a>)</cite> framework supports <em class=\"ltx_emph ltx_font_italic\">real-time</em> audio generation. Applying RFMs in these contexts would require adapting probe extraction to multi-level codebooks (for Jukebox) and to low-latency streaming architectures (for Magenta-RT). In particular, real-time models highlight the possibility of <span class=\"ltx_text ltx_font_bold\">real-time steering</span>: dynamically injecting directions during live playback, enabling interactive control (i.e. live DJ-ing). Extending MusicRFM into these setups could bridge interpretability with performance-critical generative applications such as interactive music tools and live performance systems.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_italic\">MusicRFM</span>, a framework that leverages RFM-derived, eigenvalue-ranked directions to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large model directly in activation space. By combining concept-aligned directions with layer-aware weighting and time-dependent schedules, MusicRFM enables fine-grained, interpretable control over attributes such as notes, chords, and tempo without modifying the base model or relying on per-step optimization.</p>\n\n",
                "matched_terms": [
                    "tempo",
                    "chords",
                    "notes",
                    "musicrfm",
                    "attributes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across synthetic and real-music settings, we observed consistent trade-offs governed by the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S8.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: moderate steering improves alignment to targeted concepts with limited distributional drift (FD/MMD) and minimal degradation in prompt adherence (CLAP), while aggressive steering yields stronger control at the cost of artifacts. Notes are the most reliably controllable, multi-direction steering is feasible but amplifies drift, and simple schedules (e.g., decay/rise) support intuitive manipulations like crossfades. Time-based control is accurate and true-to-schedule in terms of evaluating on softmax probability of classes. Layer pruning and stochastic (Bernoulli) application help stabilize generations by limiting cumulative bias.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "notes",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use 70/15/15 train/valid/test split on RFM training, 15 RFM iterations, and mean pooling over all timesteps. For multiclass training of simple progressions, we use 700 examples per class (there are &#160;1100 per class in dataset, but we cannot fit them given our A6000 GPU memory size. However, we note that even without all training data, we still get significantly better accuracy than baseline in this category). For all other classes, we use the entire dataset for our training &amp; validation. We use 100 random choices of hyperparameters listed below for layer-wise probes and 300 for aggregation. We maximize on AUC for layer-wise probes and accuracy for aggregation.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When tuning the number of components calculated with our RFM probes, we tried a lower number of components (2-10) for categories with less data points and less perceived complexity (e.g. notes, time signatures). For categories with larger dataset size and higher perceived complexity (e.g. simple progressions, scales), we choose number of components ranging from 8 to 24.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "notes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note we tune over a more general class of kernels <math alttext=\"K_{p,q}(x,x^{\\prime})=\\exp(-\\|x-x^{\\prime}\\|_{p}^{q}/L^{q})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>K</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><mrow><msubsup><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>x</mi><mo>&#8722;</mo><msup><mi>x</mi><mo>&#8242;</mo></msup></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mi>p</mi><mi>q</mi></msubsup><mo>/</mo><msup><mi>L</mi><mi>q</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">K_{p,q}(x,x^{\\prime})=\\exp(-\\|x-x^{\\prime}\\|_{p}^{q}/L^{q})</annotation></semantics></math> (indicated by the kernel type hyperparameter) for the aggregation model, which has been shown to improve the performance of RFM on tabular datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">2025a</a>)</cite>. We also tune over whether to center the gradients in each iteration of RFM, which can help de-noise the gradients in high-dimensional settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. Gradient centering modifies the AGOP computation to give the following centered M matrix in the RFM iteration, where <math alttext=\"\\bar{g}=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p3.m2\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>g</mi><mo>&#175;</mo></mover><mo>=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>g</mi><mi>i</mi></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\bar{g}=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We study three strategies that control how many (and how strongly) layers contribute to steering: (i) <em class=\"ltx_emph ltx_font_italic\">exponential</em> score-weighted steering, (ii) a simple <em class=\"ltx_emph ltx_font_italic\">linear</em> score-weighted scheme, and (iii) hard <em class=\"ltx_emph ltx_font_italic\">top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></em> selection. We show results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T9\" title=\"Table 9 &#8227; Discrete selection (Top-&#119870;). &#8227; C.2 Ablating Layer Pruning &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T8\" title=\"Table 8 &#8227; Discrete selection (Top-&#119870;). &#8227; C.2 Ablating Layer Pruning &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "steering"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "Steering Autoregressive Music Generation with Recursive Feature Machines",
        "caption": "Table 4: RFM steering on MusicBench (key).",
        "body": "η0\\eta_{0}\nFD ↓\\downarrow\n\nMMD ↓\\downarrow\n\nCLAP ↑\\uparrow\n\nAcc. ↑\\uparrow\n\n\n\n0.15\n0.424\n0.478\n0.315\n0.148\n\n\n0.30\n0.495\n0.908\n0.308\n0.264\n\n\n0.45\n0.576\n1.563\n0.276\n0.479\n\n\n0.60\n0.717\n2.615\n0.247\n0.619",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding:-0.75pt 3.0pt;\"><math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 3.0pt;\">FD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 3.0pt;\">MMD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 3.0pt;\">CLAP <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.75pt 3.0pt;\">Acc. <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:-0.75pt 3.0pt;\">0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.424</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.478</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.315</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.75pt 3.0pt;\">0.148</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.75pt 3.0pt;\">0.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.75pt 3.0pt;\">0.495</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.75pt 3.0pt;\">0.908</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.75pt 3.0pt;\">0.308</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.75pt 3.0pt;\">0.264</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.75pt 3.0pt;\">0.45</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.75pt 3.0pt;\">0.576</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.75pt 3.0pt;\">1.563</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.75pt 3.0pt;\">0.276</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.75pt 3.0pt;\">0.479</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:-0.75pt 3.0pt;\">0.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.75pt 3.0pt;\">0.717</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.75pt 3.0pt;\">2.615</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.75pt 3.0pt;\">0.247</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.75pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.619</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "steering",
            "clap",
            "mmd",
            "musicbench",
            "key",
            "↓downarrow",
            "η0eta0",
            "acc",
            "rfm",
            "↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To test transfer beyond synthetic data, we evaluate RFM probes on <span class=\"ltx_text ltx_font_smallcaps\">MusicBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Melechovsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib12\" title=\"\">2024</a>)</cite>, a real-music corpus with ground-truth tempo, notes, and keys. Using the same pipeline as in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS2\" title=\"3.2 MusicRFM: RFM Steering for Music Generation &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we mean-pool <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large hidden states and fit layerwise RFMs (train/val/test split 70/15/15). For tempo we report normalized MSE, for classification overall accuracy. RFM probes reach <math alttext=\"75.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>75.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">75.3\\%</annotation></semantics></math> accuracy on notes and <math alttext=\"67.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>67.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">67.5\\%</annotation></semantics></math> on keys, while tempo regression proves difficult (MSE <math alttext=\"0.862\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><mn>0.862</mn><annotation encoding=\"application/x-tex\">0.862</annotation></semantics></math>). Steering experiments (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Evaluation on MusicBench (Real Music) &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mirror <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>: higher <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases FD/MMD and reduces CLAP, showing that moderate control preserves text adherence but aggressive coefficients destabilize generations. Overall, MusicBench confirms that real-music attributes can be steered, though sensitivity varies by concept difficulty.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model&#8217;s internal gradients to produce interpretable &#8220;concept directions&#8221;, or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/astradzhao/music-rfm\" title=\"\">https://github.com/astradzhao/music-rfm</a></span></span></span> to encourage further exploration on RFMs in the music domain.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce MusicRFM, a framework that adapts RFMs to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-Large model directly in its activation space. Our approach is twofold: first, we train extremely lightweight, layer-wise RFM probes on the <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> dataset &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> to extract these potent, concept-aligned directions. Then, at inference time, we inject them into the model&#8217;s residual stream via forward hooks, enabling real-time, fine-grained control over the generated output. To ensure that audio quality and fidelity is not sacrificed for steering controllability, we introduce layer-based methods that apply steering selectively across <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s 48 decoder blocks, using top-K selection or an exponential weighting scheme based on each layer&#8217;s probe performance. For dynamic control, we implement time-based schedules that modulate steering strength throughout the generation with functions like linear fades, sinusoidal patterns, and sparse, stochastic application. MusicRFM further supports multi-direction steering, allowing for simultaneous or staggered enforcement of multiple attributes, such as jointly controlling notes and tempos. This comprehensive approach to control proves highly effective: our primary analysis shows that steering can increase the classification accuracy of a target note from 0.23 to over 0.82, while CLAP score for text alignment remains within <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.02 of the unsteered baseline.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In brief, MusicRFM establishes a general and efficient framework for fine-grained, interpretable control in text-to-music generation, requiring only the lightweight training of small RFM probes, with no model finetuning or costly optimization at inference time. Its layer- and time-aware mechanisms, coupled with support for multi-direction steering, enable flexible and controllable modulation of attributes while preserving high audio fidelity.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our overall goal is to enable fine-grained, interpretable control in AR music generation, steering towards concepts like specific notes, chord types, or slow/fast tempo. To do this, we train lightweight RFM probes to extract concept-aligned directions and re-inject them into <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span> activations at inference time. This framework allows us to generate music samples that still follow text conditioning with high accuracy, while also reflecting controlled variations in targeted musical attributes.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adapt RFMs to steer <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large (<math alttext=\"L{=}48\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">L{=}48</annotation></semantics></math> decoder blocks), a Transformer over EnCodec tokens conditioned on text <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib4\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib6\" title=\"\">2022</a>)</cite>. Our pipeline has three stages: (i) audio <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> <span class=\"ltx_text ltx_font_smallcaps\">EnCodec</span> codes, (ii) layerwise RFM probes that yield AGOP eigendirections, and (iii) steering applied at inference as described above.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each concept <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> and layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we train RFM probes for 15 iterations (fit predictor, compute AGOP, apply PSD map), keeping the probe with best validation metric (AUC for classification, MSE for regression). Binary concepts use <math alttext=\"\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,1\\}</annotation></semantics></math> labels and regression targets are z-normalized. The resulting eigendirections <math alttext=\"q_{\\ell,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">q_{\\ell,j}</annotation></semantics></math> form interpretable axes used for steering at inference. Steering is performed by the same process described in Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.E3\" title=\"In 3.1 Background on Recursive Feature Machines &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. For classification tasks, we additionally train multiclass RFMs that simply replace binary labels with one-hot-encoded target vectors, predicting through softmaxing final outputs.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Na&#239;ve steering, where we inject RFM directions uniformly across all <math alttext=\"L{=}48\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">L{=}48</annotation></semantics></math> layers at every step as is done in the original RFM paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">2025a</a>)</cite>, leads to noticeable degradation in audio quality and weaker alignment to text prompts. To address this, we introduce <em class=\"ltx_emph ltx_font_italic\">layer pruning</em> strategies at inference time that prioritize informative layers and downweight noisy ones, thereby improving both perceptual fidelity and controllability (see App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3\" title=\"Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for full results).</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We modulate steering strength over time as <math alttext=\"\\eta_{\\ell}(t)=\\eta_{0}\\,w_{\\ell}\\,\\phi(t)\\,\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{\\ell}(t)=\\eta_{0}\\,w_{\\ell}\\,\\phi(t)\\,\\psi_{p}(t)</annotation></semantics></math>, where <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> is a global coefficient, <math alttext=\"w_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">w_{\\ell}</annotation></semantics></math> a layer weight, <math alttext=\"\\phi(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(t)</annotation></semantics></math> a deterministic schedule, and <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math> an optional stochastic gate.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "η0eta0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first quantify audio similarity and distributional shift of generations steered along a <em class=\"ltx_emph ltx_font_italic\">single</em> concept direction using three standard metrics as a function of the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: (i) <span class=\"ltx_text ltx_font_bold\">Fr&#233;chet Distance (FD)</span> (lower is better), and (ii) <span class=\"ltx_text ltx_font_bold\">Maximum Mean Discrepancy (MMD)</span> (lower is better), (iii) <span class=\"ltx_text ltx_font_bold\">CLAP</span> alignment (higher is better). For the <span class=\"ltx_text ltx_font_bold\">tempos</span> category, the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> is averaged among the absolute value of the coefficient (e.g. the results from -0.15 and 0.15 are averaged into the 0.15 column). We show these results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. All results are reported on generations steered with RFM probes using stochastic application <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math> with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>; these are settings we found to be most optimal when creating high-quality, conceptually accurate generations.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "clap",
                    "mmd",
                    "η0eta0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The trends observed are as expected. Distributional metrics (FD and MMD) are consistently lower at smaller control coefficients, since weak steering leaves generations closer to the reference distribution. As <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, stronger injections deviate more from ground truth and raise FD/MMD. By contrast, CLAP alignment&#8212;measuring similarity to the conditioning text prompt&#8212;remains essentially flat across control strengths, indicating that textual conditioning is preserved regardless of steering intensity, only with slight degradation in some categories as control coefficient increases. Thus, moderate values of <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> can balance concept control with distributional fidelity while maintaining prompt adherence. We provide additional visual graphs for the reader in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A4\" title=\"Appendix D Single Direction Metric Graphs &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "clap",
                    "mmd",
                    "η0eta0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate whether MusicRFM steering actually enforces the intended musical concepts, we classify generated samples using the same multiclass RFM probes described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S4\" title=\"4 Classification Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. For each attribute, we varied the control coefficient <math alttext=\"\\eta_{0}\\in\\{0.15,0.30,0.45,0.60\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0.15</mn><mo>,</mo><mn>0.30</mn><mo>,</mo><mn>0.45</mn><mo>,</mo><mn>0.60</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}\\in\\{0.15,0.30,0.45,0.60\\}</annotation></semantics></math> and measured probe accuracy on all generations. The last column in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes results across six categories.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe that classification accuracy is generally low for all attributes except <span class=\"ltx_text ltx_font_bold\">notes</span>, where probe accuracy climbs sharply from 0.23 at <math alttext=\"\\eta_{0}{=}0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.15</annotation></semantics></math> to 0.82 at <math alttext=\"\\eta_{0}{=}0.60\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.60</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.60</annotation></semantics></math>, reflecting the inherent difficulty of enforcing more abstract or temporal musical properties. Nevertheless, the key observation is that accuracy <em class=\"ltx_emph ltx_font_italic\">monotonically increases with the control coefficient</em> in every category. For example, chords rise from 0.27 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.34, intervals from 0.12 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.22, and time signatures from 0.17 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.25. Even when overall values remain low, the consistent positive slope indicates that higher steering strength pushes generations in the expected direction of the controlled attribute.</p>\n\n",
                "matched_terms": [
                    "key",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results highlight both the promise and the limitations of probe-based evaluation. On the one hand, the monotonic response to <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> validates that MusicRFM manipulates representations in directions aligned with the intended probes. On the other hand, the <em class=\"ltx_emph ltx_font_italic\">absolute values of accuracy should not be over-interpreted</em>: the RFM probes were trained exclusively on <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>, a highly synthetic dataset with simplified musical attributes. When applied to naturalistic generations, these probes may (i) fail to generalize, or (ii) misclassify samples that do in fact satisfy the target property. Thus, the reported accuracies should be understood primarily as <em class=\"ltx_emph ltx_font_italic\">relative trends</em> across control coefficients, not as reliable ground-truth measures of musical validity.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "η0eta0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide results of a listening test, where we asked 12 participants to score 3 different audio samples for 4 control types, where they judge based on audio quality and adherence of the audio to the specified control. The 3 clips were randomly chosen base model generations (without control), na&#239;ve RFM generations, and optimal RFM generations (steering with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>). We show mean and STD of each type of steering in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T3\" title=\"Table 3 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Overall, the results indicate that both na&#239;ve and MusicRFM steering substantially improve perceived control compared to the base model, with MusicRFM consistently achieving the highest ratings across all attributes. In particular, chord and interval control benefit most from our optimizations, while tempo control shows the largest relative gain over the no-steering baseline.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the reader, we also provide representative audio samples from the listening test. Each clip is paired with its text prompt and steering metadata (<math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>, schedule), where all clips are steered with the &#8220;optimal&#8221; parameters listed above. An interactive demo of some of the clips used in our listening test is available at the project page.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://astradzhao.github.io/MusicRFMPage/\" title=\"\">https://astradzhao.github.io/MusicRFMPage/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "steering",
                    "η0eta0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate MusicRFM when (i) <em class=\"ltx_emph ltx_font_italic\">multiple</em> concept directions are injected simultaneously and (ii) when steering strength varies <em class=\"ltx_emph ltx_font_italic\">over time</em>. We report the same quantitative metrics used in the single-direction setting and for (ii) introduce temporal analyses based on RFM probe softmax scores.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe several trends:\n(i) <span class=\"ltx_text ltx_font_bold\">Probe accuracy still rises with stronger coefficients.</span> For notes in particular, accuracy increases from 0.770 at <math alttext=\"\\eta_{0}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.3</annotation></semantics></math> to 0.920 at <math alttext=\"\\eta_{0}=0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.6</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.6</annotation></semantics></math>, indicating that control strength directly improves enforcement even in multi-direction cases.\n(ii) <span class=\"ltx_text ltx_font_bold\">Distributional metrics and CLAP scores degrade at higher strengths.</span> Both FD and MMD grow substantially as <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, consistent with the single-direction case, where aggressive steering pushes samples away from the reference distribution. CLAP alignment also degrades significantly. (iii) <span class=\"ltx_text ltx_font_bold\">Accuracy in multi-direction steering exceeds single-direction.</span> We actually observe higher probe accuracy in the multi-direction setting, which we hypothesize arises because stronger aggregate constraints reduce adherence to the text prompt (lower CLAP) and, in turn, compress the generative manifold. This yields less stylistic variance in the music, making classes easier for probes to detect.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "clap",
                    "mmd",
                    "η0eta0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results highlight that multi-direction steering can indeed enforce multiple concepts, but doing so amplifies distributional drift and weakens prompt adherence. Notably, <em class=\"ltx_emph ltx_font_italic\">notes</em> remain most controllable (large probe gains with modest <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>), while more abstract concepts like intervals yield smaller improvements. This suggests that balancing coefficients across attributes or staggering them temporally (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS3.SSS3\" title=\"3.3.3 Multi-Direction and Staggered Control &#8227; 3.3 Improving Robustness in Audio-Domain Steering &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.3.3</span></a>), may be necessary for high-quality joint control.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "η0eta0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To study temporal schedules in isolation, we analyze the <span class=\"ltx_text ltx_font_bold\">notes</span> dataset with per-step steering strength <math alttext=\"\\eta_{\\ell}(t)=\\eta_{0}\\,\\rho_{\\ell}\\,\\phi(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#961;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{\\ell}(t)=\\eta_{0}\\,\\rho_{\\ell}\\,\\phi(t)</annotation></semantics></math> and track the <em class=\"ltx_emph ltx_font_italic\">softmax score of the ground-truth note class</em> under the RFM probe as a function of time (generation steps). For the experiments in this section, we only analyze on notes because are they are the highest quality in terms of following control, and also can give us a measurable accuracy when evaluating with RFM probes.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use per-direction coefficients <math alttext=\"\\eta_{0,m}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\eta_{0,m}</annotation></semantics></math> and schedules <math alttext=\"\\phi_{m}(t)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi_{m}(t)\\in[0,1]</annotation></semantics></math>, so <math alttext=\"\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)</annotation></semantics></math>. The schedules we ablate are exponential decay, linear decay &amp; increase, logistic increase, and sine wave. We put formulas used in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A5\" title=\"Appendix E Control schedules used for time control ablations on note classification &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, and record FD, MMD, and CLAP scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.T6\" title=\"Table 6 &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then log and display the RFM-probe softmax scores for both <math alttext=\"n_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">n_{1}</annotation></semantics></math> and <math alttext=\"n_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">n_{2}</annotation></semantics></math> at each timestep in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.F1.sf2\" title=\"In Figure 1 &#8227; Crossfading Between Concepts. &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>. As expected, the first note falls in probability while the second note rises. On average over 500 randomly sampled note pairs, crossfaded generations achieve FD of <math alttext=\"0.350\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mn>0.350</mn><annotation encoding=\"application/x-tex\">0.350</annotation></semantics></math>, MMD of <math alttext=\"1.922\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mn>1.922</mn><annotation encoding=\"application/x-tex\">1.922</annotation></semantics></math>, and CLAP alignment of <math alttext=\"0.250\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mn>0.250</mn><annotation encoding=\"application/x-tex\">0.250</annotation></semantics></math>, indicating modest distributional drift but stable prompt adherence.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, our experiments so far are limited to <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>-based, symbolic music-theoretic concepts such as notes, chords, and tempo. Future work could extend MusicRFM to attributes more directly tied to perceptual or production-level qualities, including instrument identity, timbre, or articulation style. While we perform preliminary analysis on <span class=\"ltx_text ltx_font_smallcaps\">MusicBench</span>, extended RFM training and steering on real-music-based datasets remains an open direction. These studies would connect RFM steering more directly to interpretability in real-world generation tasks.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering",
                    "musicbench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our experiments target <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large, but other large audio models open complementary directions for RFM steering. OpenAI&#8217;s <span class=\"ltx_text ltx_font_smallcaps\">Jukebox</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Dhariwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib5\" title=\"\">2020</a>)</cite> uses multi-scale VQ-VAE codes and hierarchical AR decoders, while Google&#8217;s recent <span class=\"ltx_text ltx_font_smallcaps\">Magenta-RT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib19\" title=\"\">2025</a>)</cite> framework supports <em class=\"ltx_emph ltx_font_italic\">real-time</em> audio generation. Applying RFMs in these contexts would require adapting probe extraction to multi-level codebooks (for Jukebox) and to low-latency streaming architectures (for Magenta-RT). In particular, real-time models highlight the possibility of <span class=\"ltx_text ltx_font_bold\">real-time steering</span>: dynamically injecting directions during live playback, enabling interactive control (i.e. live DJ-ing). Extending MusicRFM into these setups could bridge interpretability with performance-critical generative applications such as interactive music tools and live performance systems.</p>\n\n",
                "matched_terms": [
                    "rfm",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across synthetic and real-music settings, we observed consistent trade-offs governed by the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S8.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: moderate steering improves alignment to targeted concepts with limited distributional drift (FD/MMD) and minimal degradation in prompt adherence (CLAP), while aggressive steering yields stronger control at the cost of artifacts. Notes are the most reliably controllable, multi-direction steering is feasible but amplifies drift, and simple schedules (e.g., decay/rise) support intuitive manipulations like crossfades. Time-based control is accurate and true-to-schedule in terms of evaluating on softmax probability of classes. Layer pruning and stochastic (Bernoulli) application help stabilize generations by limiting cumulative bias.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "clap",
                    "η0eta0"
                ]
            }
        ]
    },
    "S6.T5": {
        "source_file": "Steering Autoregressive Music Generation with Recursive Feature Machines",
        "caption": "Table 5: Multi-direction (pairwise) steering. Each cell reports the average over 200 generations.",
        "body": "Probe Acc. ↑\\uparrow",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">Probe Acc.</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n</table>\n",
        "informative_terms_identified": [
            "steering",
            "pairwise",
            "cell",
            "reports",
            "generations",
            "probe",
            "acc",
            "average",
            "↑uparrow",
            "each",
            "multidirection",
            "over"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">At inference, we inject two steering directions per selected layer, one for each concept, following Sec&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS3.SSS3\" title=\"3.3.3 Multi-Direction and Staggered Control &#8227; 3.3 Improving Robustness in Audio-Domain Steering &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.3.3</span></a>. Each direction is scaled by an independent global coefficient <math alttext=\"\\eta_{0}\\in\\{0.3,0.6\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0.3</mn><mo>,</mo><mn>0.6</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}\\in\\{0.3,0.6\\}</annotation></semantics></math>. We evaluate all four cross-combinations {[0.3,0.3], [0.3,0.6], [0.6,0.3], [0.6,0.6]}, where the first value corresponds to category <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m2\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and the second to category <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m3\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>. For each pair, we generate <math alttext=\"N=100\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding=\"application/x-tex\">N=100</annotation></semantics></math> samples per configuration, yielding <math alttext=\"3\\times 4\\times N=1200\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow><mo>=</mo><mn>1200</mn></mrow><annotation encoding=\"application/x-tex\">3\\times 4\\times N=1200</annotation></semantics></math> total generations. To report results concisely, we reorganize outputs by attribute rather than by pair. For instance, all samples where <em class=\"ltx_emph ltx_font_italic\">notes</em> were steered with coefficient <math alttext=\"0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p2.m6\" intent=\":literal\"><semantics><mn>0.3</mn><annotation encoding=\"application/x-tex\">0.3</annotation></semantics></math>&#8212;regardless of whether they were paired with chords or intervals&#8212;are averaged together. This gives us per-category summaries across all pairings, shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.T5\" title=\"Table 5 &#8227; 6.1 Multi-Direction Steering: Pairwise Cross-Category Control &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model&#8217;s internal gradients to produce interpretable &#8220;concept directions&#8221;, or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/astradzhao/music-rfm\" title=\"\">https://github.com/astradzhao/music-rfm</a></span></span></span> to encourage further exploration on RFMs in the music domain.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large autoregressive (AR) models, powered by neural audio codecs, have made remarkable strides in text-to-music (TTM) generation, producing audio with impressive fidelity and coherence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib4\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib23\" title=\"\">2025</a>)</cite>. Despite a growing body of work in conditioning TTM AR models on time-varying controls <cite class=\"ltx_cite ltx_citemacro_citep\">(Novack et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib16\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib15\" title=\"\">a</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib22\" title=\"\">2024</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib10\" title=\"\">2023</a>; Koo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib9\" title=\"\">2025</a>)</cite>, achieving precise control over fine-grained <em class=\"ltx_emph ltx_font_italic\">music-theoretic</em> (e.g. pitch classes and chord qualities) content across time in generations remains challenging. Current approaches often focus on broad temporal controls like dynamics or polyphonic melody, and may either require intense finetuning runs or costly\nper-step optimization during inference to avoid large-scale training.</p>\n\n",
                "matched_terms": [
                    "over",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce MusicRFM, a framework that adapts RFMs to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-Large model directly in its activation space. Our approach is twofold: first, we train extremely lightweight, layer-wise RFM probes on the <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> dataset &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> to extract these potent, concept-aligned directions. Then, at inference time, we inject them into the model&#8217;s residual stream via forward hooks, enabling real-time, fine-grained control over the generated output. To ensure that audio quality and fidelity is not sacrificed for steering controllability, we introduce layer-based methods that apply steering selectively across <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s 48 decoder blocks, using top-K selection or an exponential weighting scheme based on each layer&#8217;s probe performance. For dynamic control, we implement time-based schedules that modulate steering strength throughout the generation with functions like linear fades, sinusoidal patterns, and sparse, stochastic application. MusicRFM further supports multi-direction steering, allowing for simultaneous or staggered enforcement of multiple attributes, such as jointly controlling notes and tempos. This comprehensive approach to control proves highly effective: our primary analysis shows that steering can increase the classification accuracy of a target note from 0.23 to over 0.82, while CLAP score for text alignment remains within <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.02 of the unsteered baseline.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "probe",
                    "each",
                    "multidirection",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In brief, MusicRFM establishes a general and efficient framework for fine-grained, interpretable control in text-to-music generation, requiring only the lightweight training of small RFM probes, with no model finetuning or costly optimization at inference time. Its layer- and time-aware mechanisms, coupled with support for multi-direction steering, enable flexible and controllable modulation of attributes while preserving high audio fidelity.</p>\n\n",
                "matched_terms": [
                    "multidirection",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recursive Feature Machines (RFMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> were introduced as probing methods that iteratively recondition features via AGOP matrices to uncover task-sensitive subspaces. More recently, RFM-derived directions have been re-injected into activations for <em class=\"ltx_emph ltx_font_italic\">steering</em> in LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. We extend this paradigm to autoregressive music generation with three innovations: (i) <em class=\"ltx_emph ltx_font_italic\">layer-based control</em> through top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> and exponential weighting across 48 layers, (ii) <em class=\"ltx_emph ltx_font_italic\">time-based control</em> using dynamic schedules, and (iii) <em class=\"ltx_emph ltx_font_italic\">multi-direction control</em> via simultaneous or staggered application of concept directions.</p>\n\n",
                "matched_terms": [
                    "multidirection",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work has extended RFMs to <em class=\"ltx_emph ltx_font_italic\">steering</em>: injecting a concept direction <math alttext=\"q_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">q_{j}</annotation></semantics></math> back into hidden activations biases a frozen model toward that attribute during inference <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. In practice, steering is implemented by registering hooks on a subset of layers <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> and adding a broadcast control vector to each residual stream:</p>\n\n",
                "matched_terms": [
                    "each",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adapt RFMs to steer <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large (<math alttext=\"L{=}48\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">L{=}48</annotation></semantics></math> decoder blocks), a Transformer over EnCodec tokens conditioned on text <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib4\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib6\" title=\"\">2022</a>)</cite>. Our pipeline has three stages: (i) audio <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> <span class=\"ltx_text ltx_font_smallcaps\">EnCodec</span> codes, (ii) layerwise RFM probes that yield AGOP eigendirections, and (iii) steering applied at inference as described above.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio clips are resampled to 32&#8201;kHz, encoded with <span class=\"ltx_text ltx_font_smallcaps\">EnCodec</span>, and passed through <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>. For clip <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> and layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we mean-pool over tokens,\n<math alttext=\"x_{i,\\ell}=\\tfrac{1}{T}\\sum_{t=1}^{T}h_{t,\\ell}^{(i)}\\in\\mathbb{R}^{d_{\\ell}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mi mathvariant=\"normal\">&#8467;</mi></mrow></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mi>T</mi></mfrac><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><msubsup><mi>h</mi><mrow><mi>t</mi><mo>,</mo><mi mathvariant=\"normal\">&#8467;</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><msub><mi>d</mi><mi mathvariant=\"normal\">&#8467;</mi></msub></msup></mrow><annotation encoding=\"application/x-tex\">x_{i,\\ell}=\\tfrac{1}{T}\\sum_{t=1}^{T}h_{t,\\ell}^{(i)}\\in\\mathbb{R}^{d_{\\ell}}</annotation></semantics></math>,\nyielding clip-level vectors. Unlike last-token pooling used in text-based RFMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">a</a>)</cite>, mean pooling better captures temporal structure and improves probe performance.</p>\n\n",
                "matched_terms": [
                    "probe",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each concept <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> and layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we train RFM probes for 15 iterations (fit predictor, compute AGOP, apply PSD map), keeping the probe with best validation metric (AUC for classification, MSE for regression). Binary concepts use <math alttext=\"\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,1\\}</annotation></semantics></math> labels and regression targets are z-normalized. The resulting eigendirections <math alttext=\"q_{\\ell,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">q_{\\ell,j}</annotation></semantics></math> form interpretable axes used for steering at inference. Steering is performed by the same process described in Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.E3\" title=\"In 3.1 Background on Recursive Feature Machines &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. For classification tasks, we additionally train multiclass RFMs that simply replace binary labels with one-hot-encoded target vectors, predicting through softmaxing final outputs.</p>\n\n",
                "matched_terms": [
                    "each",
                    "probe",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> selection.</span> We rank each layer <math alttext=\"\\ell\\in\\{1,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\ell\\in\\{1,\\dots,L\\}</annotation></semantics></math> by its validation probe performance <math alttext=\"\\mathrm{AUC}_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>AUC</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">\\mathrm{AUC}_{\\ell}</annotation></semantics></math>, then restrict steering to the top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> layers.</p>\n\n",
                "matched_terms": [
                    "each",
                    "probe",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exponential weighting.</span> Instead of hard pruning, we also apply continuous weighting across layers. For each layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we normalize its probe score <math alttext=\"s_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">s_{\\ell}</annotation></semantics></math> into <math alttext=\"\\hat{s}_{\\ell}\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{s}_{\\ell}\\in[0,1]</annotation></semantics></math>, and define <math alttext=\"w_{\\ell}=w_{0}\\cdot\\hat{s}_{\\ell}^{1/\\kappa}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>=</mo><mrow><msub><mi>w</mi><mn>0</mn></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msubsup><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>1</mn><mo>/</mo><mi>&#954;</mi></mrow></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">w_{\\ell}=w_{0}\\cdot\\hat{s}_{\\ell}^{1/\\kappa}</annotation></semantics></math> with <math alttext=\"\\kappa\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m5\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa\\in(0,1)</annotation></semantics></math>. This concentrates steering strength on high-performing layers, reducing unwanted artifacts and incorrect directions produced by the lower-scoring ones.</p>\n\n",
                "matched_terms": [
                    "each",
                    "probe",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We modulate steering strength over time as <math alttext=\"\\eta_{\\ell}(t)=\\eta_{0}\\,w_{\\ell}\\,\\phi(t)\\,\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{\\ell}(t)=\\eta_{0}\\,w_{\\ell}\\,\\phi(t)\\,\\psi_{p}(t)</annotation></semantics></math>, where <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> is a global coefficient, <math alttext=\"w_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">w_{\\ell}</annotation></semantics></math> a layer weight, <math alttext=\"\\phi(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(t)</annotation></semantics></math> a deterministic schedule, and <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math> an optional stochastic gate.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further extend MusicRFM to support <em class=\"ltx_emph ltx_font_italic\">multi-direction steering</em>, combining multiple concept vectors <math alttext=\"\\{q_{\\ell,j_{m}}\\}_{m=1}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><msub><mi>j</mi><mi>m</mi></msub></mrow></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><annotation encoding=\"application/x-tex\">\\{q_{\\ell,j_{m}}\\}_{m=1}^{M}</annotation></semantics></math> in parallel. At each step we inject\n<math alttext=\"h^{\\prime}_{t,\\ell}=h_{t,\\ell}+\\sum_{m=1}^{M}\\big[\\eta_{0,m}\\,w_{\\ell}\\,\\phi_{m}(t)\\,\\psi_{p}(t)\\big]\\,q_{\\ell,j_{m}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi>h</mi><mrow><mi>t</mi><mo>,</mo><mi mathvariant=\"normal\">&#8467;</mi></mrow><mo>&#8242;</mo></msubsup><mo>=</mo><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>,</mo><mi mathvariant=\"normal\">&#8467;</mi></mrow></msub><mo rspace=\"0.055em\">+</mo><mrow><msubsup><mo rspace=\"0em\">&#8721;</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mrow><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">[</mo><mrow><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">]</mo></mrow><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><msub><mi>j</mi><mi>m</mi></msub></mrow></msub></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">h^{\\prime}_{t,\\ell}=h_{t,\\ell}+\\sum_{m=1}^{M}\\big[\\eta_{0,m}\\,w_{\\ell}\\,\\phi_{m}(t)\\,\\psi_{p}(t)\\big]\\,q_{\\ell,j_{m}}</annotation></semantics></math>,\nwhere each direction <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m3\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> has its own coefficient <math alttext=\"\\eta_{0,m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\eta_{0,m}</annotation></semantics></math> and schedule <math alttext=\"\\phi_{m}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi_{m}(t)</annotation></semantics></math>. This enables both (i) <em class=\"ltx_emph ltx_font_italic\">simultaneous</em> enforcement of multiple attributes and (ii) <em class=\"ltx_emph ltx_font_italic\">staggered</em> control where different concepts are activated at different times. For example, one schedule may enforce tempo strongly during the opening segment, while another gradually ramps in harmonic structure later.</p>\n\n",
                "matched_terms": [
                    "each",
                    "multidirection",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With MusicRFM, we additionally train separate multiclass probes (different from the binary probes used to steer) to compare RFM clasification against the original probing methods used in <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>. We see that RFMs have better or comparable performance to the 2-layer FFN probes used in the original <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> paper across all categories. We highlight that RFMs beat the baseline probes in accuracy on scales, progressions, and intervals and in R2 score on the tempo dataset, resulting in a higher overall average score. We justify the mean pooling strategy by ablating RFMs over only classifying on last-token activations, showing much better performance.</p>\n\n",
                "matched_terms": [
                    "average",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The trends observed are as expected. Distributional metrics (FD and MMD) are consistently lower at smaller control coefficients, since weak steering leaves generations closer to the reference distribution. As <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, stronger injections deviate more from ground truth and raise FD/MMD. By contrast, CLAP alignment&#8212;measuring similarity to the conditioning text prompt&#8212;remains essentially flat across control strengths, indicating that textual conditioning is preserved regardless of steering intensity, only with slight degradation in some categories as control coefficient increases. Thus, moderate values of <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> can balance concept control with distributional fidelity while maintaining prompt adherence. We provide additional visual graphs for the reader in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A4\" title=\"Appendix D Single Direction Metric Graphs &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate whether MusicRFM steering actually enforces the intended musical concepts, we classify generated samples using the same multiclass RFM probes described in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S4\" title=\"4 Classification Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. For each attribute, we varied the control coefficient <math alttext=\"\\eta_{0}\\in\\{0.15,0.30,0.45,0.60\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0.15</mn><mo>,</mo><mn>0.30</mn><mo>,</mo><mn>0.45</mn><mo>,</mo><mn>0.60</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}\\in\\{0.15,0.30,0.45,0.60\\}</annotation></semantics></math> and measured probe accuracy on all generations. The last column in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> summarizes results across six categories.</p>\n\n",
                "matched_terms": [
                    "each",
                    "probe",
                    "steering",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe that classification accuracy is generally low for all attributes except <span class=\"ltx_text ltx_font_bold\">notes</span>, where probe accuracy climbs sharply from 0.23 at <math alttext=\"\\eta_{0}{=}0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.15</annotation></semantics></math> to 0.82 at <math alttext=\"\\eta_{0}{=}0.60\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.60</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.60</annotation></semantics></math>, reflecting the inherent difficulty of enforcing more abstract or temporal musical properties. Nevertheless, the key observation is that accuracy <em class=\"ltx_emph ltx_font_italic\">monotonically increases with the control coefficient</em> in every category. For example, chords rise from 0.27 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.34, intervals from 0.12 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.22, and time signatures from 0.17 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.25. Even when overall values remain low, the consistent positive slope indicates that higher steering strength pushes generations in the expected direction of the controlled attribute.</p>\n\n",
                "matched_terms": [
                    "probe",
                    "steering",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide results of a listening test, where we asked 12 participants to score 3 different audio samples for 4 control types, where they judge based on audio quality and adherence of the audio to the specified control. The 3 clips were randomly chosen base model generations (without control), na&#239;ve RFM generations, and optimal RFM generations (steering with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>). We show mean and STD of each type of steering in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T3\" title=\"Table 3 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Overall, the results indicate that both na&#239;ve and MusicRFM steering substantially improve perceived control compared to the base model, with MusicRFM consistently achieving the highest ratings across all attributes. In particular, chord and interval control benefit most from our optimizations, while tempo control shows the largest relative gain over the no-steering baseline.</p>\n\n",
                "matched_terms": [
                    "each",
                    "steering",
                    "over",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the reader, we also provide representative audio samples from the listening test. Each clip is paired with its text prompt and steering metadata (<math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>, schedule), where all clips are steered with the &#8220;optimal&#8221; parameters listed above. An interactive demo of some of the clips used in our listening test is available at the project page.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://astradzhao.github.io/MusicRFMPage/\" title=\"\">https://astradzhao.github.io/MusicRFMPage/</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "each",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test transfer beyond synthetic data, we evaluate RFM probes on <span class=\"ltx_text ltx_font_smallcaps\">MusicBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Melechovsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib12\" title=\"\">2024</a>)</cite>, a real-music corpus with ground-truth tempo, notes, and keys. Using the same pipeline as in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS2\" title=\"3.2 MusicRFM: RFM Steering for Music Generation &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we mean-pool <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large hidden states and fit layerwise RFMs (train/val/test split 70/15/15). For tempo we report normalized MSE, for classification overall accuracy. RFM probes reach <math alttext=\"75.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>75.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">75.3\\%</annotation></semantics></math> accuracy on notes and <math alttext=\"67.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>67.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">67.5\\%</annotation></semantics></math> on keys, while tempo regression proves difficult (MSE <math alttext=\"0.862\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><mn>0.862</mn><annotation encoding=\"application/x-tex\">0.862</annotation></semantics></math>). Steering experiments (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Evaluation on MusicBench (Real Music) &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mirror <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>: higher <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases FD/MMD and reduces CLAP, showing that moderate control preserves text adherence but aggressive coefficients destabilize generations. Overall, MusicBench confirms that real-music attributes can be steered, though sensitivity varies by concept difficulty.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate MusicRFM when (i) <em class=\"ltx_emph ltx_font_italic\">multiple</em> concept directions are injected simultaneously and (ii) when steering strength varies <em class=\"ltx_emph ltx_font_italic\">over time</em>. We report the same quantitative metrics used in the single-direction setting and for (ii) introduce temporal analyses based on RFM probe softmax scores.</p>\n\n",
                "matched_terms": [
                    "probe",
                    "steering",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test whether MusicRFM can jointly enforce <em class=\"ltx_emph ltx_font_italic\">multiple musical attributes</em>, we examine all pairwise combinations among {<span class=\"ltx_text ltx_font_bold\">notes</span>, <span class=\"ltx_text ltx_font_bold\">chords</span>, <span class=\"ltx_text ltx_font_bold\">intervals</span>}. For each pair <math alttext=\"(a,b)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a,b)</annotation></semantics></math>, we sample a random target class from category <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m2\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> (e.g., note <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m3\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>) and a random class from category <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p1.m4\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> (e.g., major chord), then generate music conditioned on both controls simultaneously.</p>\n\n",
                "matched_terms": [
                    "each",
                    "pairwise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe several trends:\n(i) <span class=\"ltx_text ltx_font_bold\">Probe accuracy still rises with stronger coefficients.</span> For notes in particular, accuracy increases from 0.770 at <math alttext=\"\\eta_{0}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.3</annotation></semantics></math> to 0.920 at <math alttext=\"\\eta_{0}=0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.6</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.6</annotation></semantics></math>, indicating that control strength directly improves enforcement even in multi-direction cases.\n(ii) <span class=\"ltx_text ltx_font_bold\">Distributional metrics and CLAP scores degrade at higher strengths.</span> Both FD and MMD grow substantially as <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, consistent with the single-direction case, where aggressive steering pushes samples away from the reference distribution. CLAP alignment also degrades significantly. (iii) <span class=\"ltx_text ltx_font_bold\">Accuracy in multi-direction steering exceeds single-direction.</span> We actually observe higher probe accuracy in the multi-direction setting, which we hypothesize arises because stronger aggregate constraints reduce adherence to the text prompt (lower CLAP) and, in turn, compress the generative manifold. This yields less stylistic variance in the music, making classes easier for probes to detect.</p>\n\n",
                "matched_terms": [
                    "probe",
                    "steering",
                    "multidirection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results highlight that multi-direction steering can indeed enforce multiple concepts, but doing so amplifies distributional drift and weakens prompt adherence. Notably, <em class=\"ltx_emph ltx_font_italic\">notes</em> remain most controllable (large probe gains with modest <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>), while more abstract concepts like intervals yield smaller improvements. This suggests that balancing coefficients across attributes or staggering them temporally (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS3.SSS3\" title=\"3.3.3 Multi-Direction and Staggered Control &#8227; 3.3 Improving Robustness in Audio-Domain Steering &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.3.3</span></a>), may be necessary for high-quality joint control.</p>\n\n",
                "matched_terms": [
                    "probe",
                    "steering",
                    "multidirection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To study temporal schedules in isolation, we analyze the <span class=\"ltx_text ltx_font_bold\">notes</span> dataset with per-step steering strength <math alttext=\"\\eta_{\\ell}(t)=\\eta_{0}\\,\\rho_{\\ell}\\,\\phi(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#961;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{\\ell}(t)=\\eta_{0}\\,\\rho_{\\ell}\\,\\phi(t)</annotation></semantics></math> and track the <em class=\"ltx_emph ltx_font_italic\">softmax score of the ground-truth note class</em> under the RFM probe as a function of time (generation steps). For the experiments in this section, we only analyze on notes because are they are the highest quality in terms of following control, and also can give us a measurable accuracy when evaluating with RFM probes.</p>\n\n",
                "matched_terms": [
                    "probe",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each schedule we plot the probe softmax of the correct note over time in Figure &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.F1.sf1\" title=\"In Figure 1 &#8227; Crossfading Between Concepts. &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>. We see that the distribution over time follows exactly what we would expect from each of the smooth scheduling functions - exponential &amp; linear decay look like decay functions, sine is very similar to a sine wave, and logistic &amp; linear increase show an increase in predicted probability.</p>\n\n",
                "matched_terms": [
                    "each",
                    "probe",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then log and display the RFM-probe softmax scores for both <math alttext=\"n_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">n_{1}</annotation></semantics></math> and <math alttext=\"n_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">n_{2}</annotation></semantics></math> at each timestep in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.F1.sf2\" title=\"In Figure 1 &#8227; Crossfading Between Concepts. &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>. As expected, the first note falls in probability while the second note rises. On average over 500 randomly sampled note pairs, crossfaded generations achieve FD of <math alttext=\"0.350\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mn>0.350</mn><annotation encoding=\"application/x-tex\">0.350</annotation></semantics></math>, MMD of <math alttext=\"1.922\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mn>1.922</mn><annotation encoding=\"application/x-tex\">1.922</annotation></semantics></math>, and CLAP alignment of <math alttext=\"0.250\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mn>0.250</mn><annotation encoding=\"application/x-tex\">0.250</annotation></semantics></math>, indicating modest distributional drift but stable prompt adherence.</p>\n\n",
                "matched_terms": [
                    "each",
                    "average",
                    "over",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another promising direction is to modify the steering process itself: by selectively masking portions of the autoregressive context during inference, one could reduce model over-dependence on previously generated tokens, thus increasing the model&#8217;s sensitivity to injected steering signals. Such approaches could make activation-space interventions both more controllable and also reduce the number of unwanted artifacts, particularly for longer generations.</p>\n\n",
                "matched_terms": [
                    "steering",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, our experiments target <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large, but other large audio models open complementary directions for RFM steering. OpenAI&#8217;s <span class=\"ltx_text ltx_font_smallcaps\">Jukebox</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Dhariwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib5\" title=\"\">2020</a>)</cite> uses multi-scale VQ-VAE codes and hierarchical AR decoders, while Google&#8217;s recent <span class=\"ltx_text ltx_font_smallcaps\">Magenta-RT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib19\" title=\"\">2025</a>)</cite> framework supports <em class=\"ltx_emph ltx_font_italic\">real-time</em> audio generation. Applying RFMs in these contexts would require adapting probe extraction to multi-level codebooks (for Jukebox) and to low-latency streaming architectures (for Magenta-RT). In particular, real-time models highlight the possibility of <span class=\"ltx_text ltx_font_bold\">real-time steering</span>: dynamically injecting directions during live playback, enabling interactive control (i.e. live DJ-ing). Extending MusicRFM into these setups could bridge interpretability with performance-critical generative applications such as interactive music tools and live performance systems.</p>\n\n",
                "matched_terms": [
                    "probe",
                    "steering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across synthetic and real-music settings, we observed consistent trade-offs governed by the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S8.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: moderate steering improves alignment to targeted concepts with limited distributional drift (FD/MMD) and minimal degradation in prompt adherence (CLAP), while aggressive steering yields stronger control at the cost of artifacts. Notes are the most reliably controllable, multi-direction steering is feasible but amplifies drift, and simple schedules (e.g., decay/rise) support intuitive manipulations like crossfades. Time-based control is accurate and true-to-schedule in terms of evaluating on softmax probability of classes. Layer pruning and stochastic (Bernoulli) application help stabilize generations by limiting cumulative bias.</p>\n\n",
                "matched_terms": [
                    "multidirection",
                    "steering",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note we tune over a more general class of kernels <math alttext=\"K_{p,q}(x,x^{\\prime})=\\exp(-\\|x-x^{\\prime}\\|_{p}^{q}/L^{q})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>K</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><mrow><msubsup><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>x</mi><mo>&#8722;</mo><msup><mi>x</mi><mo>&#8242;</mo></msup></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mi>p</mi><mi>q</mi></msubsup><mo>/</mo><msup><mi>L</mi><mi>q</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">K_{p,q}(x,x^{\\prime})=\\exp(-\\|x-x^{\\prime}\\|_{p}^{q}/L^{q})</annotation></semantics></math> (indicated by the kernel type hyperparameter) for the aggregation model, which has been shown to improve the performance of RFM on tabular datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">2025a</a>)</cite>. We also tune over whether to center the gradients in each iteration of RFM, which can help de-noise the gradients in high-dimensional settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. Gradient centering modifies the AGOP computation to give the following centered M matrix in the RFM iteration, where <math alttext=\"\\bar{g}=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p3.m2\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>g</mi><mo>&#175;</mo></mover><mo>=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>g</mi><mi>i</mi></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\bar{g}=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "each",
                    "over"
                ]
            }
        ]
    },
    "S6.T6": {
        "source_file": "Steering Autoregressive Music Generation with Recursive Feature Machines",
        "caption": "Table 6: Metrics on time-dependent controlled generations",
        "body": "Schedule\nFD ↓\\downarrow\n\nMMD ↓\\downarrow\n\nCLAP ↑\\uparrow\n\n\n\nLinear increase\n0.358\n1.917\n0.227\n\n\nLinear decay\n0.321\n1.636\n0.257\n\n\nExponential decay\n0.229\n1.052\n0.312\n\n\nLogistic increase\n0.360\n1.999\n0.208\n\n\nSine modulation\n0.413\n2.347\n0.225",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:-0.5pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Schedule</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.5pt 3.0pt;\">FD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.5pt 3.0pt;\">MMD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:-0.5pt 3.0pt;\">CLAP <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T6.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.5pt 3.0pt;\">Linear increase</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 3.0pt;\">0.358</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 3.0pt;\">1.917</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.5pt 3.0pt;\">0.227</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.5pt 3.0pt;\">Linear decay</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 3.0pt;\">0.321</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 3.0pt;\">1.636</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 3.0pt;\">0.257</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.5pt 3.0pt;\">Exponential decay</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 3.0pt;\">0.229</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 3.0pt;\">1.052</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 3.0pt;\">0.312</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.5pt 3.0pt;\">Logistic increase</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 3.0pt;\">0.360</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 3.0pt;\">1.999</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.5pt 3.0pt;\">0.208</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:-0.5pt 3.0pt;\">Sine modulation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 3.0pt;\">0.413</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 3.0pt;\">2.347</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.5pt 3.0pt;\">0.225</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "controlled",
            "clap",
            "mmd",
            "exponential",
            "increase",
            "sine",
            "↓downarrow",
            "generations",
            "decay",
            "logistic",
            "↑uparrow",
            "schedule",
            "linear",
            "timedependent",
            "metrics",
            "modulation"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We use per-direction coefficients <math alttext=\"\\eta_{0,m}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\eta_{0,m}</annotation></semantics></math> and schedules <math alttext=\"\\phi_{m}(t)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi_{m}(t)\\in[0,1]</annotation></semantics></math>, so <math alttext=\"\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)</annotation></semantics></math>. The schedules we ablate are exponential decay, linear decay &amp; increase, logistic increase, and sine wave. We put formulas used in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A5\" title=\"Appendix E Control schedules used for time control ablations on note classification &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, and record FD, MMD, and CLAP scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.T6\" title=\"Table 6 &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce MusicRFM, a framework that adapts RFMs to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-Large model directly in its activation space. Our approach is twofold: first, we train extremely lightweight, layer-wise RFM probes on the <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> dataset &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> to extract these potent, concept-aligned directions. Then, at inference time, we inject them into the model&#8217;s residual stream via forward hooks, enabling real-time, fine-grained control over the generated output. To ensure that audio quality and fidelity is not sacrificed for steering controllability, we introduce layer-based methods that apply steering selectively across <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s 48 decoder blocks, using top-K selection or an exponential weighting scheme based on each layer&#8217;s probe performance. For dynamic control, we implement time-based schedules that modulate steering strength throughout the generation with functions like linear fades, sinusoidal patterns, and sparse, stochastic application. MusicRFM further supports multi-direction steering, allowing for simultaneous or staggered enforcement of multiple attributes, such as jointly controlling notes and tempos. This comprehensive approach to control proves highly effective: our primary analysis shows that steering can increase the classification accuracy of a target note from 0.23 to over 0.82, while CLAP score for text alignment remains within <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.02 of the unsteered baseline.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "exponential",
                    "increase",
                    "linear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Deterministic schedules <math alttext=\"\\phi(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(t)</annotation></semantics></math>.</span> Linear/logistic <em class=\"ltx_emph ltx_font_italic\">rise</em>, linear/exponential <em class=\"ltx_emph ltx_font_italic\">decay</em>, and <em class=\"ltx_emph ltx_font_italic\">sinusoidal</em> modulation let us increase or decrease a concept&#8217;s influence over time (e.g., fade out a note class, ramp in a chord progression, or periodically modulate tempo). Closed-form expressions are given in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A5\" title=\"Appendix E Control schedules used for time control ablations on note classification &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "modulation",
                    "increase",
                    "decay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report results on how well binary directions trained using MusicRFM are able to steer generations towards interpretable concepts, exploring both quantitative and subjective metrics.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first quantify audio similarity and distributional shift of generations steered along a <em class=\"ltx_emph ltx_font_italic\">single</em> concept direction using three standard metrics as a function of the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: (i) <span class=\"ltx_text ltx_font_bold\">Fr&#233;chet Distance (FD)</span> (lower is better), and (ii) <span class=\"ltx_text ltx_font_bold\">Maximum Mean Discrepancy (MMD)</span> (lower is better), (iii) <span class=\"ltx_text ltx_font_bold\">CLAP</span> alignment (higher is better). For the <span class=\"ltx_text ltx_font_bold\">tempos</span> category, the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> is averaged among the absolute value of the coefficient (e.g. the results from -0.15 and 0.15 are averaged into the 0.15 column). We show these results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. All results are reported on generations steered with RFM probes using stochastic application <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math> with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>; these are settings we found to be most optimal when creating high-quality, conceptually accurate generations.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd",
                    "exponential",
                    "generations",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The trends observed are as expected. Distributional metrics (FD and MMD) are consistently lower at smaller control coefficients, since weak steering leaves generations closer to the reference distribution. As <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, stronger injections deviate more from ground truth and raise FD/MMD. By contrast, CLAP alignment&#8212;measuring similarity to the conditioning text prompt&#8212;remains essentially flat across control strengths, indicating that textual conditioning is preserved regardless of steering intensity, only with slight degradation in some categories as control coefficient increases. Thus, moderate values of <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> can balance concept control with distributional fidelity while maintaining prompt adherence. We provide additional visual graphs for the reader in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A4\" title=\"Appendix D Single Direction Metric Graphs &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd",
                    "metrics",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe that classification accuracy is generally low for all attributes except <span class=\"ltx_text ltx_font_bold\">notes</span>, where probe accuracy climbs sharply from 0.23 at <math alttext=\"\\eta_{0}{=}0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.15</annotation></semantics></math> to 0.82 at <math alttext=\"\\eta_{0}{=}0.60\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.60</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.60</annotation></semantics></math>, reflecting the inherent difficulty of enforcing more abstract or temporal musical properties. Nevertheless, the key observation is that accuracy <em class=\"ltx_emph ltx_font_italic\">monotonically increases with the control coefficient</em> in every category. For example, chords rise from 0.27 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.34, intervals from 0.12 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.22, and time signatures from 0.17 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.25. Even when overall values remain low, the consistent positive slope indicates that higher steering strength pushes generations in the expected direction of the controlled attribute.</p>\n\n",
                "matched_terms": [
                    "controlled",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide results of a listening test, where we asked 12 participants to score 3 different audio samples for 4 control types, where they judge based on audio quality and adherence of the audio to the specified control. The 3 clips were randomly chosen base model generations (without control), na&#239;ve RFM generations, and optimal RFM generations (steering with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>). We show mean and STD of each type of steering in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T3\" title=\"Table 3 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Overall, the results indicate that both na&#239;ve and MusicRFM steering substantially improve perceived control compared to the base model, with MusicRFM consistently achieving the highest ratings across all attributes. In particular, chord and interval control benefit most from our optimizations, while tempo control shows the largest relative gain over the no-steering baseline.</p>\n\n",
                "matched_terms": [
                    "exponential",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test transfer beyond synthetic data, we evaluate RFM probes on <span class=\"ltx_text ltx_font_smallcaps\">MusicBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Melechovsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib12\" title=\"\">2024</a>)</cite>, a real-music corpus with ground-truth tempo, notes, and keys. Using the same pipeline as in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS2\" title=\"3.2 MusicRFM: RFM Steering for Music Generation &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we mean-pool <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large hidden states and fit layerwise RFMs (train/val/test split 70/15/15). For tempo we report normalized MSE, for classification overall accuracy. RFM probes reach <math alttext=\"75.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>75.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">75.3\\%</annotation></semantics></math> accuracy on notes and <math alttext=\"67.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>67.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">67.5\\%</annotation></semantics></math> on keys, while tempo regression proves difficult (MSE <math alttext=\"0.862\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><mn>0.862</mn><annotation encoding=\"application/x-tex\">0.862</annotation></semantics></math>). Steering experiments (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Evaluation on MusicBench (Real Music) &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mirror <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>: higher <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases FD/MMD and reduces CLAP, showing that moderate control preserves text adherence but aggressive coefficients destabilize generations. Overall, MusicBench confirms that real-music attributes can be steered, though sensitivity varies by concept difficulty.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe several trends:\n(i) <span class=\"ltx_text ltx_font_bold\">Probe accuracy still rises with stronger coefficients.</span> For notes in particular, accuracy increases from 0.770 at <math alttext=\"\\eta_{0}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.3</annotation></semantics></math> to 0.920 at <math alttext=\"\\eta_{0}=0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.6</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.6</annotation></semantics></math>, indicating that control strength directly improves enforcement even in multi-direction cases.\n(ii) <span class=\"ltx_text ltx_font_bold\">Distributional metrics and CLAP scores degrade at higher strengths.</span> Both FD and MMD grow substantially as <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, consistent with the single-direction case, where aggressive steering pushes samples away from the reference distribution. CLAP alignment also degrades significantly. (iii) <span class=\"ltx_text ltx_font_bold\">Accuracy in multi-direction steering exceeds single-direction.</span> We actually observe higher probe accuracy in the multi-direction setting, which we hypothesize arises because stronger aggregate constraints reduce adherence to the text prompt (lower CLAP) and, in turn, compress the generative manifold. This yields less stylistic variance in the music, making classes easier for probes to detect.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each schedule we plot the probe softmax of the correct note over time in Figure &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.F1.sf1\" title=\"In Figure 1 &#8227; Crossfading Between Concepts. &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>. We see that the distribution over time follows exactly what we would expect from each of the smooth scheduling functions - exponential &amp; linear decay look like decay functions, sine is very similar to a sine wave, and logistic &amp; linear increase show an increase in predicted probability.</p>\n\n",
                "matched_terms": [
                    "sine",
                    "exponential",
                    "increase",
                    "decay",
                    "logistic",
                    "schedule",
                    "linear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We additionally study a controlled <em class=\"ltx_emph ltx_font_italic\">cross-fade</em> between two notes <math alttext=\"n_{1}\\!\\to\\!n_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>n</mi><mn>1</mn></msub><mo lspace=\"0.108em\" rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo><msub><mi>n</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">n_{1}\\!\\to\\!n_{2}</annotation></semantics></math> using complementary schedules over a fixed window of 0&#8211;1500 steps: for <math alttext=\"n_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">n_{1}</annotation></semantics></math> we decay from <math alttext=\"\\eta_{0}=0.45\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.45</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.45</annotation></semantics></math> to <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><mn>0</mn></math>, and for <math alttext=\"n_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">n_{2}</annotation></semantics></math> we rise from <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m6\" intent=\":literal\"><mn>0</mn></math> to <math alttext=\"0.45\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><mn>0.45</mn><annotation encoding=\"application/x-tex\">0.45</annotation></semantics></math>. Formally,</p>\n\n",
                "matched_terms": [
                    "controlled",
                    "decay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then log and display the RFM-probe softmax scores for both <math alttext=\"n_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">n_{1}</annotation></semantics></math> and <math alttext=\"n_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">n_{2}</annotation></semantics></math> at each timestep in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.F1.sf2\" title=\"In Figure 1 &#8227; Crossfading Between Concepts. &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>. As expected, the first note falls in probability while the second note rises. On average over 500 randomly sampled note pairs, crossfaded generations achieve FD of <math alttext=\"0.350\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mn>0.350</mn><annotation encoding=\"application/x-tex\">0.350</annotation></semantics></math>, MMD of <math alttext=\"1.922\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mn>1.922</mn><annotation encoding=\"application/x-tex\">1.922</annotation></semantics></math>, and CLAP alignment of <math alttext=\"0.250\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mn>0.250</mn><annotation encoding=\"application/x-tex\">0.250</annotation></semantics></math>, indicating modest distributional drift but stable prompt adherence.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across synthetic and real-music settings, we observed consistent trade-offs governed by the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S8.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: moderate steering improves alignment to targeted concepts with limited distributional drift (FD/MMD) and minimal degradation in prompt adherence (CLAP), while aggressive steering yields stronger control at the cost of artifacts. Notes are the most reliably controllable, multi-direction steering is feasible but amplifies drift, and simple schedules (e.g., decay/rise) support intuitive manipulations like crossfades. Time-based control is accurate and true-to-schedule in terms of evaluating on softmax probability of classes. Layer pruning and stochastic (Bernoulli) application help stabilize generations by limiting cumulative bias.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "generations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We study three strategies that control how many (and how strongly) layers contribute to steering: (i) <em class=\"ltx_emph ltx_font_italic\">exponential</em> score-weighted steering, (ii) a simple <em class=\"ltx_emph ltx_font_italic\">linear</em> score-weighted scheme, and (iii) hard <em class=\"ltx_emph ltx_font_italic\">top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></em> selection. We show results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T9\" title=\"Table 9 &#8227; Discrete selection (Top-&#119870;). &#8227; C.2 Ablating Layer Pruning &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T8\" title=\"Table 8 &#8227; Discrete selection (Top-&#119870;). &#8227; C.2 Ablating Layer Pruning &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "exponential",
                    "linear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\kappa\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa\\in(0,1)</annotation></semantics></math> is a <em class=\"ltx_emph ltx_font_italic\">decay rate</em> (smaller <math alttext=\"\\kappa\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>&#954;</mi><annotation encoding=\"application/x-tex\">\\kappa</annotation></semantics></math> increases contrast, concentrating weight on high-scoring layers). Linear is the minimal &#8220;from 1 to 0&#8221; mapping; exponential recovers linear as <math alttext=\"\\kappa\\!\\to\\!1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa\\!\\to\\!1</annotation></semantics></math> and becomes more selective as <math alttext=\"\\kappa\\!\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo lspace=\"0.108em\" stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\kappa\\!\\downarrow</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "linear",
                    "exponential",
                    "decay"
                ]
            }
        ]
    },
    "A2.T7": {
        "source_file": "Steering Autoregressive Music Generation with Recursive Feature Machines",
        "caption": "Table 7: Search spaces for MusicRFM on individual layers and for the aggregation model.",
        "body": "Hyperparameter\nLayer-wise\nAggregation model\n\n\nBandwidth (L)\nlog⁡𝒰​(1,100)\\log\\mathcal{U}(1,100)\nlog⁡𝒰​(1,100)\\log\\mathcal{U}(1,100)\n\n\nCenter gradients\n{False, True}\n{False, True}\n\n\nExponent qq\n\n𝒰​(0.7,1.4)\\mathcal{U}(0.7,1.4)\n𝒰​(0.7,1.4)\\mathcal{U}(0.7,1.4)\n\n\nKernel Type\nK2,qK_{2,q}\nKp,qK_{p,q}\n\n\n\npp (when kernel type is Kp,qK_{p,q})\n–\n𝒰​(q,2)\\mathcal{U}(q,2)\n\n\nRegularization\nlog⁡𝒰​(10−5,10)\\log\\mathcal{U}(10^{-5},10)\nlog⁡𝒰​(10−5,10)\\log\\mathcal{U}(10^{-5},10)",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Hyperparameter</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Layer-wise</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Aggregation model</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Bandwidth (L)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"\\log\\mathcal{U}(1,100)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T7.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo lspace=\"0.167em\">&#8289;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log\\mathcal{U}(1,100)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"\\log\\mathcal{U}(1,100)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T7.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo lspace=\"0.167em\">&#8289;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>100</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log\\mathcal{U}(1,100)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Center gradients</td>\n<td class=\"ltx_td ltx_align_left\">{False, True}</td>\n<td class=\"ltx_td ltx_align_left\">{False, True}</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Exponent <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T7.m3\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\mathcal{U}(0.7,1.4)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T7.m4\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0.7</mn><mo>,</mo><mn>1.4</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{U}(0.7,1.4)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\mathcal{U}(0.7,1.4)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T7.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0.7</mn><mo>,</mo><mn>1.4</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{U}(0.7,1.4)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Kernel Type</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"K_{2,q}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T7.m6\" intent=\":literal\"><semantics><msub><mi>K</mi><mrow><mn>2</mn><mo>,</mo><mi>q</mi></mrow></msub><annotation encoding=\"application/x-tex\">K_{2,q}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"K_{p,q}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T7.m7\" intent=\":literal\"><semantics><msub><mi>K</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow></msub><annotation encoding=\"application/x-tex\">K_{p,q}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T7.m8\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> (when kernel type is <math alttext=\"K_{p,q}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T7.m9\" intent=\":literal\"><semantics><msub><mi>K</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow></msub><annotation encoding=\"application/x-tex\">K_{p,q}</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_left\">&#8211;</td>\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"\\mathcal{U}(q,2)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T7.m10\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{U}(q,2)</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Regularization</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><math alttext=\"\\log\\mathcal{U}(10^{-5},10)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T7.m11\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo lspace=\"0.167em\">&#8289;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup><mo>,</mo><mn>10</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log\\mathcal{U}(10^{-5},10)</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><math alttext=\"\\log\\mathcal{U}(10^{-5},10)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T7.m12\" intent=\":literal\"><semantics><mrow><mrow><mi>log</mi><mo lspace=\"0.167em\">&#8289;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup><mo>,</mo><mn>10</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\log\\mathcal{U}(10^{-5},10)</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "𝒰​q2mathcaluq2",
            "true",
            "search",
            "false",
            "individual",
            "kernel",
            "spaces",
            "regularization",
            "k2qk2q",
            "exponent",
            "log⁡𝒰​1100logmathcalu1100",
            "𝒰​0714mathcalu0714",
            "log⁡𝒰​10−510logmathcalu10510",
            "musicrfm",
            "center",
            "bandwidth",
            "kpqkpq",
            "hyperparameter",
            "gradients",
            "aggregation",
            "layerwise",
            "layers",
            "when",
            "model",
            "type"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model&#8217;s internal gradients to produce interpretable &#8220;concept directions&#8221;, or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/astradzhao/music-rfm\" title=\"\">https://github.com/astradzhao/music-rfm</a></span></span></span> to encourage further exploration on RFMs in the music domain.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gradients",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce MusicRFM, a framework that adapts RFMs to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-Large model directly in its activation space. Our approach is twofold: first, we train extremely lightweight, layer-wise RFM probes on the <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> dataset &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> to extract these potent, concept-aligned directions. Then, at inference time, we inject them into the model&#8217;s residual stream via forward hooks, enabling real-time, fine-grained control over the generated output. To ensure that audio quality and fidelity is not sacrificed for steering controllability, we introduce layer-based methods that apply steering selectively across <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s 48 decoder blocks, using top-K selection or an exponential weighting scheme based on each layer&#8217;s probe performance. For dynamic control, we implement time-based schedules that modulate steering strength throughout the generation with functions like linear fades, sinusoidal patterns, and sparse, stochastic application. MusicRFM further supports multi-direction steering, allowing for simultaneous or staggered enforcement of multiple attributes, such as jointly controlling notes and tempos. This comprehensive approach to control proves highly effective: our primary analysis shows that steering can increase the classification accuracy of a target note from 0.23 to over 0.82, while CLAP score for text alignment remains within <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.02 of the unsteered baseline.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "model",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In brief, MusicRFM establishes a general and efficient framework for fine-grained, interpretable control in text-to-music generation, requiring only the lightweight training of small RFM probes, with no model finetuning or costly optimization at inference time. Its layer- and time-aware mechanisms, coupled with support for multi-direction steering, enable flexible and controllable modulation of attributes while preserving high audio fidelity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work has extended RFMs to <em class=\"ltx_emph ltx_font_italic\">steering</em>: injecting a concept direction <math alttext=\"q_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><msub><mi>q</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">q_{j}</annotation></semantics></math> back into hidden activations biases a frozen model toward that attribute during inference <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. In practice, steering is implemented by registering hooks on a subset of layers <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> and adding a broadcast control vector to each residual stream:</p>\n\n",
                "matched_terms": [
                    "model",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> is a recently designed synthetic dataset made to study interpretable representations of music theory concepts in large models, divided into 7 categories: tempo, notes, chord progressions, chord types, scales, intervals, and time signatures. Compared to prior music datasets, <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> offers clean, fine-grained supervision of musical properties, enabling controlled experiments on model interpretability and controllability. This dataset is particularly well-suited for probing approaches, as its labeled attributes align directly with theoretical concepts that can be mapped onto latent representations. In our setting, <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> allows us to train lightweight RFM probes on layerwise activations of MusicGen, yielding gradient-based directions that correspond to human-interpretable musical attributes.</p>\n\n",
                "matched_terms": [
                    "model",
                    "layerwise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results highlight both the promise and the limitations of probe-based evaluation. On the one hand, the monotonic response to <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> validates that MusicRFM manipulates representations in directions aligned with the intended probes. On the other hand, the <em class=\"ltx_emph ltx_font_italic\">absolute values of accuracy should not be over-interpreted</em>: the RFM probes were trained exclusively on <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>, a highly synthetic dataset with simplified musical attributes. When applied to naturalistic generations, these probes may (i) fail to generalize, or (ii) misclassify samples that do in fact satisfy the target property. Thus, the reported accuracies should be understood primarily as <em class=\"ltx_emph ltx_font_italic\">relative trends</em> across control coefficients, not as reliable ground-truth measures of musical validity.</p>\n\n",
                "matched_terms": [
                    "when",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide results of a listening test, where we asked 12 participants to score 3 different audio samples for 4 control types, where they judge based on audio quality and adherence of the audio to the specified control. The 3 clips were randomly chosen base model generations (without control), na&#239;ve RFM generations, and optimal RFM generations (steering with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>). We show mean and STD of each type of steering in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T3\" title=\"Table 3 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Overall, the results indicate that both na&#239;ve and MusicRFM steering substantially improve perceived control compared to the base model, with MusicRFM consistently achieving the highest ratings across all attributes. In particular, chord and interval control benefit most from our optimizations, while tempo control shows the largest relative gain over the no-steering baseline.</p>\n\n",
                "matched_terms": [
                    "model",
                    "type",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate MusicRFM when (i) <em class=\"ltx_emph ltx_font_italic\">multiple</em> concept directions are injected simultaneously and (ii) when steering strength varies <em class=\"ltx_emph ltx_font_italic\">over time</em>. We report the same quantitative metrics used in the single-direction setting and for (ii) introduce temporal analyses based on RFM probe softmax scores.</p>\n\n",
                "matched_terms": [
                    "when",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our probes rely on mean-pooled features, which discard temporal ordering. This limits performance on concepts with strong sequential dependencies, such as scales, chord progressions, and time signatures, where the temporal dynamics are essential for accurate classification and control. As a result, RFM probes underperform on these attributes compared to temporally local concepts like notes or chords. Future work should explore temporally aware pooling strategies (e.g., attention pooling, recurrent aggregation, convolutional pooling) or sequence-level RFMs that directly model time-evolving representations. Similarly, extending beyond the top eigenvector to incorporate multiple components could capture richer subspaces of variation, but we have not yet performed variance analyses to quantify how much information higher-order components retain.</p>\n\n",
                "matched_terms": [
                    "model",
                    "aggregation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_italic\">MusicRFM</span>, a framework that leverages RFM-derived, eigenvalue-ranked directions to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large model directly in activation space. By combining concept-aligned directions with layer-aware weighting and time-dependent schedules, MusicRFM enables fine-grained, interpretable control over attributes such as notes, chords, and tempo without modifying the base model or relying on per-step optimization.</p>\n\n",
                "matched_terms": [
                    "model",
                    "musicrfm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Kernel ridge regression (KRR) is the base model with which we apply the RFM procedure for iterative feature learning via the AGOP. We briefly explain the KRR model. Let <math alttext=\"X\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math> denote training samples with <math alttext=\"{x^{(i)}}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mmultiscripts><mi>x</mi><mrow/><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow><mrow/><mi>T</mi></mmultiscripts><annotation encoding=\"application/x-tex\">{x^{(i)}}^{T}</annotation></semantics></math> denoting the sample in the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math><sup class=\"ltx_sup\">th</sup> row of <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> for <math alttext=\"i\\in[n]\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">i\\in[n]</annotation></semantics></math> and <math alttext=\"y\\in\\mathbb{R}^{n\\times c}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>y</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>c</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">y\\in\\mathbb{R}^{n\\times c}</annotation></semantics></math> denote training labels, where <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m7\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is the number of output channels (e.g. one-hot encoded classes for <math alttext=\"c&gt;2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo>&gt;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">c&gt;2</annotation></semantics></math> classes). Let <math alttext=\"K:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m9\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><mrow><msup><mi>&#8477;</mi><mi>d</mi></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>&#8477;</mi><mi>d</mi></msup></mrow><mo stretchy=\"false\">&#8594;</mo><mi>&#8477;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">K:\\mathbb{R}^{d}\\times\\mathbb{R}^{d}\\to\\mathbb{R}</annotation></semantics></math> denote a kernel function (a positive semi-definite, symmetric function), such as the Gaussian/RBF kernel (<math alttext=\"K(x,z)=\\exp(-\\|x-z\\|_{2}^{2})/L^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m10\" intent=\":literal\"><semantics><mrow><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><msubsup><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>x</mi><mo>&#8722;</mo><mi>z</mi></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><msup><mi>L</mi><mn>2</mn></msup></mrow></mrow><annotation encoding=\"application/x-tex\">K(x,z)=\\exp(-\\|x-z\\|_{2}^{2})/L^{2}</annotation></semantics></math>), or the Laplace kernel (<math alttext=\"K(x,z)=\\exp(-\\|x-z\\|_{2})/L\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m11\" intent=\":literal\"><semantics><mrow><mrow><mi>K</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><msub><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>x</mi><mo>&#8722;</mo><mi>z</mi></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mn>2</mn></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>/</mo><mi>L</mi></mrow></mrow><annotation encoding=\"application/x-tex\">K(x,z)=\\exp(-\\|x-z\\|_{2})/L</annotation></semantics></math>) used in this work. Given a ridge regularization parameter <math alttext=\"\\lambda\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m12\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda\\geq 0</annotation></semantics></math>, KRR solved on the data <math alttext=\"(X,y)\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m13\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>X</mi><mo>,</mo><mi>y</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(X,y)</annotation></semantics></math> gives a predictor, <math alttext=\"\\hat{f}:\\mathbb{R}^{d}\\to\\mathbb{R}^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m14\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>f</mi><mo>^</mo></mover><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mrow><msup><mi>&#8477;</mi><mi>d</mi></msup><mo stretchy=\"false\">&#8594;</mo><msup><mi>&#8477;</mi><mi>c</mi></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{f}:\\mathbb{R}^{d}\\to\\mathbb{R}^{c}</annotation></semantics></math>, of the form:</p>\n\n",
                "matched_terms": [
                    "regularization",
                    "model",
                    "kernel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use 70/15/15 train/valid/test split on RFM training, 15 RFM iterations, and mean pooling over all timesteps. For multiclass training of simple progressions, we use 700 examples per class (there are &#160;1100 per class in dataset, but we cannot fit them given our A6000 GPU memory size. However, we note that even without all training data, we still get significantly better accuracy than baseline in this category). For all other classes, we use the entire dataset for our training &amp; validation. We use 100 random choices of hyperparameters listed below for layer-wise probes and 300 for aggregation. We maximize on AUC for layer-wise probes and accuracy for aggregation.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "aggregation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note we tune over a more general class of kernels <math alttext=\"K_{p,q}(x,x^{\\prime})=\\exp(-\\|x-x^{\\prime}\\|_{p}^{q}/L^{q})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>K</mi><mrow><mi>p</mi><mo>,</mo><mi>q</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mi>exp</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mo>&#8722;</mo><mrow><msubsup><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>x</mi><mo>&#8722;</mo><msup><mi>x</mi><mo>&#8242;</mo></msup></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mi>p</mi><mi>q</mi></msubsup><mo>/</mo><msup><mi>L</mi><mi>q</mi></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">K_{p,q}(x,x^{\\prime})=\\exp(-\\|x-x^{\\prime}\\|_{p}^{q}/L^{q})</annotation></semantics></math> (indicated by the kernel type hyperparameter) for the aggregation model, which has been shown to improve the performance of RFM on tabular datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">2025a</a>)</cite>. We also tune over whether to center the gradients in each iteration of RFM, which can help de-noise the gradients in high-dimensional settings <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. Gradient centering modifies the AGOP computation to give the following centered M matrix in the RFM iteration, where <math alttext=\"\\bar{g}=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p3.m2\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>g</mi><mo>&#175;</mo></mover><mo>=</mo><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>g</mi><mi>i</mi></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\bar{g}=\\frac{1}{n}\\sum_{i=1}^{n}g_{i}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "hyperparameter",
                    "gradients",
                    "aggregation",
                    "kernel",
                    "center",
                    "model",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generation, we ablate two steering knobs that most strongly impact generation quality and concept alignment: (i) the <em class=\"ltx_emph ltx_font_italic\">effective number of layers</em> contributing control via both a flat top-<span class=\"ltx_text ltx_font_italic\">k</span> value and an exponential, score-weighted layer scheme (&#8220;layer pruning&#8221;), and (ii) a <em class=\"ltx_emph ltx_font_italic\">per-timestep injection probability</em> <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> that sparsifies when control is applied.</p>\n\n",
                "matched_terms": [
                    "when",
                    "layers"
                ]
            }
        ]
    },
    "A3.T8": {
        "source_file": "Steering Autoregressive Music Generation with Recursive Feature Machines",
        "caption": "Table 8: Layer weighting ablation (continuous schemes). Exponential decay κ\\kappa interpolates between flat (κ→1\\kappa\\!\\to\\!1) and highly concentrated (κ→0\\kappa\\!\\to\\!0). Linear maps the best layer to w0w_{0} and the worst to 0.",
        "body": "Scheme\nHyperparams\n\nFD ↓\\downarrow\n\n\nMMD ↓\\downarrow\n\n\nCLAP ↑\\uparrow\n\n\nClassification Acc. ↑\\uparrow\n\n\n\nLinear\nwℓ=w0​s^ℓw_{\\ell}=w_{0}\\hat{s}_{\\ell}\n0.482\n2.701\n0.166\n0.959\n\n\nExponential\nκ=0.98\\kappa=0.98\n0.487\n2.710\n0.186\n0.954\n\n\nExponential (ours)\nκ=0.95\\kappa=0.95\n0.465\n2.575\n0.194\n0.961\n\n\nExponential\nκ=0.92\\kappa=0.92\n0.483\n2.687\n0.175\n0.954\n\n\nUniform (naive)\n–\n0.599\n3.44\n0.155\n0.964",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Scheme</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Hyperparams</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">FD</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">MMD</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">CLAP</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">Classification Acc.</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Linear</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"w_{\\ell}=w_{0}\\hat{s}_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m5\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>=</mo><mrow><msub><mi>w</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">w_{\\ell}=w_{0}\\hat{s}_{\\ell}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.482</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.701</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.166</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.959</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Exponential</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\kappa=0.98\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m6\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.98</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.98</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">0.487</td>\n<td class=\"ltx_td ltx_align_center\">2.710</td>\n<td class=\"ltx_td ltx_align_center\">0.186</td>\n<td class=\"ltx_td ltx_align_center\">0.954</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Exponential (ours)</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m7\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">0.465</td>\n<td class=\"ltx_td ltx_align_center\">2.575</td>\n<td class=\"ltx_td ltx_align_center\">0.194</td>\n<td class=\"ltx_td ltx_align_center\">0.961</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Exponential</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\kappa=0.92\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m8\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.92</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.92</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">0.483</td>\n<td class=\"ltx_td ltx_align_center\">2.687</td>\n<td class=\"ltx_td ltx_align_center\">0.175</td>\n<td class=\"ltx_td ltx_align_center\">0.954</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Uniform (naive)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.599</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.155</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.964</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "κ→1kappato1",
            "schemes",
            "κ→0kappato0",
            "exponential",
            "highly",
            "decay",
            "flat",
            "classification",
            "↑uparrow",
            "scheme",
            "κ098kappa098",
            "concentrated",
            "uniform",
            "κkappa",
            "best",
            "wℓw0​sℓwellw0hatsell",
            "continuous",
            "linear",
            "weighting",
            "layer",
            "interpolates",
            "ablation",
            "↓downarrow",
            "acc",
            "κ092kappa092",
            "w0w0",
            "hyperparams",
            "ours",
            "clap",
            "mmd",
            "naive",
            "κ095kappa095",
            "maps",
            "between",
            "worst"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We study three strategies that control how many (and how strongly) layers contribute to steering: (i) <em class=\"ltx_emph ltx_font_italic\">exponential</em> score-weighted steering, (ii) a simple <em class=\"ltx_emph ltx_font_italic\">linear</em> score-weighted scheme, and (iii) hard <em class=\"ltx_emph ltx_font_italic\">top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></em> selection. We show results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T9\" title=\"Table 9 &#8227; Discrete selection (Top-&#119870;). &#8227; C.2 Ablating Layer Pruning &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T8\" title=\"Table 8 &#8227; Discrete selection (Top-&#119870;). &#8227; C.2 Ablating Layer Pruning &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce MusicRFM, a framework that adapts RFMs to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-Large model directly in its activation space. Our approach is twofold: first, we train extremely lightweight, layer-wise RFM probes on the <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> dataset &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> to extract these potent, concept-aligned directions. Then, at inference time, we inject them into the model&#8217;s residual stream via forward hooks, enabling real-time, fine-grained control over the generated output. To ensure that audio quality and fidelity is not sacrificed for steering controllability, we introduce layer-based methods that apply steering selectively across <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s 48 decoder blocks, using top-K selection or an exponential weighting scheme based on each layer&#8217;s probe performance. For dynamic control, we implement time-based schedules that modulate steering strength throughout the generation with functions like linear fades, sinusoidal patterns, and sparse, stochastic application. MusicRFM further supports multi-direction steering, allowing for simultaneous or staggered enforcement of multiple attributes, such as jointly controlling notes and tempos. This comprehensive approach to control proves highly effective: our primary analysis shows that steering can increase the classification accuracy of a target note from 0.23 to over 0.82, while CLAP score for text alignment remains within <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.02 of the unsteered baseline.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "exponential",
                    "highly",
                    "classification",
                    "linear",
                    "weighting",
                    "scheme"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recursive Feature Machines (RFMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> were introduced as probing methods that iteratively recondition features via AGOP matrices to uncover task-sensitive subspaces. More recently, RFM-derived directions have been re-injected into activations for <em class=\"ltx_emph ltx_font_italic\">steering</em> in LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. We extend this paradigm to autoregressive music generation with three innovations: (i) <em class=\"ltx_emph ltx_font_italic\">layer-based control</em> through top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> and exponential weighting across 48 layers, (ii) <em class=\"ltx_emph ltx_font_italic\">time-based control</em> using dynamic schedules, and (iii) <em class=\"ltx_emph ltx_font_italic\">multi-direction control</em> via simultaneous or staggered application of concept directions.</p>\n\n",
                "matched_terms": [
                    "weighting",
                    "exponential"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each concept <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> and layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we train RFM probes for 15 iterations (fit predictor, compute AGOP, apply PSD map), keeping the probe with best validation metric (AUC for classification, MSE for regression). Binary concepts use <math alttext=\"\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,1\\}</annotation></semantics></math> labels and regression targets are z-normalized. The resulting eigendirections <math alttext=\"q_{\\ell,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">q_{\\ell,j}</annotation></semantics></math> form interpretable axes used for steering at inference. Steering is performed by the same process described in Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.E3\" title=\"In 3.1 Background on Recursive Feature Machines &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. For classification tasks, we additionally train multiclass RFMs that simply replace binary labels with one-hot-encoded target vectors, predicting through softmaxing final outputs.</p>\n\n",
                "matched_terms": [
                    "best",
                    "layer",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As we extend the existing <em class=\"ltx_emph ltx_font_italic\">text</em>-steering framework of RFMs provided by <cite class=\"ltx_cite ltx_citemacro_cite\">Beaglehole et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite> to audio domain music, we introduce additional modifications to help reduce out-of-distribution behavior and improve control. In particular, given the difference between the discrete, variable-sampling rate nature of text and the continuous, fixed-sampling rate nature of audio-domain music, we found that many of the algorithmic choices made by <cite class=\"ltx_cite ltx_citemacro_citet\">Beaglehole et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite> were ill-suited for TTM generation. All modifications are <span class=\"ltx_text ltx_font_bold\">only applied during inference time</span>.</p>\n\n",
                "matched_terms": [
                    "between",
                    "continuous"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exponential weighting.</span> Instead of hard pruning, we also apply continuous weighting across layers. For each layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we normalize its probe score <math alttext=\"s_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">s_{\\ell}</annotation></semantics></math> into <math alttext=\"\\hat{s}_{\\ell}\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{s}_{\\ell}\\in[0,1]</annotation></semantics></math>, and define <math alttext=\"w_{\\ell}=w_{0}\\cdot\\hat{s}_{\\ell}^{1/\\kappa}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>=</mo><mrow><msub><mi>w</mi><mn>0</mn></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msubsup><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>1</mn><mo>/</mo><mi>&#954;</mi></mrow></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">w_{\\ell}=w_{0}\\cdot\\hat{s}_{\\ell}^{1/\\kappa}</annotation></semantics></math> with <math alttext=\"\\kappa\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m5\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa\\in(0,1)</annotation></semantics></math>. This concentrates steering strength on high-performing layers, reducing unwanted artifacts and incorrect directions produced by the lower-scoring ones.</p>\n\n",
                "matched_terms": [
                    "continuous",
                    "weighting",
                    "layer",
                    "exponential"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first quantify audio similarity and distributional shift of generations steered along a <em class=\"ltx_emph ltx_font_italic\">single</em> concept direction using three standard metrics as a function of the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: (i) <span class=\"ltx_text ltx_font_bold\">Fr&#233;chet Distance (FD)</span> (lower is better), and (ii) <span class=\"ltx_text ltx_font_bold\">Maximum Mean Discrepancy (MMD)</span> (lower is better), (iii) <span class=\"ltx_text ltx_font_bold\">CLAP</span> alignment (higher is better). For the <span class=\"ltx_text ltx_font_bold\">tempos</span> category, the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> is averaged among the absolute value of the coefficient (e.g. the results from -0.15 and 0.15 are averaged into the 0.15 column). We show these results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. All results are reported on generations steered with RFM probes using stochastic application <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math> with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>; these are settings we found to be most optimal when creating high-quality, conceptually accurate generations.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "clap",
                    "mmd",
                    "exponential",
                    "κ095kappa095",
                    "weighting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The trends observed are as expected. Distributional metrics (FD and MMD) are consistently lower at smaller control coefficients, since weak steering leaves generations closer to the reference distribution. As <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, stronger injections deviate more from ground truth and raise FD/MMD. By contrast, CLAP alignment&#8212;measuring similarity to the conditioning text prompt&#8212;remains essentially flat across control strengths, indicating that textual conditioning is preserved regardless of steering intensity, only with slight degradation in some categories as control coefficient increases. Thus, moderate values of <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> can balance concept control with distributional fidelity while maintaining prompt adherence. We provide additional visual graphs for the reader in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A4\" title=\"Appendix D Single Direction Metric Graphs &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd",
                    "flat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide results of a listening test, where we asked 12 participants to score 3 different audio samples for 4 control types, where they judge based on audio quality and adherence of the audio to the specified control. The 3 clips were randomly chosen base model generations (without control), na&#239;ve RFM generations, and optimal RFM generations (steering with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>). We show mean and STD of each type of steering in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T3\" title=\"Table 3 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Overall, the results indicate that both na&#239;ve and MusicRFM steering substantially improve perceived control compared to the base model, with MusicRFM consistently achieving the highest ratings across all attributes. In particular, chord and interval control benefit most from our optimizations, while tempo control shows the largest relative gain over the no-steering baseline.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "weighting",
                    "exponential",
                    "κ095kappa095"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test transfer beyond synthetic data, we evaluate RFM probes on <span class=\"ltx_text ltx_font_smallcaps\">MusicBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Melechovsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib12\" title=\"\">2024</a>)</cite>, a real-music corpus with ground-truth tempo, notes, and keys. Using the same pipeline as in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS2\" title=\"3.2 MusicRFM: RFM Steering for Music Generation &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we mean-pool <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large hidden states and fit layerwise RFMs (train/val/test split 70/15/15). For tempo we report normalized MSE, for classification overall accuracy. RFM probes reach <math alttext=\"75.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>75.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">75.3\\%</annotation></semantics></math> accuracy on notes and <math alttext=\"67.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>67.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">67.5\\%</annotation></semantics></math> on keys, while tempo regression proves difficult (MSE <math alttext=\"0.862\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><mn>0.862</mn><annotation encoding=\"application/x-tex\">0.862</annotation></semantics></math>). Steering experiments (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Evaluation on MusicBench (Real Music) &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mirror <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>: higher <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases FD/MMD and reduces CLAP, showing that moderate control preserves text adherence but aggressive coefficients destabilize generations. Overall, MusicBench confirms that real-music attributes can be steered, though sensitivity varies by concept difficulty.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe several trends:\n(i) <span class=\"ltx_text ltx_font_bold\">Probe accuracy still rises with stronger coefficients.</span> For notes in particular, accuracy increases from 0.770 at <math alttext=\"\\eta_{0}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.3</annotation></semantics></math> to 0.920 at <math alttext=\"\\eta_{0}=0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.6</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.6</annotation></semantics></math>, indicating that control strength directly improves enforcement even in multi-direction cases.\n(ii) <span class=\"ltx_text ltx_font_bold\">Distributional metrics and CLAP scores degrade at higher strengths.</span> Both FD and MMD grow substantially as <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, consistent with the single-direction case, where aggressive steering pushes samples away from the reference distribution. CLAP alignment also degrades significantly. (iii) <span class=\"ltx_text ltx_font_bold\">Accuracy in multi-direction steering exceeds single-direction.</span> We actually observe higher probe accuracy in the multi-direction setting, which we hypothesize arises because stronger aggregate constraints reduce adherence to the text prompt (lower CLAP) and, in turn, compress the generative manifold. This yields less stylistic variance in the music, making classes easier for probes to detect.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use per-direction coefficients <math alttext=\"\\eta_{0,m}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\eta_{0,m}</annotation></semantics></math> and schedules <math alttext=\"\\phi_{m}(t)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi_{m}(t)\\in[0,1]</annotation></semantics></math>, so <math alttext=\"\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)</annotation></semantics></math>. The schedules we ablate are exponential decay, linear decay &amp; increase, logistic increase, and sine wave. We put formulas used in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A5\" title=\"Appendix E Control schedules used for time control ablations on note classification &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, and record FD, MMD, and CLAP scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.T6\" title=\"Table 6 &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd",
                    "exponential",
                    "decay",
                    "linear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each schedule we plot the probe softmax of the correct note over time in Figure &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.F1.sf1\" title=\"In Figure 1 &#8227; Crossfading Between Concepts. &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>. We see that the distribution over time follows exactly what we would expect from each of the smooth scheduling functions - exponential &amp; linear decay look like decay functions, sine is very similar to a sine wave, and logistic &amp; linear increase show an increase in predicted probability.</p>\n\n",
                "matched_terms": [
                    "linear",
                    "exponential",
                    "decay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We additionally study a controlled <em class=\"ltx_emph ltx_font_italic\">cross-fade</em> between two notes <math alttext=\"n_{1}\\!\\to\\!n_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>n</mi><mn>1</mn></msub><mo lspace=\"0.108em\" rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo><msub><mi>n</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">n_{1}\\!\\to\\!n_{2}</annotation></semantics></math> using complementary schedules over a fixed window of 0&#8211;1500 steps: for <math alttext=\"n_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">n_{1}</annotation></semantics></math> we decay from <math alttext=\"\\eta_{0}=0.45\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.45</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.45</annotation></semantics></math> to <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><mn>0</mn></math>, and for <math alttext=\"n_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">n_{2}</annotation></semantics></math> we rise from <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m6\" intent=\":literal\"><mn>0</mn></math> to <math alttext=\"0.45\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><mn>0.45</mn><annotation encoding=\"application/x-tex\">0.45</annotation></semantics></math>. Formally,</p>\n\n",
                "matched_terms": [
                    "between",
                    "decay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then log and display the RFM-probe softmax scores for both <math alttext=\"n_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">n_{1}</annotation></semantics></math> and <math alttext=\"n_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">n_{2}</annotation></semantics></math> at each timestep in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.F1.sf2\" title=\"In Figure 1 &#8227; Crossfading Between Concepts. &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>. As expected, the first note falls in probability while the second note rises. On average over 500 randomly sampled note pairs, crossfaded generations achieve FD of <math alttext=\"0.350\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mn>0.350</mn><annotation encoding=\"application/x-tex\">0.350</annotation></semantics></math>, MMD of <math alttext=\"1.922\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mn>1.922</mn><annotation encoding=\"application/x-tex\">1.922</annotation></semantics></math>, and CLAP alignment of <math alttext=\"0.250\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mn>0.250</mn><annotation encoding=\"application/x-tex\">0.250</annotation></semantics></math>, indicating modest distributional drift but stable prompt adherence.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across synthetic and real-music settings, we observed consistent trade-offs governed by the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S8.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: moderate steering improves alignment to targeted concepts with limited distributional drift (FD/MMD) and minimal degradation in prompt adherence (CLAP), while aggressive steering yields stronger control at the cost of artifacts. Notes are the most reliably controllable, multi-direction steering is feasible but amplifies drift, and simple schedules (e.g., decay/rise) support intuitive manipulations like crossfades. Time-based control is accurate and true-to-schedule in terms of evaluating on softmax probability of classes. Layer pruning and stochastic (Bernoulli) application help stabilize generations by limiting cumulative bias.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generation, we ablate two steering knobs that most strongly impact generation quality and concept alignment: (i) the <em class=\"ltx_emph ltx_font_italic\">effective number of layers</em> contributing control via both a flat top-<span class=\"ltx_text ltx_font_italic\">k</span> value and an exponential, score-weighted layer scheme (&#8220;layer pruning&#8221;), and (ii) a <em class=\"ltx_emph ltx_font_italic\">per-timestep injection probability</em> <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> that sparsifies when control is applied.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "exponential",
                    "scheme",
                    "flat"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where for ablations we set <math alttext=\"\\phi(t)\\equiv 1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8801;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\phi(t)\\equiv 1</annotation></semantics></math> and vary layer weighting and <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "weighting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\kappa\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa\\in(0,1)</annotation></semantics></math> is a <em class=\"ltx_emph ltx_font_italic\">decay rate</em> (smaller <math alttext=\"\\kappa\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>&#954;</mi><annotation encoding=\"application/x-tex\">\\kappa</annotation></semantics></math> increases contrast, concentrating weight on high-scoring layers). Linear is the minimal &#8220;from 1 to 0&#8221; mapping; exponential recovers linear as <math alttext=\"\\kappa\\!\\to\\!1\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\" stretchy=\"false\">&#8594;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa\\!\\to\\!1</annotation></semantics></math> and becomes more selective as <math alttext=\"\\kappa\\!\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo lspace=\"0.108em\" stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">\\kappa\\!\\downarrow</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "κ→1kappato1",
                    "exponential",
                    "κkappa",
                    "decay",
                    "linear"
                ]
            }
        ]
    },
    "A3.T9": {
        "source_file": "Steering Autoregressive Music Generation with Recursive Feature Machines",
        "caption": "Table 9: Layer selection ablation (top-KK hard pruning). KK controls the effective number of controlled layers.",
        "body": "Top-KK\n\nFD ↓\\downarrow\n\n\nMMD ↓\\downarrow\n\n\nCLAP ↑\\uparrow\n\n\nClassification Acc. ↑\\uparrow\n\n\n\nK=4K=4\n0.109\n0.192\n0.309\n0.398\n\n\nK=8K=8\n0.157\n0.448\n0.291\n0.678\n\n\nK=12K=12\n0.225\n0.919\n0.263\n0.882\n\n\nK=16K=16\n0.347\n1.781\n0.225\n0.941\n\n\nK=24K=24\n0.555\n3.218\n0.158\n0.967\n\n\nK=32K=32\n0.586\n3.395\n0.158\n0.958\n\n\n\nK=48K=48 (naive)\n0.599\n3.44\n0.155\n0.964",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">FD</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">MMD</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">CLAP</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">Classification Acc.</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"K=4\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m6\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">K=4</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.109</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.192</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.309</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.398</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"K=8\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m7\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">K=8</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">0.157</td>\n<td class=\"ltx_td ltx_align_center\">0.448</td>\n<td class=\"ltx_td ltx_align_center\">0.291</td>\n<td class=\"ltx_td ltx_align_center\">0.678</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"K=12\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m8\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">K=12</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">0.225</td>\n<td class=\"ltx_td ltx_align_center\">0.919</td>\n<td class=\"ltx_td ltx_align_center\">0.263</td>\n<td class=\"ltx_td ltx_align_center\">0.882</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"K=16\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m9\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">K=16</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">0.347</td>\n<td class=\"ltx_td ltx_align_center\">1.781</td>\n<td class=\"ltx_td ltx_align_center\">0.225</td>\n<td class=\"ltx_td ltx_align_center\">0.941</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"K=24\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m10\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>24</mn></mrow><annotation encoding=\"application/x-tex\">K=24</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">0.555</td>\n<td class=\"ltx_td ltx_align_center\">3.218</td>\n<td class=\"ltx_td ltx_align_center\">0.158</td>\n<td class=\"ltx_td ltx_align_center\">0.967</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"K=32\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m11\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">K=32</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">0.586</td>\n<td class=\"ltx_td ltx_align_center\">3.395</td>\n<td class=\"ltx_td ltx_align_center\">0.158</td>\n<td class=\"ltx_td ltx_align_center\">0.958</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<math alttext=\"K=48\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T9.m12\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">K=48</annotation></semantics></math> (naive)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.599</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.155</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.964</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "k4k4",
            "hard",
            "classification",
            "↑uparrow",
            "k16k16",
            "selection",
            "pruning",
            "layer",
            "k8k8",
            "ablation",
            "↓downarrow",
            "number",
            "k24k24",
            "k32k32",
            "acc",
            "k12k12",
            "effective",
            "controlled",
            "clap",
            "mmd",
            "naive",
            "k48k48",
            "topkk",
            "layers",
            "controls"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We study three strategies that control how many (and how strongly) layers contribute to steering: (i) <em class=\"ltx_emph ltx_font_italic\">exponential</em> score-weighted steering, (ii) a simple <em class=\"ltx_emph ltx_font_italic\">linear</em> score-weighted scheme, and (iii) hard <em class=\"ltx_emph ltx_font_italic\">top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></em> selection. We show results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T9\" title=\"Table 9 &#8227; Discrete selection (Top-&#119870;). &#8227; C.2 Ablating Layer Pruning &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T8\" title=\"Table 8 &#8227; Discrete selection (Top-&#119870;). &#8227; C.2 Ablating Layer Pruning &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce MusicRFM, a framework that adapts RFMs to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-Large model directly in its activation space. Our approach is twofold: first, we train extremely lightweight, layer-wise RFM probes on the <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> dataset &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> to extract these potent, concept-aligned directions. Then, at inference time, we inject them into the model&#8217;s residual stream via forward hooks, enabling real-time, fine-grained control over the generated output. To ensure that audio quality and fidelity is not sacrificed for steering controllability, we introduce layer-based methods that apply steering selectively across <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s 48 decoder blocks, using top-K selection or an exponential weighting scheme based on each layer&#8217;s probe performance. For dynamic control, we implement time-based schedules that modulate steering strength throughout the generation with functions like linear fades, sinusoidal patterns, and sparse, stochastic application. MusicRFM further supports multi-direction steering, allowing for simultaneous or staggered enforcement of multiple attributes, such as jointly controlling notes and tempos. This comprehensive approach to control proves highly effective: our primary analysis shows that steering can increase the classification accuracy of a target note from 0.23 to over 0.82, while CLAP score for text alignment remains within <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.02 of the unsteered baseline.</p>\n\n",
                "matched_terms": [
                    "effective",
                    "selection",
                    "classification",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We focus particularly on text-to-music (TTM) generation that relies on neural audio codecs and autoregressive sequence models in architectures like <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>, <span class=\"ltx_text ltx_font_smallcaps\">MusicLM</span>, and <span class=\"ltx_text ltx_font_smallcaps\">Jukebox</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib4\" title=\"\">2024</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib6\" title=\"\">2022</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib1\" title=\"\">2023</a>; Dhariwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib5\" title=\"\">2020</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib23\" title=\"\">2025</a>; Team et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib19\" title=\"\">2025</a>)</cite>. Additionally, a number of controllable TTM systems exist in the parallel diffusion domain <cite class=\"ltx_cite ltx_citemacro_citep\">(Novack et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib16\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib15\" title=\"\">a</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib22\" title=\"\">2024</a>; Nistal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib13\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib14\" title=\"\">b</a>)</cite>. Most existing controllable methods for AR focus on multi-modal controls (e.g.&#160;video) <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib8\" title=\"\">2025</a>)</cite> or common musical controls like piano rolls <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib11\" title=\"\">2024</a>)</cite>. These approaches, while generally performant, still require reasonably compute-heavy finetuning runs and thus necessitate changing the base model, potentially breaking its core generative capabilities if the finetuning data is ill-chosen.</p>\n\n",
                "matched_terms": [
                    "number",
                    "controls"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recursive Feature Machines (RFMs) <cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> were introduced as probing methods that iteratively recondition features via AGOP matrices to uncover task-sensitive subspaces. More recently, RFM-derived directions have been re-injected into activations for <em class=\"ltx_emph ltx_font_italic\">steering</em> in LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib3\" title=\"\">2025b</a>)</cite>. We extend this paradigm to autoregressive music generation with three innovations: (i) <em class=\"ltx_emph ltx_font_italic\">layer-based control</em> through top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> and exponential weighting across 48 layers, (ii) <em class=\"ltx_emph ltx_font_italic\">time-based control</em> using dynamic schedules, and (iii) <em class=\"ltx_emph ltx_font_italic\">multi-direction control</em> via simultaneous or staggered application of concept directions.</p>\n\n",
                "matched_terms": [
                    "topkk",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each concept <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> and layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we train RFM probes for 15 iterations (fit predictor, compute AGOP, apply PSD map), keeping the probe with best validation metric (AUC for classification, MSE for regression). Binary concepts use <math alttext=\"\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0,1\\}</annotation></semantics></math> labels and regression targets are z-normalized. The resulting eigendirections <math alttext=\"q_{\\ell,j}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msub><mi>q</mi><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>j</mi></mrow></msub><annotation encoding=\"application/x-tex\">q_{\\ell,j}</annotation></semantics></math> form interpretable axes used for steering at inference. Steering is performed by the same process described in Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.E3\" title=\"In 3.1 Background on Recursive Feature Machines &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. For classification tasks, we additionally train multiclass RFMs that simply replace binary labels with one-hot-encoded target vectors, predicting through softmaxing final outputs.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Na&#239;ve steering, where we inject RFM directions uniformly across all <math alttext=\"L{=}48\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">L{=}48</annotation></semantics></math> layers at every step as is done in the original RFM paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Beaglehole et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib2\" title=\"\">2025a</a>)</cite>, leads to noticeable degradation in audio quality and weaker alignment to text prompts. To address this, we introduce <em class=\"ltx_emph ltx_font_italic\">layer pruning</em> strategies at inference time that prioritize informative layers and downweight noisy ones, thereby improving both perceptual fidelity and controllability (see App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3\" title=\"Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for full results).</p>\n\n",
                "matched_terms": [
                    "layer",
                    "pruning",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> selection.</span> We rank each layer <math alttext=\"\\ell\\in\\{1,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\ell\\in\\{1,\\dots,L\\}</annotation></semantics></math> by its validation probe performance <math alttext=\"\\mathrm{AUC}_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>AUC</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">\\mathrm{AUC}_{\\ell}</annotation></semantics></math>, then restrict steering to the top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> layers.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "selection",
                    "topkk",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exponential weighting.</span> Instead of hard pruning, we also apply continuous weighting across layers. For each layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we normalize its probe score <math alttext=\"s_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">s_{\\ell}</annotation></semantics></math> into <math alttext=\"\\hat{s}_{\\ell}\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{s}_{\\ell}\\in[0,1]</annotation></semantics></math>, and define <math alttext=\"w_{\\ell}=w_{0}\\cdot\\hat{s}_{\\ell}^{1/\\kappa}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>=</mo><mrow><msub><mi>w</mi><mn>0</mn></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msubsup><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>1</mn><mo>/</mo><mi>&#954;</mi></mrow></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">w_{\\ell}=w_{0}\\cdot\\hat{s}_{\\ell}^{1/\\kappa}</annotation></semantics></math> with <math alttext=\"\\kappa\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m5\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa\\in(0,1)</annotation></semantics></math>. This concentrates steering strength on high-performing layers, reducing unwanted artifacts and incorrect directions produced by the lower-scoring ones.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "hard",
                    "pruning",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stochastic application <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math>.</span> At each step, apply control with probability <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p3.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> (Bernoulli gating). Similarly to layer pruning, this method reduces over-steering and cumulative artifacts while preserving the expected bias toward the target. Ablations are in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3\" title=\"Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first quantify audio similarity and distributional shift of generations steered along a <em class=\"ltx_emph ltx_font_italic\">single</em> concept direction using three standard metrics as a function of the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: (i) <span class=\"ltx_text ltx_font_bold\">Fr&#233;chet Distance (FD)</span> (lower is better), and (ii) <span class=\"ltx_text ltx_font_bold\">Maximum Mean Discrepancy (MMD)</span> (lower is better), (iii) <span class=\"ltx_text ltx_font_bold\">CLAP</span> alignment (higher is better). For the <span class=\"ltx_text ltx_font_bold\">tempos</span> category, the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> is averaged among the absolute value of the coefficient (e.g. the results from -0.15 and 0.15 are averaged into the 0.15 column). We show these results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. All results are reported on generations steered with RFM probes using stochastic application <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math> with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>; these are settings we found to be most optimal when creating high-quality, conceptually accurate generations.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "clap",
                    "mmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The trends observed are as expected. Distributional metrics (FD and MMD) are consistently lower at smaller control coefficients, since weak steering leaves generations closer to the reference distribution. As <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, stronger injections deviate more from ground truth and raise FD/MMD. By contrast, CLAP alignment&#8212;measuring similarity to the conditioning text prompt&#8212;remains essentially flat across control strengths, indicating that textual conditioning is preserved regardless of steering intensity, only with slight degradation in some categories as control coefficient increases. Thus, moderate values of <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> can balance concept control with distributional fidelity while maintaining prompt adherence. We provide additional visual graphs for the reader in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A4\" title=\"Appendix D Single Direction Metric Graphs &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe that classification accuracy is generally low for all attributes except <span class=\"ltx_text ltx_font_bold\">notes</span>, where probe accuracy climbs sharply from 0.23 at <math alttext=\"\\eta_{0}{=}0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.15</annotation></semantics></math> to 0.82 at <math alttext=\"\\eta_{0}{=}0.60\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.60</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.60</annotation></semantics></math>, reflecting the inherent difficulty of enforcing more abstract or temporal musical properties. Nevertheless, the key observation is that accuracy <em class=\"ltx_emph ltx_font_italic\">monotonically increases with the control coefficient</em> in every category. For example, chords rise from 0.27 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.34, intervals from 0.12 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.22, and time signatures from 0.17 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.25. Even when overall values remain low, the consistent positive slope indicates that higher steering strength pushes generations in the expected direction of the controlled attribute.</p>\n\n",
                "matched_terms": [
                    "controlled",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test transfer beyond synthetic data, we evaluate RFM probes on <span class=\"ltx_text ltx_font_smallcaps\">MusicBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Melechovsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib12\" title=\"\">2024</a>)</cite>, a real-music corpus with ground-truth tempo, notes, and keys. Using the same pipeline as in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS2\" title=\"3.2 MusicRFM: RFM Steering for Music Generation &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we mean-pool <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large hidden states and fit layerwise RFMs (train/val/test split 70/15/15). For tempo we report normalized MSE, for classification overall accuracy. RFM probes reach <math alttext=\"75.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>75.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">75.3\\%</annotation></semantics></math> accuracy on notes and <math alttext=\"67.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>67.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">67.5\\%</annotation></semantics></math> on keys, while tempo regression proves difficult (MSE <math alttext=\"0.862\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><mn>0.862</mn><annotation encoding=\"application/x-tex\">0.862</annotation></semantics></math>). Steering experiments (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Evaluation on MusicBench (Real Music) &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mirror <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>: higher <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases FD/MMD and reduces CLAP, showing that moderate control preserves text adherence but aggressive coefficients destabilize generations. Overall, MusicBench confirms that real-music attributes can be steered, though sensitivity varies by concept difficulty.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe several trends:\n(i) <span class=\"ltx_text ltx_font_bold\">Probe accuracy still rises with stronger coefficients.</span> For notes in particular, accuracy increases from 0.770 at <math alttext=\"\\eta_{0}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.3</annotation></semantics></math> to 0.920 at <math alttext=\"\\eta_{0}=0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.6</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.6</annotation></semantics></math>, indicating that control strength directly improves enforcement even in multi-direction cases.\n(ii) <span class=\"ltx_text ltx_font_bold\">Distributional metrics and CLAP scores degrade at higher strengths.</span> Both FD and MMD grow substantially as <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, consistent with the single-direction case, where aggressive steering pushes samples away from the reference distribution. CLAP alignment also degrades significantly. (iii) <span class=\"ltx_text ltx_font_bold\">Accuracy in multi-direction steering exceeds single-direction.</span> We actually observe higher probe accuracy in the multi-direction setting, which we hypothesize arises because stronger aggregate constraints reduce adherence to the text prompt (lower CLAP) and, in turn, compress the generative manifold. This yields less stylistic variance in the music, making classes easier for probes to detect.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use per-direction coefficients <math alttext=\"\\eta_{0,m}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\eta_{0,m}</annotation></semantics></math> and schedules <math alttext=\"\\phi_{m}(t)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi_{m}(t)\\in[0,1]</annotation></semantics></math>, so <math alttext=\"\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)</annotation></semantics></math>. The schedules we ablate are exponential decay, linear decay &amp; increase, logistic increase, and sine wave. We put formulas used in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A5\" title=\"Appendix E Control schedules used for time control ablations on note classification &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, and record FD, MMD, and CLAP scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.T6\" title=\"Table 6 &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then log and display the RFM-probe softmax scores for both <math alttext=\"n_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">n_{1}</annotation></semantics></math> and <math alttext=\"n_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">n_{2}</annotation></semantics></math> at each timestep in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.F1.sf2\" title=\"In Figure 1 &#8227; Crossfading Between Concepts. &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>. As expected, the first note falls in probability while the second note rises. On average over 500 randomly sampled note pairs, crossfaded generations achieve FD of <math alttext=\"0.350\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mn>0.350</mn><annotation encoding=\"application/x-tex\">0.350</annotation></semantics></math>, MMD of <math alttext=\"1.922\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mn>1.922</mn><annotation encoding=\"application/x-tex\">1.922</annotation></semantics></math>, and CLAP alignment of <math alttext=\"0.250\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mn>0.250</mn><annotation encoding=\"application/x-tex\">0.250</annotation></semantics></math>, indicating modest distributional drift but stable prompt adherence.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across synthetic and real-music settings, we observed consistent trade-offs governed by the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S8.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: moderate steering improves alignment to targeted concepts with limited distributional drift (FD/MMD) and minimal degradation in prompt adherence (CLAP), while aggressive steering yields stronger control at the cost of artifacts. Notes are the most reliably controllable, multi-direction steering is feasible but amplifies drift, and simple schedules (e.g., decay/rise) support intuitive manipulations like crossfades. Time-based control is accurate and true-to-schedule in terms of evaluating on softmax probability of classes. Layer pruning and stochastic (Bernoulli) application help stabilize generations by limiting cumulative bias.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "clap",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generation, we ablate two steering knobs that most strongly impact generation quality and concept alignment: (i) the <em class=\"ltx_emph ltx_font_italic\">effective number of layers</em> contributing control via both a flat top-<span class=\"ltx_text ltx_font_italic\">k</span> value and an exponential, score-weighted layer scheme (&#8220;layer pruning&#8221;), and (ii) a <em class=\"ltx_emph ltx_font_italic\">per-timestep injection probability</em> <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> that sparsifies when control is applied.</p>\n\n",
                "matched_terms": [
                    "number",
                    "layer",
                    "effective",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also ablate a hard selection mask <math alttext=\"m_{\\ell}^{(K)}\\in\\{0,1\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>m</mi><mi mathvariant=\"normal\">&#8467;</mi><mrow><mo stretchy=\"false\">(</mo><mi>K</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{\\ell}^{(K)}\\in\\{0,1\\}</annotation></semantics></math> over the top-<math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> layers by <math alttext=\"\\hat{s}_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">\\hat{s}_{\\ell}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "hard",
                    "selection",
                    "layers",
                    "topkk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sweep <math alttext=\"K\\in\\{4,8,12,16,24,32,48\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>4</mn><mo>,</mo><mn>8</mn><mo>,</mo><mn>12</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>24</mn><mo>,</mo><mn>32</mn><mo>,</mo><mn>48</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">K\\in\\{4,8,12,16,24,32,48\\}</annotation></semantics></math>, with <math alttext=\"K{=}48\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">K{=}48</annotation></semantics></math> meaning all layers.</p>\n\n",
                "matched_terms": [
                    "k48k48",
                    "layers"
                ]
            }
        ]
    },
    "A3.T10": {
        "source_file": "Steering Autoregressive Music Generation with Recursive Feature Machines",
        "caption": "Table 10: Injection probability ablation. Lower pp reduces artifacts but may weaken alignment; higher pp increases control strength but risks over-steering.",
        "body": "pp\n\nFD ↓\\downarrow\n\n\nMMD ↓\\downarrow\n\n\nCLAP ↑\\uparrow\n\n\nClassification Acc. ↑\\uparrow\n\n\n\n0.15\n0.108\n0.163\n0.348\n0.348\n\n\n0.30 (ours)\n0.118\n0.272\n0.306\n0.697\n\n\n0.45\n0.197\n0.769\n0.287\n0.884\n\n\n0.6\n0.281\n1.343\n0.265\n0.931\n\n\n0.75\n0.399\n2.145\n0.207\n0.961\n\n\n0.9\n0.510\n2.853\n0.172\n0.962\n\n\n1.0 (naive)\n0.599\n3.44\n0.155\n0.964",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T10.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">FD</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T10.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">MMD</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T10.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">CLAP</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T10.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\">Classification Acc.</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T10.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.108</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.163</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.348</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.348</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">0.30 (ours)</td>\n<td class=\"ltx_td ltx_align_center\">0.118</td>\n<td class=\"ltx_td ltx_align_center\">0.272</td>\n<td class=\"ltx_td ltx_align_center\">0.306</td>\n<td class=\"ltx_td ltx_align_center\">0.697</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">0.45</td>\n<td class=\"ltx_td ltx_align_center\">0.197</td>\n<td class=\"ltx_td ltx_align_center\">0.769</td>\n<td class=\"ltx_td ltx_align_center\">0.287</td>\n<td class=\"ltx_td ltx_align_center\">0.884</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">0.6</td>\n<td class=\"ltx_td ltx_align_center\">0.281</td>\n<td class=\"ltx_td ltx_align_center\">1.343</td>\n<td class=\"ltx_td ltx_align_center\">0.265</td>\n<td class=\"ltx_td ltx_align_center\">0.931</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">0.75</td>\n<td class=\"ltx_td ltx_align_center\">0.399</td>\n<td class=\"ltx_td ltx_align_center\">2.145</td>\n<td class=\"ltx_td ltx_align_center\">0.207</td>\n<td class=\"ltx_td ltx_align_center\">0.961</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">0.9</td>\n<td class=\"ltx_td ltx_align_center\">0.510</td>\n<td class=\"ltx_td ltx_align_center\">2.853</td>\n<td class=\"ltx_td ltx_align_center\">0.172</td>\n<td class=\"ltx_td ltx_align_center\">0.962</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">1.0 (naive)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.599</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.155</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.964</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "control",
            "risks",
            "increases",
            "lower",
            "classification",
            "injection",
            "↑uparrow",
            "oversteering",
            "reduces",
            "ablation",
            "↓downarrow",
            "acc",
            "alignment",
            "higher",
            "ours",
            "clap",
            "mmd",
            "strength",
            "naive",
            "probability",
            "weaken",
            "artifacts"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">so control fires stochastically with probability <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS3.p1.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>. We show results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3.T10\" title=\"Table 10 &#8227; C.3 Ablating Injection Probability &#119901; &#8227; Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radhakrishnan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib18\" title=\"\">2023</a>)</cite> to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model&#8217;s internal gradients to produce interpretable &#8220;concept directions&#8221;, or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/astradzhao/music-rfm\" title=\"\">https://github.com/astradzhao/music-rfm</a></span></span></span> to encourage further exploration on RFMs in the music domain.</p>\n\n",
                "matched_terms": [
                    "artifacts",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we introduce MusicRFM, a framework that adapts RFMs to steer a frozen <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-Large model directly in its activation space. Our approach is twofold: first, we train extremely lightweight, layer-wise RFM probes on the <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span> dataset &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib21\" title=\"\">2024</a>)</cite> to extract these potent, concept-aligned directions. Then, at inference time, we inject them into the model&#8217;s residual stream via forward hooks, enabling real-time, fine-grained control over the generated output. To ensure that audio quality and fidelity is not sacrificed for steering controllability, we introduce layer-based methods that apply steering selectively across <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>&#8217;s 48 decoder blocks, using top-K selection or an exponential weighting scheme based on each layer&#8217;s probe performance. For dynamic control, we implement time-based schedules that modulate steering strength throughout the generation with functions like linear fades, sinusoidal patterns, and sparse, stochastic application. MusicRFM further supports multi-direction steering, allowing for simultaneous or staggered enforcement of multiple attributes, such as jointly controlling notes and tempos. This comprehensive approach to control proves highly effective: our primary analysis shows that steering can increase the classification accuracy of a target note from 0.23 to over 0.82, while CLAP score for text alignment remains within <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.02 of the unsteered baseline.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "strength",
                    "control",
                    "classification",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exponential weighting.</span> Instead of hard pruning, we also apply continuous weighting across layers. For each layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, we normalize its probe score <math alttext=\"s_{\\ell}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><annotation encoding=\"application/x-tex\">s_{\\ell}</annotation></semantics></math> into <math alttext=\"\\hat{s}_{\\ell}\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m3\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{s}_{\\ell}\\in[0,1]</annotation></semantics></math>, and define <math alttext=\"w_{\\ell}=w_{0}\\cdot\\hat{s}_{\\ell}^{1/\\kappa}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo>=</mo><mrow><msub><mi>w</mi><mn>0</mn></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msubsup><mover accent=\"true\"><mi>s</mi><mo>^</mo></mover><mi mathvariant=\"normal\">&#8467;</mi><mrow><mn>1</mn><mo>/</mo><mi>&#954;</mi></mrow></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">w_{\\ell}=w_{0}\\cdot\\hat{s}_{\\ell}^{1/\\kappa}</annotation></semantics></math> with <math alttext=\"\\kappa\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p3.m5\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa\\in(0,1)</annotation></semantics></math>. This concentrates steering strength on high-performing layers, reducing unwanted artifacts and incorrect directions produced by the lower-scoring ones.</p>\n\n",
                "matched_terms": [
                    "artifacts",
                    "strength"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stochastic application <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math>.</span> At each step, apply control with probability <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p3.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> (Bernoulli gating). Similarly to layer pruning, this method reduces over-steering and cumulative artifacts while preserving the expected bias toward the target. Ablations are in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A3\" title=\"Appendix C Steering Ablations &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "reduces",
                    "control",
                    "probability",
                    "oversteering",
                    "artifacts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first quantify audio similarity and distributional shift of generations steered along a <em class=\"ltx_emph ltx_font_italic\">single</em> concept direction using three standard metrics as a function of the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: (i) <span class=\"ltx_text ltx_font_bold\">Fr&#233;chet Distance (FD)</span> (lower is better), and (ii) <span class=\"ltx_text ltx_font_bold\">Maximum Mean Discrepancy (MMD)</span> (lower is better), (iii) <span class=\"ltx_text ltx_font_bold\">CLAP</span> alignment (higher is better). For the <span class=\"ltx_text ltx_font_bold\">tempos</span> category, the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> is averaged among the absolute value of the coefficient (e.g. the results from -0.15 and 0.15 are averaged into the 0.15 column). We show these results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T2\" title=\"Table 2 &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. All results are reported on generations steered with RFM probes using stochastic application <math alttext=\"\\psi_{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#968;</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\psi_{p}(t)</annotation></semantics></math> with <math alttext=\"p=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p=0.3</annotation></semantics></math> and exponential layer weighting with <math alttext=\"w_{0}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{0}=1</annotation></semantics></math> and <math alttext=\"\\kappa=0.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mn>0.95</mn></mrow><annotation encoding=\"application/x-tex\">\\kappa=0.95</annotation></semantics></math>; these are settings we found to be most optimal when creating high-quality, conceptually accurate generations.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd",
                    "control",
                    "lower",
                    "alignment",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The trends observed are as expected. Distributional metrics (FD and MMD) are consistently lower at smaller control coefficients, since weak steering leaves generations closer to the reference distribution. As <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, stronger injections deviate more from ground truth and raise FD/MMD. By contrast, CLAP alignment&#8212;measuring similarity to the conditioning text prompt&#8212;remains essentially flat across control strengths, indicating that textual conditioning is preserved regardless of steering intensity, only with slight degradation in some categories as control coefficient increases. Thus, moderate values of <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> can balance concept control with distributional fidelity while maintaining prompt adherence. We provide additional visual graphs for the reader in App.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A4\" title=\"Appendix D Single Direction Metric Graphs &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd",
                    "control",
                    "increases",
                    "lower"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe that classification accuracy is generally low for all attributes except <span class=\"ltx_text ltx_font_bold\">notes</span>, where probe accuracy climbs sharply from 0.23 at <math alttext=\"\\eta_{0}{=}0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.15</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.15</annotation></semantics></math> to 0.82 at <math alttext=\"\\eta_{0}{=}0.60\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.60</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}{=}0.60</annotation></semantics></math>, reflecting the inherent difficulty of enforcing more abstract or temporal musical properties. Nevertheless, the key observation is that accuracy <em class=\"ltx_emph ltx_font_italic\">monotonically increases with the control coefficient</em> in every category. For example, chords rise from 0.27 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.34, intervals from 0.12 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.22, and time signatures from 0.17 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 0.25. Even when overall values remain low, the consistent positive slope indicates that higher steering strength pushes generations in the expected direction of the controlled attribute.</p>\n\n",
                "matched_terms": [
                    "strength",
                    "control",
                    "increases",
                    "classification",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To test transfer beyond synthetic data, we evaluate RFM probes on <span class=\"ltx_text ltx_font_smallcaps\">MusicBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Melechovsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#bib.bib12\" title=\"\">2024</a>)</cite>, a real-music corpus with ground-truth tempo, notes, and keys. Using the same pipeline as in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S3.SS2\" title=\"3.2 MusicRFM: RFM Steering for Music Generation &#8227; 3 Methods &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we mean-pool <span class=\"ltx_text ltx_font_smallcaps\">MusicGen</span>-large hidden states and fit layerwise RFMs (train/val/test split 70/15/15). For tempo we report normalized MSE, for classification overall accuracy. RFM probes reach <math alttext=\"75.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>75.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">75.3\\%</annotation></semantics></math> accuracy on notes and <math alttext=\"67.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>67.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">67.5\\%</annotation></semantics></math> on keys, while tempo regression proves difficult (MSE <math alttext=\"0.862\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><mn>0.862</mn><annotation encoding=\"application/x-tex\">0.862</annotation></semantics></math>). Steering experiments (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S5.T4\" title=\"Table 4 &#8227; 5.4 Evaluation on MusicBench (Real Music) &#8227; 5 Single-Direction MusicRFM Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) mirror <span class=\"ltx_text ltx_font_smallcaps\">SynTheory</span>: higher <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases FD/MMD and reduces CLAP, showing that moderate control preserves text adherence but aggressive coefficients destabilize generations. Overall, MusicBench confirms that real-music attributes can be steered, though sensitivity varies by concept difficulty.</p>\n\n",
                "matched_terms": [
                    "reduces",
                    "clap",
                    "control",
                    "increases",
                    "classification",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe several trends:\n(i) <span class=\"ltx_text ltx_font_bold\">Probe accuracy still rises with stronger coefficients.</span> For notes in particular, accuracy increases from 0.770 at <math alttext=\"\\eta_{0}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.3</annotation></semantics></math> to 0.920 at <math alttext=\"\\eta_{0}=0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo>=</mo><mn>0.6</mn></mrow><annotation encoding=\"application/x-tex\">\\eta_{0}=0.6</annotation></semantics></math>, indicating that control strength directly improves enforcement even in multi-direction cases.\n(ii) <span class=\"ltx_text ltx_font_bold\">Distributional metrics and CLAP scores degrade at higher strengths.</span> Both FD and MMD grow substantially as <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math> increases, consistent with the single-direction case, where aggressive steering pushes samples away from the reference distribution. CLAP alignment also degrades significantly. (iii) <span class=\"ltx_text ltx_font_bold\">Accuracy in multi-direction steering exceeds single-direction.</span> We actually observe higher probe accuracy in the multi-direction setting, which we hypothesize arises because stronger aggregate constraints reduce adherence to the text prompt (lower CLAP) and, in turn, compress the generative manifold. This yields less stylistic variance in the music, making classes easier for probes to detect.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd",
                    "control",
                    "strength",
                    "increases",
                    "lower",
                    "alignment",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To study temporal schedules in isolation, we analyze the <span class=\"ltx_text ltx_font_bold\">notes</span> dataset with per-step steering strength <math alttext=\"\\eta_{\\ell}(t)=\\eta_{0}\\,\\rho_{\\ell}\\,\\phi(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mn>0</mn></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#961;</mi><mi mathvariant=\"normal\">&#8467;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{\\ell}(t)=\\eta_{0}\\,\\rho_{\\ell}\\,\\phi(t)</annotation></semantics></math> and track the <em class=\"ltx_emph ltx_font_italic\">softmax score of the ground-truth note class</em> under the RFM probe as a function of time (generation steps). For the experiments in this section, we only analyze on notes because are they are the highest quality in terms of following control, and also can give us a measurable accuracy when evaluating with RFM probes.</p>\n\n",
                "matched_terms": [
                    "strength",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use per-direction coefficients <math alttext=\"\\eta_{0,m}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\eta_{0,m}</annotation></semantics></math> and schedules <math alttext=\"\\phi_{m}(t)\\in[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi_{m}(t)\\in[0,1]</annotation></semantics></math>, so <math alttext=\"\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#951;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>&#951;</mi><mrow><mn>0</mn><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#981;</mi><mi>m</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\eta_{m}(t)=\\eta_{0,m}\\,\\phi_{m}(t)</annotation></semantics></math>. The schedules we ablate are exponential decay, linear decay &amp; increase, logistic increase, and sine wave. We put formulas used in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#A5\" title=\"Appendix E Control schedules used for time control ablations on note classification &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, and record FD, MMD, and CLAP scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.T6\" title=\"Table 6 &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "mmd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then log and display the RFM-probe softmax scores for both <math alttext=\"n_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">n_{1}</annotation></semantics></math> and <math alttext=\"n_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">n_{2}</annotation></semantics></math> at each timestep in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19127v1#S6.F1.sf2\" title=\"In Figure 1 &#8227; Crossfading Between Concepts. &#8227; 6.2 Time-Dependent Control: Smooth Schedules &#8227; 6 Multi-Direction and Time-Based Steering Results &#8227; Steering Autoregressive Music Generation with Recursive Feature Machines\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>. As expected, the first note falls in probability while the second note rises. On average over 500 randomly sampled note pairs, crossfaded generations achieve FD of <math alttext=\"0.350\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mn>0.350</mn><annotation encoding=\"application/x-tex\">0.350</annotation></semantics></math>, MMD of <math alttext=\"1.922\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mn>1.922</mn><annotation encoding=\"application/x-tex\">1.922</annotation></semantics></math>, and CLAP alignment of <math alttext=\"0.250\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mn>0.250</mn><annotation encoding=\"application/x-tex\">0.250</annotation></semantics></math>, indicating modest distributional drift but stable prompt adherence.</p>\n\n",
                "matched_terms": [
                    "probability",
                    "clap",
                    "mmd",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our probes rely on mean-pooled features, which discard temporal ordering. This limits performance on concepts with strong sequential dependencies, such as scales, chord progressions, and time signatures, where the temporal dynamics are essential for accurate classification and control. As a result, RFM probes underperform on these attributes compared to temporally local concepts like notes or chords. Future work should explore temporally aware pooling strategies (e.g., attention pooling, recurrent aggregation, convolutional pooling) or sequence-level RFMs that directly model time-evolving representations. Similarly, extending beyond the top eigenvector to incorporate multiple components could capture richer subspaces of variation, but we have not yet performed variance analyses to quantify how much information higher-order components retain.</p>\n\n",
                "matched_terms": [
                    "control",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across synthetic and real-music settings, we observed consistent trade-offs governed by the control coefficient <math alttext=\"\\eta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S8.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#951;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\eta_{0}</annotation></semantics></math>: moderate steering improves alignment to targeted concepts with limited distributional drift (FD/MMD) and minimal degradation in prompt adherence (CLAP), while aggressive steering yields stronger control at the cost of artifacts. Notes are the most reliably controllable, multi-direction steering is feasible but amplifies drift, and simple schedules (e.g., decay/rise) support intuitive manipulations like crossfades. Time-based control is accurate and true-to-schedule in terms of evaluating on softmax probability of classes. Layer pruning and stochastic (Bernoulli) application help stabilize generations by limiting cumulative bias.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "control",
                    "probability",
                    "alignment",
                    "artifacts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When tuning the number of components calculated with our RFM probes, we tried a lower number of components (2-10) for categories with less data points and less perceived complexity (e.g. notes, time signatures). For categories with larger dataset size and higher perceived complexity (e.g. simple progressions, scales), we choose number of components ranging from 8 to 24.</p>\n\n",
                "matched_terms": [
                    "lower",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For generation, we ablate two steering knobs that most strongly impact generation quality and concept alignment: (i) the <em class=\"ltx_emph ltx_font_italic\">effective number of layers</em> contributing control via both a flat top-<span class=\"ltx_text ltx_font_italic\">k</span> value and an exponential, score-weighted layer scheme (&#8220;layer pruning&#8221;), and (ii) a <em class=\"ltx_emph ltx_font_italic\">per-timestep injection probability</em> <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math> that sparsifies when control is applied.</p>\n\n",
                "matched_terms": [
                    "probability",
                    "injection",
                    "control",
                    "alignment"
                ]
            }
        ]
    }
}