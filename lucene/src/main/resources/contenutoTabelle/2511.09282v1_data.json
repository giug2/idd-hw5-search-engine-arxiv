{
    "Sx4.T1": {
        "caption": "Table 1: Experimental datasets. Datasets marked with asterisks are filtered to ensure one-to-one correspondence between problems and contexts.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Language</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Type</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Size</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Question</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Context</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Train</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Val</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Test</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Spoken-SQuAD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">English</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">37,107</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5,351</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Spoken-SQuAD*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">English</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">29,227</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3,884</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">LibriSQA</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">English</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">104,014</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2620</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">SLUE-SQA-5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">English</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">46,186</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1,939</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2,382</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">DRCD*</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Chinese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">25,321</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1,425</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">-</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "drcd",
            "type",
            "marked",
            "asterisks",
            "size",
            "experimental",
            "correspondence",
            "context",
            "val",
            "ensure",
            "between",
            "test",
            "librisqa",
            "contexts",
            "text",
            "english",
            "language",
            "filtered",
            "spokensquad",
            "speech",
            "datasets",
            "problems",
            "sluesqa5",
            "train",
            "question",
            "onetoone",
            "dataset",
            "chinese"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We conduct experiments on four datasets: Spoken-SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite>, LibriSQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib42\" title=\"\">2024</a>)</cite>, SLUE-SQA-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib33\" title=\"\">2022</a>)</cite>, and DRCD. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T1\" title=\"Table 1 &#8227; Configuration &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> displays detailed information about these datasets. <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite> use Google text-to-speech (TTS) system to generate the spoken versions of the articles in SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajpurkar <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib29\" title=\"\">2016</a>)</cite>. Given that SQuAD is a many-to-one dataset, where multiple questions correspond to the same context, it is unsuitable for training text-speech retrievers. So, we filter the original Spoken-SQuAD dataset to ensure that each question corresponds one-to-one with its context; the filtered dataset is referred to as Spoken SQuAD*. LibriSQA is adapted from the ASR dataset LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib27\" title=\"\">2015</a>)</cite>. The authors input the textual document of each speech segment from LibriSpeech into ChatGPT and request the generation of corresponding text question-answer pairs. We use the first part of LibriSQA, which presents questions without options, and the answers are complete sentences. SLUE-SQA-5 is adapted from five text QA datasets, and both the questions and contexts consist of authentic audio recordings. DRCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Shao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib30\" title=\"\">2018</a>)</cite> is originally a Chinese QA dataset. Similar to SQuAD, it is also a many-to-one dataset. We first filter it into a one-to-one dataset and then use the TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib22\" title=\"\">2020</a>)</cite> to synthesize the speech versions of each question-context pair for its training set. <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib19\" title=\"\">2018</a>)</cite> offer spoken version of DRCD&#8217;s development set, which we use for testing.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Significant progress has been made in spoken question answering (SQA) in recent years. However, many existing methods, including large audio language models, struggle with processing long audio. Follow the success of retrieval augmented generation, a speech-related retriever shows promising in help preprocessing long-form speech. But the performance of existing speech-related retrievers is lacking. To address this challenge, we propose CLSR, an end-to-end contrastive language-speech retriever that efficiently extracts question-relevant segments from long audio recordings for downstream SQA task. Unlike conventional speech-text contrastive models, CLSR incorporates an intermediate step that converts acoustic features into text-like representations prior to alignment, thereby more effectively bridging the gap between modalities. Experimental results across four cross-modal retrieval datasets demonstrate that CLSR surpasses both end-to-end speech related retrievers and pipeline approaches combining speech recognition with text retrieval, providing a robust foundation for advancing practical long-form SQA applications.</p>\n\n",
                "matched_terms": [
                    "language",
                    "question",
                    "between",
                    "speech",
                    "text",
                    "datasets",
                    "experimental"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The question answering (QA) task requires the model to find the answer to a question from a given context. When the answer is a specific segment within the context, the task is classified as extractive QA; conversely, if the answer cannot be directly derived from the context and necessitates additional reasoning by the model, it is termed abstractive QA <cite class=\"ltx_cite ltx_citemacro_citep\">(Shih et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib31\" title=\"\">2023a</a>)</cite>. In the realm of spoken question answering, the context is presented in audio format <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite>, and certain complex SQA tasks also require the questions to be delivered in audio format <cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib33\" title=\"\">2022</a>)</cite>. Despite advancements in SQA methodologies <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, Chen, and Lee <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib18\" title=\"\">2019</a>; You et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib40\" title=\"\">2022</a>)</cite>, the majority of existing SQA models are limited to processing short audio segments (under one minute). However, many real-world dialogue scenarios, such as meetings, lectures, and online discussions, often involve voice recordings that exceed ten minutes, posing challenges for current SQA techniques.</p>\n\n",
                "matched_terms": [
                    "question",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of large language models (LLMs) is advancing rapidly. Notable examples, such as GPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib2\" title=\"\">2020</a>)</cite> and LLaMA <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib34\" title=\"\">2023</a>)</cite>, among others <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib24\" title=\"\">2023</a>; Yao, Li, and Zhao <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib39\" title=\"\">2024</a>)</cite>, have demonstrated significant success across various traditional natural language processing (NLP) tasks, including question answering. In the speech domain, numerous large audio language models (LALMs) have emerged, showcasing remarkable capabilities in speech comprehension <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib5\" title=\"\">2023</a>; Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib28\" title=\"\">2023</a>)</cite>. The retrieval-augmented generation (RAG) paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib20\" title=\"\">2020</a>)</cite> enhances LLMs&#8217; natural language understanding by integrating external knowledge <cite class=\"ltx_cite ltx_citemacro_citep\">(Gupta, Ranjan, and Singh <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib14\" title=\"\">2024</a>)</cite>. Specifically, RAG employs a retriever to assess the similarity between user queries and segments within a knowledge database, selecting the top-k most relevant segments as supplementary context for the LLM. This approach enables the LLM to better comprehend queries and generate more accurate responses. Currently, RAG is commonly used for long-context reasoning tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib23\" title=\"\">2025</a>; Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib12\" title=\"\">2025</a>)</cite>. For example, in question-answering tasks that involve lengthy input contexts like full articles, RAG works by identifying and retrieving the most pertinent context segments. This process minimizes the inclusion of irrelevant information, which can otherwise introduce errors into the answer and reduce inference speed. Given the effectiveness of RAG in text-based long-context QA, a pertinent question arises for long-form SQA: Can RAG be similarly employed to extract problem-relevant segments from audio inputs to serve as context for subsequent LALM processing?</p>\n\n",
                "matched_terms": [
                    "language",
                    "context",
                    "question",
                    "between",
                    "speech",
                    "contexts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose CLSR, an end-to-end (E2E) contrastive language-speech retriever designed to distill lengthy speech recordings into a selection of audio clips that are most pertinent to a given query. These audio clips are used for subsequent LALM inference. Unlike conventional E2E speech-text contrastive learning models, CLSR does not endeavor to align acoustic representations and text representations directly. Instead, it first converts acoustic representations into text-like representations, which are then aligned with actual text representations. The extraction of text-like representations primarily employs continuous integrate-and-fire (CIF) to map acoustic representations from temporal steps to token numbers, followed by the application of a vector quantizer (VQ) based adaptor to refine these acoustic representations into text-like forms. We conduct a comparative analysis of CLSR against standard E2E speech-text retrievers and pipeline retrievers, which integrate speech-to-text models with text contrastive learning models, across four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD. The experimental findings demonstrate that CLSR exhibits superior retrieval performance, suggesting that the use of text-like representations as an intermediary between acoustic and text representations enables CLSR to more effectively discern the similarities and distinctions between these two modalities, thereby enhancing the accuracy of pairing speech with text or speech with speech. The contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "drcd",
                    "spokensquad",
                    "between",
                    "speech",
                    "librisqa",
                    "text",
                    "datasets",
                    "experimental"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The CLSR demonstrates superior performance compared to both E2E and cascade speech retrieval systems on four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD.</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "drcd",
                    "spokensquad",
                    "speech",
                    "librisqa",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, there are many works related to SQA. <cite class=\"ltx_cite ltx_citemacro_citet\">Chuang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib6\" title=\"\">2019</a>)</cite> propose a pre-trained model called SpeechBERT for the E2E SQA task. Through the training stage called initial phonetic spatial joint embedding for audio words, it aligns the generated audio embeddings with the text embeddings produced by BERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib7\" title=\"\">2019</a>)</cite> within the same hidden space. <cite class=\"ltx_cite ltx_citemacro_citet\">Shih et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib31\" title=\"\">2023a</a>)</cite> introduce GSQA, which enables the SQA system to perform abstractive reasoning. They first utilize HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib15\" title=\"\">2021</a>)</cite> to convert the input speech into discrete units, and then employ a sequence-to-sequence SQA model finetuned from the text QA model LongT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib13\" title=\"\">2022</a>)</cite> to generate answers in the form of discrete units. <cite class=\"ltx_cite ltx_citemacro_citet\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib25\" title=\"\">2024</a>)</cite> focus on open-domain SQA in scenarios whose paired speech-text data is unavailable. They propose SpeechDPR, which utilizes a bi-encoder retriever framework and learns a sentence-level semantic representation space by extracting knowledge from a combined model of automatic speech recognition (ASR) and text retrieval. <cite class=\"ltx_cite ltx_citemacro_citet\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib16\" title=\"\">2024</a>)</cite> introduce a retriever that employs deep Q-learning to bypass irrelevant audio segments in longer audio files, thereby enhancing the efficiency of SQA. The latter two articles are related to retriever, which is similar to our paper; however, they have limitations: the performance of the former is inferior to that of the pipeline model, and the latter poorly adapts to the high-dimensional, complex feature space of audio, easily falls into local optima during training due to unbalanced exploration and exploitation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the inception of GPT, RAG has advanced rapidly <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib41\" title=\"\">2025</a>)</cite>, while research on speech RAG has been comparatively limited. <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib38\" title=\"\">2024</a>)</cite> utilize RAG for spoken language understanding (SLU). They first employ a pre-trained ASR encoder to extract acoustic features. Next, they perform a similarity calculation to identify audio-text label pairs in the training set that are similar, subsequently incorporating this label information into the SLU decoder through a cross-attention mechanism. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib36\" title=\"\">2024</a>)</cite> propose a joint speech and language model based on RAG, which enhances performance on the named entity recognition task. They compute the similarity between the input speech query embeddings and the entity embeddings in the database to extract the K entities most relevant to the query, using these entities as additional inputs to the model. Currently, there is no SRAG model designed for long-form SQA task.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Consider the SQA task, where questions are presented in text format and contexts are delivered in speech format. Let <math alttext=\"X=\\{x_{1},x_{2},\\dots,x_{t}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=\\{x_{1},x_{2},\\dots,x_{t}\\}</annotation></semantics></math> denote the speech context, represented as a sequence of <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> frames. Let <math alttext=\"Z=\\{z_{1},z_{2},\\dots,z_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=\\{z_{1},z_{2},\\dots,z_{m}\\}</annotation></semantics></math> denote the question text, represented as a sequence of <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> tokens, where each token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> belongs to a vocabulary <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m6\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> (i.e., <math alttext=\"z_{i}\\in V\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">z_{i}\\in V</annotation></semantics></math>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F2\" title=\"Figure 2 &#8227; Preliminary &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the architecture of a typical E2E audio-text contrastive model, such as CLAP <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib37\" title=\"\">2023</a>)</cite>. This model employs a speech encoder <math alttext=\"A(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">A(\\cdot)</annotation></semantics></math> and a text encoder <math alttext=\"B(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m9\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B(\\cdot)</annotation></semantics></math> to extract acoustic features and textual features, respectively. The similarity <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m10\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> between the two feature representations is then quantified using cosine similarity. The formula is as follows, where <math alttext=\"||.||\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"Sx3.SSx1.p1.m11\" intent=\":literal\"><semantics><mrow><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0.167em\" rspace=\"0.167em\">.</mo><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">||.||</annotation></semantics></math> denotes the L2 norm.</p>\n\n",
                "matched_terms": [
                    "context",
                    "question",
                    "between",
                    "speech",
                    "contexts",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, the model learns to minimize the negative log-likelihood (NLL) loss between the representations of paired questions and contexts. This NLL loss comprises two symmetric components: one for retrieving the context given the question, and the other for retrieving the question given the context. The specific formulation is as follows:</p>\n\n",
                "matched_terms": [
                    "question",
                    "contexts",
                    "context",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F3\" title=\"Figure 3 &#8227; Overview &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the specific architecture of CLSR. The left half features a non-autoregressive attention encoder-decoder (AED) framework based on CIF <cite class=\"ltx_cite ltx_citemacro_citep\">(Dong and Xu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib8\" title=\"\">2020</a>)</cite>. It takes the speech context <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> as input and produces the corresponding token probability distribution <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>, <math alttext=\"D=\\{d_{1},d_{2},d_{3},&#8230;,d_{n}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>D</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><msub><mi>d</mi><mn>2</mn></msub><mo>,</mo><msub><mi>d</mi><mn>3</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>d</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">D=\\{d_{1},d_{2},d_{3},&#8230;,d_{n}\\}</annotation></semantics></math>. Both the speech encoder and decoder adopt the SAN-M <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib10\" title=\"\">2020</a>)</cite> structure, which is a specialized Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib35\" title=\"\">2017</a>)</cite> layer that integrates a self-attention mechanism with deep feed-forward sequential memory networks (DFSMN). Initially, the framework uses the speech encoder to extract acoustic features <math alttext=\"H^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m4\" intent=\":literal\"><semantics><msup><mi>H</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">H^{s}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then, as shown in the following formula, it predicts the corresponding token distribution through the speech decoder and a fully connected layer. In addition, following <cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib11\" title=\"\">2022</a>)</cite>, we use a sampler to optimize the training process of this framework. The sampler does not contain any learnable parameters and is designed to enhance the context modeling capability of the decoder by sampling text features into <math alttext=\"E^{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p3.m1\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>a</mi></msup><annotation encoding=\"application/x-tex\">E^{a}</annotation></semantics></math>. It will be elaborated on in subsequent chapter.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The right half of CLSR is a Transformer-based text encoder that receives either text embeddings or text-like embeddings as input and output corresponding text representations. We obtain the sentence-level representation by inserting the <span class=\"ltx_text ltx_font_typewriter\">CLS</span> token. When aligning the text question with the speech context, we input the text-like embeddings <math alttext=\"E^{Y^{{}^{\\prime}}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p6.m1\" intent=\":literal\"><semantics><msup><mi>E</mi><msup><mi>Y</mi><msup><mi/><mo>&#8242;</mo></msup></msup></msup><annotation encoding=\"application/x-tex\">E^{Y^{{}^{\\prime}}}</annotation></semantics></math> of the context and the text embeddings <math alttext=\"E^{Z}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p6.m2\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>Z</mi></msup><annotation encoding=\"application/x-tex\">E^{Z}</annotation></semantics></math> of the question into the text encoder to obtain their respective sentence-level representations. We then use cosine similarity to evaluate the similarity between them.</p>\n\n",
                "matched_terms": [
                    "context",
                    "question",
                    "between",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding the real text embeddings <math alttext=\"E^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p7.m1\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>c</mi></msup><annotation encoding=\"application/x-tex\">E^{c}</annotation></semantics></math>, <cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib11\" title=\"\">2022</a>)</cite> uses the embedding layer of the speech decoder to derive them. However, in our proposed model, this layer is not trained, and its weights may not effectively represent the text embedding space. Consequently, we use the weights of the linear layer, which is used to generate the probability distribution of the tokens, to calculate <math alttext=\"E^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p7.m2\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>c</mi></msup><annotation encoding=\"application/x-tex\">E^{c}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also use NLL loss to optimize the model&#8217;s ability in aligning the question representation with the context representation. The overall loss function can be expressed as follows, where <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx6.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx6.p2.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> are parameters that control the proportions of the CIF loss and the contrastive loss, with <math alttext=\"\\alpha\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx6.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in(0,1)</annotation></semantics></math> and <math alttext=\"\\beta\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx6.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#946;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\beta\\in(0,1)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use 220M Paraformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib11\" title=\"\">2022</a>)</cite> and BGE-base <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib3\" title=\"\">2024</a>)</cite> to build CLSR. BGE is frozen during joint training. We consider two models as baseline: one is the E2E text-speech contrastive model like Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F2\" title=\"Figure 2 &#8227; Preliminary &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, and the other is the cascaded model that first uses ASR model to convert speech into text, followed by a text QA task. For the former, we choose CLAP and SpeechDPR for comparison. For the latter, we use Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib28\" title=\"\">2023</a>)</cite> as the ASR module and BGE-base as the text QA module. The Whisper&#8217;s size is 244M. In the experiment, word error rate (WER) is used to measure the ASR performance, and top-k question-to-context and context-to-question retrieval recall are used to assess retrieval performance. We establish the experimental environment based on Funasr <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib9\" title=\"\">2023</a>)</cite> and ModelScope. The <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> of the loss is set to <math alttext=\"\\frac{1}{3}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m3\" intent=\":literal\"><semantics><mfrac><mn>1</mn><mn>3</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{3}</annotation></semantics></math>. We train the model until convergence, consistently using the Adam optimizer with a learning rate of 5e-5.</p>\n\n",
                "matched_terms": [
                    "train",
                    "size",
                    "speech",
                    "text",
                    "experimental"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T2\" title=\"Table 2 &#8227; Main Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison results of CLSR and other models across four datasets. We additionally provide the results of using BGE for clean text question-context retrieval. In terms of E2E text-to-speech contrastive models, the results of CLSR are significantly better than those of CLAP and SpeechDPR. We found that CLAP cannot learn the relevance between text questions and speech contexts effectively on four datasets, suggesting that CLAP is not well-suited for text-to-speech content alignment. In fact, CLAP is more appropriate for sound and text alignment.</p>\n\n",
                "matched_terms": [
                    "between",
                    "speech",
                    "contexts",
                    "text",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SpeechDPR is committed to using text-less data for training. Although they use ASR models and text QA models for knowledge distillation, the scarcity of data hampers its ability to achieve optimal performance. It is worth noting that we do not conduct large-scale pre-training prior to training CLSR. All leading contrastive learning models, such as BGE, have undergone extensive pre-training, which enhances their retrieval capabilities. Nevertheless, CLSR still achieves results that are second only to BGE in clean text retrieval and even surpasses BGE&#8217;s performance on Spoken-SQuAD*, highlighting the advantages of CLSR&#8217;s architecture.</p>\n\n",
                "matched_terms": [
                    "text",
                    "spokensquad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to conventional E2E contrastive models that directly perform text-to-speech or speech-to-speech alignment, CLSR utilizes text-like representations to alleviate the differences between speech and text modalities. It first maps speech representations into text-like representations and then aligns these text-like representations with actual text representations (or aligns text-like representations with other text-like representations) within the text modality. Leveraging the robust performance of text contrastive models, this approach enhances the alignment between speech and text (or between speech and speech), thereby facilitating more accurate pairing with the context most relevant to the question.</p>\n\n",
                "matched_terms": [
                    "context",
                    "question",
                    "between",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When conducting a comparative analysis of CLSR and Whisper+BGE, we find that their retrieval performances on three English datasets are quite similar; however, CLSR demonstrates certain advantages. In terms of transcription ability, CLSR significantly outperforms Whisper+BGE. This indicates that the joint training of CLSR effectively optimizes both the ASR module and the contrastive learning module. Given that Whisper&#8217;s performance in Chinese speech recognition is not exceptional, we have opted not to train Whisper on DRCD*.</p>\n\n",
                "matched_terms": [
                    "drcd",
                    "english",
                    "train",
                    "speech",
                    "chinese",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before joint training, we can pre-train the ASR module and the BGE module of CLSR separately. In the experiment, we use 460 hours of clean LibriSpeech data to pre-train Paraformer, and use Spoken-SQuAD&#8217;s clean text question-context pairs to train BGE. Comparing the second and fourth rows of the experimental results, it is not difficult to find that pre-training the BGE is beneficial; incorporating the pre-trained BGE during joint training enhances various retrieval metrics of the CLSR. In addition, through the comparison between the fourth and sixth rows, it can be found that pre-training Paraformer can improve the model&#8217;s transcription performance while also slightly improving its retrieval ability. It should be noted that in order to improve the training speed of the model, we froze BGE, which has strong retrieval performance, during joint training. Therefore, we can freeze the ASR module after joint training and train BGE for a few epochs separately, which is called post-train in the table. It is hoped that this approach can make BGE better adapt to the text-like representation provided by the ASR module. Unfortunately, post-train can only slightly improve the performance of the model, as evidenced by rows 2 and 3, 4 and 5, 7 and 8 in the table. In short, through ablation experiments, we have shown that both quantizers and samplers are inseparable for CLSR, and that pre-training the ASR module and BGE module of CLSR is of significant importance.</p>\n\n",
                "matched_terms": [
                    "train",
                    "text",
                    "between",
                    "experimental"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to further demonstrate the superiority of the proposed model over the traditional E2E speech-related contrastive model, which consists of two encoders, we construct a new baseline: ParaBGE, to compare the retrieval capability with CLSR. ParaBGE is composed of the speech encoder from Paraformer and the text encoder from BGE. The sizes of each module in both models are identical to those in CLSR. The experimental results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T4\" title=\"Table 4 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. All retrieval metrics of CLSR far exceed ParaBGE, indicating that CLSR has a stronger question-context alignment ability. Although ParaBGE can optimize parameters towards the direction of aligning question and context representation during training, its performance is not ideal. As we mentioned earlier, such model heavily rely on pre-training with large-scale corpora. However, high-quality speech-text pairs are already very scarce, so for E2E speech related retrieval models, it is difficult to achieve excellent results. However, CLSR alleviates the modal differences between speech and text by using text-like representation as a bridge, shifting the alignment of speech to text alignment. With the powerful generalization ability of text contrastive learning models, it can achieve excellent retrieval capabilities comparable to cascade models and text contrastive models without the need for long-term, large-scale pre-training.</p>\n\n",
                "matched_terms": [
                    "context",
                    "question",
                    "between",
                    "speech",
                    "text",
                    "experimental"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate our model for real-world long audio SQA&#8212;enabling downstream LALMs to simplify inputs and enhance inference speed and accuracy&#8212;we conduct SQA tasks on a modified SLUE-SQA-5 dataset using CLSR and Qwen-Audio. To simulate long-context inference, we replace the contextual audio in 500 randomly selected test instances with full documents from the Spoken Wikipedia corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;hn, Stegen, and Baumann <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib17\" title=\"\">2016</a>)</cite>, each averaging &#160;30 minutes in length.</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "dataset",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without CLSR, Qwen-Audio directly generates answers from the input speech document and text question. With CLSR, we first segment the speech document into 40-second intervals, assess each segment&#8217;s similarity to the text question, and select the most relevant one as Qwen-Audio&#8217;s contextual input. Prior to testing, both Qwen-Audio and CLSR were trained using the original training subset of SLUE-SQA-5. The results of the testing are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T5\" title=\"Table 5 &#8227; Long-form SQA Evaluation &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Following the application of CLSR for long audio reduction, there is a notable enhancement in Qwen-Audio&#8217;s exact match (EM) and macro-F1 scores for SQA, alongside a tenfold reduction in inference time. These findings underscore the significance of CLSR in the preprocessing of long audio, demonstrating its capacity to not only enhance the inference accuracy of downstream LALM but also to substantially decrease inference time.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "question",
                    "sluesqa5",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce CLSR, an E2E contrastive language-speech retriever designed to distill lengthy speech recordings into a limited number of clips that are most pertinent to a given query. By employing a text-like representation as an intermediary state, CLSR exhibits strong capability of cross-modal question-context alignment. Experimental findings demonstrate that CLSR&#8217;s retrieval performance significantly outstrips that of existing E2E speech-related retrievers and is competitive with both cascaded models and text-based retrievers.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "experimental"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of the Whisper+BGE pipeline following fine-tuning with noisy data, we transcribe the training data using a trained Whisper model and subsequently provide it to BGE for pre-training. The results of this experimentation are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.T7\" title=\"Table 7 &#8227; Does CLSR have an advantage over Whisper+BGE pipeline during runtime? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, where the fine-tuned Whisper+BGE is referred to as Whisper+BGE*. The findings suggest that Whisper+BGE* yields only a marginal improvement in retrieval capability on the LibriSQA dataset, while performance declines on the other two datasets. CLSR continues to demonstrate a superior ability in retrieving speech content.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "librisqa",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two representative cases from the LibriSQA inference results and visualize the similarity distributions of CLSR and ParaBGE when aligning textual questions with speech contexts in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.F7\" title=\"Figure 7 &#8227; Case Study: Structural Superiority of CLSR over Traditional End-to-End Dual-Encoder Retrievers. &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which demonstrates CLSR&#8217;s architectural superiority over traditional dual-encoder retrievers. As shown, CLSR achieves precise alignment between semantically related audio-text segments (e.g., matching &#8220;mary&#8221; in Case 1 and &#8220;humble grass&#8221; in Case 2), whereas ParaBGE exhibits uniformly distributed similarity scores without capturing token-level correspondences. This granular alignment enables CLSR to identify context segments more relevant to queries, enhancing retrieval accuracy.</p>\n\n",
                "matched_terms": [
                    "context",
                    "between",
                    "speech",
                    "librisqa",
                    "contexts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CLSR&#8217;s unique architecture facilitates such granular alignment: It first utilizes the CIF module to project acoustic features from time steps to token positions; then uses VQ-based refinement to convert these features into text-like representations; finally, in text space, leverages pre-trained text retrieval model&#8217;s power to align text-like representations with ordinary text representations token by token. Since text-like representations retain acoustic-feature similarity, this achieves fine-grained alignment between acoustic and text representations. This architectural superiority is absent in ParaBGE and similar dual-encoder retrievers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "between"
                ]
            }
        ]
    },
    "Sx4.T2": {
        "caption": "Table 2: Main results of the proposed model across four datasets. Results for BGE are included as a reference benchmark, showing theoretical limits under optimal ASR conditions (100% accuracy). The SpeechDPRs paper only provides the result of R@20. CLAP is composed of HTSAT (Chen etal. 2022) and RoBERTa (Liu 2019).",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Dataset</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Paradigm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Type</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">ASR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Q-C Retrieval (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">C-Q Retrieval (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Question</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Context</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">WER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Spoken-SQuAD*</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">BGE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">67.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">85.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">89.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">65.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">84.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">89.06</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLAP</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">2.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">9.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">14.84</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">10.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">15.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper+BGE</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Pipeline</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Transcript</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">19.39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">69.93</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">86.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">90.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">67.97</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.76</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">89.65</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">15.14</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">70.03</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">86.90</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">90.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">67.84</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">85.69</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">90.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">LibriSQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">BGE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">86.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">94.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">95.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">86.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">94.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">96.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLAP</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.19</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.08</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.19</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper+BGE</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Pipeline</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Transcript</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">4.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">83.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">93.28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">94.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">85.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">93.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">95.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">4.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">93.36</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">95.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">94.01</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">95.57</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">SLUE-SQA-5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">BGE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">38.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">72.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">84.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">35.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">70.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">82.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLAP</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">11.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">28.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">38.16</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">11.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">28.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">38.12</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">SpeechDPR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">19.94*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper+BGE</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Pipeline</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Transcript</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Transcript</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">36.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.98</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">60.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">72.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.85</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">60.75</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">73.47</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">16.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">30.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">62.19</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">74.43</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">29.89</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">62.18</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">73.05</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">DRCD*</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">BGE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">90.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">97.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">98.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">89.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">97.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">98.39</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLAP</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Text</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">2.95</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.61</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">5.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">76.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">87.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">90.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">75.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">88.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">91.51</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "four",
            "drcd",
            "r20",
            "type",
            "wer",
            "whisperbge",
            "composed",
            "optimal",
            "speechdpr",
            "bge",
            "e2e",
            "downarrow",
            "htsat",
            "limits",
            "showing",
            "context",
            "conditions",
            "accuracy",
            "clsr",
            "librisqa",
            "text",
            "retrieval",
            "across",
            "clap",
            "main",
            "result",
            "pipeline",
            "under",
            "spokensquad",
            "transcript",
            "results",
            "asr",
            "roberta",
            "speech",
            "only",
            "datasets",
            "benchmark",
            "sluesqa5",
            "uparrow",
            "proposed",
            "provides",
            "included",
            "chen",
            "theoretical",
            "model",
            "speechdprs",
            "liu",
            "question",
            "r10",
            "paper",
            "paradigm",
            "dataset",
            "reference"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T2\" title=\"Table 2 &#8227; Main Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison results of CLSR and other models across four datasets. We additionally provide the results of using BGE for clean text question-context retrieval. In terms of E2E text-to-speech contrastive models, the results of CLSR are significantly better than those of CLAP and SpeechDPR. We found that CLAP cannot learn the relevance between text questions and speech contexts effectively on four datasets, suggesting that CLAP is not well-suited for text-to-speech content alignment. In fact, CLAP is more appropriate for sound and text alignment.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Significant progress has been made in spoken question answering (SQA) in recent years. However, many existing methods, including large audio language models, struggle with processing long audio. Follow the success of retrieval augmented generation, a speech-related retriever shows promising in help preprocessing long-form speech. But the performance of existing speech-related retrievers is lacking. To address this challenge, we propose CLSR, an end-to-end contrastive language-speech retriever that efficiently extracts question-relevant segments from long audio recordings for downstream SQA task. Unlike conventional speech-text contrastive models, CLSR incorporates an intermediate step that converts acoustic features into text-like representations prior to alignment, thereby more effectively bridging the gap between modalities. Experimental results across four cross-modal retrieval datasets demonstrate that CLSR surpasses both end-to-end speech related retrievers and pipeline approaches combining speech recognition with text retrieval, providing a robust foundation for advancing practical long-form SQA applications.</p>\n\n",
                "matched_terms": [
                    "across",
                    "four",
                    "datasets",
                    "pipeline",
                    "question",
                    "speech",
                    "results",
                    "clsr",
                    "text",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The question answering (QA) task requires the model to find the answer to a question from a given context. When the answer is a specific segment within the context, the task is classified as extractive QA; conversely, if the answer cannot be directly derived from the context and necessitates additional reasoning by the model, it is termed abstractive QA <cite class=\"ltx_cite ltx_citemacro_citep\">(Shih et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib31\" title=\"\">2023a</a>)</cite>. In the realm of spoken question answering, the context is presented in audio format <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite>, and certain complex SQA tasks also require the questions to be delivered in audio format <cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib33\" title=\"\">2022</a>)</cite>. Despite advancements in SQA methodologies <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, Chen, and Lee <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib18\" title=\"\">2019</a>; You et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib40\" title=\"\">2022</a>)</cite>, the majority of existing SQA models are limited to processing short audio segments (under one minute). However, many real-world dialogue scenarios, such as meetings, lectures, and online discussions, often involve voice recordings that exceed ten minutes, posing challenges for current SQA techniques.</p>\n\n",
                "matched_terms": [
                    "chen",
                    "model",
                    "context",
                    "question",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of large language models (LLMs) is advancing rapidly. Notable examples, such as GPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib2\" title=\"\">2020</a>)</cite> and LLaMA <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib34\" title=\"\">2023</a>)</cite>, among others <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib24\" title=\"\">2023</a>; Yao, Li, and Zhao <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib39\" title=\"\">2024</a>)</cite>, have demonstrated significant success across various traditional natural language processing (NLP) tasks, including question answering. In the speech domain, numerous large audio language models (LALMs) have emerged, showcasing remarkable capabilities in speech comprehension <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib5\" title=\"\">2023</a>; Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib28\" title=\"\">2023</a>)</cite>. The retrieval-augmented generation (RAG) paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib20\" title=\"\">2020</a>)</cite> enhances LLMs&#8217; natural language understanding by integrating external knowledge <cite class=\"ltx_cite ltx_citemacro_citep\">(Gupta, Ranjan, and Singh <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib14\" title=\"\">2024</a>)</cite>. Specifically, RAG employs a retriever to assess the similarity between user queries and segments within a knowledge database, selecting the top-k most relevant segments as supplementary context for the LLM. This approach enables the LLM to better comprehend queries and generate more accurate responses. Currently, RAG is commonly used for long-context reasoning tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib23\" title=\"\">2025</a>; Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib12\" title=\"\">2025</a>)</cite>. For example, in question-answering tasks that involve lengthy input contexts like full articles, RAG works by identifying and retrieving the most pertinent context segments. This process minimizes the inclusion of irrelevant information, which can otherwise introduce errors into the answer and reduce inference speed. Given the effectiveness of RAG in text-based long-context QA, a pertinent question arises for long-form SQA: Can RAG be similarly employed to extract problem-relevant segments from audio inputs to serve as context for subsequent LALM processing?</p>\n\n",
                "matched_terms": [
                    "across",
                    "context",
                    "paradigm",
                    "question",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose CLSR, an end-to-end (E2E) contrastive language-speech retriever designed to distill lengthy speech recordings into a selection of audio clips that are most pertinent to a given query. These audio clips are used for subsequent LALM inference. Unlike conventional E2E speech-text contrastive learning models, CLSR does not endeavor to align acoustic representations and text representations directly. Instead, it first converts acoustic representations into text-like representations, which are then aligned with actual text representations. The extraction of text-like representations primarily employs continuous integrate-and-fire (CIF) to map acoustic representations from temporal steps to token numbers, followed by the application of a vector quantizer (VQ) based adaptor to refine these acoustic representations into text-like forms. We conduct a comparative analysis of CLSR against standard E2E speech-text retrievers and pipeline retrievers, which integrate speech-to-text models with text contrastive learning models, across four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD. The experimental findings demonstrate that CLSR exhibits superior retrieval performance, suggesting that the use of text-like representations as an intermediary between acoustic and text representations enables CLSR to more effectively discern the similarities and distinctions between these two modalities, thereby enhancing the accuracy of pairing speech with text or speech with speech. The contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "across",
                    "four",
                    "drcd",
                    "datasets",
                    "pipeline",
                    "librisqa",
                    "speech",
                    "spokensquad",
                    "paper",
                    "e2e",
                    "accuracy",
                    "text",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The proposed model initially transforms acoustic representations into text-like representations, subsequently aligning these text-like representations with text representations, which effectively mitigates modal discrepancies and facilitates cross-modal alignment.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "proposed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The CLSR demonstrates superior performance compared to both E2E and cascade speech retrieval systems on four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD.</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "four",
                    "drcd",
                    "e2e",
                    "spokensquad",
                    "speech",
                    "librisqa",
                    "retrieval",
                    "datasets",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, there are many works related to SQA. <cite class=\"ltx_cite ltx_citemacro_citet\">Chuang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib6\" title=\"\">2019</a>)</cite> propose a pre-trained model called SpeechBERT for the E2E SQA task. Through the training stage called initial phonetic spatial joint embedding for audio words, it aligns the generated audio embeddings with the text embeddings produced by BERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib7\" title=\"\">2019</a>)</cite> within the same hidden space. <cite class=\"ltx_cite ltx_citemacro_citet\">Shih et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib31\" title=\"\">2023a</a>)</cite> introduce GSQA, which enables the SQA system to perform abstractive reasoning. They first utilize HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib15\" title=\"\">2021</a>)</cite> to convert the input speech into discrete units, and then employ a sequence-to-sequence SQA model finetuned from the text QA model LongT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib13\" title=\"\">2022</a>)</cite> to generate answers in the form of discrete units. <cite class=\"ltx_cite ltx_citemacro_citet\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib25\" title=\"\">2024</a>)</cite> focus on open-domain SQA in scenarios whose paired speech-text data is unavailable. They propose SpeechDPR, which utilizes a bi-encoder retriever framework and learns a sentence-level semantic representation space by extracting knowledge from a combined model of automatic speech recognition (ASR) and text retrieval. <cite class=\"ltx_cite ltx_citemacro_citet\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib16\" title=\"\">2024</a>)</cite> introduce a retriever that employs deep Q-learning to bypass irrelevant audio segments in longer audio files, thereby enhancing the efficiency of SQA. The latter two articles are related to retriever, which is similar to our paper; however, they have limitations: the performance of the former is inferior to that of the pipeline model, and the latter poorly adapts to the high-dimensional, complex feature space of audio, easily falls into local optima during training due to unbalanced exploration and exploitation.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "model",
                    "speechdpr",
                    "speech",
                    "paper",
                    "asr",
                    "e2e",
                    "text",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the inception of GPT, RAG has advanced rapidly <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib41\" title=\"\">2025</a>)</cite>, while research on speech RAG has been comparatively limited. <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib38\" title=\"\">2024</a>)</cite> utilize RAG for spoken language understanding (SLU). They first employ a pre-trained ASR encoder to extract acoustic features. Next, they perform a similarity calculation to identify audio-text label pairs in the training set that are similar, subsequently incorporating this label information into the SLU decoder through a cross-attention mechanism. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib36\" title=\"\">2024</a>)</cite> propose a joint speech and language model based on RAG, which enhances performance on the named entity recognition task. They compute the similarity between the input speech query embeddings and the entity embeddings in the database to extract the K entities most relevant to the query, using these entities as additional inputs to the model. Currently, there is no SRAG model designed for long-form SQA task.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Consider the SQA task, where questions are presented in text format and contexts are delivered in speech format. Let <math alttext=\"X=\\{x_{1},x_{2},\\dots,x_{t}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=\\{x_{1},x_{2},\\dots,x_{t}\\}</annotation></semantics></math> denote the speech context, represented as a sequence of <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> frames. Let <math alttext=\"Z=\\{z_{1},z_{2},\\dots,z_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=\\{z_{1},z_{2},\\dots,z_{m}\\}</annotation></semantics></math> denote the question text, represented as a sequence of <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> tokens, where each token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> belongs to a vocabulary <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m6\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> (i.e., <math alttext=\"z_{i}\\in V\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">z_{i}\\in V</annotation></semantics></math>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F2\" title=\"Figure 2 &#8227; Preliminary &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the architecture of a typical E2E audio-text contrastive model, such as CLAP <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib37\" title=\"\">2023</a>)</cite>. This model employs a speech encoder <math alttext=\"A(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">A(\\cdot)</annotation></semantics></math> and a text encoder <math alttext=\"B(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m9\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B(\\cdot)</annotation></semantics></math> to extract acoustic features and textual features, respectively. The similarity <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m10\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> between the two feature representations is then quantified using cosine similarity. The formula is as follows, where <math alttext=\"||.||\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"Sx3.SSx1.p1.m11\" intent=\":literal\"><semantics><mrow><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0.167em\" rspace=\"0.167em\">.</mo><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">||.||</annotation></semantics></math> denotes the L2 norm.</p>\n\n",
                "matched_terms": [
                    "clap",
                    "model",
                    "context",
                    "question",
                    "speech",
                    "e2e",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, the model learns to minimize the negative log-likelihood (NLL) loss between the representations of paired questions and contexts. This NLL loss comprises two symmetric components: one for retrieving the context given the question, and the other for retrieving the question given the context. The specific formulation is as follows:</p>\n\n",
                "matched_terms": [
                    "question",
                    "model",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F3\" title=\"Figure 3 &#8227; Overview &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the specific architecture of CLSR. The left half features a non-autoregressive attention encoder-decoder (AED) framework based on CIF <cite class=\"ltx_cite ltx_citemacro_citep\">(Dong and Xu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib8\" title=\"\">2020</a>)</cite>. It takes the speech context <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m1\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math> as input and produces the corresponding token probability distribution <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>, <math alttext=\"D=\\{d_{1},d_{2},d_{3},&#8230;,d_{n}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>D</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><msub><mi>d</mi><mn>2</mn></msub><mo>,</mo><msub><mi>d</mi><mn>3</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>d</mi><mi>n</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">D=\\{d_{1},d_{2},d_{3},&#8230;,d_{n}\\}</annotation></semantics></math>. Both the speech encoder and decoder adopt the SAN-M <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib10\" title=\"\">2020</a>)</cite> structure, which is a specialized Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib35\" title=\"\">2017</a>)</cite> layer that integrates a self-attention mechanism with deep feed-forward sequential memory networks (DFSMN). Initially, the framework uses the speech encoder to extract acoustic features <math alttext=\"H^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p1.m4\" intent=\":literal\"><semantics><msup><mi>H</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">H^{s}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "speech",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then, as shown in the following formula, it predicts the corresponding token distribution through the speech decoder and a fully connected layer. In addition, following <cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib11\" title=\"\">2022</a>)</cite>, we use a sampler to optimize the training process of this framework. The sampler does not contain any learnable parameters and is designed to enhance the context modeling capability of the decoder by sampling text features into <math alttext=\"E^{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p3.m1\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>a</mi></msup><annotation encoding=\"application/x-tex\">E^{a}</annotation></semantics></math>. It will be elaborated on in subsequent chapter.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The right half of CLSR is a Transformer-based text encoder that receives either text embeddings or text-like embeddings as input and output corresponding text representations. We obtain the sentence-level representation by inserting the <span class=\"ltx_text ltx_font_typewriter\">CLS</span> token. When aligning the text question with the speech context, we input the text-like embeddings <math alttext=\"E^{Y^{{}^{\\prime}}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p6.m1\" intent=\":literal\"><semantics><msup><mi>E</mi><msup><mi>Y</mi><msup><mi/><mo>&#8242;</mo></msup></msup></msup><annotation encoding=\"application/x-tex\">E^{Y^{{}^{\\prime}}}</annotation></semantics></math> of the context and the text embeddings <math alttext=\"E^{Z}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p6.m2\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>Z</mi></msup><annotation encoding=\"application/x-tex\">E^{Z}</annotation></semantics></math> of the question into the text encoder to obtain their respective sentence-level representations. We then use cosine similarity to evaluate the similarity between them.</p>\n\n",
                "matched_terms": [
                    "context",
                    "question",
                    "speech",
                    "clsr",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance the capacity of the selected non autoregressive AED framework to model token probability distributions, we introduce a training optimization module called the sampler. When the sampler is enabled, the training process of the framework consists of two rounds. In the first round, we do not utilize the sampler; instead, we directly employ the acoustic features <math alttext=\"E^{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m1\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>a</mi></msup><annotation encoding=\"application/x-tex\">E^{a}</annotation></semantics></math> obtained from the CIF module to predict the probability distribution of tokens. By applying <math alttext=\"argmax\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">argmax</annotation></semantics></math>, we can derive the transcription result <math alttext=\"Y^{asr}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m3\" intent=\":literal\"><semantics><msup><mi>Y</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msup><annotation encoding=\"application/x-tex\">Y^{asr}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "result",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we proceed to the second round of training and initiate sampling. We first compare the ASR output <math alttext=\"Y^{asr}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m4\" intent=\":literal\"><semantics><msup><mi>Y</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msup><annotation encoding=\"application/x-tex\">Y^{asr}</annotation></semantics></math> with the ground-truth context <math alttext=\"Y^{con}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m5\" intent=\":literal\"><semantics><msup><mi>Y</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow></msup><annotation encoding=\"application/x-tex\">Y^{con}</annotation></semantics></math> to identify tokens containing transcription errors and their respective positions. Then merge the correct embeddings of erroneous tokens from <math alttext=\"E^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m6\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>c</mi></msup><annotation encoding=\"application/x-tex\">E^{c}</annotation></semantics></math> (the embeddings of <math alttext=\"Y^{con}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m7\" intent=\":literal\"><semantics><msup><mi>Y</mi><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi></mrow></msup><annotation encoding=\"application/x-tex\">Y^{con}</annotation></semantics></math>) into the acoustic features <math alttext=\"E^{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m8\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>a</mi></msup><annotation encoding=\"application/x-tex\">E^{a}</annotation></semantics></math> through selective replacement, generating semantic features <math alttext=\"E^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p1.m9\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">E^{s}</annotation></semantics></math>. This process is formalized as:</p>\n\n",
                "matched_terms": [
                    "context",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding the real text embeddings <math alttext=\"E^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p7.m1\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>c</mi></msup><annotation encoding=\"application/x-tex\">E^{c}</annotation></semantics></math>, <cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib11\" title=\"\">2022</a>)</cite> uses the embedding layer of the speech decoder to derive them. However, in our proposed model, this layer is not trained, and its weights may not effectively represent the text embedding space. Consequently, we use the weights of the linear layer, which is used to generate the probability distribution of the tokens, to calculate <math alttext=\"E^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p7.m2\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>c</mi></msup><annotation encoding=\"application/x-tex\">E^{c}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "text",
                    "proposed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also use NLL loss to optimize the model&#8217;s ability in aligning the question representation with the context representation. The overall loss function can be expressed as follows, where <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx6.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx6.p2.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> are parameters that control the proportions of the CIF loss and the contrastive loss, with <math alttext=\"\\alpha\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx6.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in(0,1)</annotation></semantics></math> and <math alttext=\"\\beta\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx6.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#946;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\beta\\in(0,1)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "question",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments on four datasets: Spoken-SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite>, LibriSQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib42\" title=\"\">2024</a>)</cite>, SLUE-SQA-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib33\" title=\"\">2022</a>)</cite>, and DRCD. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T1\" title=\"Table 1 &#8227; Configuration &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> displays detailed information about these datasets. <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite> use Google text-to-speech (TTS) system to generate the spoken versions of the articles in SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajpurkar <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib29\" title=\"\">2016</a>)</cite>. Given that SQuAD is a many-to-one dataset, where multiple questions correspond to the same context, it is unsuitable for training text-speech retrievers. So, we filter the original Spoken-SQuAD dataset to ensure that each question corresponds one-to-one with its context; the filtered dataset is referred to as Spoken SQuAD*. LibriSQA is adapted from the ASR dataset LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib27\" title=\"\">2015</a>)</cite>. The authors input the textual document of each speech segment from LibriSpeech into ChatGPT and request the generation of corresponding text question-answer pairs. We use the first part of LibriSQA, which presents questions without options, and the answers are complete sentences. SLUE-SQA-5 is adapted from five text QA datasets, and both the questions and contexts consist of authentic audio recordings. DRCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Shao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib30\" title=\"\">2018</a>)</cite> is originally a Chinese QA dataset. Similar to SQuAD, it is also a many-to-one dataset. We first filter it into a one-to-one dataset and then use the TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib22\" title=\"\">2020</a>)</cite> to synthesize the speech versions of each question-context pair for its training set. <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib19\" title=\"\">2018</a>)</cite> offer spoken version of DRCD&#8217;s development set, which we use for testing.</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "four",
                    "drcd",
                    "model",
                    "context",
                    "dataset",
                    "question",
                    "spokensquad",
                    "asr",
                    "speech",
                    "librisqa",
                    "text",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use 220M Paraformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib11\" title=\"\">2022</a>)</cite> and BGE-base <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib3\" title=\"\">2024</a>)</cite> to build CLSR. BGE is frozen during joint training. We consider two models as baseline: one is the E2E text-speech contrastive model like Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F2\" title=\"Figure 2 &#8227; Preliminary &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, and the other is the cascaded model that first uses ASR model to convert speech into text, followed by a text QA task. For the former, we choose CLAP and SpeechDPR for comparison. For the latter, we use Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib28\" title=\"\">2023</a>)</cite> as the ASR module and BGE-base as the text QA module. The Whisper&#8217;s size is 244M. In the experiment, word error rate (WER) is used to measure the ASR performance, and top-k question-to-context and context-to-question retrieval recall are used to assess retrieval performance. We establish the experimental environment based on Funasr <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib9\" title=\"\">2023</a>)</cite> and ModelScope. The <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> of the loss is set to <math alttext=\"\\frac{1}{3}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m3\" intent=\":literal\"><semantics><mfrac><mn>1</mn><mn>3</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{3}</annotation></semantics></math>. We train the model until convergence, consistently using the Adam optimizer with a learning rate of 5e-5.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "clap",
                    "chen",
                    "model",
                    "speechdpr",
                    "speech",
                    "asr",
                    "bge",
                    "e2e",
                    "text",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SpeechDPR is committed to using text-less data for training. Although they use ASR models and text QA models for knowledge distillation, the scarcity of data hampers its ability to achieve optimal performance. It is worth noting that we do not conduct large-scale pre-training prior to training CLSR. All leading contrastive learning models, such as BGE, have undergone extensive pre-training, which enhances their retrieval capabilities. Nevertheless, CLSR still achieves results that are second only to BGE in clean text retrieval and even surpasses BGE&#8217;s performance on Spoken-SQuAD*, highlighting the advantages of CLSR&#8217;s architecture.</p>\n\n",
                "matched_terms": [
                    "optimal",
                    "speechdpr",
                    "spokensquad",
                    "results",
                    "asr",
                    "bge",
                    "only",
                    "text",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to conventional E2E contrastive models that directly perform text-to-speech or speech-to-speech alignment, CLSR utilizes text-like representations to alleviate the differences between speech and text modalities. It first maps speech representations into text-like representations and then aligns these text-like representations with actual text representations (or aligns text-like representations with other text-like representations) within the text modality. Leveraging the robust performance of text contrastive models, this approach enhances the alignment between speech and text (or between speech and speech), thereby facilitating more accurate pairing with the context most relevant to the question.</p>\n\n",
                "matched_terms": [
                    "context",
                    "question",
                    "speech",
                    "e2e",
                    "text",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When conducting a comparative analysis of CLSR and Whisper+BGE, we find that their retrieval performances on three English datasets are quite similar; however, CLSR demonstrates certain advantages. In terms of transcription ability, CLSR significantly outperforms Whisper+BGE. This indicates that the joint training of CLSR effectively optimizes both the ASR module and the contrastive learning module. Given that Whisper&#8217;s performance in Chinese speech recognition is not exceptional, we have opted not to train Whisper on DRCD*.</p>\n\n",
                "matched_terms": [
                    "drcd",
                    "datasets",
                    "whisperbge",
                    "speech",
                    "asr",
                    "clsr",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the effectiveness of the quantizer and sampler in CLSR, as well as the potential for multi-stage training to improve model performance. We conduct a series of ablation experiments on Spoken-SQuAD. The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T3\" title=\"Table 3 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The first two rows of the results show the value of the quantizer. When the quantizer is not utilized, the model may achieve a lower WER; however, its comparative learning ability significantly diminishes. The top-10 retrieval recall rate of &#8220;CLSR w/o VQ&#8221; is only comparable to the top-1 retrieval recall rate of &#8220;CLSR w/ VQ&#8221;. The results in the sixth and seventh rows show the effectiveness of the sampler. After introducing the sampler, CLSR not only improves retrieval ability, but also improves ASR performance.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "model",
                    "spokensquad",
                    "results",
                    "asr",
                    "clsr",
                    "only",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before joint training, we can pre-train the ASR module and the BGE module of CLSR separately. In the experiment, we use 460 hours of clean LibriSpeech data to pre-train Paraformer, and use Spoken-SQuAD&#8217;s clean text question-context pairs to train BGE. Comparing the second and fourth rows of the experimental results, it is not difficult to find that pre-training the BGE is beneficial; incorporating the pre-trained BGE during joint training enhances various retrieval metrics of the CLSR. In addition, through the comparison between the fourth and sixth rows, it can be found that pre-training Paraformer can improve the model&#8217;s transcription performance while also slightly improving its retrieval ability. It should be noted that in order to improve the training speed of the model, we froze BGE, which has strong retrieval performance, during joint training. Therefore, we can freeze the ASR module after joint training and train BGE for a few epochs separately, which is called post-train in the table. It is hoped that this approach can make BGE better adapt to the text-like representation provided by the ASR module. Unfortunately, post-train can only slightly improve the performance of the model, as evidenced by rows 2 and 3, 4 and 5, 7 and 8 in the table. In short, through ablation experiments, we have shown that both quantizers and samplers are inseparable for CLSR, and that pre-training the ASR module and BGE module of CLSR is of significant importance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "asr",
                    "bge",
                    "only",
                    "text",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess transcription error impact on CLSR&#8217;s retrieval ability, we evaluate it on Spoken-SQuAD (results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.F6\" title=\"Figure 6 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Lower WER correlates with higher recall rates. Crucially, a WER of &#160;16.75% marks a threshold: recall drops significantly above this value.</p>\n\n",
                "matched_terms": [
                    "spokensquad",
                    "retrieval",
                    "results",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to further demonstrate the superiority of the proposed model over the traditional E2E speech-related contrastive model, which consists of two encoders, we construct a new baseline: ParaBGE, to compare the retrieval capability with CLSR. ParaBGE is composed of the speech encoder from Paraformer and the text encoder from BGE. The sizes of each module in both models are identical to those in CLSR. The experimental results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T4\" title=\"Table 4 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. All retrieval metrics of CLSR far exceed ParaBGE, indicating that CLSR has a stronger question-context alignment ability. Although ParaBGE can optimize parameters towards the direction of aligning question and context representation during training, its performance is not ideal. As we mentioned earlier, such model heavily rely on pre-training with large-scale corpora. However, high-quality speech-text pairs are already very scarce, so for E2E speech related retrieval models, it is difficult to achieve excellent results. However, CLSR alleviates the modal differences between speech and text by using text-like representation as a bridge, shifting the alignment of speech to text alignment. With the powerful generalization ability of text contrastive learning models, it can achieve excellent retrieval capabilities comparable to cascade models and text contrastive models without the need for long-term, large-scale pre-training.</p>\n\n",
                "matched_terms": [
                    "proposed",
                    "composed",
                    "model",
                    "context",
                    "question",
                    "speech",
                    "results",
                    "bge",
                    "e2e",
                    "text",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate our model for real-world long audio SQA&#8212;enabling downstream LALMs to simplify inputs and enhance inference speed and accuracy&#8212;we conduct SQA tasks on a modified SLUE-SQA-5 dataset using CLSR and Qwen-Audio. To simulate long-context inference, we replace the contextual audio in 500 randomly selected test instances with full documents from the Spoken Wikipedia corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;hn, Stegen, and Baumann <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib17\" title=\"\">2016</a>)</cite>, each averaging &#160;30 minutes in length.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "dataset",
                    "sluesqa5",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without CLSR, Qwen-Audio directly generates answers from the input speech document and text question. With CLSR, we first segment the speech document into 40-second intervals, assess each segment&#8217;s similarity to the text question, and select the most relevant one as Qwen-Audio&#8217;s contextual input. Prior to testing, both Qwen-Audio and CLSR were trained using the original training subset of SLUE-SQA-5. The results of the testing are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T5\" title=\"Table 5 &#8227; Long-form SQA Evaluation &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Following the application of CLSR for long audio reduction, there is a notable enhancement in Qwen-Audio&#8217;s exact match (EM) and macro-F1 scores for SQA, alongside a tenfold reduction in inference time. These findings underscore the significance of CLSR in the preprocessing of long audio, demonstrating its capacity to not only enhance the inference accuracy of downstream LALM but also to substantially decrease inference time.</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "question",
                    "speech",
                    "results",
                    "clsr",
                    "accuracy",
                    "only",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce CLSR, an E2E contrastive language-speech retriever designed to distill lengthy speech recordings into a limited number of clips that are most pertinent to a given query. By employing a text-like representation as an intermediary state, CLSR exhibits strong capability of cross-modal question-context alignment. Experimental findings demonstrate that CLSR&#8217;s retrieval performance significantly outstrips that of existing E2E speech-related retrievers and is competitive with both cascaded models and text-based retrievers.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "paper",
                    "e2e",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CLSR functions as an E2E model, providing a more rapid inference speed in comparison to pipeline models. The speed is a critical runtime metric for a RAG retriever, which must efficiently interface with downstream LALM for long audio inference. We assess the inference speed of CLSR relative to Whisper+BGE across three datasets, with the results detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.T6\" title=\"Table 6 &#8227; Does CLSR have an advantage over Whisper+BGE pipeline during runtime? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. While CLSR demonstrates a slight enhancement in transcription and retrieval performance compared to Whisper+BGE, it significantly outperforms in terms of inference speed, indicating that CLSR is more suitable as a RAG retriever.</p>\n\n",
                "matched_terms": [
                    "across",
                    "datasets",
                    "whisperbge",
                    "pipeline",
                    "model",
                    "results",
                    "e2e",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of the Whisper+BGE pipeline following fine-tuning with noisy data, we transcribe the training data using a trained Whisper model and subsequently provide it to BGE for pre-training. The results of this experimentation are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.T7\" title=\"Table 7 &#8227; Does CLSR have an advantage over Whisper+BGE pipeline during runtime? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, where the fine-tuned Whisper+BGE is referred to as Whisper+BGE*. The findings suggest that Whisper+BGE* yields only a marginal improvement in retrieval capability on the LibriSQA dataset, while performance declines on the other two datasets. CLSR continues to demonstrate a superior ability in retrieving speech content.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "whisperbge",
                    "pipeline",
                    "model",
                    "dataset",
                    "speech",
                    "results",
                    "bge",
                    "librisqa",
                    "only",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two representative cases from the LibriSQA inference results and visualize the similarity distributions of CLSR and ParaBGE when aligning textual questions with speech contexts in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.F7\" title=\"Figure 7 &#8227; Case Study: Structural Superiority of CLSR over Traditional End-to-End Dual-Encoder Retrievers. &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which demonstrates CLSR&#8217;s architectural superiority over traditional dual-encoder retrievers. As shown, CLSR achieves precise alignment between semantically related audio-text segments (e.g., matching &#8220;mary&#8221; in Case 1 and &#8220;humble grass&#8221; in Case 2), whereas ParaBGE exhibits uniformly distributed similarity scores without capturing token-level correspondences. This granular alignment enables CLSR to identify context segments more relevant to queries, enhancing retrieval accuracy.</p>\n\n",
                "matched_terms": [
                    "librisqa",
                    "context",
                    "speech",
                    "results",
                    "clsr",
                    "accuracy",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CLSR&#8217;s unique architecture facilitates such granular alignment: It first utilizes the CIF module to project acoustic features from time steps to token positions; then uses VQ-based refinement to convert these features into text-like representations; finally, in text space, leverages pre-trained text retrieval model&#8217;s power to align text-like representations with ordinary text representations token by token. Since text-like representations retain acoustic-feature similarity, this achieves fine-grained alignment between acoustic and text representations. This architectural superiority is absent in ParaBGE and similar dual-encoder retrievers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "retrieval"
                ]
            }
        ]
    },
    "Sx4.T3": {
        "caption": "Table 3: Ablation results in Spoken-SQuAD.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Pre-train</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Joint-train</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Post-train</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">ASR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Q-C Retrieval (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">C-Q Retrieval (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">ASR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">BGE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">VQ</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Sampler</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">BGE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">WER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@10</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">16.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">15.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">34.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">44.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">15.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">36.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">46.16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">17.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">42.52</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">71.46</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">78.36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">46.86</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">72.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">79.95</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">17.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">45.11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">75.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">82.90</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.05</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">75.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">83.18</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">17.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">78.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">84.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">49.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">76.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">83.42</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">17.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">78.55</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">84.73</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">50.08</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">77.16</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">83.68</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">16.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">49.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">79.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">85.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">50.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">77.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">84.21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">15.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">49.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">79.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">85.91</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">50.59</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">77.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">84.38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">15.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">49.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">79.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">50.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">77.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">84.56</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "posttrain",
            "uparrow",
            "wer",
            "pretrain",
            "ablation",
            "r10",
            "jointtrain",
            "spokensquad",
            "results",
            "asr",
            "bge",
            "downarrow",
            "retrieval",
            "sampler"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To demonstrate the effectiveness of the quantizer and sampler in CLSR, as well as the potential for multi-stage training to improve model performance. We conduct a series of ablation experiments on Spoken-SQuAD. The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T3\" title=\"Table 3 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The first two rows of the results show the value of the quantizer. When the quantizer is not utilized, the model may achieve a lower WER; however, its comparative learning ability significantly diminishes. The top-10 retrieval recall rate of &#8220;CLSR w/o VQ&#8221; is only comparable to the top-1 retrieval recall rate of &#8220;CLSR w/ VQ&#8221;. The results in the sixth and seventh rows show the effectiveness of the sampler. After introducing the sampler, CLSR not only improves retrieval ability, but also improves ASR performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Significant progress has been made in spoken question answering (SQA) in recent years. However, many existing methods, including large audio language models, struggle with processing long audio. Follow the success of retrieval augmented generation, a speech-related retriever shows promising in help preprocessing long-form speech. But the performance of existing speech-related retrievers is lacking. To address this challenge, we propose CLSR, an end-to-end contrastive language-speech retriever that efficiently extracts question-relevant segments from long audio recordings for downstream SQA task. Unlike conventional speech-text contrastive models, CLSR incorporates an intermediate step that converts acoustic features into text-like representations prior to alignment, thereby more effectively bridging the gap between modalities. Experimental results across four cross-modal retrieval datasets demonstrate that CLSR surpasses both end-to-end speech related retrievers and pipeline approaches combining speech recognition with text retrieval, providing a robust foundation for advancing practical long-form SQA applications.</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose CLSR, an end-to-end (E2E) contrastive language-speech retriever designed to distill lengthy speech recordings into a selection of audio clips that are most pertinent to a given query. These audio clips are used for subsequent LALM inference. Unlike conventional E2E speech-text contrastive learning models, CLSR does not endeavor to align acoustic representations and text representations directly. Instead, it first converts acoustic representations into text-like representations, which are then aligned with actual text representations. The extraction of text-like representations primarily employs continuous integrate-and-fire (CIF) to map acoustic representations from temporal steps to token numbers, followed by the application of a vector quantizer (VQ) based adaptor to refine these acoustic representations into text-like forms. We conduct a comparative analysis of CLSR against standard E2E speech-text retrievers and pipeline retrievers, which integrate speech-to-text models with text contrastive learning models, across four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD. The experimental findings demonstrate that CLSR exhibits superior retrieval performance, suggesting that the use of text-like representations as an intermediary between acoustic and text representations enables CLSR to more effectively discern the similarities and distinctions between these two modalities, thereby enhancing the accuracy of pairing speech with text or speech with speech. The contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "spokensquad",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The CLSR demonstrates superior performance compared to both E2E and cascade speech retrieval systems on four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD.</p>\n\n",
                "matched_terms": [
                    "spokensquad",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, there are many works related to SQA. <cite class=\"ltx_cite ltx_citemacro_citet\">Chuang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib6\" title=\"\">2019</a>)</cite> propose a pre-trained model called SpeechBERT for the E2E SQA task. Through the training stage called initial phonetic spatial joint embedding for audio words, it aligns the generated audio embeddings with the text embeddings produced by BERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib7\" title=\"\">2019</a>)</cite> within the same hidden space. <cite class=\"ltx_cite ltx_citemacro_citet\">Shih et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib31\" title=\"\">2023a</a>)</cite> introduce GSQA, which enables the SQA system to perform abstractive reasoning. They first utilize HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib15\" title=\"\">2021</a>)</cite> to convert the input speech into discrete units, and then employ a sequence-to-sequence SQA model finetuned from the text QA model LongT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib13\" title=\"\">2022</a>)</cite> to generate answers in the form of discrete units. <cite class=\"ltx_cite ltx_citemacro_citet\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib25\" title=\"\">2024</a>)</cite> focus on open-domain SQA in scenarios whose paired speech-text data is unavailable. They propose SpeechDPR, which utilizes a bi-encoder retriever framework and learns a sentence-level semantic representation space by extracting knowledge from a combined model of automatic speech recognition (ASR) and text retrieval. <cite class=\"ltx_cite ltx_citemacro_citet\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib16\" title=\"\">2024</a>)</cite> introduce a retriever that employs deep Q-learning to bypass irrelevant audio segments in longer audio files, thereby enhancing the efficiency of SQA. The latter two articles are related to retriever, which is similar to our paper; however, they have limitations: the performance of the former is inferior to that of the pipeline model, and the latter poorly adapts to the high-dimensional, complex feature space of audio, easily falls into local optima during training due to unbalanced exploration and exploitation.</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">It is important to note that during the initial training phase, no gradient backpropagation is performed, and <math alttext=\"Y^{asr}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p6.m1\" intent=\":literal\"><semantics><msup><mi>Y</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msup><annotation encoding=\"application/x-tex\">Y^{asr}</annotation></semantics></math> is solely utilized to determine the sampling number for the sampler. <math alttext=\"D^{{}^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx4.p6.m2\" intent=\":literal\"><semantics><msup><mi>D</mi><msup><mi/><mo>&#8242;</mo></msup></msup><annotation encoding=\"application/x-tex\">D^{{}^{\\prime}}</annotation></semantics></math> obtained in the second phase is then used to calculate the ASR loss.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "sampler"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments on four datasets: Spoken-SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite>, LibriSQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib42\" title=\"\">2024</a>)</cite>, SLUE-SQA-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib33\" title=\"\">2022</a>)</cite>, and DRCD. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T1\" title=\"Table 1 &#8227; Configuration &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> displays detailed information about these datasets. <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite> use Google text-to-speech (TTS) system to generate the spoken versions of the articles in SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajpurkar <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib29\" title=\"\">2016</a>)</cite>. Given that SQuAD is a many-to-one dataset, where multiple questions correspond to the same context, it is unsuitable for training text-speech retrievers. So, we filter the original Spoken-SQuAD dataset to ensure that each question corresponds one-to-one with its context; the filtered dataset is referred to as Spoken SQuAD*. LibriSQA is adapted from the ASR dataset LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib27\" title=\"\">2015</a>)</cite>. The authors input the textual document of each speech segment from LibriSpeech into ChatGPT and request the generation of corresponding text question-answer pairs. We use the first part of LibriSQA, which presents questions without options, and the answers are complete sentences. SLUE-SQA-5 is adapted from five text QA datasets, and both the questions and contexts consist of authentic audio recordings. DRCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Shao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib30\" title=\"\">2018</a>)</cite> is originally a Chinese QA dataset. Similar to SQuAD, it is also a many-to-one dataset. We first filter it into a one-to-one dataset and then use the TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib22\" title=\"\">2020</a>)</cite> to synthesize the speech versions of each question-context pair for its training set. <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib19\" title=\"\">2018</a>)</cite> offer spoken version of DRCD&#8217;s development set, which we use for testing.</p>\n\n",
                "matched_terms": [
                    "spokensquad",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use 220M Paraformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib11\" title=\"\">2022</a>)</cite> and BGE-base <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib3\" title=\"\">2024</a>)</cite> to build CLSR. BGE is frozen during joint training. We consider two models as baseline: one is the E2E text-speech contrastive model like Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F2\" title=\"Figure 2 &#8227; Preliminary &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, and the other is the cascaded model that first uses ASR model to convert speech into text, followed by a text QA task. For the former, we choose CLAP and SpeechDPR for comparison. For the latter, we use Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib28\" title=\"\">2023</a>)</cite> as the ASR module and BGE-base as the text QA module. The Whisper&#8217;s size is 244M. In the experiment, word error rate (WER) is used to measure the ASR performance, and top-k question-to-context and context-to-question retrieval recall are used to assess retrieval performance. We establish the experimental environment based on Funasr <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib9\" title=\"\">2023</a>)</cite> and ModelScope. The <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> of the loss is set to <math alttext=\"\\frac{1}{3}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m3\" intent=\":literal\"><semantics><mfrac><mn>1</mn><mn>3</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{3}</annotation></semantics></math>. We train the model until convergence, consistently using the Adam optimizer with a learning rate of 5e-5.</p>\n\n",
                "matched_terms": [
                    "bge",
                    "retrieval",
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T2\" title=\"Table 2 &#8227; Main Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison results of CLSR and other models across four datasets. We additionally provide the results of using BGE for clean text question-context retrieval. In terms of E2E text-to-speech contrastive models, the results of CLSR are significantly better than those of CLAP and SpeechDPR. We found that CLAP cannot learn the relevance between text questions and speech contexts effectively on four datasets, suggesting that CLAP is not well-suited for text-to-speech content alignment. In fact, CLAP is more appropriate for sound and text alignment.</p>\n\n",
                "matched_terms": [
                    "bge",
                    "retrieval",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SpeechDPR is committed to using text-less data for training. Although they use ASR models and text QA models for knowledge distillation, the scarcity of data hampers its ability to achieve optimal performance. It is worth noting that we do not conduct large-scale pre-training prior to training CLSR. All leading contrastive learning models, such as BGE, have undergone extensive pre-training, which enhances their retrieval capabilities. Nevertheless, CLSR still achieves results that are second only to BGE in clean text retrieval and even surpasses BGE&#8217;s performance on Spoken-SQuAD*, highlighting the advantages of CLSR&#8217;s architecture.</p>\n\n",
                "matched_terms": [
                    "spokensquad",
                    "asr",
                    "results",
                    "bge",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When conducting a comparative analysis of CLSR and Whisper+BGE, we find that their retrieval performances on three English datasets are quite similar; however, CLSR demonstrates certain advantages. In terms of transcription ability, CLSR significantly outperforms Whisper+BGE. This indicates that the joint training of CLSR effectively optimizes both the ASR module and the contrastive learning module. Given that Whisper&#8217;s performance in Chinese speech recognition is not exceptional, we have opted not to train Whisper on DRCD*.</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before joint training, we can pre-train the ASR module and the BGE module of CLSR separately. In the experiment, we use 460 hours of clean LibriSpeech data to pre-train Paraformer, and use Spoken-SQuAD&#8217;s clean text question-context pairs to train BGE. Comparing the second and fourth rows of the experimental results, it is not difficult to find that pre-training the BGE is beneficial; incorporating the pre-trained BGE during joint training enhances various retrieval metrics of the CLSR. In addition, through the comparison between the fourth and sixth rows, it can be found that pre-training Paraformer can improve the model&#8217;s transcription performance while also slightly improving its retrieval ability. It should be noted that in order to improve the training speed of the model, we froze BGE, which has strong retrieval performance, during joint training. Therefore, we can freeze the ASR module after joint training and train BGE for a few epochs separately, which is called post-train in the table. It is hoped that this approach can make BGE better adapt to the text-like representation provided by the ASR module. Unfortunately, post-train can only slightly improve the performance of the model, as evidenced by rows 2 and 3, 4 and 5, 7 and 8 in the table. In short, through ablation experiments, we have shown that both quantizers and samplers are inseparable for CLSR, and that pre-training the ASR module and BGE module of CLSR is of significant importance.</p>\n\n",
                "matched_terms": [
                    "posttrain",
                    "ablation",
                    "pretrain",
                    "asr",
                    "results",
                    "bge",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess transcription error impact on CLSR&#8217;s retrieval ability, we evaluate it on Spoken-SQuAD (results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.F6\" title=\"Figure 6 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Lower WER correlates with higher recall rates. Crucially, a WER of &#160;16.75% marks a threshold: recall drops significantly above this value.</p>\n\n",
                "matched_terms": [
                    "spokensquad",
                    "retrieval",
                    "results",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to further demonstrate the superiority of the proposed model over the traditional E2E speech-related contrastive model, which consists of two encoders, we construct a new baseline: ParaBGE, to compare the retrieval capability with CLSR. ParaBGE is composed of the speech encoder from Paraformer and the text encoder from BGE. The sizes of each module in both models are identical to those in CLSR. The experimental results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T4\" title=\"Table 4 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. All retrieval metrics of CLSR far exceed ParaBGE, indicating that CLSR has a stronger question-context alignment ability. Although ParaBGE can optimize parameters towards the direction of aligning question and context representation during training, its performance is not ideal. As we mentioned earlier, such model heavily rely on pre-training with large-scale corpora. However, high-quality speech-text pairs are already very scarce, so for E2E speech related retrieval models, it is difficult to achieve excellent results. However, CLSR alleviates the modal differences between speech and text by using text-like representation as a bridge, shifting the alignment of speech to text alignment. With the powerful generalization ability of text contrastive learning models, it can achieve excellent retrieval capabilities comparable to cascade models and text contrastive models without the need for long-term, large-scale pre-training.</p>\n\n",
                "matched_terms": [
                    "bge",
                    "retrieval",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CLSR functions as an E2E model, providing a more rapid inference speed in comparison to pipeline models. The speed is a critical runtime metric for a RAG retriever, which must efficiently interface with downstream LALM for long audio inference. We assess the inference speed of CLSR relative to Whisper+BGE across three datasets, with the results detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.T6\" title=\"Table 6 &#8227; Does CLSR have an advantage over Whisper+BGE pipeline during runtime? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. While CLSR demonstrates a slight enhancement in transcription and retrieval performance compared to Whisper+BGE, it significantly outperforms in terms of inference speed, indicating that CLSR is more suitable as a RAG retriever.</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of the Whisper+BGE pipeline following fine-tuning with noisy data, we transcribe the training data using a trained Whisper model and subsequently provide it to BGE for pre-training. The results of this experimentation are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.T7\" title=\"Table 7 &#8227; Does CLSR have an advantage over Whisper+BGE pipeline during runtime? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, where the fine-tuned Whisper+BGE is referred to as Whisper+BGE*. The findings suggest that Whisper+BGE* yields only a marginal improvement in retrieval capability on the LibriSQA dataset, while performance declines on the other two datasets. CLSR continues to demonstrate a superior ability in retrieving speech content.</p>\n\n",
                "matched_terms": [
                    "bge",
                    "retrieval",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Follow <cite class=\"ltx_cite ltx_citemacro_citet\">Gao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib11\" title=\"\">2022</a>)</cite>, we use the sampler to optimize the training process of CLSR&#8217;s ASR module. The sampler combines the correct embeddings of error tokens in <math alttext=\"E^{c}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SSx3.p1.m1\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>c</mi></msup><annotation encoding=\"application/x-tex\">E^{c}</annotation></semantics></math> into <math alttext=\"E^{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SSx3.p1.m2\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>a</mi></msup><annotation encoding=\"application/x-tex\">E^{a}</annotation></semantics></math>, and generates the semantic features <math alttext=\"E^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.SSx3.p1.m3\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>s</mi></msup><annotation encoding=\"application/x-tex\">E^{s}</annotation></semantics></math>. The specific pseudocode is shown in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#alg1\" title=\"Algorithm 1 &#8227; What is the exact mechanism of the sampler? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "sampler"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two representative cases from the LibriSQA inference results and visualize the similarity distributions of CLSR and ParaBGE when aligning textual questions with speech contexts in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.F7\" title=\"Figure 7 &#8227; Case Study: Structural Superiority of CLSR over Traditional End-to-End Dual-Encoder Retrievers. &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which demonstrates CLSR&#8217;s architectural superiority over traditional dual-encoder retrievers. As shown, CLSR achieves precise alignment between semantically related audio-text segments (e.g., matching &#8220;mary&#8221; in Case 1 and &#8220;humble grass&#8221; in Case 2), whereas ParaBGE exhibits uniformly distributed similarity scores without capturing token-level correspondences. This granular alignment enables CLSR to identify context segments more relevant to queries, enhancing retrieval accuracy.</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "results"
                ]
            }
        ]
    },
    "Sx4.T4": {
        "caption": "Table 4: Comparison results between traditional E2E contrastive model and CLSR.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Paradigm</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">ASR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Q-C Retrieval (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">C-Q Retrieval (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">WER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@10</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Spoken-SQuAD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">ParaBGE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">17.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">38.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">17.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">38.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">15.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">49.82</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">79.63</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">50.63</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">77.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">84.56</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">LibriSQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">ParaBGE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">50.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">59.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">20.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">39.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">49.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">4.09</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">93.36</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">95.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">94.01</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">95.57</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">SLUE-SQA-5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">ParaBGE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">7.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">21.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">32.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">7.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">21.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">33.12</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">E2E</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">16.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">30.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">62.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">74.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">29.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">62.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">73.05</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wer",
            "contrastive",
            "e2e",
            "downarrow",
            "comparison",
            "between",
            "clsr",
            "librisqa",
            "retrieval",
            "traditional",
            "spokensquad",
            "results",
            "asr",
            "parabge",
            "sluesqa5",
            "uparrow",
            "model",
            "r10",
            "paradigm",
            "dataset"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In order to further demonstrate the superiority of the proposed model over the traditional E2E speech-related contrastive model, which consists of two encoders, we construct a new baseline: ParaBGE, to compare the retrieval capability with CLSR. ParaBGE is composed of the speech encoder from Paraformer and the text encoder from BGE. The sizes of each module in both models are identical to those in CLSR. The experimental results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T4\" title=\"Table 4 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. All retrieval metrics of CLSR far exceed ParaBGE, indicating that CLSR has a stronger question-context alignment ability. Although ParaBGE can optimize parameters towards the direction of aligning question and context representation during training, its performance is not ideal. As we mentioned earlier, such model heavily rely on pre-training with large-scale corpora. However, high-quality speech-text pairs are already very scarce, so for E2E speech related retrieval models, it is difficult to achieve excellent results. However, CLSR alleviates the modal differences between speech and text by using text-like representation as a bridge, shifting the alignment of speech to text alignment. With the powerful generalization ability of text contrastive learning models, it can achieve excellent retrieval capabilities comparable to cascade models and text contrastive models without the need for long-term, large-scale pre-training.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Significant progress has been made in spoken question answering (SQA) in recent years. However, many existing methods, including large audio language models, struggle with processing long audio. Follow the success of retrieval augmented generation, a speech-related retriever shows promising in help preprocessing long-form speech. But the performance of existing speech-related retrievers is lacking. To address this challenge, we propose CLSR, an end-to-end contrastive language-speech retriever that efficiently extracts question-relevant segments from long audio recordings for downstream SQA task. Unlike conventional speech-text contrastive models, CLSR incorporates an intermediate step that converts acoustic features into text-like representations prior to alignment, thereby more effectively bridging the gap between modalities. Experimental results across four cross-modal retrieval datasets demonstrate that CLSR surpasses both end-to-end speech related retrievers and pipeline approaches combining speech recognition with text retrieval, providing a robust foundation for advancing practical long-form SQA applications.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "results",
                    "between",
                    "clsr",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of large language models (LLMs) is advancing rapidly. Notable examples, such as GPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib2\" title=\"\">2020</a>)</cite> and LLaMA <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib34\" title=\"\">2023</a>)</cite>, among others <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib24\" title=\"\">2023</a>; Yao, Li, and Zhao <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib39\" title=\"\">2024</a>)</cite>, have demonstrated significant success across various traditional natural language processing (NLP) tasks, including question answering. In the speech domain, numerous large audio language models (LALMs) have emerged, showcasing remarkable capabilities in speech comprehension <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib5\" title=\"\">2023</a>; Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib28\" title=\"\">2023</a>)</cite>. The retrieval-augmented generation (RAG) paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib20\" title=\"\">2020</a>)</cite> enhances LLMs&#8217; natural language understanding by integrating external knowledge <cite class=\"ltx_cite ltx_citemacro_citep\">(Gupta, Ranjan, and Singh <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib14\" title=\"\">2024</a>)</cite>. Specifically, RAG employs a retriever to assess the similarity between user queries and segments within a knowledge database, selecting the top-k most relevant segments as supplementary context for the LLM. This approach enables the LLM to better comprehend queries and generate more accurate responses. Currently, RAG is commonly used for long-context reasoning tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib23\" title=\"\">2025</a>; Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib12\" title=\"\">2025</a>)</cite>. For example, in question-answering tasks that involve lengthy input contexts like full articles, RAG works by identifying and retrieving the most pertinent context segments. This process minimizes the inclusion of irrelevant information, which can otherwise introduce errors into the answer and reduce inference speed. Given the effectiveness of RAG in text-based long-context QA, a pertinent question arises for long-form SQA: Can RAG be similarly employed to extract problem-relevant segments from audio inputs to serve as context for subsequent LALM processing?</p>\n\n",
                "matched_terms": [
                    "paradigm",
                    "traditional",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose CLSR, an end-to-end (E2E) contrastive language-speech retriever designed to distill lengthy speech recordings into a selection of audio clips that are most pertinent to a given query. These audio clips are used for subsequent LALM inference. Unlike conventional E2E speech-text contrastive learning models, CLSR does not endeavor to align acoustic representations and text representations directly. Instead, it first converts acoustic representations into text-like representations, which are then aligned with actual text representations. The extraction of text-like representations primarily employs continuous integrate-and-fire (CIF) to map acoustic representations from temporal steps to token numbers, followed by the application of a vector quantizer (VQ) based adaptor to refine these acoustic representations into text-like forms. We conduct a comparative analysis of CLSR against standard E2E speech-text retrievers and pipeline retrievers, which integrate speech-to-text models with text contrastive learning models, across four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD. The experimental findings demonstrate that CLSR exhibits superior retrieval performance, suggesting that the use of text-like representations as an intermediary between acoustic and text representations enables CLSR to more effectively discern the similarities and distinctions between these two modalities, thereby enhancing the accuracy of pairing speech with text or speech with speech. The contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "contrastive",
                    "spokensquad",
                    "between",
                    "e2e",
                    "librisqa",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The CLSR demonstrates superior performance compared to both E2E and cascade speech retrieval systems on four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD.</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "e2e",
                    "spokensquad",
                    "clsr",
                    "librisqa",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, there are many works related to SQA. <cite class=\"ltx_cite ltx_citemacro_citet\">Chuang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib6\" title=\"\">2019</a>)</cite> propose a pre-trained model called SpeechBERT for the E2E SQA task. Through the training stage called initial phonetic spatial joint embedding for audio words, it aligns the generated audio embeddings with the text embeddings produced by BERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib7\" title=\"\">2019</a>)</cite> within the same hidden space. <cite class=\"ltx_cite ltx_citemacro_citet\">Shih et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib31\" title=\"\">2023a</a>)</cite> introduce GSQA, which enables the SQA system to perform abstractive reasoning. They first utilize HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib15\" title=\"\">2021</a>)</cite> to convert the input speech into discrete units, and then employ a sequence-to-sequence SQA model finetuned from the text QA model LongT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib13\" title=\"\">2022</a>)</cite> to generate answers in the form of discrete units. <cite class=\"ltx_cite ltx_citemacro_citet\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib25\" title=\"\">2024</a>)</cite> focus on open-domain SQA in scenarios whose paired speech-text data is unavailable. They propose SpeechDPR, which utilizes a bi-encoder retriever framework and learns a sentence-level semantic representation space by extracting knowledge from a combined model of automatic speech recognition (ASR) and text retrieval. <cite class=\"ltx_cite ltx_citemacro_citet\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib16\" title=\"\">2024</a>)</cite> introduce a retriever that employs deep Q-learning to bypass irrelevant audio segments in longer audio files, thereby enhancing the efficiency of SQA. The latter two articles are related to retriever, which is similar to our paper; however, they have limitations: the performance of the former is inferior to that of the pipeline model, and the latter poorly adapts to the high-dimensional, complex feature space of audio, easily falls into local optima during training due to unbalanced exploration and exploitation.</p>\n\n",
                "matched_terms": [
                    "e2e",
                    "model",
                    "retrieval",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the inception of GPT, RAG has advanced rapidly <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib41\" title=\"\">2025</a>)</cite>, while research on speech RAG has been comparatively limited. <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib38\" title=\"\">2024</a>)</cite> utilize RAG for spoken language understanding (SLU). They first employ a pre-trained ASR encoder to extract acoustic features. Next, they perform a similarity calculation to identify audio-text label pairs in the training set that are similar, subsequently incorporating this label information into the SLU decoder through a cross-attention mechanism. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib36\" title=\"\">2024</a>)</cite> propose a joint speech and language model based on RAG, which enhances performance on the named entity recognition task. They compute the similarity between the input speech query embeddings and the entity embeddings in the database to extract the K entities most relevant to the query, using these entities as additional inputs to the model. Currently, there is no SRAG model designed for long-form SQA task.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Consider the SQA task, where questions are presented in text format and contexts are delivered in speech format. Let <math alttext=\"X=\\{x_{1},x_{2},\\dots,x_{t}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=\\{x_{1},x_{2},\\dots,x_{t}\\}</annotation></semantics></math> denote the speech context, represented as a sequence of <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> frames. Let <math alttext=\"Z=\\{z_{1},z_{2},\\dots,z_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=\\{z_{1},z_{2},\\dots,z_{m}\\}</annotation></semantics></math> denote the question text, represented as a sequence of <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> tokens, where each token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> belongs to a vocabulary <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m6\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> (i.e., <math alttext=\"z_{i}\\in V\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">z_{i}\\in V</annotation></semantics></math>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F2\" title=\"Figure 2 &#8227; Preliminary &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the architecture of a typical E2E audio-text contrastive model, such as CLAP <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib37\" title=\"\">2023</a>)</cite>. This model employs a speech encoder <math alttext=\"A(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">A(\\cdot)</annotation></semantics></math> and a text encoder <math alttext=\"B(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m9\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B(\\cdot)</annotation></semantics></math> to extract acoustic features and textual features, respectively. The similarity <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m10\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> between the two feature representations is then quantified using cosine similarity. The formula is as follows, where <math alttext=\"||.||\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"Sx3.SSx1.p1.m11\" intent=\":literal\"><semantics><mrow><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0.167em\" rspace=\"0.167em\">.</mo><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">||.||</annotation></semantics></math> denotes the L2 norm.</p>\n\n",
                "matched_terms": [
                    "e2e",
                    "contrastive",
                    "model",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, the model learns to minimize the negative log-likelihood (NLL) loss between the representations of paired questions and contexts. This NLL loss comprises two symmetric components: one for retrieving the context given the question, and the other for retrieving the question given the context. The specific formulation is as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The right half of CLSR is a Transformer-based text encoder that receives either text embeddings or text-like embeddings as input and output corresponding text representations. We obtain the sentence-level representation by inserting the <span class=\"ltx_text ltx_font_typewriter\">CLS</span> token. When aligning the text question with the speech context, we input the text-like embeddings <math alttext=\"E^{Y^{{}^{\\prime}}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p6.m1\" intent=\":literal\"><semantics><msup><mi>E</mi><msup><mi>Y</mi><msup><mi/><mo>&#8242;</mo></msup></msup></msup><annotation encoding=\"application/x-tex\">E^{Y^{{}^{\\prime}}}</annotation></semantics></math> of the context and the text embeddings <math alttext=\"E^{Z}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p6.m2\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>Z</mi></msup><annotation encoding=\"application/x-tex\">E^{Z}</annotation></semantics></math> of the question into the text encoder to obtain their respective sentence-level representations. We then use cosine similarity to evaluate the similarity between them.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments on four datasets: Spoken-SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite>, LibriSQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib42\" title=\"\">2024</a>)</cite>, SLUE-SQA-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib33\" title=\"\">2022</a>)</cite>, and DRCD. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T1\" title=\"Table 1 &#8227; Configuration &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> displays detailed information about these datasets. <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite> use Google text-to-speech (TTS) system to generate the spoken versions of the articles in SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajpurkar <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib29\" title=\"\">2016</a>)</cite>. Given that SQuAD is a many-to-one dataset, where multiple questions correspond to the same context, it is unsuitable for training text-speech retrievers. So, we filter the original Spoken-SQuAD dataset to ensure that each question corresponds one-to-one with its context; the filtered dataset is referred to as Spoken SQuAD*. LibriSQA is adapted from the ASR dataset LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib27\" title=\"\">2015</a>)</cite>. The authors input the textual document of each speech segment from LibriSpeech into ChatGPT and request the generation of corresponding text question-answer pairs. We use the first part of LibriSQA, which presents questions without options, and the answers are complete sentences. SLUE-SQA-5 is adapted from five text QA datasets, and both the questions and contexts consist of authentic audio recordings. DRCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Shao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib30\" title=\"\">2018</a>)</cite> is originally a Chinese QA dataset. Similar to SQuAD, it is also a many-to-one dataset. We first filter it into a one-to-one dataset and then use the TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib22\" title=\"\">2020</a>)</cite> to synthesize the speech versions of each question-context pair for its training set. <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib19\" title=\"\">2018</a>)</cite> offer spoken version of DRCD&#8217;s development set, which we use for testing.</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "model",
                    "dataset",
                    "spokensquad",
                    "asr",
                    "librisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use 220M Paraformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib11\" title=\"\">2022</a>)</cite> and BGE-base <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib3\" title=\"\">2024</a>)</cite> to build CLSR. BGE is frozen during joint training. We consider two models as baseline: one is the E2E text-speech contrastive model like Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F2\" title=\"Figure 2 &#8227; Preliminary &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, and the other is the cascaded model that first uses ASR model to convert speech into text, followed by a text QA task. For the former, we choose CLAP and SpeechDPR for comparison. For the latter, we use Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib28\" title=\"\">2023</a>)</cite> as the ASR module and BGE-base as the text QA module. The Whisper&#8217;s size is 244M. In the experiment, word error rate (WER) is used to measure the ASR performance, and top-k question-to-context and context-to-question retrieval recall are used to assess retrieval performance. We establish the experimental environment based on Funasr <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib9\" title=\"\">2023</a>)</cite> and ModelScope. The <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> of the loss is set to <math alttext=\"\\frac{1}{3}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m3\" intent=\":literal\"><semantics><mfrac><mn>1</mn><mn>3</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{3}</annotation></semantics></math>. We train the model until convergence, consistently using the Adam optimizer with a learning rate of 5e-5.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "contrastive",
                    "model",
                    "asr",
                    "e2e",
                    "retrieval",
                    "comparison",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T2\" title=\"Table 2 &#8227; Main Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison results of CLSR and other models across four datasets. We additionally provide the results of using BGE for clean text question-context retrieval. In terms of E2E text-to-speech contrastive models, the results of CLSR are significantly better than those of CLAP and SpeechDPR. We found that CLAP cannot learn the relevance between text questions and speech contexts effectively on four datasets, suggesting that CLAP is not well-suited for text-to-speech content alignment. In fact, CLAP is more appropriate for sound and text alignment.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "results",
                    "between",
                    "e2e",
                    "retrieval",
                    "comparison",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SpeechDPR is committed to using text-less data for training. Although they use ASR models and text QA models for knowledge distillation, the scarcity of data hampers its ability to achieve optimal performance. It is worth noting that we do not conduct large-scale pre-training prior to training CLSR. All leading contrastive learning models, such as BGE, have undergone extensive pre-training, which enhances their retrieval capabilities. Nevertheless, CLSR still achieves results that are second only to BGE in clean text retrieval and even surpasses BGE&#8217;s performance on Spoken-SQuAD*, highlighting the advantages of CLSR&#8217;s architecture.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "spokensquad",
                    "results",
                    "asr",
                    "clsr",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to conventional E2E contrastive models that directly perform text-to-speech or speech-to-speech alignment, CLSR utilizes text-like representations to alleviate the differences between speech and text modalities. It first maps speech representations into text-like representations and then aligns these text-like representations with actual text representations (or aligns text-like representations with other text-like representations) within the text modality. Leveraging the robust performance of text contrastive models, this approach enhances the alignment between speech and text (or between speech and speech), thereby facilitating more accurate pairing with the context most relevant to the question.</p>\n\n",
                "matched_terms": [
                    "e2e",
                    "contrastive",
                    "clsr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When conducting a comparative analysis of CLSR and Whisper+BGE, we find that their retrieval performances on three English datasets are quite similar; however, CLSR demonstrates certain advantages. In terms of transcription ability, CLSR significantly outperforms Whisper+BGE. This indicates that the joint training of CLSR effectively optimizes both the ASR module and the contrastive learning module. Given that Whisper&#8217;s performance in Chinese speech recognition is not exceptional, we have opted not to train Whisper on DRCD*.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "contrastive",
                    "retrieval",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the effectiveness of the quantizer and sampler in CLSR, as well as the potential for multi-stage training to improve model performance. We conduct a series of ablation experiments on Spoken-SQuAD. The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T3\" title=\"Table 3 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The first two rows of the results show the value of the quantizer. When the quantizer is not utilized, the model may achieve a lower WER; however, its comparative learning ability significantly diminishes. The top-10 retrieval recall rate of &#8220;CLSR w/o VQ&#8221; is only comparable to the top-1 retrieval recall rate of &#8220;CLSR w/ VQ&#8221;. The results in the sixth and seventh rows show the effectiveness of the sampler. After introducing the sampler, CLSR not only improves retrieval ability, but also improves ASR performance.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "model",
                    "spokensquad",
                    "results",
                    "asr",
                    "clsr",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before joint training, we can pre-train the ASR module and the BGE module of CLSR separately. In the experiment, we use 460 hours of clean LibriSpeech data to pre-train Paraformer, and use Spoken-SQuAD&#8217;s clean text question-context pairs to train BGE. Comparing the second and fourth rows of the experimental results, it is not difficult to find that pre-training the BGE is beneficial; incorporating the pre-trained BGE during joint training enhances various retrieval metrics of the CLSR. In addition, through the comparison between the fourth and sixth rows, it can be found that pre-training Paraformer can improve the model&#8217;s transcription performance while also slightly improving its retrieval ability. It should be noted that in order to improve the training speed of the model, we froze BGE, which has strong retrieval performance, during joint training. Therefore, we can freeze the ASR module after joint training and train BGE for a few epochs separately, which is called post-train in the table. It is hoped that this approach can make BGE better adapt to the text-like representation provided by the ASR module. Unfortunately, post-train can only slightly improve the performance of the model, as evidenced by rows 2 and 3, 4 and 5, 7 and 8 in the table. In short, through ablation experiments, we have shown that both quantizers and samplers are inseparable for CLSR, and that pre-training the ASR module and BGE module of CLSR is of significant importance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "between",
                    "clsr",
                    "asr",
                    "retrieval",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess transcription error impact on CLSR&#8217;s retrieval ability, we evaluate it on Spoken-SQuAD (results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.F6\" title=\"Figure 6 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Lower WER correlates with higher recall rates. Crucially, a WER of &#160;16.75% marks a threshold: recall drops significantly above this value.</p>\n\n",
                "matched_terms": [
                    "spokensquad",
                    "retrieval",
                    "results",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate our model for real-world long audio SQA&#8212;enabling downstream LALMs to simplify inputs and enhance inference speed and accuracy&#8212;we conduct SQA tasks on a modified SLUE-SQA-5 dataset using CLSR and Qwen-Audio. To simulate long-context inference, we replace the contextual audio in 500 randomly selected test instances with full documents from the Spoken Wikipedia corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;hn, Stegen, and Baumann <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib17\" title=\"\">2016</a>)</cite>, each averaging &#160;30 minutes in length.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "dataset",
                    "sluesqa5",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without CLSR, Qwen-Audio directly generates answers from the input speech document and text question. With CLSR, we first segment the speech document into 40-second intervals, assess each segment&#8217;s similarity to the text question, and select the most relevant one as Qwen-Audio&#8217;s contextual input. Prior to testing, both Qwen-Audio and CLSR were trained using the original training subset of SLUE-SQA-5. The results of the testing are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T5\" title=\"Table 5 &#8227; Long-form SQA Evaluation &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Following the application of CLSR for long audio reduction, there is a notable enhancement in Qwen-Audio&#8217;s exact match (EM) and macro-F1 scores for SQA, alongside a tenfold reduction in inference time. These findings underscore the significance of CLSR in the preprocessing of long audio, demonstrating its capacity to not only enhance the inference accuracy of downstream LALM but also to substantially decrease inference time.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "sluesqa5",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce CLSR, an E2E contrastive language-speech retriever designed to distill lengthy speech recordings into a limited number of clips that are most pertinent to a given query. By employing a text-like representation as an intermediary state, CLSR exhibits strong capability of cross-modal question-context alignment. Experimental findings demonstrate that CLSR&#8217;s retrieval performance significantly outstrips that of existing E2E speech-related retrievers and is competitive with both cascaded models and text-based retrievers.</p>\n\n",
                "matched_terms": [
                    "e2e",
                    "contrastive",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CLSR functions as an E2E model, providing a more rapid inference speed in comparison to pipeline models. The speed is a critical runtime metric for a RAG retriever, which must efficiently interface with downstream LALM for long audio inference. We assess the inference speed of CLSR relative to Whisper+BGE across three datasets, with the results detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.T6\" title=\"Table 6 &#8227; Does CLSR have an advantage over Whisper+BGE pipeline during runtime? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. While CLSR demonstrates a slight enhancement in transcription and retrieval performance compared to Whisper+BGE, it significantly outperforms in terms of inference speed, indicating that CLSR is more suitable as a RAG retriever.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "clsr",
                    "e2e",
                    "retrieval",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of the Whisper+BGE pipeline following fine-tuning with noisy data, we transcribe the training data using a trained Whisper model and subsequently provide it to BGE for pre-training. The results of this experimentation are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.T7\" title=\"Table 7 &#8227; Does CLSR have an advantage over Whisper+BGE pipeline during runtime? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, where the fine-tuned Whisper+BGE is referred to as Whisper+BGE*. The findings suggest that Whisper+BGE* yields only a marginal improvement in retrieval capability on the LibriSQA dataset, while performance declines on the other two datasets. CLSR continues to demonstrate a superior ability in retrieving speech content.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset",
                    "results",
                    "clsr",
                    "librisqa",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two representative cases from the LibriSQA inference results and visualize the similarity distributions of CLSR and ParaBGE when aligning textual questions with speech contexts in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.F7\" title=\"Figure 7 &#8227; Case Study: Structural Superiority of CLSR over Traditional End-to-End Dual-Encoder Retrievers. &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which demonstrates CLSR&#8217;s architectural superiority over traditional dual-encoder retrievers. As shown, CLSR achieves precise alignment between semantically related audio-text segments (e.g., matching &#8220;mary&#8221; in Case 1 and &#8220;humble grass&#8221; in Case 2), whereas ParaBGE exhibits uniformly distributed similarity scores without capturing token-level correspondences. This granular alignment enables CLSR to identify context segments more relevant to queries, enhancing retrieval accuracy.</p>\n\n",
                "matched_terms": [
                    "traditional",
                    "parabge",
                    "results",
                    "between",
                    "clsr",
                    "librisqa",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CLSR&#8217;s unique architecture facilitates such granular alignment: It first utilizes the CIF module to project acoustic features from time steps to token positions; then uses VQ-based refinement to convert these features into text-like representations; finally, in text space, leverages pre-trained text retrieval model&#8217;s power to align text-like representations with ordinary text representations token by token. Since text-like representations retain acoustic-feature similarity, this achieves fine-grained alignment between acoustic and text representations. This architectural superiority is absent in ParaBGE and similar dual-encoder retrievers.</p>\n\n",
                "matched_terms": [
                    "parabge",
                    "retrieval",
                    "between"
                ]
            }
        ]
    },
    "Sx4.T5": {
        "caption": "Table 5: The effectiveness of CLSR when applied to long audio SQA.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">w/ CLSR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">EM (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">F1 (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Cost time (s)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">SpeedUP</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10005;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">18.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">23.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">7935.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.00X</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">27.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">35.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">783.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">10.13X</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "time",
            "effectiveness",
            "uparrow",
            "audio",
            "1013x",
            "sqa",
            "100x",
            "speedup",
            "long",
            "when",
            "clsr",
            "cost",
            "applied"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Without CLSR, Qwen-Audio directly generates answers from the input speech document and text question. With CLSR, we first segment the speech document into 40-second intervals, assess each segment&#8217;s similarity to the text question, and select the most relevant one as Qwen-Audio&#8217;s contextual input. Prior to testing, both Qwen-Audio and CLSR were trained using the original training subset of SLUE-SQA-5. The results of the testing are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T5\" title=\"Table 5 &#8227; Long-form SQA Evaluation &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Following the application of CLSR for long audio reduction, there is a notable enhancement in Qwen-Audio&#8217;s exact match (EM) and macro-F1 scores for SQA, alongside a tenfold reduction in inference time. These findings underscore the significance of CLSR in the preprocessing of long audio, demonstrating its capacity to not only enhance the inference accuracy of downstream LALM but also to substantially decrease inference time.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Significant progress has been made in spoken question answering (SQA) in recent years. However, many existing methods, including large audio language models, struggle with processing long audio. Follow the success of retrieval augmented generation, a speech-related retriever shows promising in help preprocessing long-form speech. But the performance of existing speech-related retrievers is lacking. To address this challenge, we propose CLSR, an end-to-end contrastive language-speech retriever that efficiently extracts question-relevant segments from long audio recordings for downstream SQA task. Unlike conventional speech-text contrastive models, CLSR incorporates an intermediate step that converts acoustic features into text-like representations prior to alignment, thereby more effectively bridging the gap between modalities. Experimental results across four cross-modal retrieval datasets demonstrate that CLSR surpasses both end-to-end speech related retrievers and pipeline approaches combining speech recognition with text retrieval, providing a robust foundation for advancing practical long-form SQA applications.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "audio",
                    "sqa",
                    "long"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The question answering (QA) task requires the model to find the answer to a question from a given context. When the answer is a specific segment within the context, the task is classified as extractive QA; conversely, if the answer cannot be directly derived from the context and necessitates additional reasoning by the model, it is termed abstractive QA <cite class=\"ltx_cite ltx_citemacro_citep\">(Shih et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib31\" title=\"\">2023a</a>)</cite>. In the realm of spoken question answering, the context is presented in audio format <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite>, and certain complex SQA tasks also require the questions to be delivered in audio format <cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib33\" title=\"\">2022</a>)</cite>. Despite advancements in SQA methodologies <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee, Chen, and Lee <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib18\" title=\"\">2019</a>; You et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib40\" title=\"\">2022</a>)</cite>, the majority of existing SQA models are limited to processing short audio segments (under one minute). However, many real-world dialogue scenarios, such as meetings, lectures, and online discussions, often involve voice recordings that exceed ten minutes, posing challenges for current SQA techniques.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sqa",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of large language models (LLMs) is advancing rapidly. Notable examples, such as GPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib2\" title=\"\">2020</a>)</cite> and LLaMA <cite class=\"ltx_cite ltx_citemacro_citep\">(Touvron et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib34\" title=\"\">2023</a>)</cite>, among others <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib24\" title=\"\">2023</a>; Yao, Li, and Zhao <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib39\" title=\"\">2024</a>)</cite>, have demonstrated significant success across various traditional natural language processing (NLP) tasks, including question answering. In the speech domain, numerous large audio language models (LALMs) have emerged, showcasing remarkable capabilities in speech comprehension <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib5\" title=\"\">2023</a>; Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib28\" title=\"\">2023</a>)</cite>. The retrieval-augmented generation (RAG) paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">(Lewis et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib20\" title=\"\">2020</a>)</cite> enhances LLMs&#8217; natural language understanding by integrating external knowledge <cite class=\"ltx_cite ltx_citemacro_citep\">(Gupta, Ranjan, and Singh <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib14\" title=\"\">2024</a>)</cite>. Specifically, RAG employs a retriever to assess the similarity between user queries and segments within a knowledge database, selecting the top-k most relevant segments as supplementary context for the LLM. This approach enables the LLM to better comprehend queries and generate more accurate responses. Currently, RAG is commonly used for long-context reasoning tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib23\" title=\"\">2025</a>; Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib12\" title=\"\">2025</a>)</cite>. For example, in question-answering tasks that involve lengthy input contexts like full articles, RAG works by identifying and retrieving the most pertinent context segments. This process minimizes the inclusion of irrelevant information, which can otherwise introduce errors into the answer and reduce inference speed. Given the effectiveness of RAG in text-based long-context QA, a pertinent question arises for long-form SQA: Can RAG be similarly employed to extract problem-relevant segments from audio inputs to serve as context for subsequent LALM processing?</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "audio",
                    "sqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose CLSR, an end-to-end (E2E) contrastive language-speech retriever designed to distill lengthy speech recordings into a selection of audio clips that are most pertinent to a given query. These audio clips are used for subsequent LALM inference. Unlike conventional E2E speech-text contrastive learning models, CLSR does not endeavor to align acoustic representations and text representations directly. Instead, it first converts acoustic representations into text-like representations, which are then aligned with actual text representations. The extraction of text-like representations primarily employs continuous integrate-and-fire (CIF) to map acoustic representations from temporal steps to token numbers, followed by the application of a vector quantizer (VQ) based adaptor to refine these acoustic representations into text-like forms. We conduct a comparative analysis of CLSR against standard E2E speech-text retrievers and pipeline retrievers, which integrate speech-to-text models with text contrastive learning models, across four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD. The experimental findings demonstrate that CLSR exhibits superior retrieval performance, suggesting that the use of text-like representations as an intermediary between acoustic and text representations enables CLSR to more effectively discern the similarities and distinctions between these two modalities, thereby enhancing the accuracy of pairing speech with text or speech with speech. The contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, there are many works related to SQA. <cite class=\"ltx_cite ltx_citemacro_citet\">Chuang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib6\" title=\"\">2019</a>)</cite> propose a pre-trained model called SpeechBERT for the E2E SQA task. Through the training stage called initial phonetic spatial joint embedding for audio words, it aligns the generated audio embeddings with the text embeddings produced by BERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib7\" title=\"\">2019</a>)</cite> within the same hidden space. <cite class=\"ltx_cite ltx_citemacro_citet\">Shih et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib31\" title=\"\">2023a</a>)</cite> introduce GSQA, which enables the SQA system to perform abstractive reasoning. They first utilize HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib15\" title=\"\">2021</a>)</cite> to convert the input speech into discrete units, and then employ a sequence-to-sequence SQA model finetuned from the text QA model LongT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib13\" title=\"\">2022</a>)</cite> to generate answers in the form of discrete units. <cite class=\"ltx_cite ltx_citemacro_citet\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib25\" title=\"\">2024</a>)</cite> focus on open-domain SQA in scenarios whose paired speech-text data is unavailable. They propose SpeechDPR, which utilizes a bi-encoder retriever framework and learns a sentence-level semantic representation space by extracting knowledge from a combined model of automatic speech recognition (ASR) and text retrieval. <cite class=\"ltx_cite ltx_citemacro_citet\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib16\" title=\"\">2024</a>)</cite> introduce a retriever that employs deep Q-learning to bypass irrelevant audio segments in longer audio files, thereby enhancing the efficiency of SQA. The latter two articles are related to retriever, which is similar to our paper; however, they have limitations: the performance of the former is inferior to that of the pipeline model, and the latter poorly adapts to the high-dimensional, complex feature space of audio, easily falls into local optima during training due to unbalanced exploration and exploitation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The right half of CLSR is a Transformer-based text encoder that receives either text embeddings or text-like embeddings as input and output corresponding text representations. We obtain the sentence-level representation by inserting the <span class=\"ltx_text ltx_font_typewriter\">CLS</span> token. When aligning the text question with the speech context, we input the text-like embeddings <math alttext=\"E^{Y^{{}^{\\prime}}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p6.m1\" intent=\":literal\"><semantics><msup><mi>E</mi><msup><mi>Y</mi><msup><mi/><mo>&#8242;</mo></msup></msup></msup><annotation encoding=\"application/x-tex\">E^{Y^{{}^{\\prime}}}</annotation></semantics></math> of the context and the text embeddings <math alttext=\"E^{Z}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p6.m2\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>Z</mi></msup><annotation encoding=\"application/x-tex\">E^{Z}</annotation></semantics></math> of the question into the text encoder to obtain their respective sentence-level representations. We then use cosine similarity to evaluate the similarity between them.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When conducting a comparative analysis of CLSR and Whisper+BGE, we find that their retrieval performances on three English datasets are quite similar; however, CLSR demonstrates certain advantages. In terms of transcription ability, CLSR significantly outperforms Whisper+BGE. This indicates that the joint training of CLSR effectively optimizes both the ASR module and the contrastive learning module. Given that Whisper&#8217;s performance in Chinese speech recognition is not exceptional, we have opted not to train Whisper on DRCD*.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the effectiveness of the quantizer and sampler in CLSR, as well as the potential for multi-stage training to improve model performance. We conduct a series of ablation experiments on Spoken-SQuAD. The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T3\" title=\"Table 3 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The first two rows of the results show the value of the quantizer. When the quantizer is not utilized, the model may achieve a lower WER; however, its comparative learning ability significantly diminishes. The top-10 retrieval recall rate of &#8220;CLSR w/o VQ&#8221; is only comparable to the top-1 retrieval recall rate of &#8220;CLSR w/ VQ&#8221;. The results in the sixth and seventh rows show the effectiveness of the sampler. After introducing the sampler, CLSR not only improves retrieval ability, but also improves ASR performance.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "effectiveness",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate our model for real-world long audio SQA&#8212;enabling downstream LALMs to simplify inputs and enhance inference speed and accuracy&#8212;we conduct SQA tasks on a modified SLUE-SQA-5 dataset using CLSR and Qwen-Audio. To simulate long-context inference, we replace the contextual audio in 500 randomly selected test instances with full documents from the Spoken Wikipedia corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;hn, Stegen, and Baumann <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib17\" title=\"\">2016</a>)</cite>, each averaging &#160;30 minutes in length.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "audio",
                    "sqa",
                    "long"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CLSR functions as an E2E model, providing a more rapid inference speed in comparison to pipeline models. The speed is a critical runtime metric for a RAG retriever, which must efficiently interface with downstream LALM for long audio inference. We assess the inference speed of CLSR relative to Whisper+BGE across three datasets, with the results detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.T6\" title=\"Table 6 &#8227; Does CLSR have an advantage over Whisper+BGE pipeline during runtime? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. While CLSR demonstrates a slight enhancement in transcription and retrieval performance compared to Whisper+BGE, it significantly outperforms in terms of inference speed, indicating that CLSR is more suitable as a RAG retriever.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "audio",
                    "long"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two representative cases from the LibriSQA inference results and visualize the similarity distributions of CLSR and ParaBGE when aligning textual questions with speech contexts in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.F7\" title=\"Figure 7 &#8227; Case Study: Structural Superiority of CLSR over Traditional End-to-End Dual-Encoder Retrievers. &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which demonstrates CLSR&#8217;s architectural superiority over traditional dual-encoder retrievers. As shown, CLSR achieves precise alignment between semantically related audio-text segments (e.g., matching &#8220;mary&#8221; in Case 1 and &#8220;humble grass&#8221; in Case 2), whereas ParaBGE exhibits uniformly distributed similarity scores without capturing token-level correspondences. This granular alignment enables CLSR to identify context segments more relevant to queries, enhancing retrieval accuracy.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "when"
                ]
            }
        ]
    },
    "A1.T6": {
        "caption": "Table 6: Runtime comparison results between CLSR and Whisper+BGE pipeline.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">WER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Q-C R@1 (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">C-Q R@1 (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Cost Time (s)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">SpeedUp</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Spoken-SQuAD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper+BGE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">19.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">69.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">67.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3733.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.00X</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">15.14</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">70.03</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">67.84</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">355.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">10.52X</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">LibriSQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper+BGE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">4.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">83.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">85.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1470.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.00X</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">4.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">186.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">7.91X</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">SLUE-SQA-5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper+BGE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">36.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">6141.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1.00X</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">16.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">30.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">29.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">745.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">8.25X</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wer",
            "whisperbge",
            "speedup",
            "100x",
            "downarrow",
            "comparison",
            "time",
            "791x",
            "825x",
            "1052x",
            "between",
            "librisqa",
            "cost",
            "clsr",
            "runtime",
            "pipeline",
            "spokensquad",
            "results",
            "sluesqa5",
            "uparrow",
            "model",
            "dataset"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">CLSR functions as an E2E model, providing a more rapid inference speed in comparison to pipeline models. The speed is a critical runtime metric for a RAG retriever, which must efficiently interface with downstream LALM for long audio inference. We assess the inference speed of CLSR relative to Whisper+BGE across three datasets, with the results detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.T6\" title=\"Table 6 &#8227; Does CLSR have an advantage over Whisper+BGE pipeline during runtime? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. While CLSR demonstrates a slight enhancement in transcription and retrieval performance compared to Whisper+BGE, it significantly outperforms in terms of inference speed, indicating that CLSR is more suitable as a RAG retriever.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Significant progress has been made in spoken question answering (SQA) in recent years. However, many existing methods, including large audio language models, struggle with processing long audio. Follow the success of retrieval augmented generation, a speech-related retriever shows promising in help preprocessing long-form speech. But the performance of existing speech-related retrievers is lacking. To address this challenge, we propose CLSR, an end-to-end contrastive language-speech retriever that efficiently extracts question-relevant segments from long audio recordings for downstream SQA task. Unlike conventional speech-text contrastive models, CLSR incorporates an intermediate step that converts acoustic features into text-like representations prior to alignment, thereby more effectively bridging the gap between modalities. Experimental results across four cross-modal retrieval datasets demonstrate that CLSR surpasses both end-to-end speech related retrievers and pipeline approaches combining speech recognition with text retrieval, providing a robust foundation for advancing practical long-form SQA applications.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "pipeline",
                    "results",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose CLSR, an end-to-end (E2E) contrastive language-speech retriever designed to distill lengthy speech recordings into a selection of audio clips that are most pertinent to a given query. These audio clips are used for subsequent LALM inference. Unlike conventional E2E speech-text contrastive learning models, CLSR does not endeavor to align acoustic representations and text representations directly. Instead, it first converts acoustic representations into text-like representations, which are then aligned with actual text representations. The extraction of text-like representations primarily employs continuous integrate-and-fire (CIF) to map acoustic representations from temporal steps to token numbers, followed by the application of a vector quantizer (VQ) based adaptor to refine these acoustic representations into text-like forms. We conduct a comparative analysis of CLSR against standard E2E speech-text retrievers and pipeline retrievers, which integrate speech-to-text models with text contrastive learning models, across four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD. The experimental findings demonstrate that CLSR exhibits superior retrieval performance, suggesting that the use of text-like representations as an intermediary between acoustic and text representations enables CLSR to more effectively discern the similarities and distinctions between these two modalities, thereby enhancing the accuracy of pairing speech with text or speech with speech. The contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "pipeline",
                    "spokensquad",
                    "between",
                    "clsr",
                    "librisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The CLSR demonstrates superior performance compared to both E2E and cascade speech retrieval systems on four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD.</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "librisqa",
                    "spokensquad",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, there are many works related to SQA. <cite class=\"ltx_cite ltx_citemacro_citet\">Chuang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib6\" title=\"\">2019</a>)</cite> propose a pre-trained model called SpeechBERT for the E2E SQA task. Through the training stage called initial phonetic spatial joint embedding for audio words, it aligns the generated audio embeddings with the text embeddings produced by BERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib7\" title=\"\">2019</a>)</cite> within the same hidden space. <cite class=\"ltx_cite ltx_citemacro_citet\">Shih et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib31\" title=\"\">2023a</a>)</cite> introduce GSQA, which enables the SQA system to perform abstractive reasoning. They first utilize HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib15\" title=\"\">2021</a>)</cite> to convert the input speech into discrete units, and then employ a sequence-to-sequence SQA model finetuned from the text QA model LongT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib13\" title=\"\">2022</a>)</cite> to generate answers in the form of discrete units. <cite class=\"ltx_cite ltx_citemacro_citet\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib25\" title=\"\">2024</a>)</cite> focus on open-domain SQA in scenarios whose paired speech-text data is unavailable. They propose SpeechDPR, which utilizes a bi-encoder retriever framework and learns a sentence-level semantic representation space by extracting knowledge from a combined model of automatic speech recognition (ASR) and text retrieval. <cite class=\"ltx_cite ltx_citemacro_citet\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib16\" title=\"\">2024</a>)</cite> introduce a retriever that employs deep Q-learning to bypass irrelevant audio segments in longer audio files, thereby enhancing the efficiency of SQA. The latter two articles are related to retriever, which is similar to our paper; however, they have limitations: the performance of the former is inferior to that of the pipeline model, and the latter poorly adapts to the high-dimensional, complex feature space of audio, easily falls into local optima during training due to unbalanced exploration and exploitation.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the inception of GPT, RAG has advanced rapidly <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib41\" title=\"\">2025</a>)</cite>, while research on speech RAG has been comparatively limited. <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib38\" title=\"\">2024</a>)</cite> utilize RAG for spoken language understanding (SLU). They first employ a pre-trained ASR encoder to extract acoustic features. Next, they perform a similarity calculation to identify audio-text label pairs in the training set that are similar, subsequently incorporating this label information into the SLU decoder through a cross-attention mechanism. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib36\" title=\"\">2024</a>)</cite> propose a joint speech and language model based on RAG, which enhances performance on the named entity recognition task. They compute the similarity between the input speech query embeddings and the entity embeddings in the database to extract the K entities most relevant to the query, using these entities as additional inputs to the model. Currently, there is no SRAG model designed for long-form SQA task.</p>\n\n",
                "matched_terms": [
                    "model",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Consider the SQA task, where questions are presented in text format and contexts are delivered in speech format. Let <math alttext=\"X=\\{x_{1},x_{2},\\dots,x_{t}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=\\{x_{1},x_{2},\\dots,x_{t}\\}</annotation></semantics></math> denote the speech context, represented as a sequence of <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> frames. Let <math alttext=\"Z=\\{z_{1},z_{2},\\dots,z_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=\\{z_{1},z_{2},\\dots,z_{m}\\}</annotation></semantics></math> denote the question text, represented as a sequence of <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> tokens, where each token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> belongs to a vocabulary <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m6\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> (i.e., <math alttext=\"z_{i}\\in V\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">z_{i}\\in V</annotation></semantics></math>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F2\" title=\"Figure 2 &#8227; Preliminary &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the architecture of a typical E2E audio-text contrastive model, such as CLAP <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib37\" title=\"\">2023</a>)</cite>. This model employs a speech encoder <math alttext=\"A(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">A(\\cdot)</annotation></semantics></math> and a text encoder <math alttext=\"B(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m9\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B(\\cdot)</annotation></semantics></math> to extract acoustic features and textual features, respectively. The similarity <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m10\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> between the two feature representations is then quantified using cosine similarity. The formula is as follows, where <math alttext=\"||.||\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"Sx3.SSx1.p1.m11\" intent=\":literal\"><semantics><mrow><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0.167em\" rspace=\"0.167em\">.</mo><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">||.||</annotation></semantics></math> denotes the L2 norm.</p>\n\n",
                "matched_terms": [
                    "model",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, the model learns to minimize the negative log-likelihood (NLL) loss between the representations of paired questions and contexts. This NLL loss comprises two symmetric components: one for retrieving the context given the question, and the other for retrieving the question given the context. The specific formulation is as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The right half of CLSR is a Transformer-based text encoder that receives either text embeddings or text-like embeddings as input and output corresponding text representations. We obtain the sentence-level representation by inserting the <span class=\"ltx_text ltx_font_typewriter\">CLS</span> token. When aligning the text question with the speech context, we input the text-like embeddings <math alttext=\"E^{Y^{{}^{\\prime}}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p6.m1\" intent=\":literal\"><semantics><msup><mi>E</mi><msup><mi>Y</mi><msup><mi/><mo>&#8242;</mo></msup></msup></msup><annotation encoding=\"application/x-tex\">E^{Y^{{}^{\\prime}}}</annotation></semantics></math> of the context and the text embeddings <math alttext=\"E^{Z}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p6.m2\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>Z</mi></msup><annotation encoding=\"application/x-tex\">E^{Z}</annotation></semantics></math> of the question into the text encoder to obtain their respective sentence-level representations. We then use cosine similarity to evaluate the similarity between them.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments on four datasets: Spoken-SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite>, LibriSQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib42\" title=\"\">2024</a>)</cite>, SLUE-SQA-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib33\" title=\"\">2022</a>)</cite>, and DRCD. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T1\" title=\"Table 1 &#8227; Configuration &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> displays detailed information about these datasets. <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite> use Google text-to-speech (TTS) system to generate the spoken versions of the articles in SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajpurkar <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib29\" title=\"\">2016</a>)</cite>. Given that SQuAD is a many-to-one dataset, where multiple questions correspond to the same context, it is unsuitable for training text-speech retrievers. So, we filter the original Spoken-SQuAD dataset to ensure that each question corresponds one-to-one with its context; the filtered dataset is referred to as Spoken SQuAD*. LibriSQA is adapted from the ASR dataset LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib27\" title=\"\">2015</a>)</cite>. The authors input the textual document of each speech segment from LibriSpeech into ChatGPT and request the generation of corresponding text question-answer pairs. We use the first part of LibriSQA, which presents questions without options, and the answers are complete sentences. SLUE-SQA-5 is adapted from five text QA datasets, and both the questions and contexts consist of authentic audio recordings. DRCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Shao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib30\" title=\"\">2018</a>)</cite> is originally a Chinese QA dataset. Similar to SQuAD, it is also a many-to-one dataset. We first filter it into a one-to-one dataset and then use the TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib22\" title=\"\">2020</a>)</cite> to synthesize the speech versions of each question-context pair for its training set. <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib19\" title=\"\">2018</a>)</cite> offer spoken version of DRCD&#8217;s development set, which we use for testing.</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "model",
                    "dataset",
                    "spokensquad",
                    "librisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use 220M Paraformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib11\" title=\"\">2022</a>)</cite> and BGE-base <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib3\" title=\"\">2024</a>)</cite> to build CLSR. BGE is frozen during joint training. We consider two models as baseline: one is the E2E text-speech contrastive model like Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F2\" title=\"Figure 2 &#8227; Preliminary &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, and the other is the cascaded model that first uses ASR model to convert speech into text, followed by a text QA task. For the former, we choose CLAP and SpeechDPR for comparison. For the latter, we use Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib28\" title=\"\">2023</a>)</cite> as the ASR module and BGE-base as the text QA module. The Whisper&#8217;s size is 244M. In the experiment, word error rate (WER) is used to measure the ASR performance, and top-k question-to-context and context-to-question retrieval recall are used to assess retrieval performance. We establish the experimental environment based on Funasr <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib9\" title=\"\">2023</a>)</cite> and ModelScope. The <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> of the loss is set to <math alttext=\"\\frac{1}{3}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m3\" intent=\":literal\"><semantics><mfrac><mn>1</mn><mn>3</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{3}</annotation></semantics></math>. We train the model until convergence, consistently using the Adam optimizer with a learning rate of 5e-5.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "model",
                    "comparison",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T2\" title=\"Table 2 &#8227; Main Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison results of CLSR and other models across four datasets. We additionally provide the results of using BGE for clean text question-context retrieval. In terms of E2E text-to-speech contrastive models, the results of CLSR are significantly better than those of CLAP and SpeechDPR. We found that CLAP cannot learn the relevance between text questions and speech contexts effectively on four datasets, suggesting that CLAP is not well-suited for text-to-speech content alignment. In fact, CLAP is more appropriate for sound and text alignment.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "comparison",
                    "results",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SpeechDPR is committed to using text-less data for training. Although they use ASR models and text QA models for knowledge distillation, the scarcity of data hampers its ability to achieve optimal performance. It is worth noting that we do not conduct large-scale pre-training prior to training CLSR. All leading contrastive learning models, such as BGE, have undergone extensive pre-training, which enhances their retrieval capabilities. Nevertheless, CLSR still achieves results that are second only to BGE in clean text retrieval and even surpasses BGE&#8217;s performance on Spoken-SQuAD*, highlighting the advantages of CLSR&#8217;s architecture.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "spokensquad",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to conventional E2E contrastive models that directly perform text-to-speech or speech-to-speech alignment, CLSR utilizes text-like representations to alleviate the differences between speech and text modalities. It first maps speech representations into text-like representations and then aligns these text-like representations with actual text representations (or aligns text-like representations with other text-like representations) within the text modality. Leveraging the robust performance of text contrastive models, this approach enhances the alignment between speech and text (or between speech and speech), thereby facilitating more accurate pairing with the context most relevant to the question.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When conducting a comparative analysis of CLSR and Whisper+BGE, we find that their retrieval performances on three English datasets are quite similar; however, CLSR demonstrates certain advantages. In terms of transcription ability, CLSR significantly outperforms Whisper+BGE. This indicates that the joint training of CLSR effectively optimizes both the ASR module and the contrastive learning module. Given that Whisper&#8217;s performance in Chinese speech recognition is not exceptional, we have opted not to train Whisper on DRCD*.</p>\n\n",
                "matched_terms": [
                    "whisperbge",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the effectiveness of the quantizer and sampler in CLSR, as well as the potential for multi-stage training to improve model performance. We conduct a series of ablation experiments on Spoken-SQuAD. The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T3\" title=\"Table 3 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The first two rows of the results show the value of the quantizer. When the quantizer is not utilized, the model may achieve a lower WER; however, its comparative learning ability significantly diminishes. The top-10 retrieval recall rate of &#8220;CLSR w/o VQ&#8221; is only comparable to the top-1 retrieval recall rate of &#8220;CLSR w/ VQ&#8221;. The results in the sixth and seventh rows show the effectiveness of the sampler. After introducing the sampler, CLSR not only improves retrieval ability, but also improves ASR performance.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "model",
                    "spokensquad",
                    "results",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before joint training, we can pre-train the ASR module and the BGE module of CLSR separately. In the experiment, we use 460 hours of clean LibriSpeech data to pre-train Paraformer, and use Spoken-SQuAD&#8217;s clean text question-context pairs to train BGE. Comparing the second and fourth rows of the experimental results, it is not difficult to find that pre-training the BGE is beneficial; incorporating the pre-trained BGE during joint training enhances various retrieval metrics of the CLSR. In addition, through the comparison between the fourth and sixth rows, it can be found that pre-training Paraformer can improve the model&#8217;s transcription performance while also slightly improving its retrieval ability. It should be noted that in order to improve the training speed of the model, we froze BGE, which has strong retrieval performance, during joint training. Therefore, we can freeze the ASR module after joint training and train BGE for a few epochs separately, which is called post-train in the table. It is hoped that this approach can make BGE better adapt to the text-like representation provided by the ASR module. Unfortunately, post-train can only slightly improve the performance of the model, as evidenced by rows 2 and 3, 4 and 5, 7 and 8 in the table. In short, through ablation experiments, we have shown that both quantizers and samplers are inseparable for CLSR, and that pre-training the ASR module and BGE module of CLSR is of significant importance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "between",
                    "clsr",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess transcription error impact on CLSR&#8217;s retrieval ability, we evaluate it on Spoken-SQuAD (results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.F6\" title=\"Figure 6 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Lower WER correlates with higher recall rates. Crucially, a WER of &#160;16.75% marks a threshold: recall drops significantly above this value.</p>\n\n",
                "matched_terms": [
                    "spokensquad",
                    "results",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to further demonstrate the superiority of the proposed model over the traditional E2E speech-related contrastive model, which consists of two encoders, we construct a new baseline: ParaBGE, to compare the retrieval capability with CLSR. ParaBGE is composed of the speech encoder from Paraformer and the text encoder from BGE. The sizes of each module in both models are identical to those in CLSR. The experimental results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T4\" title=\"Table 4 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. All retrieval metrics of CLSR far exceed ParaBGE, indicating that CLSR has a stronger question-context alignment ability. Although ParaBGE can optimize parameters towards the direction of aligning question and context representation during training, its performance is not ideal. As we mentioned earlier, such model heavily rely on pre-training with large-scale corpora. However, high-quality speech-text pairs are already very scarce, so for E2E speech related retrieval models, it is difficult to achieve excellent results. However, CLSR alleviates the modal differences between speech and text by using text-like representation as a bridge, shifting the alignment of speech to text alignment. With the powerful generalization ability of text contrastive learning models, it can achieve excellent retrieval capabilities comparable to cascade models and text contrastive models without the need for long-term, large-scale pre-training.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "model",
                    "results",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate our model for real-world long audio SQA&#8212;enabling downstream LALMs to simplify inputs and enhance inference speed and accuracy&#8212;we conduct SQA tasks on a modified SLUE-SQA-5 dataset using CLSR and Qwen-Audio. To simulate long-context inference, we replace the contextual audio in 500 randomly selected test instances with full documents from the Spoken Wikipedia corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;hn, Stegen, and Baumann <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib17\" title=\"\">2016</a>)</cite>, each averaging &#160;30 minutes in length.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "dataset",
                    "sluesqa5",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without CLSR, Qwen-Audio directly generates answers from the input speech document and text question. With CLSR, we first segment the speech document into 40-second intervals, assess each segment&#8217;s similarity to the text question, and select the most relevant one as Qwen-Audio&#8217;s contextual input. Prior to testing, both Qwen-Audio and CLSR were trained using the original training subset of SLUE-SQA-5. The results of the testing are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T5\" title=\"Table 5 &#8227; Long-form SQA Evaluation &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Following the application of CLSR for long audio reduction, there is a notable enhancement in Qwen-Audio&#8217;s exact match (EM) and macro-F1 scores for SQA, alongside a tenfold reduction in inference time. These findings underscore the significance of CLSR in the preprocessing of long audio, demonstrating its capacity to not only enhance the inference accuracy of downstream LALM but also to substantially decrease inference time.</p>\n\n",
                "matched_terms": [
                    "time",
                    "results",
                    "sluesqa5",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of the Whisper+BGE pipeline following fine-tuning with noisy data, we transcribe the training data using a trained Whisper model and subsequently provide it to BGE for pre-training. The results of this experimentation are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.T7\" title=\"Table 7 &#8227; Does CLSR have an advantage over Whisper+BGE pipeline during runtime? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, where the fine-tuned Whisper+BGE is referred to as Whisper+BGE*. The findings suggest that Whisper+BGE* yields only a marginal improvement in retrieval capability on the LibriSQA dataset, while performance declines on the other two datasets. CLSR continues to demonstrate a superior ability in retrieving speech content.</p>\n\n",
                "matched_terms": [
                    "whisperbge",
                    "pipeline",
                    "model",
                    "dataset",
                    "results",
                    "clsr",
                    "librisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two representative cases from the LibriSQA inference results and visualize the similarity distributions of CLSR and ParaBGE when aligning textual questions with speech contexts in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.F7\" title=\"Figure 7 &#8227; Case Study: Structural Superiority of CLSR over Traditional End-to-End Dual-Encoder Retrievers. &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which demonstrates CLSR&#8217;s architectural superiority over traditional dual-encoder retrievers. As shown, CLSR achieves precise alignment between semantically related audio-text segments (e.g., matching &#8220;mary&#8221; in Case 1 and &#8220;humble grass&#8221; in Case 2), whereas ParaBGE exhibits uniformly distributed similarity scores without capturing token-level correspondences. This granular alignment enables CLSR to identify context segments more relevant to queries, enhancing retrieval accuracy.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "librisqa",
                    "results",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CLSR&#8217;s unique architecture facilitates such granular alignment: It first utilizes the CIF module to project acoustic features from time steps to token positions; then uses VQ-based refinement to convert these features into text-like representations; finally, in text space, leverages pre-trained text retrieval model&#8217;s power to align text-like representations with ordinary text representations token by token. Since text-like representations retain acoustic-feature similarity, this achieves fine-grained alignment between acoustic and text representations. This architectural superiority is absent in ParaBGE and similar dual-encoder retrievers.</p>\n\n",
                "matched_terms": [
                    "time",
                    "between"
                ]
            }
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Comparison results between E2E and Whisper+BGE pipeline with noisy fine-tuned.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">ASR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Q-C Retrieval (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">C-Q Retrieval (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">WER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@5</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">R@10</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Spoken-SQuAD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper+BGE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">19.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">69.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">86.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">90.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">67.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">89.65</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper+BGE*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">19.39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">69.05</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">86.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">90.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">67.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">85.48</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">89.93</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">15.14</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">70.03</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">86.90</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">90.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">67.84</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">85.69</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">90.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">LibriSQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper+BGE</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">4.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">83.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">93.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">94.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">85.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">93.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">95.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper+BGE*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">4.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">84.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">92.86</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">95.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">83.74</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">93.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">94.85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">4.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">93.36</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">95.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.53</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">94.01</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">95.57</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" rowspan=\"3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">SLUE-SQA-5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper+BGE*</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">36.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">60.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">72.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">60.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">73.47</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper+BGE*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">36.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">23.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">48.49</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">60.41</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">23.05</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">51.34</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">63.18</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">CLSR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">16.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">30.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">62.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">74.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">29.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">62.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">73.05</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wer",
            "whisperbge",
            "finetuned",
            "e2e",
            "downarrow",
            "comparison",
            "between",
            "clsr",
            "librisqa",
            "retrieval",
            "pipeline",
            "spokensquad",
            "results",
            "asr",
            "sluesqa5",
            "uparrow",
            "model",
            "r10",
            "dataset",
            "noisy"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To evaluate the performance of the Whisper+BGE pipeline following fine-tuning with noisy data, we transcribe the training data using a trained Whisper model and subsequently provide it to BGE for pre-training. The results of this experimentation are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.T7\" title=\"Table 7 &#8227; Does CLSR have an advantage over Whisper+BGE pipeline during runtime? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, where the fine-tuned Whisper+BGE is referred to as Whisper+BGE*. The findings suggest that Whisper+BGE* yields only a marginal improvement in retrieval capability on the LibriSQA dataset, while performance declines on the other two datasets. CLSR continues to demonstrate a superior ability in retrieving speech content.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Significant progress has been made in spoken question answering (SQA) in recent years. However, many existing methods, including large audio language models, struggle with processing long audio. Follow the success of retrieval augmented generation, a speech-related retriever shows promising in help preprocessing long-form speech. But the performance of existing speech-related retrievers is lacking. To address this challenge, we propose CLSR, an end-to-end contrastive language-speech retriever that efficiently extracts question-relevant segments from long audio recordings for downstream SQA task. Unlike conventional speech-text contrastive models, CLSR incorporates an intermediate step that converts acoustic features into text-like representations prior to alignment, thereby more effectively bridging the gap between modalities. Experimental results across four cross-modal retrieval datasets demonstrate that CLSR surpasses both end-to-end speech related retrievers and pipeline approaches combining speech recognition with text retrieval, providing a robust foundation for advancing practical long-form SQA applications.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "results",
                    "between",
                    "clsr",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose CLSR, an end-to-end (E2E) contrastive language-speech retriever designed to distill lengthy speech recordings into a selection of audio clips that are most pertinent to a given query. These audio clips are used for subsequent LALM inference. Unlike conventional E2E speech-text contrastive learning models, CLSR does not endeavor to align acoustic representations and text representations directly. Instead, it first converts acoustic representations into text-like representations, which are then aligned with actual text representations. The extraction of text-like representations primarily employs continuous integrate-and-fire (CIF) to map acoustic representations from temporal steps to token numbers, followed by the application of a vector quantizer (VQ) based adaptor to refine these acoustic representations into text-like forms. We conduct a comparative analysis of CLSR against standard E2E speech-text retrievers and pipeline retrievers, which integrate speech-to-text models with text contrastive learning models, across four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD. The experimental findings demonstrate that CLSR exhibits superior retrieval performance, suggesting that the use of text-like representations as an intermediary between acoustic and text representations enables CLSR to more effectively discern the similarities and distinctions between these two modalities, thereby enhancing the accuracy of pairing speech with text or speech with speech. The contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "pipeline",
                    "spokensquad",
                    "between",
                    "e2e",
                    "librisqa",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The CLSR demonstrates superior performance compared to both E2E and cascade speech retrieval systems on four datasets: Spoken-SQuAD, LibriSQA, SLUE-SQA-5, and DRCD.</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "e2e",
                    "spokensquad",
                    "clsr",
                    "librisqa",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Currently, there are many works related to SQA. <cite class=\"ltx_cite ltx_citemacro_citet\">Chuang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib6\" title=\"\">2019</a>)</cite> propose a pre-trained model called SpeechBERT for the E2E SQA task. Through the training stage called initial phonetic spatial joint embedding for audio words, it aligns the generated audio embeddings with the text embeddings produced by BERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib7\" title=\"\">2019</a>)</cite> within the same hidden space. <cite class=\"ltx_cite ltx_citemacro_citet\">Shih et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib31\" title=\"\">2023a</a>)</cite> introduce GSQA, which enables the SQA system to perform abstractive reasoning. They first utilize HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib15\" title=\"\">2021</a>)</cite> to convert the input speech into discrete units, and then employ a sequence-to-sequence SQA model finetuned from the text QA model LongT5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib13\" title=\"\">2022</a>)</cite> to generate answers in the form of discrete units. <cite class=\"ltx_cite ltx_citemacro_citet\">Lin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib25\" title=\"\">2024</a>)</cite> focus on open-domain SQA in scenarios whose paired speech-text data is unavailable. They propose SpeechDPR, which utilizes a bi-encoder retriever framework and learns a sentence-level semantic representation space by extracting knowledge from a combined model of automatic speech recognition (ASR) and text retrieval. <cite class=\"ltx_cite ltx_citemacro_citet\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib16\" title=\"\">2024</a>)</cite> introduce a retriever that employs deep Q-learning to bypass irrelevant audio segments in longer audio files, thereby enhancing the efficiency of SQA. The latter two articles are related to retriever, which is similar to our paper; however, they have limitations: the performance of the former is inferior to that of the pipeline model, and the latter poorly adapts to the high-dimensional, complex feature space of audio, easily falls into local optima during training due to unbalanced exploration and exploitation.</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "pipeline",
                    "model",
                    "asr",
                    "e2e",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the inception of GPT, RAG has advanced rapidly <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib41\" title=\"\">2025</a>)</cite>, while research on speech RAG has been comparatively limited. <cite class=\"ltx_cite ltx_citemacro_citet\">Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib38\" title=\"\">2024</a>)</cite> utilize RAG for spoken language understanding (SLU). They first employ a pre-trained ASR encoder to extract acoustic features. Next, they perform a similarity calculation to identify audio-text label pairs in the training set that are similar, subsequently incorporating this label information into the SLU decoder through a cross-attention mechanism. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib36\" title=\"\">2024</a>)</cite> propose a joint speech and language model based on RAG, which enhances performance on the named entity recognition task. They compute the similarity between the input speech query embeddings and the entity embeddings in the database to extract the K entities most relevant to the query, using these entities as additional inputs to the model. Currently, there is no SRAG model designed for long-form SQA task.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Consider the SQA task, where questions are presented in text format and contexts are delivered in speech format. Let <math alttext=\"X=\\{x_{1},x_{2},\\dots,x_{t}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=\\{x_{1},x_{2},\\dots,x_{t}\\}</annotation></semantics></math> denote the speech context, represented as a sequence of <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> frames. Let <math alttext=\"Z=\\{z_{1},z_{2},\\dots,z_{m}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m3\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><mi>m</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=\\{z_{1},z_{2},\\dots,z_{m}\\}</annotation></semantics></math> denote the question text, represented as a sequence of <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> tokens, where each token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m5\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> belongs to a vocabulary <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m6\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> (i.e., <math alttext=\"z_{i}\\in V\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>&#8712;</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">z_{i}\\in V</annotation></semantics></math>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F2\" title=\"Figure 2 &#8227; Preliminary &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the architecture of a typical E2E audio-text contrastive model, such as CLAP <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib37\" title=\"\">2023</a>)</cite>. This model employs a speech encoder <math alttext=\"A(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m8\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">A(\\cdot)</annotation></semantics></math> and a text encoder <math alttext=\"B(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m9\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">B(\\cdot)</annotation></semantics></math> to extract acoustic features and textual features, respectively. The similarity <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m10\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> between the two feature representations is then quantified using cosine similarity. The formula is as follows, where <math alttext=\"||.||\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"Sx3.SSx1.p1.m11\" intent=\":literal\"><semantics><mrow><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0.167em\" rspace=\"0.167em\">.</mo><mo fence=\"false\" rspace=\"0.167em\" stretchy=\"false\">|</mo><mo fence=\"false\" stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">||.||</annotation></semantics></math> denotes the L2 norm.</p>\n\n",
                "matched_terms": [
                    "e2e",
                    "model",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, the model learns to minimize the negative log-likelihood (NLL) loss between the representations of paired questions and contexts. This NLL loss comprises two symmetric components: one for retrieving the context given the question, and the other for retrieving the question given the context. The specific formulation is as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The right half of CLSR is a Transformer-based text encoder that receives either text embeddings or text-like embeddings as input and output corresponding text representations. We obtain the sentence-level representation by inserting the <span class=\"ltx_text ltx_font_typewriter\">CLS</span> token. When aligning the text question with the speech context, we input the text-like embeddings <math alttext=\"E^{Y^{{}^{\\prime}}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p6.m1\" intent=\":literal\"><semantics><msup><mi>E</mi><msup><mi>Y</mi><msup><mi/><mo>&#8242;</mo></msup></msup></msup><annotation encoding=\"application/x-tex\">E^{Y^{{}^{\\prime}}}</annotation></semantics></math> of the context and the text embeddings <math alttext=\"E^{Z}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p6.m2\" intent=\":literal\"><semantics><msup><mi>E</mi><mi>Z</mi></msup><annotation encoding=\"application/x-tex\">E^{Z}</annotation></semantics></math> of the question into the text encoder to obtain their respective sentence-level representations. We then use cosine similarity to evaluate the similarity between them.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments on four datasets: Spoken-SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite>, LibriSQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib42\" title=\"\">2024</a>)</cite>, SLUE-SQA-5 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shon et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib33\" title=\"\">2022</a>)</cite>, and DRCD. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T1\" title=\"Table 1 &#8227; Configuration &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> displays detailed information about these datasets. <cite class=\"ltx_cite ltx_citemacro_citet\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib21\" title=\"\">2018</a>)</cite> use Google text-to-speech (TTS) system to generate the spoken versions of the articles in SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajpurkar <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib29\" title=\"\">2016</a>)</cite>. Given that SQuAD is a many-to-one dataset, where multiple questions correspond to the same context, it is unsuitable for training text-speech retrievers. So, we filter the original Spoken-SQuAD dataset to ensure that each question corresponds one-to-one with its context; the filtered dataset is referred to as Spoken SQuAD*. LibriSQA is adapted from the ASR dataset LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib27\" title=\"\">2015</a>)</cite>. The authors input the textual document of each speech segment from LibriSpeech into ChatGPT and request the generation of corresponding text question-answer pairs. We use the first part of LibriSQA, which presents questions without options, and the answers are complete sentences. SLUE-SQA-5 is adapted from five text QA datasets, and both the questions and contexts consist of authentic audio recordings. DRCD <cite class=\"ltx_cite ltx_citemacro_citep\">(Shao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib30\" title=\"\">2018</a>)</cite> is originally a Chinese QA dataset. Similar to SQuAD, it is also a many-to-one dataset. We first filter it into a one-to-one dataset and then use the TTS model <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib22\" title=\"\">2020</a>)</cite> to synthesize the speech versions of each question-context pair for its training set. <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib19\" title=\"\">2018</a>)</cite> offer spoken version of DRCD&#8217;s development set, which we use for testing.</p>\n\n",
                "matched_terms": [
                    "sluesqa5",
                    "model",
                    "dataset",
                    "spokensquad",
                    "asr",
                    "librisqa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use 220M Paraformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib11\" title=\"\">2022</a>)</cite> and BGE-base <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib3\" title=\"\">2024</a>)</cite> to build CLSR. BGE is frozen during joint training. We consider two models as baseline: one is the E2E text-speech contrastive model like Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx3.F2\" title=\"Figure 2 &#8227; Preliminary &#8227; Method &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, and the other is the cascaded model that first uses ASR model to convert speech into text, followed by a text QA task. For the former, we choose CLAP and SpeechDPR for comparison. For the latter, we use Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib28\" title=\"\">2023</a>)</cite> as the ASR module and BGE-base as the text QA module. The Whisper&#8217;s size is 244M. In the experiment, word error rate (WER) is used to measure the ASR performance, and top-k question-to-context and context-to-question retrieval recall are used to assess retrieval performance. We establish the experimental environment based on Funasr <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib9\" title=\"\">2023</a>)</cite> and ModelScope. The <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> and <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m2\" intent=\":literal\"><semantics><mi>&#946;</mi><annotation encoding=\"application/x-tex\">\\beta</annotation></semantics></math> of the loss is set to <math alttext=\"\\frac{1}{3}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m3\" intent=\":literal\"><semantics><mfrac><mn>1</mn><mn>3</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{1}{3}</annotation></semantics></math>. We train the model until convergence, consistently using the Adam optimizer with a learning rate of 5e-5.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "model",
                    "asr",
                    "clsr",
                    "e2e",
                    "retrieval",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T2\" title=\"Table 2 &#8227; Main Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the comparison results of CLSR and other models across four datasets. We additionally provide the results of using BGE for clean text question-context retrieval. In terms of E2E text-to-speech contrastive models, the results of CLSR are significantly better than those of CLAP and SpeechDPR. We found that CLAP cannot learn the relevance between text questions and speech contexts effectively on four datasets, suggesting that CLAP is not well-suited for text-to-speech content alignment. In fact, CLAP is more appropriate for sound and text alignment.</p>\n\n",
                "matched_terms": [
                    "between",
                    "results",
                    "clsr",
                    "e2e",
                    "retrieval",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SpeechDPR is committed to using text-less data for training. Although they use ASR models and text QA models for knowledge distillation, the scarcity of data hampers its ability to achieve optimal performance. It is worth noting that we do not conduct large-scale pre-training prior to training CLSR. All leading contrastive learning models, such as BGE, have undergone extensive pre-training, which enhances their retrieval capabilities. Nevertheless, CLSR still achieves results that are second only to BGE in clean text retrieval and even surpasses BGE&#8217;s performance on Spoken-SQuAD*, highlighting the advantages of CLSR&#8217;s architecture.</p>\n\n",
                "matched_terms": [
                    "spokensquad",
                    "asr",
                    "results",
                    "clsr",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared to conventional E2E contrastive models that directly perform text-to-speech or speech-to-speech alignment, CLSR utilizes text-like representations to alleviate the differences between speech and text modalities. It first maps speech representations into text-like representations and then aligns these text-like representations with actual text representations (or aligns text-like representations with other text-like representations) within the text modality. Leveraging the robust performance of text contrastive models, this approach enhances the alignment between speech and text (or between speech and speech), thereby facilitating more accurate pairing with the context most relevant to the question.</p>\n\n",
                "matched_terms": [
                    "e2e",
                    "clsr",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When conducting a comparative analysis of CLSR and Whisper+BGE, we find that their retrieval performances on three English datasets are quite similar; however, CLSR demonstrates certain advantages. In terms of transcription ability, CLSR significantly outperforms Whisper+BGE. This indicates that the joint training of CLSR effectively optimizes both the ASR module and the contrastive learning module. Given that Whisper&#8217;s performance in Chinese speech recognition is not exceptional, we have opted not to train Whisper on DRCD*.</p>\n\n",
                "matched_terms": [
                    "whisperbge",
                    "asr",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the effectiveness of the quantizer and sampler in CLSR, as well as the potential for multi-stage training to improve model performance. We conduct a series of ablation experiments on Spoken-SQuAD. The results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T3\" title=\"Table 3 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The first two rows of the results show the value of the quantizer. When the quantizer is not utilized, the model may achieve a lower WER; however, its comparative learning ability significantly diminishes. The top-10 retrieval recall rate of &#8220;CLSR w/o VQ&#8221; is only comparable to the top-1 retrieval recall rate of &#8220;CLSR w/ VQ&#8221;. The results in the sixth and seventh rows show the effectiveness of the sampler. After introducing the sampler, CLSR not only improves retrieval ability, but also improves ASR performance.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "model",
                    "spokensquad",
                    "results",
                    "asr",
                    "clsr",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Before joint training, we can pre-train the ASR module and the BGE module of CLSR separately. In the experiment, we use 460 hours of clean LibriSpeech data to pre-train Paraformer, and use Spoken-SQuAD&#8217;s clean text question-context pairs to train BGE. Comparing the second and fourth rows of the experimental results, it is not difficult to find that pre-training the BGE is beneficial; incorporating the pre-trained BGE during joint training enhances various retrieval metrics of the CLSR. In addition, through the comparison between the fourth and sixth rows, it can be found that pre-training Paraformer can improve the model&#8217;s transcription performance while also slightly improving its retrieval ability. It should be noted that in order to improve the training speed of the model, we froze BGE, which has strong retrieval performance, during joint training. Therefore, we can freeze the ASR module after joint training and train BGE for a few epochs separately, which is called post-train in the table. It is hoped that this approach can make BGE better adapt to the text-like representation provided by the ASR module. Unfortunately, post-train can only slightly improve the performance of the model, as evidenced by rows 2 and 3, 4 and 5, 7 and 8 in the table. In short, through ablation experiments, we have shown that both quantizers and samplers are inseparable for CLSR, and that pre-training the ASR module and BGE module of CLSR is of significant importance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "between",
                    "clsr",
                    "asr",
                    "retrieval",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess transcription error impact on CLSR&#8217;s retrieval ability, we evaluate it on Spoken-SQuAD (results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.F6\" title=\"Figure 6 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Lower WER correlates with higher recall rates. Crucially, a WER of &#160;16.75% marks a threshold: recall drops significantly above this value.</p>\n\n",
                "matched_terms": [
                    "spokensquad",
                    "retrieval",
                    "results",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to further demonstrate the superiority of the proposed model over the traditional E2E speech-related contrastive model, which consists of two encoders, we construct a new baseline: ParaBGE, to compare the retrieval capability with CLSR. ParaBGE is composed of the speech encoder from Paraformer and the text encoder from BGE. The sizes of each module in both models are identical to those in CLSR. The experimental results are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T4\" title=\"Table 4 &#8227; Ablation Result &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. All retrieval metrics of CLSR far exceed ParaBGE, indicating that CLSR has a stronger question-context alignment ability. Although ParaBGE can optimize parameters towards the direction of aligning question and context representation during training, its performance is not ideal. As we mentioned earlier, such model heavily rely on pre-training with large-scale corpora. However, high-quality speech-text pairs are already very scarce, so for E2E speech related retrieval models, it is difficult to achieve excellent results. However, CLSR alleviates the modal differences between speech and text by using text-like representation as a bridge, shifting the alignment of speech to text alignment. With the powerful generalization ability of text contrastive learning models, it can achieve excellent retrieval capabilities comparable to cascade models and text contrastive models without the need for long-term, large-scale pre-training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "between",
                    "e2e",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate our model for real-world long audio SQA&#8212;enabling downstream LALMs to simplify inputs and enhance inference speed and accuracy&#8212;we conduct SQA tasks on a modified SLUE-SQA-5 dataset using CLSR and Qwen-Audio. To simulate long-context inference, we replace the contextual audio in 500 randomly selected test instances with full documents from the Spoken Wikipedia corpus <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;hn, Stegen, and Baumann <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#bib.bib17\" title=\"\">2016</a>)</cite>, each averaging &#160;30 minutes in length.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "dataset",
                    "sluesqa5",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without CLSR, Qwen-Audio directly generates answers from the input speech document and text question. With CLSR, we first segment the speech document into 40-second intervals, assess each segment&#8217;s similarity to the text question, and select the most relevant one as Qwen-Audio&#8217;s contextual input. Prior to testing, both Qwen-Audio and CLSR were trained using the original training subset of SLUE-SQA-5. The results of the testing are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#Sx4.T5\" title=\"Table 5 &#8227; Long-form SQA Evaluation &#8227; Experiment &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Following the application of CLSR for long audio reduction, there is a notable enhancement in Qwen-Audio&#8217;s exact match (EM) and macro-F1 scores for SQA, alongside a tenfold reduction in inference time. These findings underscore the significance of CLSR in the preprocessing of long audio, demonstrating its capacity to not only enhance the inference accuracy of downstream LALM but also to substantially decrease inference time.</p>\n\n",
                "matched_terms": [
                    "clsr",
                    "sluesqa5",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce CLSR, an E2E contrastive language-speech retriever designed to distill lengthy speech recordings into a limited number of clips that are most pertinent to a given query. By employing a text-like representation as an intermediary state, CLSR exhibits strong capability of cross-modal question-context alignment. Experimental findings demonstrate that CLSR&#8217;s retrieval performance significantly outstrips that of existing E2E speech-related retrievers and is competitive with both cascaded models and text-based retrievers.</p>\n\n",
                "matched_terms": [
                    "e2e",
                    "retrieval",
                    "clsr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CLSR functions as an E2E model, providing a more rapid inference speed in comparison to pipeline models. The speed is a critical runtime metric for a RAG retriever, which must efficiently interface with downstream LALM for long audio inference. We assess the inference speed of CLSR relative to Whisper+BGE across three datasets, with the results detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.T6\" title=\"Table 6 &#8227; Does CLSR have an advantage over Whisper+BGE pipeline during runtime? &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. While CLSR demonstrates a slight enhancement in transcription and retrieval performance compared to Whisper+BGE, it significantly outperforms in terms of inference speed, indicating that CLSR is more suitable as a RAG retriever.</p>\n\n",
                "matched_terms": [
                    "whisperbge",
                    "pipeline",
                    "model",
                    "results",
                    "clsr",
                    "e2e",
                    "retrieval",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select two representative cases from the LibriSQA inference results and visualize the similarity distributions of CLSR and ParaBGE when aligning textual questions with speech contexts in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.09282v1#A1.F7\" title=\"Figure 7 &#8227; Case Study: Structural Superiority of CLSR over Traditional End-to-End Dual-Encoder Retrievers. &#8227; Appendix A Appendix &#8227; End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, which demonstrates CLSR&#8217;s architectural superiority over traditional dual-encoder retrievers. As shown, CLSR achieves precise alignment between semantically related audio-text segments (e.g., matching &#8220;mary&#8221; in Case 1 and &#8220;humble grass&#8221; in Case 2), whereas ParaBGE exhibits uniformly distributed similarity scores without capturing token-level correspondences. This granular alignment enables CLSR to identify context segments more relevant to queries, enhancing retrieval accuracy.</p>\n\n",
                "matched_terms": [
                    "results",
                    "between",
                    "clsr",
                    "librisqa",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">CLSR&#8217;s unique architecture facilitates such granular alignment: It first utilizes the CIF module to project acoustic features from time steps to token positions; then uses VQ-based refinement to convert these features into text-like representations; finally, in text space, leverages pre-trained text retrieval model&#8217;s power to align text-like representations with ordinary text representations token by token. Since text-like representations retain acoustic-feature similarity, this achieves fine-grained alignment between acoustic and text representations. This architectural superiority is absent in ParaBGE and similar dual-encoder retrievers.</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "between"
                ]
            }
        ]
    }
}