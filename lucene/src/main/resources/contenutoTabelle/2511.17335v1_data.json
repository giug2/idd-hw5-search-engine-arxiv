{
    "S6.T1": {
        "caption": "TABLE I: YouCookII [zhou2018towards]:\n“Clip [sec]” show the average clip duration.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">#Videos</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">#Clips</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Clip [sec]</span></th>\n<td class=\"ltx_td ltx_border_tt\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Training</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1173</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">8743</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.7</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Validation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">416</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">3117</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.8</span></td>\n<td class=\"ltx_td ltx_border_bb\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "clip",
            "validation",
            "average",
            "sec”",
            "“clip",
            "sec",
            "show",
            "duration",
            "training",
            "zhou2018towards",
            "youcookii",
            "clips",
            "videos"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our proposed methods were tested using YouCook2 consisting of cooking action video clips aligned with human action instruction in natural language. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T1\" title=\"TABLE I &#8227; VI-A Setup &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> shows the data statistics. In this work, we used the validation set for evaluation since the test set was not publicly available.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment.\nThis paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding.\nThe state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps.\nAlthough actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos.\nFurthermore, this paper proposes a text-conditioning approach\nto feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former.\nExperiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning.\nFurthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "show",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are several works on robotic manipulation actions at a high level that have proposed how instructions can be stored and analyzed&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tenorth2013automated</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2014cognitive</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2015robot</span>]</cite>.\nInitial work utilized contrastive learning to learn a reward function to train reinforcement learning (RL) agents&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sermanet2018time</span>]</cite>.\nMore recently, there have been some works\nusing robot primitives and extraction of cost functions from task videos to enable imitation of demonstrated tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bahl2022human</span>]</cite>.\nThere has also been some work on training perception modules on large sets of manipulation data to simplify learning of manipulation tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">R3M_2022</span>]</cite>. Finally, there has been growing interest in using vision and language for learning diverse robot skills. There are some works on training visual language models using human instruction videos that are well aligned to robot actions to generate action sequences <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Visualtranslating4robot2018</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2D/3D_RN_Robot2021</span>]</cite>.\nThen, multimodal language models have been applied to robot action planning using various features, such as audio, visual, speech, and text, to expand knowledge acquisition of the tasks from human demonstration videos.\nInitially, multimodal scene-understanding-based robot action planning applied an audio-visual transformer trained from cooking videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori23_interspeech</span>]</cite>, where the action sequence was translated from human action descriptions.</p>\n\n",
                "matched_terms": [
                    "videos",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, automatic action planning errors are inevitable because of remaining discrepancies with the training environments and unexpected circumstances.\nTo avoid robots executing incorrect actions, a neural action replanning approach based on human error correction was proposed, where humans intervene to correct the action plans confirmed by robots using natural language before executing the planned actions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>.\nIn this approach, the robot manipulation was segmented into a skill acquisition phase and a knowledge acquisition phase.\nAlthough there have been some other research on language acquisition by robots to find associations between actions, objects, properties, and effects, and to map those associations to language&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">HRI_8616857</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">HRI_6082460</span>]</cite>, it is unrealistic to train models handling a huge vocabulary using human demonstration videos through real human-robot interaction.\nTo transfer (2) action planning trained from human action demonstration videos to (3) robot action generation/execution, a robot simulator was applied to generate and optimize executable robot actions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kai_ICRA25_WS</span>]</cite>.\nIn the next step, we build a system that can handle the real world beyond simulators&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Stone_ICRA25_WS</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025flarerobotlearningimplicit</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_2025_arxiv</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "videos",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this research trend, this paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation.\nThe state-of-the-art approach leveraging AVBLIP generates robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although action sequence dependency can be captured through an entire video to achieve the goal of a long-horizon task, the current single-clip-based approaches do not apply such long context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, the conventional AVBLIP feeds text embeddings into multimodal Q-former and could unexpectedly be degraded due to the high abstraction of the language information in the multimodal semantic space, thereby losing rich information represented in concrete words. To retain the original rich language information, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder that generates robot action confirmation and robot action steps.\nFinally, we show that the long-context Q-former improves the confirmation and action planning by integrating\nVideoLLaMA3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "show",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of AVBLIP consists of two stages: (1) vision-language representation learning with frozen multimodal encoders and (2) vision-to-language generative learning with a frozen LLM.\nIn the second stage, we connect the Q-former to the frozen LLM Decoder and perform multimodal action sequence generation. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the extracted multimodal features from the video clip are converted to token embeddings using the Q-former. The embedding vectors are then projected to the LLM embedding space using a fully-connected layer. Then, the LLM Decoder generates action sequences and descriptions (confirmation sentences) for the given video clip. We use the cross-entropy loss function for the ground-truth sequences in this stage.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The AVBLIP model in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> can generate action sequences and confirmation sentences for short video clips of around 10-20 seconds. However, these clips are part of a longer instruction video, consisting of 5-10 sequential clips that aim at a single goal, e.g., &#8220;cooking meatloaf&#8221;.\nTherefore, incorporating contextual information from the previous and succeeding video clips is a promising extension of the model to generate more accurate sequences, because the video clips are interdependent: for example, some items cooked in a clip may be used in the next clip.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "clips"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F3\" title=\"Figure 3 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows an extended AVBLIP architecture based on a long-context Q-former, which contains two Q-former modules, one processes multi-modal features from the current clip as originally used and the other processes the features from the surrounding clips to utilize them to enhance the output for the current clip, where the surrounding clips could contain the current one.\nBoth Q-formers output token embeddings, which are then combined using a transformer encoder.\nFinally, the LLM decoder receives the combined embeddings to generate micro-step actions for the robot and action description (as a confirmation) for the human.\nThe two Q-formers and the transformer encoder are jointly trained in the same manner as the original AVBLIP framework described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S3\" title=\"III Robot action generation using AVBLIP &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "clips"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance was evaluated using the BLEU-2 and METEOR scores computed between the generated and ground-truth sequences used in the robotics field <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Visualtranslating4robot2018</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2D/3D_RN_Robot2021</span>]</cite>.\nWe applied cross-validation, where one half of the validation set was used to select the best-epoch model, and the other half was used to measure model performance.\nThus, the shown scores are the average over the two subsets.</p>\n\n",
                "matched_terms": [
                    "average",
                    "validation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can see certain gains in BLEU and METEOR scores by using the long-context Q-former. The three rows after the baseline correspond to the results of left-context expansion, where introducing the previous two clips is sufficient, and we do not have to consider the full left context.\nThe next three rows display the results of introducing both left and right contexts, yielding further gains. Similar to the left-context expansion, it is sufficient to include two previous and two following clips to achieve the best quality.\nThese results demonstrate that the contextual information helps action and description generation and the long-context Q-former can effectively utilize that information.\nNote that the training time of the long-context Q-former with full-video context (*,0,*) took 8.2 hours on a single A40 GPU, compared to 5.5 hours for the baseline, when performing two-stage training for 30 epochs with batch size 16. The average inference time was 5.8 seconds compared to 4.1 seconds for the baseline. Thus, the computational cost for the long-context model is not crucial.</p>\n\n",
                "matched_terms": [
                    "clips",
                    "average",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we show the impact of text information on the feature side and the prompt side, i.e., text conditioning.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S7.T6\" title=\"TABLE VI &#8227; VII Analysis and discussions &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a> shows the quality of action sequences and descriptions in different feature combinations.\nThe first row shows the baseline result, where we used &#8220;base&#8221; features consisting of image, video, audio, and subtitle features.\nThe second row shows the result when we removed subtitles from the base features. Compared to the baseline, the BLEU and METEOR scores do not change significantly, meaning that the baseline model did not effectively utilize the subtitle feature.\nThe third row indicates that text conditioning with subtitles substantially improves the sequence quality.\nThe fourth row corresponds to the result when we use subtitles on both sides, showing that adding subtitles to the features does not help.\nWe also used VideoLLaMA descriptions as features and/or text conditioning, and obtained similar results to the case of subtitles.\nThe last two rows show the results when we removed the base features, where we used only subtitles or VideoLLaMA descriptions, because the text may already contain rich semantic information of the video, and therefore, we want to check if the base features are still necessary for this task.\nWe can see substantial degradation of the quality, which means that the multimodal features are still important for this task even though a subtitle or rich video description is provided.\nFurthermore, we found that not all videos have valid subtitles. They include transcripts of unrelated utterances, non-English subtitles, and background music alone, which may not contribute to the performance. Therefore, the multimodal features are essential to compensate for invalid subtitles.</p>\n\n",
                "matched_terms": [
                    "show",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposed a method for robot action sequence and confirmation sentence generation that leverages (a) a long-context Q-former considering left and right context dependency in full videos and (b) a text-conditioned LLM decoder to retain the precise language information.\nWe trained the above proposed models, generating single-arm robot micro-step action sequences and robot action confirmation in natural language using the YouCook2 dataset.\nTo mitigate the sparseness of the human action descriptions, we leveraged the video descriptions obtained from VideoLLaMA3.\nExperimental results show that our proposed long-context Q-former outperformed the baseline model for all metrics.\nWe confirmed that the combination of all multimodal features and text-conditioning performed the best.\nFuture work includes (1) multitask training of robot action steps and action descriptions, (2) comparison with other multimodal fusion approaches such as simple sequence concatenation or pooling+MLP, (3) evaluation for tasks other than cooking, and (4) evaluation using a simulator or a real robot.</p>\n\n",
                "matched_terms": [
                    "show",
                    "videos",
                    "training"
                ]
            }
        ]
    },
    "S6.T2": {
        "caption": "TABLE II: Examples of robot action confirmation sentence and robot action steps. \nObject-level errors are highlighted in red. Corrected object names are highlighted in blue. If the corrected objects are supported by the subtitle, the VideoLLaMA3 description, and the context, they are also highlighted in blue.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Video clip Id</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Source</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Action sequence</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Action description (confirmation)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"6\"><span class=\"ltx_text\" style=\"font-size:90%;\">sjh57ujp52M,6</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Reference</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">pick fish, place fish on towel</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">remove the fish and drain the oil</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Baseline</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">pick </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">spring roll</span><span class=\"ltx_text\" style=\"font-size:90%;\">, place </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">spring roll</span><span class=\"ltx_text\" style=\"font-size:90%;\"> in oil</span>\n</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">remove the </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">patties</span><span class=\"ltx_text\" style=\"font-size:90%;\"> from the oil and drain on paper towels</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Proposed</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">pick </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">fish</span><span class=\"ltx_text\" style=\"font-size:90%;\">, place </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">fish</span><span class=\"ltx_text\" style=\"font-size:90%;\"> on paper </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">towel</span>\n</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">remove the </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">fish</span><span class=\"ltx_text\" style=\"font-size:90%;\"> from the pan and drain on the paper towels</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Subtitle</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">is take them off and I&#8217;m just going to place them on a little bit of paper tissue just to let them drain off so when you serve them up they&#8217;re not all</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_left\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">oily look at that</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">VideoLLaMA3</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">The video showcases the process of frying </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">fish</span><span class=\"ltx_text\" style=\"font-size:90%;\"> in a pan. The </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">fish</span><span class=\"ltx_text\" style=\"font-size:90%;\"> is initially placed into hot oil, where it sizzles and turns golden brown. Once cooked</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_left\" colspan=\"2\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">to perfection, the </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">fish</span><span class=\"ltx_text\" style=\"font-size:90%;\"> is lifted out with a spatula and transferred onto paper </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">towels</span><span class=\"ltx_text\" style=\"font-size:90%;\"> to drain excess oil. This action takes place on a stove top, indicating</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_left\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">that the cooking is taking place at home or in a small kitchen setting.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"6\"><span class=\"ltx_text\" style=\"font-size:90%;\">Z5bpo2sBsl8,2</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Reference</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">pick knife from counter, cut cabbage with knife, place knife on counter</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">continue chopping the cabbage</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Baseline</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">pick </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">potato</span><span class=\"ltx_text\" style=\"font-size:90%;\"> from plate, place </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">potato</span><span class=\"ltx_text\" style=\"font-size:90%;\"> in bowl, pick bowl from counter, place bowl on counter,</span>\n</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">grate some </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">parmesan cheese</span><span class=\"ltx_text\" style=\"font-size:90%;\"> on top of the </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">salad</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">pick knife from counter, cut </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#FF0000;\">potato</span><span class=\"ltx_text\" style=\"font-size:90%;\"> in bowl, place knife on counter</span>\n</td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Proposed</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">pick </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">cabbage</span><span class=\"ltx_text\" style=\"font-size:90%;\"> from plate, place </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">cabbage</span><span class=\"ltx_text\" style=\"font-size:90%;\"> on counter, pick knife from counter, cut </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">cabbage</span>\n</td>\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">chop the </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">cabbage</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">with knife, place knife on counter</span></td>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Subtitle</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">What weight? Probably about 280. Wow. And I&#8217;ve had a&#8230;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">VideoLLaMA3</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"2\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">In the video, a man and a woman are in a kitchen preparing food. The man is talking while standing next to the woman who is cutting </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">cabbage</span><span class=\"ltx_text\" style=\"font-size:90%;\"> on a wooden board.</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Context</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" colspan=\"2\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">[previous] chop the </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">cabbage</span><span class=\"ltx_text\" style=\"font-size:90%;\"> / [next] continue chopping the </span><span class=\"ltx_text\" style=\"font-size:90%;--ltx-fg-color:#0000FF;\">cabbage</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "blue",
            "subtitle",
            "brown",
            "who",
            "cutting",
            "probably",
            "they’re",
            "also",
            "wooden",
            "from",
            "let",
            "top",
            "continue",
            "showcases",
            "talking",
            "names",
            "oily",
            "errors",
            "where",
            "drain",
            "i’ve",
            "i’m",
            "woman",
            "tissue",
            "steps",
            "food",
            "standing",
            "salad",
            "supported",
            "reference",
            "they",
            "you",
            "sizzles",
            "onto",
            "patties",
            "man",
            "not",
            "examples",
            "golden",
            "oil",
            "cooking",
            "look",
            "context",
            "spatula",
            "next",
            "take",
            "sentence",
            "frying",
            "about",
            "roll",
            "bit",
            "pick",
            "indicating",
            "taking",
            "cheese",
            "perfection",
            "initially",
            "proposed",
            "some",
            "hot",
            "description",
            "parmesan",
            "preparing",
            "little",
            "cabbage",
            "what",
            "once",
            "setting",
            "excess",
            "when",
            "z5bpo2sbsl82",
            "object",
            "chopping",
            "remove",
            "transferred",
            "confirmation",
            "them",
            "small",
            "towels",
            "chop",
            "off",
            "into",
            "stove",
            "bowl",
            "cut",
            "wow",
            "weight",
            "red",
            "video",
            "all",
            "paper",
            "robot",
            "baseline",
            "grate",
            "placed",
            "while",
            "out",
            "going",
            "pan",
            "knife",
            "source",
            "towel",
            "just",
            "sjh57ujp52m6",
            "clip",
            "takes",
            "counter",
            "potato",
            "had",
            "fish",
            "serve",
            "action",
            "lifted",
            "home",
            "turns",
            "place",
            "cooked",
            "highlighted",
            "board",
            "spring",
            "objectlevel",
            "corrected",
            "sequence",
            "process",
            "plate",
            "previous",
            "videollama3",
            "kitchen",
            "objects"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T2\" title=\"TABLE II &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> shows examples of generated action sequences and descriptions, where the rows contain the reference, the baseline result, the sequence generated by the proposed method, the speech subtitle, and the VideoLLaMA3 description. They also include generated descriptions for the previous and the following clips in the second example. The proposed method used both long-context expansion and text conditioning.\nIn the first example, the errors in the baseline result are corrected by the proposed method, where the word &#8220;spring roll&#8221; is substituted with the correct word &#8220;fish&#8221;. This shows that the VideoLLaMA description successfully supported the correct word through text conditioning.\nIn the second example, the object &#8220;potato&#8221; in the action sequence and &#8220;parmesan cheese&#8221; in the action description are corrected to &#8220;cabbage&#8221; using the proposed model. This correction could be made using information from the multimodal features of the previous and following clips, as the model predicted the word. Although the VideoLLaMA description also included &#8220;cabbage&#8221;, the error could not be fixed when the long-context features were not used. We also have several examples in which speech subtitles support correcting the action sequences and descriptions, but we omit them due to space limitations.\nAs in those examples, the long context and text conditioning provide helpful information to generate accurate action sequences and descriptions.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment.\nThis paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding.\nThe state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps.\nAlthough actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos.\nFurthermore, this paper proposes a text-conditioning approach\nto feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former.\nExperiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning.\nFurthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "action",
                    "into",
                    "video",
                    "videollama3",
                    "context",
                    "steps",
                    "confirmation",
                    "from",
                    "paper",
                    "robot",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Human-robot interaction using natural language has the potential to be the most effective way for human-robot collaboration for shared tasks in daily life. Human-to-human collaboration is easily achieved because humans share the required knowledge about tasks and surrounding environments, and can understand other humans&#8217; behaviors.\nIf there are unknown things, humans can use natural language to confirm what to do and how to do it.\nSuch fundamental functions for human-robot collaboration can be built using multimodal scene understanding that enables robots to interpret their environment and interact with humans based on such understanding.\nThis goal lies at the intersection of multiple avenues of research in speech understanding, audio event detection, object and action recognition in computer vision, physical sensing/manipulation for robot control, and natural language generation to interact with humans.\nAchieving effective human-robot collaboration requires significant advances in the following areas: (1) multimodal scene understanding for human and robot environments,\n(2) robot action planning based on multimodal understanding and logical and physical affordance,\n(3) robot action generation/execution according to the planning,\n(4) robot action replanning if necessary, and (5) human-robot interaction via dialogue to achieve the goals efficiently.</p>\n\n",
                "matched_terms": [
                    "action",
                    "what",
                    "robot",
                    "object",
                    "about"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are several works on robotic manipulation actions at a high level that have proposed how instructions can be stored and analyzed&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tenorth2013automated</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2014cognitive</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2015robot</span>]</cite>.\nInitial work utilized contrastive learning to learn a reward function to train reinforcement learning (RL) agents&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sermanet2018time</span>]</cite>.\nMore recently, there have been some works\nusing robot primitives and extraction of cost functions from task videos to enable imitation of demonstrated tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bahl2022human</span>]</cite>.\nThere has also been some work on training perception modules on large sets of manipulation data to simplify learning of manipulation tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">R3M_2022</span>]</cite>. Finally, there has been growing interest in using vision and language for learning diverse robot skills. There are some works on training visual language models using human instruction videos that are well aligned to robot actions to generate action sequences <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Visualtranslating4robot2018</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2D/3D_RN_Robot2021</span>]</cite>.\nThen, multimodal language models have been applied to robot action planning using various features, such as audio, visual, speech, and text, to expand knowledge acquisition of the tasks from human demonstration videos.\nInitially, multimodal scene-understanding-based robot action planning applied an audio-visual transformer trained from cooking videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori23_interspeech</span>]</cite>, where the action sequence was translated from human action descriptions.</p>\n\n",
                "matched_terms": [
                    "action",
                    "initially",
                    "proposed",
                    "some",
                    "sequence",
                    "also",
                    "cooking",
                    "from",
                    "robot",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, neural language models such as Large Language Models (LLMs) have been applied to bridge the gaps between sensing information and abstract-level understanding for robot action planning.\nThis framework allows us to implement an end-to-end approach to build a human-robot collaboration system directly from multimodal scene understanding addressing all the steps (1) to (5).\nPromising use cases of LLMs were reported in the robotics research field, such as CLIPort <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cliport2021</span>]</cite> and SayCan <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SayCan2022</span>]</cite> in creating robotic agents that perform open-vocabulary tasks.\nPROGPROMPT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singh2023progprompt</span>]</cite> introduces a programmatic LLM prompt structure that facilitates the generation of plans in diverse environments, robot functionalities, and tasks.\nLLM-POP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Lingfeng_ICRA24</span>]</cite> targets partially observable task planning, where LLMs expand vocabulary and context considerations, while visual grounding LLMs enhance spatial reasoning capabilities. COWP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2023integrating</span>]</cite> introduces an LLM-based open-world task planning system for robots.\nSome works explore using LLMs to directly predict a dense sequence of end-effector poses for robot actions with vision models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kwon2024language</span>]</cite>.\nAnother study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">raman2022planning</span>]</cite> explores re-prompting strategies to enhance the executability and accuracy of LLM-generated plans but relies strictly on templated prompts.\nMultimodal scene understanding has significantly advanced with AV-transformers, and has been applied to robot action planning with human action videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori23_interspeech</span>]</cite>.\nDespite its success, its capability for multimodal reasoning is still limited because it was impractical to cover all combinations of multimodal inputs from the existing videos.</p>\n\n",
                "matched_terms": [
                    "action",
                    "while",
                    "some",
                    "sequence",
                    "context",
                    "steps",
                    "all",
                    "from",
                    "robot",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the discrepancy issues between the training and inference stages due to the data sparsity in multimodal fusion, BLIP2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BLIP2_ICML23</span>]</cite>, narrowing down the semantic space, was applied to train a multimodal large language model (AVBLIP) for robot action planning <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>]</cite>.\nVarious features are embedded into the semantic space of LLMs by training a Q-former (cross-token transformer),\nwhich is an effective method in terms of computational efficiency and information retention compared to conventional fusion methods\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">an2025llmcentricmultimodalfusionsurvey</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wadekar2024evolutionmultimodalmodelarchitectures</span>]</cite>,\nusing both contrastive loss and action generation loss in this approach.\nThe Q-former-based multimodal LLM contributes to enhancing the performance of robot action planning.</p>\n\n",
                "matched_terms": [
                    "action",
                    "into",
                    "robot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, automatic action planning errors are inevitable because of remaining discrepancies with the training environments and unexpected circumstances.\nTo avoid robots executing incorrect actions, a neural action replanning approach based on human error correction was proposed, where humans intervene to correct the action plans confirmed by robots using natural language before executing the planned actions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>.\nIn this approach, the robot manipulation was segmented into a skill acquisition phase and a knowledge acquisition phase.\nAlthough there have been some other research on language acquisition by robots to find associations between actions, objects, properties, and effects, and to map those associations to language&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">HRI_8616857</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">HRI_6082460</span>]</cite>, it is unrealistic to train models handling a huge vocabulary using human demonstration videos through real human-robot interaction.\nTo transfer (2) action planning trained from human action demonstration videos to (3) robot action generation/execution, a robot simulator was applied to generate and optimize executable robot actions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kai_ICRA25_WS</span>]</cite>.\nIn the next step, we build a system that can handle the real world beyond simulators&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Stone_ICRA25_WS</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025flarerobotlearningimplicit</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_2025_arxiv</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "action",
                    "errors",
                    "proposed",
                    "some",
                    "into",
                    "next",
                    "from",
                    "robot",
                    "objects",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this research trend, this paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation.\nThe state-of-the-art approach leveraging AVBLIP generates robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although action sequence dependency can be captured through an entire video to achieve the goal of a long-horizon task, the current single-clip-based approaches do not apply such long context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, the conventional AVBLIP feeds text embeddings into multimodal Q-former and could unexpectedly be degraded due to the high abstraction of the language information in the multimodal semantic space, thereby losing rich information represented in concrete words. To retain the original rich language information, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder that generates robot action confirmation and robot action steps.\nFinally, we show that the long-context Q-former improves the confirmation and action planning by integrating\nVideoLLaMA3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "action",
                    "into",
                    "sequence",
                    "video",
                    "videollama3",
                    "context",
                    "steps",
                    "confirmation",
                    "from",
                    "paper",
                    "robot",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main contributions of this work consist of (a) proposing a long-context Q-former for robot action confirmation sentence generation,\n(b) introducing a text conditioning that feeds text embedding vectors to an LLM decoder to retain original language information, (c) demonstrating the effectiveness of the proposed approaches for robot confirmation sentence and robot action sequence generation in the cooking domain, and\n(d) examining the contribution of VideoLLaMA.</p>\n\n",
                "matched_terms": [
                    "action",
                    "proposed",
                    "sequence",
                    "cooking",
                    "confirmation",
                    "robot",
                    "sentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">a) Robot Action Description:</span>\nYouCook2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>http://youcook2.eecs.umich.edu/</span></span></span>\nis already annotated with human instructions in natural language to describe human cooking action steps, as introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhou2018towards</span>]</cite>.\nThe cooking action steps for each video are annotated with start and end time stamps and English description. An example of the description is &#8220;Grill the tomatoes in a pan and then put them on a plate&#8221; starting from 00:21 and ending at 00:52.</p>\n\n",
                "matched_terms": [
                    "action",
                    "cooking",
                    "video",
                    "description",
                    "steps",
                    "pan",
                    "them",
                    "from",
                    "robot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">b) Robot Micro-step Action:</span>\nIn the work of <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>,\nthe human instructions were translated into\na micro-step action sequence such that a single-arm robot could achieve the same goals as humans demonstrated actions, as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nAlthough a humanoid robot is illustrated in this example, action steps are\ndesigned for comoditized single-arm robots.\nThe micro-step action sequences for single-arm robot action are represented by &#8220;single-arm action&#8221;, &#8220;target object&#8221;, &#8220;preposition&#8221;, and &#8220;place&#8221;, to achieve the same actions as humans.\nWe borrowed the same data conditions used in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>.\nSingle-arm actions were selected from the following 12 candidates:\nOpen, Close, Pick, Place, Pour, Stir, TurnOn, TurnOff, Wipe, Cut, Scoop, Squeeze. The target objects were selected as one of the nouns in the human action instruction as much as possible.</p>\n\n",
                "matched_terms": [
                    "action",
                    "into",
                    "sequence",
                    "place",
                    "steps",
                    "cut",
                    "pick",
                    "from",
                    "robot",
                    "objects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work employs AVBLIP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>]</cite>, where BLIP-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BLIP2_ICML23</span>]</cite>, a vision-language pre-training method, is extended to handle multimodal features.\nBLIP-2 bootstraps from a frozen image encoder and a frozen LLM, where a Querying Transformer (Q-former)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">carion2020end</span>]</cite> is trained to bridge the gap between the vision and text modalities.\nThe image encoder in BLIP-2 is replaced with audio-visual encoders for video, audio, and text feature sequences in AVBLIP.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates an architecture of AVBLIP consisting of a Q-former and an LLM decoder.\nThe Q-former is trained to extract a fixed number of embeddings from a multimodal encoder that outputs sequences with different lengths.\nThe self-attention layers are shared between two transformer submodules: (1) a multimodal transformer that interacts with the frozen audio-visual encoders and (2) a text transformer that works as a text encoder and a text decoder.\nA set of learnable query embeddings is input to the multimodal transformer.\nThe queries interact with each other through self-attention layers and interact with audio-visual features through cross-attention layers.\nThe queries can additionally interact with the text through the same cross-attention layers. Finally, the queries are converted to an output feature.</p>\n\n",
                "matched_terms": [
                    "video",
                    "from",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LLM Decoder generates a sequence of micro-step actions for a single-arm robot from multimodal features aligned to language features obtained by the Q-former.\nThe LLM Decoder is constructed with a frozen LLM and a feed-forward layer. By using the LLM as a decoder, it leverages the LLM&#8217;s inference capabilities when generating action sequences. In this study, we use OPT-2.7B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022opt</span>]</cite> as the LLM.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "from",
                    "when",
                    "robot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of AVBLIP consists of two stages: (1) vision-language representation learning with frozen multimodal encoders and (2) vision-to-language generative learning with a frozen LLM.\nIn the second stage, we connect the Q-former to the frozen LLM Decoder and perform multimodal action sequence generation. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the extracted multimodal features from the video clip are converted to token embeddings using the Q-former. The embedding vectors are then projected to the LLM embedding space using a fully-connected layer. Then, the LLM Decoder generates action sequences and descriptions (confirmation sentences) for the given video clip. We use the cross-entropy loss function for the ground-truth sequences in this stage.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "action",
                    "sequence",
                    "video",
                    "confirmation",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The AVBLIP model in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> can generate action sequences and confirmation sentences for short video clips of around 10-20 seconds. However, these clips are part of a longer instruction video, consisting of 5-10 sequential clips that aim at a single goal, e.g., &#8220;cooking meatloaf&#8221;.\nTherefore, incorporating contextual information from the previous and succeeding video clips is a promising extension of the model to generate more accurate sequences, because the video clips are interdependent: for example, some items cooked in a clip may be used in the next clip.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "action",
                    "previous",
                    "some",
                    "video",
                    "cooked",
                    "confirmation",
                    "next",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F3\" title=\"Figure 3 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows an extended AVBLIP architecture based on a long-context Q-former, which contains two Q-former modules, one processes multi-modal features from the current clip as originally used and the other processes the features from the surrounding clips to utilize them to enhance the output for the current clip, where the surrounding clips could contain the current one.\nBoth Q-formers output token embeddings, which are then combined using a transformer encoder.\nFinally, the LLM decoder receives the combined embeddings to generate micro-step actions for the robot and action description (as a confirmation) for the human.\nThe two Q-formers and the transformer encoder are jointly trained in the same manner as the original AVBLIP framework described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S3\" title=\"III Robot action generation using AVBLIP &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "action",
                    "description",
                    "confirmation",
                    "from",
                    "them",
                    "robot",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>, the AVBLIP model can generate both micro-step action sequences and action descriptions by feeding query token embeddings to a pre-trained LLM, where the LLM will not further be trained or finetuned.\nThus, the Q-former is trained to generate token embeddings that convey the semantic information of the video clip as well as instructions to the LLM to generate a micro-step action sequence or an action description.\nAlthough the set of embeddings is a compact and effective representation, it may not retain the detailed information necessary for the LLM to generate fine-grained action sequences.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "action",
                    "sequence",
                    "video",
                    "description",
                    "not",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In principle, directly feeding multi-modal features and a natural language instruction to the LLM could be more effective in generating accurate action sequences.\nHowever, it is challenging to fine-tune the LLM to understand the multimodal features using limited training data and further to manually create prompts that generate specialized sequences, such as robot micro-step actions. In other words, the Q-former-based approach remains beneficial for obtaining suitable token embeddings, which serve as prompts for the pre-trained LLM.\nHowever, there is a concern that precise semantic information could be lost. In particular, text information, such as speech subtitles obtained by automatic speech recognition, often contains exact keywords indicating specific ingredients and utensils. Therefore, it is reasonable that only the text information is fed to the LLM directly with minimal information loss.</p>\n\n",
                "matched_terms": [
                    "action",
                    "serve",
                    "indicating",
                    "robot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work applies text conditioning to the LLM by prepending the text information to the Q-former token embeddings as in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F3\" title=\"Figure 3 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, where the text is tokenized and embedded by the same LLM.\nThe text can be subtitles in the video and/or automatic video descriptions generated by an external multimodal LLM such as VideoLLaMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2023videollama</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">videoSALMONN</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "video",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed methods were tested using YouCook2 consisting of cooking action video clips aligned with human action instruction in natural language. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T1\" title=\"TABLE I &#8227; VI-A Setup &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> shows the data statistics. In this work, we used the validation set for evaluation since the test set was not publicly available.</p>\n\n",
                "matched_terms": [
                    "action",
                    "proposed",
                    "cooking",
                    "video",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We borrowed micro-step action sequences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>.\nThe action set contains 2,790 unique phrases of 195 verbs, 2,229 objects, 33 prepositions, and 1002 places.</p>\n\n",
                "matched_terms": [
                    "action",
                    "objects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal features such as video, image, and audio were extracted using Omnivore <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">girdhar2022omnivore</span>]</cite>, Contrastive Language-Image Pre-Training (CLIP) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2021learning</span>]</cite>, and Audio Spectrogram Transformer (AST) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong21b_interspeech</span>]</cite>, respectively.\nThe image and video features were concatenated and projected to a single video feature sequence before feeding them to the encoder.\nIf a subtitle was available in the video, text features were extracted by Glove word embedding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pennington2014glove</span>]</cite>. Otherwise, we fed an embedding vector for the <span class=\"ltx_text ltx_font_typewriter\">&lt;unk&gt;</span> label.\nThe numbers of dimensions of the audio, visual, and text features are <math alttext=\"768\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mn>768</mn><annotation encoding=\"application/x-tex\">768</annotation></semantics></math>, <math alttext=\"1024\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m2\" intent=\":literal\"><semantics><mn>1024</mn><annotation encoding=\"application/x-tex\">1024</annotation></semantics></math>, and <math alttext=\"300\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m3\" intent=\":literal\"><semantics><mn>300</mn><annotation encoding=\"application/x-tex\">300</annotation></semantics></math>, respectively.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "sequence",
                    "video",
                    "them",
                    "subtitle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialized the Q-former with the pre-trained weights of BERTbase&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span>]</cite>, while the cross-attention layers were randomly initialized. We set the number of dimensions of the hidden layers to 768, which results in 188M parameters in total.\nIn the experiments, we used 32 queries, where each query has a 768-dimensional vector, which equals the hidden dimension of the Q-former.\nFor the long-context Q-former, we used 32 queries for each of the Q-former modules.\nThe transformer encoder to combine the contextual and current token embeddings had two transformer blocks, each of which had a self-attention layer and a full-connected layer with 768-dimensional hidden activations. These Q-formers and the transformer were jointly trained from scratch without any shared parameters.\nFor the text conditioning, speech subtitles annotated by YouTube and video descriptions obtained from VideoLLaMA3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite> were fed into the LLM decoder.</p>\n\n",
                "matched_terms": [
                    "while",
                    "into",
                    "video",
                    "videollama3",
                    "from",
                    "had",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T3\" title=\"TABLE III &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> shows the quality of action sequences and descriptions generated by the long-context Q-former.\n&#8220;Baseline&#8221; denotes the Q-former model trained with pairs of multimodal features and their target sequences without contextual information.\nUnlike the method in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite> that trains a single model to generate both action sequences and descriptions in a multitask manner, our baseline model was trained to generate action sequences and descriptions separately. Although we can use the same technique to achieve slightly better quality, we omitted that process for simplicity. In addition, our baseline model already achieves a quality comparable to the result of multitask training in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>. This could be because there are some differences in the hyperparameter setting, where we tuned the batch size, the learning rate, and the number of epochs in preliminary experiments.</p>\n\n",
                "matched_terms": [
                    "action",
                    "some",
                    "process",
                    "setting",
                    "baseline",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can see certain gains in BLEU and METEOR scores by using the long-context Q-former. The three rows after the baseline correspond to the results of left-context expansion, where introducing the previous two clips is sufficient, and we do not have to consider the full left context.\nThe next three rows display the results of introducing both left and right contexts, yielding further gains. Similar to the left-context expansion, it is sufficient to include two previous and two following clips to achieve the best quality.\nThese results demonstrate that the contextual information helps action and description generation and the long-context Q-former can effectively utilize that information.\nNote that the training time of the long-context Q-former with full-video context (*,0,*) took 8.2 hours on a single A40 GPU, compared to 5.5 hours for the baseline, when performing two-stage training for 30 epochs with batch size 16. The average inference time was 5.8 seconds compared to 4.1 seconds for the baseline. Thus, the computational cost for the long-context model is not crucial.</p>\n\n",
                "matched_terms": [
                    "action",
                    "previous",
                    "context",
                    "description",
                    "next",
                    "when",
                    "baseline",
                    "not",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T4\" title=\"TABLE IV &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> shows the quality of action sequences and descriptions generated by the Q-former with text conditioning.\nFirst, speech subtitles provided a substantial gain from the baseline in all metrics.\nThis demonstrates that text conditioning of the LLM decoder is very effective for this task.\nThen, we introduced a different text conditioning with video descriptions generated by the VideoLLaMA 7B model that achieves state-of-the-art performance in multiple video understanding benchmarks <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite>.\nTo ask VideoLLaMA3 to generate rich video descriptions for text conditioning, we used a simple prompt &#8220;Describe the cooking video in detail.&#8221;\nThis approach also improved the quality of the action sequences and descriptions, which are close to those with speech subtitles.\nThen, we concatenated two text sequences and fed them to the LLM (&#8220;VideoLLaMA3+subtitle&#8221;), and obtained further improvement.\nThis result shows that rich and precise information is very useful when it is provided as an LLM&#8217;s context.\nFurther, we tested the cases that use generated action descriptions and ground-truth descriptions.\nWith the descriptions generated by VideoLLaMA3+subtitle conditioning, the action sequence quality degraded. This is because the generated descriptions were much shorter than those of VideoLLaMA3+subtitle, and a certain level of information could be lost.\nThe ground-truth descriptions were concise, yet they provided accurate descriptions, achieving the highest quality, which is considered the upper bound of the text conditioning approach.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "also",
                    "cooking",
                    "video",
                    "videollama3",
                    "context",
                    "all",
                    "from",
                    "them",
                    "when",
                    "baseline",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we combined the long-context Q-former with text conditioning.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T5\" title=\"TABLE V &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> shows the quality of the generated sequences using the combined approach.\nThe results indicate that the performance gains by long-context Q-former and text conditioning are additive, achieving the best BLEU and METEOR scores.\nThe relative gains of the scores from the baseline are 16.7% and 11.9% for action sequences and 17.9% and 15.1% for action descriptions, respectively.</p>\n\n",
                "matched_terms": [
                    "action",
                    "baseline",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we show the impact of text information on the feature side and the prompt side, i.e., text conditioning.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S7.T6\" title=\"TABLE VI &#8227; VII Analysis and discussions &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a> shows the quality of action sequences and descriptions in different feature combinations.\nThe first row shows the baseline result, where we used &#8220;base&#8221; features consisting of image, video, audio, and subtitle features.\nThe second row shows the result when we removed subtitles from the base features. Compared to the baseline, the BLEU and METEOR scores do not change significantly, meaning that the baseline model did not effectively utilize the subtitle feature.\nThe third row indicates that text conditioning with subtitles substantially improves the sequence quality.\nThe fourth row corresponds to the result when we use subtitles on both sides, showing that adding subtitles to the features does not help.\nWe also used VideoLLaMA descriptions as features and/or text conditioning, and obtained similar results to the case of subtitles.\nThe last two rows show the results when we removed the base features, where we used only subtitles or VideoLLaMA descriptions, because the text may already contain rich semantic information of the video, and therefore, we want to check if the base features are still necessary for this task.\nWe can see substantial degradation of the quality, which means that the multimodal features are still important for this task even though a subtitle or rich video description is provided.\nFurthermore, we found that not all videos have valid subtitles. They include transcripts of unrelated utterances, non-English subtitles, and background music alone, which may not contribute to the performance. Therefore, the multimodal features are essential to compensate for invalid subtitles.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "also",
                    "video",
                    "description",
                    "all",
                    "from",
                    "when",
                    "subtitle",
                    "baseline",
                    "not",
                    "where",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposed a method for robot action sequence and confirmation sentence generation that leverages (a) a long-context Q-former considering left and right context dependency in full videos and (b) a text-conditioned LLM decoder to retain the precise language information.\nWe trained the above proposed models, generating single-arm robot micro-step action sequences and robot action confirmation in natural language using the YouCook2 dataset.\nTo mitigate the sparseness of the human action descriptions, we leveraged the video descriptions obtained from VideoLLaMA3.\nExperimental results show that our proposed long-context Q-former outperformed the baseline model for all metrics.\nWe confirmed that the combination of all multimodal features and text-conditioning performed the best.\nFuture work includes (1) multitask training of robot action steps and action descriptions, (2) comparison with other multimodal fusion approaches such as simple sequence concatenation or pooling+MLP, (3) evaluation for tasks other than cooking, and (4) evaluation using a simulator or a real robot.</p>\n\n",
                "matched_terms": [
                    "action",
                    "baseline",
                    "proposed",
                    "sequence",
                    "cooking",
                    "video",
                    "videollama3",
                    "context",
                    "steps",
                    "all",
                    "confirmation",
                    "from",
                    "paper",
                    "robot",
                    "sentence"
                ]
            }
        ]
    },
    "S6.T3": {
        "caption": "TABLE III: Quality of action sequences and descriptions generated by Long-context Q-former. The second column shows the context design. For example, “-1,0,+1” represents the context of previous (-1), current (0), and next (+1) clips. ‘*’ denotes all previous or following clips.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Action sequence</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Action description</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Model</th>\n<td class=\"ltx_td ltx_align_center\">Context</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">BLEU-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">METEOR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">BLUE-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">METEOR</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Baseline</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.370</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.260</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.229</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.159</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\">-1,0</td>\n<td class=\"ltx_td ltx_align_center\">0.374</td>\n<td class=\"ltx_td ltx_align_center\">0.264</td>\n<td class=\"ltx_td ltx_align_center\">0.235</td>\n<td class=\"ltx_td ltx_align_center\">0.166</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\">-2,-1,0</td>\n<td class=\"ltx_td ltx_align_center\">0.376</td>\n<td class=\"ltx_td ltx_align_center\">0.265</td>\n<td class=\"ltx_td ltx_align_center\">0.237</td>\n<td class=\"ltx_td ltx_align_center\">0.168</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Long-</th>\n<td class=\"ltx_td ltx_align_center\">*,0</td>\n<td class=\"ltx_td ltx_align_center\">0.376</td>\n<td class=\"ltx_td ltx_align_center\">0.266</td>\n<td class=\"ltx_td ltx_align_center\">0.237</td>\n<td class=\"ltx_td ltx_align_center\">0.169</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Context</th>\n<td class=\"ltx_td ltx_align_center\">-1,0,+1</td>\n<td class=\"ltx_td ltx_align_center\">0.379</td>\n<td class=\"ltx_td ltx_align_center\">0.269</td>\n<td class=\"ltx_td ltx_align_center\">0.240</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.170</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\">-2,-1,0,+1,+2</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.381</span></td>\n<td class=\"ltx_td ltx_align_center\">0.269</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.242</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.170</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<td class=\"ltx_td ltx_align_center\">*,0,*</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.381</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.270</span></td>\n<td class=\"ltx_td ltx_align_center\">0.241</td>\n<td class=\"ltx_td ltx_align_center\">0.169</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Multitask<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.370</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.257</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.220</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.158</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">ErrorCorrect<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.375</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.258</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.231</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.161</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "second",
            "column",
            "current",
            "example",
            "errorcorrecthori2025icassp",
            "meteor",
            "denotes",
            "bleu2",
            "qformer",
            "longcontext",
            "iii",
            "context",
            "multitaskhori2025icassp",
            "next",
            "long",
            "descriptions",
            "represents",
            "action",
            "following",
            "sequences",
            "blue2",
            "previous",
            "design",
            "sequence",
            "“101”",
            "model",
            "generated",
            "description",
            "all",
            "shows",
            "clips",
            "baseline",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T3\" title=\"TABLE III &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> shows the quality of action sequences and descriptions generated by the long-context Q-former.\n&#8220;Baseline&#8221; denotes the Q-former model trained with pairs of multimodal features and their target sequences without contextual information.\nUnlike the method in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite> that trains a single model to generate both action sequences and descriptions in a multitask manner, our baseline model was trained to generate action sequences and descriptions separately. Although we can use the same technique to achieve slightly better quality, we omitted that process for simplicity. In addition, our baseline model already achieves a quality comparable to the result of multitask training in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>. This could be because there are some differences in the hyperparameter setting, where we tuned the batch size, the learning rate, and the number of epochs in preliminary experiments.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment.\nThis paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding.\nThe state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps.\nAlthough actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos.\nFurthermore, this paper proposes a text-conditioning approach\nto feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former.\nExperiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning.\nFurthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.</p>\n\n",
                "matched_terms": [
                    "action",
                    "current",
                    "qformer",
                    "context",
                    "longcontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Human-robot interaction using natural language has the potential to be the most effective way for human-robot collaboration for shared tasks in daily life. Human-to-human collaboration is easily achieved because humans share the required knowledge about tasks and surrounding environments, and can understand other humans&#8217; behaviors.\nIf there are unknown things, humans can use natural language to confirm what to do and how to do it.\nSuch fundamental functions for human-robot collaboration can be built using multimodal scene understanding that enables robots to interpret their environment and interact with humans based on such understanding.\nThis goal lies at the intersection of multiple avenues of research in speech understanding, audio event detection, object and action recognition in computer vision, physical sensing/manipulation for robot control, and natural language generation to interact with humans.\nAchieving effective human-robot collaboration requires significant advances in the following areas: (1) multimodal scene understanding for human and robot environments,\n(2) robot action planning based on multimodal understanding and logical and physical affordance,\n(3) robot action generation/execution according to the planning,\n(4) robot action replanning if necessary, and (5) human-robot interaction via dialogue to achieve the goals efficiently.</p>\n\n",
                "matched_terms": [
                    "action",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are several works on robotic manipulation actions at a high level that have proposed how instructions can be stored and analyzed&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tenorth2013automated</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2014cognitive</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2015robot</span>]</cite>.\nInitial work utilized contrastive learning to learn a reward function to train reinforcement learning (RL) agents&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sermanet2018time</span>]</cite>.\nMore recently, there have been some works\nusing robot primitives and extraction of cost functions from task videos to enable imitation of demonstrated tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bahl2022human</span>]</cite>.\nThere has also been some work on training perception modules on large sets of manipulation data to simplify learning of manipulation tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">R3M_2022</span>]</cite>. Finally, there has been growing interest in using vision and language for learning diverse robot skills. There are some works on training visual language models using human instruction videos that are well aligned to robot actions to generate action sequences <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Visualtranslating4robot2018</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2D/3D_RN_Robot2021</span>]</cite>.\nThen, multimodal language models have been applied to robot action planning using various features, such as audio, visual, speech, and text, to expand knowledge acquisition of the tasks from human demonstration videos.\nInitially, multimodal scene-understanding-based robot action planning applied an audio-visual transformer trained from cooking videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori23_interspeech</span>]</cite>, where the action sequence was translated from human action descriptions.</p>\n\n",
                "matched_terms": [
                    "action",
                    "descriptions",
                    "sequences",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, neural language models such as Large Language Models (LLMs) have been applied to bridge the gaps between sensing information and abstract-level understanding for robot action planning.\nThis framework allows us to implement an end-to-end approach to build a human-robot collaboration system directly from multimodal scene understanding addressing all the steps (1) to (5).\nPromising use cases of LLMs were reported in the robotics research field, such as CLIPort <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cliport2021</span>]</cite> and SayCan <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SayCan2022</span>]</cite> in creating robotic agents that perform open-vocabulary tasks.\nPROGPROMPT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singh2023progprompt</span>]</cite> introduces a programmatic LLM prompt structure that facilitates the generation of plans in diverse environments, robot functionalities, and tasks.\nLLM-POP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Lingfeng_ICRA24</span>]</cite> targets partially observable task planning, where LLMs expand vocabulary and context considerations, while visual grounding LLMs enhance spatial reasoning capabilities. COWP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2023integrating</span>]</cite> introduces an LLM-based open-world task planning system for robots.\nSome works explore using LLMs to directly predict a dense sequence of end-effector poses for robot actions with vision models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kwon2024language</span>]</cite>.\nAnother study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">raman2022planning</span>]</cite> explores re-prompting strategies to enhance the executability and accuracy of LLM-generated plans but relies strictly on templated prompts.\nMultimodal scene understanding has significantly advanced with AV-transformers, and has been applied to robot action planning with human action videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori23_interspeech</span>]</cite>.\nDespite its success, its capability for multimodal reasoning is still limited because it was impractical to cover all combinations of multimodal inputs from the existing videos.</p>\n\n",
                "matched_terms": [
                    "action",
                    "all",
                    "context",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the discrepancy issues between the training and inference stages due to the data sparsity in multimodal fusion, BLIP2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BLIP2_ICML23</span>]</cite>, narrowing down the semantic space, was applied to train a multimodal large language model (AVBLIP) for robot action planning <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>]</cite>.\nVarious features are embedded into the semantic space of LLMs by training a Q-former (cross-token transformer),\nwhich is an effective method in terms of computational efficiency and information retention compared to conventional fusion methods\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">an2025llmcentricmultimodalfusionsurvey</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wadekar2024evolutionmultimodalmodelarchitectures</span>]</cite>,\nusing both contrastive loss and action generation loss in this approach.\nThe Q-former-based multimodal LLM contributes to enhancing the performance of robot action planning.</p>\n\n",
                "matched_terms": [
                    "action",
                    "model",
                    "qformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, automatic action planning errors are inevitable because of remaining discrepancies with the training environments and unexpected circumstances.\nTo avoid robots executing incorrect actions, a neural action replanning approach based on human error correction was proposed, where humans intervene to correct the action plans confirmed by robots using natural language before executing the planned actions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>.\nIn this approach, the robot manipulation was segmented into a skill acquisition phase and a knowledge acquisition phase.\nAlthough there have been some other research on language acquisition by robots to find associations between actions, objects, properties, and effects, and to map those associations to language&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">HRI_8616857</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">HRI_6082460</span>]</cite>, it is unrealistic to train models handling a huge vocabulary using human demonstration videos through real human-robot interaction.\nTo transfer (2) action planning trained from human action demonstration videos to (3) robot action generation/execution, a robot simulator was applied to generate and optimize executable robot actions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kai_ICRA25_WS</span>]</cite>.\nIn the next step, we build a system that can handle the real world beyond simulators&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Stone_ICRA25_WS</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zheng2025flarerobotlearningimplicit</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li_2025_arxiv</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "action",
                    "next"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this research trend, this paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation.\nThe state-of-the-art approach leveraging AVBLIP generates robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although action sequence dependency can be captured through an entire video to achieve the goal of a long-horizon task, the current single-clip-based approaches do not apply such long context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, the conventional AVBLIP feeds text embeddings into multimodal Q-former and could unexpectedly be degraded due to the high abstraction of the language information in the multimodal semantic space, thereby losing rich information represented in concrete words. To retain the original rich language information, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder that generates robot action confirmation and robot action steps.\nFinally, we show that the long-context Q-former improves the confirmation and action planning by integrating\nVideoLLaMA3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "action",
                    "current",
                    "sequence",
                    "qformer",
                    "context",
                    "long",
                    "longcontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main contributions of this work consist of (a) proposing a long-context Q-former for robot action confirmation sentence generation,\n(b) introducing a text conditioning that feeds text embedding vectors to an LLM decoder to retain original language information, (c) demonstrating the effectiveness of the proposed approaches for robot confirmation sentence and robot action sequence generation in the cooking domain, and\n(d) examining the contribution of VideoLLaMA.</p>\n\n",
                "matched_terms": [
                    "longcontext",
                    "sequence",
                    "action",
                    "qformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">a) Robot Action Description:</span>\nYouCook2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>http://youcook2.eecs.umich.edu/</span></span></span>\nis already annotated with human instructions in natural language to describe human cooking action steps, as introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhou2018towards</span>]</cite>.\nThe cooking action steps for each video are annotated with start and end time stamps and English description. An example of the description is &#8220;Grill the tomatoes in a pan and then put them on a plate&#8221; starting from 00:21 and ending at 00:52.</p>\n\n",
                "matched_terms": [
                    "action",
                    "description",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">b) Robot Micro-step Action:</span>\nIn the work of <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>,\nthe human instructions were translated into\na micro-step action sequence such that a single-arm robot could achieve the same goals as humans demonstrated actions, as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nAlthough a humanoid robot is illustrated in this example, action steps are\ndesigned for comoditized single-arm robots.\nThe micro-step action sequences for single-arm robot action are represented by &#8220;single-arm action&#8221;, &#8220;target object&#8221;, &#8220;preposition&#8221;, and &#8220;place&#8221;, to achieve the same actions as humans.\nWe borrowed the same data conditions used in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>.\nSingle-arm actions were selected from the following 12 candidates:\nOpen, Close, Pick, Place, Pour, Stir, TurnOn, TurnOff, Wipe, Cut, Scoop, Squeeze. The target objects were selected as one of the nouns in the human action instruction as much as possible.</p>\n\n",
                "matched_terms": [
                    "action",
                    "following",
                    "sequence",
                    "example",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work employs AVBLIP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>]</cite>, where BLIP-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BLIP2_ICML23</span>]</cite>, a vision-language pre-training method, is extended to handle multimodal features.\nBLIP-2 bootstraps from a frozen image encoder and a frozen LLM, where a Querying Transformer (Q-former)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">carion2020end</span>]</cite> is trained to bridge the gap between the vision and text modalities.\nThe image encoder in BLIP-2 is replaced with audio-visual encoders for video, audio, and text feature sequences in AVBLIP.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates an architecture of AVBLIP consisting of a Q-former and an LLM decoder.\nThe Q-former is trained to extract a fixed number of embeddings from a multimodal encoder that outputs sequences with different lengths.\nThe self-attention layers are shared between two transformer submodules: (1) a multimodal transformer that interacts with the frozen audio-visual encoders and (2) a text transformer that works as a text encoder and a text decoder.\nA set of learnable query embeddings is input to the multimodal transformer.\nThe queries interact with each other through self-attention layers and interact with audio-visual features through cross-attention layers.\nThe queries can additionally interact with the text through the same cross-attention layers. Finally, the queries are converted to an output feature.</p>\n\n",
                "matched_terms": [
                    "sequences",
                    "qformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LLM Decoder generates a sequence of micro-step actions for a single-arm robot from multimodal features aligned to language features obtained by the Q-former.\nThe LLM Decoder is constructed with a frozen LLM and a feed-forward layer. By using the LLM as a decoder, it leverages the LLM&#8217;s inference capabilities when generating action sequences. In this study, we use OPT-2.7B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022opt</span>]</cite> as the LLM.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "sequences",
                    "qformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of AVBLIP consists of two stages: (1) vision-language representation learning with frozen multimodal encoders and (2) vision-to-language generative learning with a frozen LLM.\nIn the second stage, we connect the Q-former to the frozen LLM Decoder and perform multimodal action sequence generation. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the extracted multimodal features from the video clip are converted to token embeddings using the Q-former. The embedding vectors are then projected to the LLM embedding space using a fully-connected layer. Then, the LLM Decoder generates action sequences and descriptions (confirmation sentences) for the given video clip. We use the cross-entropy loss function for the ground-truth sequences in this stage.</p>\n\n",
                "matched_terms": [
                    "action",
                    "second",
                    "sequence",
                    "sequences",
                    "qformer",
                    "descriptions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The AVBLIP model in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> can generate action sequences and confirmation sentences for short video clips of around 10-20 seconds. However, these clips are part of a longer instruction video, consisting of 5-10 sequential clips that aim at a single goal, e.g., &#8220;cooking meatloaf&#8221;.\nTherefore, incorporating contextual information from the previous and succeeding video clips is a promising extension of the model to generate more accurate sequences, because the video clips are interdependent: for example, some items cooked in a clip may be used in the next clip.</p>\n\n",
                "matched_terms": [
                    "action",
                    "previous",
                    "model",
                    "example",
                    "next",
                    "sequences",
                    "clips"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F3\" title=\"Figure 3 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows an extended AVBLIP architecture based on a long-context Q-former, which contains two Q-former modules, one processes multi-modal features from the current clip as originally used and the other processes the features from the surrounding clips to utilize them to enhance the output for the current clip, where the surrounding clips could contain the current one.\nBoth Q-formers output token embeddings, which are then combined using a transformer encoder.\nFinally, the LLM decoder receives the combined embeddings to generate micro-step actions for the robot and action description (as a confirmation) for the human.\nThe two Q-formers and the transformer encoder are jointly trained in the same manner as the original AVBLIP framework described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S3\" title=\"III Robot action generation using AVBLIP &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>.</p>\n\n",
                "matched_terms": [
                    "action",
                    "current",
                    "qformer",
                    "iii",
                    "description",
                    "shows",
                    "longcontext",
                    "clips"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>, the AVBLIP model can generate both micro-step action sequences and action descriptions by feeding query token embeddings to a pre-trained LLM, where the LLM will not further be trained or finetuned.\nThus, the Q-former is trained to generate token embeddings that convey the semantic information of the video clip as well as instructions to the LLM to generate a micro-step action sequence or an action description.\nAlthough the set of embeddings is a compact and effective representation, it may not retain the detailed information necessary for the LLM to generate fine-grained action sequences.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "model",
                    "description",
                    "sequences",
                    "qformer",
                    "descriptions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In principle, directly feeding multi-modal features and a natural language instruction to the LLM could be more effective in generating accurate action sequences.\nHowever, it is challenging to fine-tune the LLM to understand the multimodal features using limited training data and further to manually create prompts that generate specialized sequences, such as robot micro-step actions. In other words, the Q-former-based approach remains beneficial for obtaining suitable token embeddings, which serve as prompts for the pre-trained LLM.\nHowever, there is a concern that precise semantic information could be lost. In particular, text information, such as speech subtitles obtained by automatic speech recognition, often contains exact keywords indicating specific ingredients and utensils. Therefore, it is reasonable that only the text information is fed to the LLM directly with minimal information loss.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work applies text conditioning to the LLM by prepending the text information to the Q-former token embeddings as in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F3\" title=\"Figure 3 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, where the text is tokenized and embedded by the same LLM.\nThe text can be subtitles in the video and/or automatic video descriptions generated by an external multimodal LLM such as VideoLLaMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2023videollama</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">videoSALMONN</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "descriptions",
                    "generated",
                    "qformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed methods were tested using YouCook2 consisting of cooking action video clips aligned with human action instruction in natural language. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T1\" title=\"TABLE I &#8227; VI-A Setup &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> shows the data statistics. In this work, we used the validation set for evaluation since the test set was not publicly available.</p>\n\n",
                "matched_terms": [
                    "action",
                    "shows",
                    "clips"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We borrowed micro-step action sequences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>.\nThe action set contains 2,790 unique phrases of 195 verbs, 2,229 objects, 33 prepositions, and 1002 places.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialized the Q-former with the pre-trained weights of BERTbase&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span>]</cite>, while the cross-attention layers were randomly initialized. We set the number of dimensions of the hidden layers to 768, which results in 188M parameters in total.\nIn the experiments, we used 32 queries, where each query has a 768-dimensional vector, which equals the hidden dimension of the Q-former.\nFor the long-context Q-former, we used 32 queries for each of the Q-former modules.\nThe transformer encoder to combine the contextual and current token embeddings had two transformer blocks, each of which had a self-attention layer and a full-connected layer with 768-dimensional hidden activations. These Q-formers and the transformer were jointly trained from scratch without any shared parameters.\nFor the text conditioning, speech subtitles annotated by YouTube and video descriptions obtained from VideoLLaMA3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite> were fed into the LLM decoder.</p>\n\n",
                "matched_terms": [
                    "longcontext",
                    "descriptions",
                    "current",
                    "qformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance was evaluated using the BLEU-2 and METEOR scores computed between the generated and ground-truth sequences used in the robotics field <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Visualtranslating4robot2018</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2D/3D_RN_Robot2021</span>]</cite>.\nWe applied cross-validation, where one half of the validation set was used to select the best-epoch model, and the other half was used to measure model performance.\nThus, the shown scores are the average over the two subsets.</p>\n\n",
                "matched_terms": [
                    "model",
                    "generated",
                    "meteor",
                    "bleu2",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can see certain gains in BLEU and METEOR scores by using the long-context Q-former. The three rows after the baseline correspond to the results of left-context expansion, where introducing the previous two clips is sufficient, and we do not have to consider the full left context.\nThe next three rows display the results of introducing both left and right contexts, yielding further gains. Similar to the left-context expansion, it is sufficient to include two previous and two following clips to achieve the best quality.\nThese results demonstrate that the contextual information helps action and description generation and the long-context Q-former can effectively utilize that information.\nNote that the training time of the long-context Q-former with full-video context (*,0,*) took 8.2 hours on a single A40 GPU, compared to 5.5 hours for the baseline, when performing two-stage training for 30 epochs with batch size 16. The average inference time was 5.8 seconds compared to 4.1 seconds for the baseline. Thus, the computational cost for the long-context model is not crucial.</p>\n\n",
                "matched_terms": [
                    "action",
                    "previous",
                    "following",
                    "qformer",
                    "model",
                    "context",
                    "meteor",
                    "description",
                    "next",
                    "longcontext",
                    "clips",
                    "baseline",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T4\" title=\"TABLE IV &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> shows the quality of action sequences and descriptions generated by the Q-former with text conditioning.\nFirst, speech subtitles provided a substantial gain from the baseline in all metrics.\nThis demonstrates that text conditioning of the LLM decoder is very effective for this task.\nThen, we introduced a different text conditioning with video descriptions generated by the VideoLLaMA 7B model that achieves state-of-the-art performance in multiple video understanding benchmarks <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite>.\nTo ask VideoLLaMA3 to generate rich video descriptions for text conditioning, we used a simple prompt &#8220;Describe the cooking video in detail.&#8221;\nThis approach also improved the quality of the action sequences and descriptions, which are close to those with speech subtitles.\nThen, we concatenated two text sequences and fed them to the LLM (&#8220;VideoLLaMA3+subtitle&#8221;), and obtained further improvement.\nThis result shows that rich and precise information is very useful when it is provided as an LLM&#8217;s context.\nFurther, we tested the cases that use generated action descriptions and ground-truth descriptions.\nWith the descriptions generated by VideoLLaMA3+subtitle conditioning, the action sequence quality degraded. This is because the generated descriptions were much shorter than those of VideoLLaMA3+subtitle, and a certain level of information could be lost.\nThe ground-truth descriptions were concise, yet they provided accurate descriptions, achieving the highest quality, which is considered the upper bound of the text conditioning approach.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "model",
                    "context",
                    "generated",
                    "all",
                    "shows",
                    "sequences",
                    "qformer",
                    "descriptions",
                    "baseline",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we combined the long-context Q-former with text conditioning.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T5\" title=\"TABLE V &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> shows the quality of the generated sequences using the combined approach.\nThe results indicate that the performance gains by long-context Q-former and text conditioning are additive, achieving the best BLEU and METEOR scores.\nThe relative gains of the scores from the baseline are 16.7% and 11.9% for action sequences and 17.9% and 15.1% for action descriptions, respectively.</p>\n\n",
                "matched_terms": [
                    "action",
                    "longcontext",
                    "generated",
                    "meteor",
                    "shows",
                    "sequences",
                    "qformer",
                    "descriptions",
                    "baseline",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T2\" title=\"TABLE II &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> shows examples of generated action sequences and descriptions, where the rows contain the reference, the baseline result, the sequence generated by the proposed method, the speech subtitle, and the VideoLLaMA3 description. They also include generated descriptions for the previous and the following clips in the second example. The proposed method used both long-context expansion and text conditioning.\nIn the first example, the errors in the baseline result are corrected by the proposed method, where the word &#8220;spring roll&#8221; is substituted with the correct word &#8220;fish&#8221;. This shows that the VideoLLaMA description successfully supported the correct word through text conditioning.\nIn the second example, the object &#8220;potato&#8221; in the action sequence and &#8220;parmesan cheese&#8221; in the action description are corrected to &#8220;cabbage&#8221; using the proposed model. This correction could be made using information from the multimodal features of the previous and following clips, as the model predicted the word. Although the VideoLLaMA description also included &#8220;cabbage&#8221;, the error could not be fixed when the long-context features were not used. We also have several examples in which speech subtitles support correcting the action sequences and descriptions, but we omit them due to space limitations.\nAs in those examples, the long context and text conditioning provide helpful information to generate accurate action sequences and descriptions.</p>\n\n",
                "matched_terms": [
                    "action",
                    "previous",
                    "second",
                    "following",
                    "sequence",
                    "model",
                    "context",
                    "example",
                    "generated",
                    "description",
                    "shows",
                    "sequences",
                    "long",
                    "longcontext",
                    "clips",
                    "descriptions",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we show the impact of text information on the feature side and the prompt side, i.e., text conditioning.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S7.T6\" title=\"TABLE VI &#8227; VII Analysis and discussions &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a> shows the quality of action sequences and descriptions in different feature combinations.\nThe first row shows the baseline result, where we used &#8220;base&#8221; features consisting of image, video, audio, and subtitle features.\nThe second row shows the result when we removed subtitles from the base features. Compared to the baseline, the BLEU and METEOR scores do not change significantly, meaning that the baseline model did not effectively utilize the subtitle feature.\nThe third row indicates that text conditioning with subtitles substantially improves the sequence quality.\nThe fourth row corresponds to the result when we use subtitles on both sides, showing that adding subtitles to the features does not help.\nWe also used VideoLLaMA descriptions as features and/or text conditioning, and obtained similar results to the case of subtitles.\nThe last two rows show the results when we removed the base features, where we used only subtitles or VideoLLaMA descriptions, because the text may already contain rich semantic information of the video, and therefore, we want to check if the base features are still necessary for this task.\nWe can see substantial degradation of the quality, which means that the multimodal features are still important for this task even though a subtitle or rich video description is provided.\nFurthermore, we found that not all videos have valid subtitles. They include transcripts of unrelated utterances, non-English subtitles, and background music alone, which may not contribute to the performance. Therefore, the multimodal features are essential to compensate for invalid subtitles.</p>\n\n",
                "matched_terms": [
                    "action",
                    "second",
                    "sequence",
                    "model",
                    "meteor",
                    "description",
                    "all",
                    "shows",
                    "sequences",
                    "descriptions",
                    "baseline",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposed a method for robot action sequence and confirmation sentence generation that leverages (a) a long-context Q-former considering left and right context dependency in full videos and (b) a text-conditioned LLM decoder to retain the precise language information.\nWe trained the above proposed models, generating single-arm robot micro-step action sequences and robot action confirmation in natural language using the YouCook2 dataset.\nTo mitigate the sparseness of the human action descriptions, we leveraged the video descriptions obtained from VideoLLaMA3.\nExperimental results show that our proposed long-context Q-former outperformed the baseline model for all metrics.\nWe confirmed that the combination of all multimodal features and text-conditioning performed the best.\nFuture work includes (1) multitask training of robot action steps and action descriptions, (2) comparison with other multimodal fusion approaches such as simple sequence concatenation or pooling+MLP, (3) evaluation for tasks other than cooking, and (4) evaluation using a simulator or a real robot.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "qformer",
                    "model",
                    "context",
                    "all",
                    "sequences",
                    "longcontext",
                    "descriptions",
                    "baseline"
                ]
            }
        ]
    },
    "S6.T4": {
        "caption": "TABLE IV: Quality of action sequences and descriptions enhanced by text conditioning.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Action sequence</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Action description</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Conditioning</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">BLEU-2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">METEOR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">BLEU-2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">METEOR</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Baseline (no conditioning)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.370</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.260</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.229</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.159</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Speech subtitle</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.411</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.281</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.257</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.170</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">VideoLLaMA3</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.401</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.275</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.255</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.168</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">VideoLLaMA3+subtitle</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.424</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.290</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.260</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.178</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Generated description</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.398</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.272</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ground-truth description</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.499</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.337</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "action",
            "enhanced",
            "subtitle",
            "conditioning",
            "sequence",
            "groundtruth",
            "videollama3",
            "generated",
            "description",
            "meteor",
            "bleu2",
            "sequences",
            "videollama3subtitle",
            "speech",
            "descriptions",
            "baseline",
            "text",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T4\" title=\"TABLE IV &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> shows the quality of action sequences and descriptions generated by the Q-former with text conditioning.\nFirst, speech subtitles provided a substantial gain from the baseline in all metrics.\nThis demonstrates that text conditioning of the LLM decoder is very effective for this task.\nThen, we introduced a different text conditioning with video descriptions generated by the VideoLLaMA 7B model that achieves state-of-the-art performance in multiple video understanding benchmarks <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite>.\nTo ask VideoLLaMA3 to generate rich video descriptions for text conditioning, we used a simple prompt &#8220;Describe the cooking video in detail.&#8221;\nThis approach also improved the quality of the action sequences and descriptions, which are close to those with speech subtitles.\nThen, we concatenated two text sequences and fed them to the LLM (&#8220;VideoLLaMA3+subtitle&#8221;), and obtained further improvement.\nThis result shows that rich and precise information is very useful when it is provided as an LLM&#8217;s context.\nFurther, we tested the cases that use generated action descriptions and ground-truth descriptions.\nWith the descriptions generated by VideoLLaMA3+subtitle conditioning, the action sequence quality degraded. This is because the generated descriptions were much shorter than those of VideoLLaMA3+subtitle, and a certain level of information could be lost.\nThe ground-truth descriptions were concise, yet they provided accurate descriptions, achieving the highest quality, which is considered the upper bound of the text conditioning approach.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment.\nThis paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding.\nThe state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps.\nAlthough actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos.\nFurthermore, this paper proposes a text-conditioning approach\nto feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former.\nExperiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning.\nFurthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.</p>\n\n",
                "matched_terms": [
                    "action",
                    "videollama3",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Human-robot interaction using natural language has the potential to be the most effective way for human-robot collaboration for shared tasks in daily life. Human-to-human collaboration is easily achieved because humans share the required knowledge about tasks and surrounding environments, and can understand other humans&#8217; behaviors.\nIf there are unknown things, humans can use natural language to confirm what to do and how to do it.\nSuch fundamental functions for human-robot collaboration can be built using multimodal scene understanding that enables robots to interpret their environment and interact with humans based on such understanding.\nThis goal lies at the intersection of multiple avenues of research in speech understanding, audio event detection, object and action recognition in computer vision, physical sensing/manipulation for robot control, and natural language generation to interact with humans.\nAchieving effective human-robot collaboration requires significant advances in the following areas: (1) multimodal scene understanding for human and robot environments,\n(2) robot action planning based on multimodal understanding and logical and physical affordance,\n(3) robot action generation/execution according to the planning,\n(4) robot action replanning if necessary, and (5) human-robot interaction via dialogue to achieve the goals efficiently.</p>\n\n",
                "matched_terms": [
                    "action",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are several works on robotic manipulation actions at a high level that have proposed how instructions can be stored and analyzed&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tenorth2013automated</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2014cognitive</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2015robot</span>]</cite>.\nInitial work utilized contrastive learning to learn a reward function to train reinforcement learning (RL) agents&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sermanet2018time</span>]</cite>.\nMore recently, there have been some works\nusing robot primitives and extraction of cost functions from task videos to enable imitation of demonstrated tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bahl2022human</span>]</cite>.\nThere has also been some work on training perception modules on large sets of manipulation data to simplify learning of manipulation tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">R3M_2022</span>]</cite>. Finally, there has been growing interest in using vision and language for learning diverse robot skills. There are some works on training visual language models using human instruction videos that are well aligned to robot actions to generate action sequences <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Visualtranslating4robot2018</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2D/3D_RN_Robot2021</span>]</cite>.\nThen, multimodal language models have been applied to robot action planning using various features, such as audio, visual, speech, and text, to expand knowledge acquisition of the tasks from human demonstration videos.\nInitially, multimodal scene-understanding-based robot action planning applied an audio-visual transformer trained from cooking videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori23_interspeech</span>]</cite>, where the action sequence was translated from human action descriptions.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "sequences",
                    "speech",
                    "descriptions",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, neural language models such as Large Language Models (LLMs) have been applied to bridge the gaps between sensing information and abstract-level understanding for robot action planning.\nThis framework allows us to implement an end-to-end approach to build a human-robot collaboration system directly from multimodal scene understanding addressing all the steps (1) to (5).\nPromising use cases of LLMs were reported in the robotics research field, such as CLIPort <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cliport2021</span>]</cite> and SayCan <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SayCan2022</span>]</cite> in creating robotic agents that perform open-vocabulary tasks.\nPROGPROMPT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singh2023progprompt</span>]</cite> introduces a programmatic LLM prompt structure that facilitates the generation of plans in diverse environments, robot functionalities, and tasks.\nLLM-POP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Lingfeng_ICRA24</span>]</cite> targets partially observable task planning, where LLMs expand vocabulary and context considerations, while visual grounding LLMs enhance spatial reasoning capabilities. COWP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2023integrating</span>]</cite> introduces an LLM-based open-world task planning system for robots.\nSome works explore using LLMs to directly predict a dense sequence of end-effector poses for robot actions with vision models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kwon2024language</span>]</cite>.\nAnother study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">raman2022planning</span>]</cite> explores re-prompting strategies to enhance the executability and accuracy of LLM-generated plans but relies strictly on templated prompts.\nMultimodal scene understanding has significantly advanced with AV-transformers, and has been applied to robot action planning with human action videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori23_interspeech</span>]</cite>.\nDespite its success, its capability for multimodal reasoning is still limited because it was impractical to cover all combinations of multimodal inputs from the existing videos.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this research trend, this paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation.\nThe state-of-the-art approach leveraging AVBLIP generates robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although action sequence dependency can be captured through an entire video to achieve the goal of a long-horizon task, the current single-clip-based approaches do not apply such long context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, the conventional AVBLIP feeds text embeddings into multimodal Q-former and could unexpectedly be degraded due to the high abstraction of the language information in the multimodal semantic space, thereby losing rich information represented in concrete words. To retain the original rich language information, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder that generates robot action confirmation and robot action steps.\nFinally, we show that the long-context Q-former improves the confirmation and action planning by integrating\nVideoLLaMA3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "action",
                    "videollama3",
                    "text",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main contributions of this work consist of (a) proposing a long-context Q-former for robot action confirmation sentence generation,\n(b) introducing a text conditioning that feeds text embedding vectors to an LLM decoder to retain original language information, (c) demonstrating the effectiveness of the proposed approaches for robot confirmation sentence and robot action sequence generation in the cooking domain, and\n(d) examining the contribution of VideoLLaMA.</p>\n\n",
                "matched_terms": [
                    "action",
                    "text",
                    "conditioning",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">a) Robot Action Description:</span>\nYouCook2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>http://youcook2.eecs.umich.edu/</span></span></span>\nis already annotated with human instructions in natural language to describe human cooking action steps, as introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhou2018towards</span>]</cite>.\nThe cooking action steps for each video are annotated with start and end time stamps and English description. An example of the description is &#8220;Grill the tomatoes in a pan and then put them on a plate&#8221; starting from 00:21 and ending at 00:52.</p>\n\n",
                "matched_terms": [
                    "action",
                    "description"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">b) Robot Micro-step Action:</span>\nIn the work of <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>,\nthe human instructions were translated into\na micro-step action sequence such that a single-arm robot could achieve the same goals as humans demonstrated actions, as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nAlthough a humanoid robot is illustrated in this example, action steps are\ndesigned for comoditized single-arm robots.\nThe micro-step action sequences for single-arm robot action are represented by &#8220;single-arm action&#8221;, &#8220;target object&#8221;, &#8220;preposition&#8221;, and &#8220;place&#8221;, to achieve the same actions as humans.\nWe borrowed the same data conditions used in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>.\nSingle-arm actions were selected from the following 12 candidates:\nOpen, Close, Pick, Place, Pour, Stir, TurnOn, TurnOff, Wipe, Cut, Scoop, Squeeze. The target objects were selected as one of the nouns in the human action instruction as much as possible.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequences",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work employs AVBLIP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>]</cite>, where BLIP-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BLIP2_ICML23</span>]</cite>, a vision-language pre-training method, is extended to handle multimodal features.\nBLIP-2 bootstraps from a frozen image encoder and a frozen LLM, where a Querying Transformer (Q-former)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">carion2020end</span>]</cite> is trained to bridge the gap between the vision and text modalities.\nThe image encoder in BLIP-2 is replaced with audio-visual encoders for video, audio, and text feature sequences in AVBLIP.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates an architecture of AVBLIP consisting of a Q-former and an LLM decoder.\nThe Q-former is trained to extract a fixed number of embeddings from a multimodal encoder that outputs sequences with different lengths.\nThe self-attention layers are shared between two transformer submodules: (1) a multimodal transformer that interacts with the frozen audio-visual encoders and (2) a text transformer that works as a text encoder and a text decoder.\nA set of learnable query embeddings is input to the multimodal transformer.\nThe queries interact with each other through self-attention layers and interact with audio-visual features through cross-attention layers.\nThe queries can additionally interact with the text through the same cross-attention layers. Finally, the queries are converted to an output feature.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LLM Decoder generates a sequence of micro-step actions for a single-arm robot from multimodal features aligned to language features obtained by the Q-former.\nThe LLM Decoder is constructed with a frozen LLM and a feed-forward layer. By using the LLM as a decoder, it leverages the LLM&#8217;s inference capabilities when generating action sequences. In this study, we use OPT-2.7B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022opt</span>]</cite> as the LLM.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequences",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of AVBLIP consists of two stages: (1) vision-language representation learning with frozen multimodal encoders and (2) vision-to-language generative learning with a frozen LLM.\nIn the second stage, we connect the Q-former to the frozen LLM Decoder and perform multimodal action sequence generation. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the extracted multimodal features from the video clip are converted to token embeddings using the Q-former. The embedding vectors are then projected to the LLM embedding space using a fully-connected layer. Then, the LLM Decoder generates action sequences and descriptions (confirmation sentences) for the given video clip. We use the cross-entropy loss function for the ground-truth sequences in this stage.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "groundtruth",
                    "sequences",
                    "descriptions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The AVBLIP model in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> can generate action sequences and confirmation sentences for short video clips of around 10-20 seconds. However, these clips are part of a longer instruction video, consisting of 5-10 sequential clips that aim at a single goal, e.g., &#8220;cooking meatloaf&#8221;.\nTherefore, incorporating contextual information from the previous and succeeding video clips is a promising extension of the model to generate more accurate sequences, because the video clips are interdependent: for example, some items cooked in a clip may be used in the next clip.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F3\" title=\"Figure 3 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows an extended AVBLIP architecture based on a long-context Q-former, which contains two Q-former modules, one processes multi-modal features from the current clip as originally used and the other processes the features from the surrounding clips to utilize them to enhance the output for the current clip, where the surrounding clips could contain the current one.\nBoth Q-formers output token embeddings, which are then combined using a transformer encoder.\nFinally, the LLM decoder receives the combined embeddings to generate micro-step actions for the robot and action description (as a confirmation) for the human.\nThe two Q-formers and the transformer encoder are jointly trained in the same manner as the original AVBLIP framework described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S3\" title=\"III Robot action generation using AVBLIP &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>.</p>\n\n",
                "matched_terms": [
                    "action",
                    "description"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>, the AVBLIP model can generate both micro-step action sequences and action descriptions by feeding query token embeddings to a pre-trained LLM, where the LLM will not further be trained or finetuned.\nThus, the Q-former is trained to generate token embeddings that convey the semantic information of the video clip as well as instructions to the LLM to generate a micro-step action sequence or an action description.\nAlthough the set of embeddings is a compact and effective representation, it may not retain the detailed information necessary for the LLM to generate fine-grained action sequences.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "description",
                    "sequences",
                    "descriptions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In principle, directly feeding multi-modal features and a natural language instruction to the LLM could be more effective in generating accurate action sequences.\nHowever, it is challenging to fine-tune the LLM to understand the multimodal features using limited training data and further to manually create prompts that generate specialized sequences, such as robot micro-step actions. In other words, the Q-former-based approach remains beneficial for obtaining suitable token embeddings, which serve as prompts for the pre-trained LLM.\nHowever, there is a concern that precise semantic information could be lost. In particular, text information, such as speech subtitles obtained by automatic speech recognition, often contains exact keywords indicating specific ingredients and utensils. Therefore, it is reasonable that only the text information is fed to the LLM directly with minimal information loss.</p>\n\n",
                "matched_terms": [
                    "action",
                    "text",
                    "speech",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work applies text conditioning to the LLM by prepending the text information to the Q-former token embeddings as in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F3\" title=\"Figure 3 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, where the text is tokenized and embedded by the same LLM.\nThe text can be subtitles in the video and/or automatic video descriptions generated by an external multimodal LLM such as VideoLLaMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2023videollama</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">videoSALMONN</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "descriptions",
                    "text",
                    "conditioning",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We borrowed micro-step action sequences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>.\nThe action set contains 2,790 unique phrases of 195 verbs, 2,229 objects, 33 prepositions, and 1002 places.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal features such as video, image, and audio were extracted using Omnivore <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">girdhar2022omnivore</span>]</cite>, Contrastive Language-Image Pre-Training (CLIP) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2021learning</span>]</cite>, and Audio Spectrogram Transformer (AST) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong21b_interspeech</span>]</cite>, respectively.\nThe image and video features were concatenated and projected to a single video feature sequence before feeding them to the encoder.\nIf a subtitle was available in the video, text features were extracted by Glove word embedding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pennington2014glove</span>]</cite>. Otherwise, we fed an embedding vector for the <span class=\"ltx_text ltx_font_typewriter\">&lt;unk&gt;</span> label.\nThe numbers of dimensions of the audio, visual, and text features are <math alttext=\"768\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mn>768</mn><annotation encoding=\"application/x-tex\">768</annotation></semantics></math>, <math alttext=\"1024\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m2\" intent=\":literal\"><semantics><mn>1024</mn><annotation encoding=\"application/x-tex\">1024</annotation></semantics></math>, and <math alttext=\"300\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m3\" intent=\":literal\"><semantics><mn>300</mn><annotation encoding=\"application/x-tex\">300</annotation></semantics></math>, respectively.</p>\n\n",
                "matched_terms": [
                    "subtitle",
                    "text",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialized the Q-former with the pre-trained weights of BERTbase&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span>]</cite>, while the cross-attention layers were randomly initialized. We set the number of dimensions of the hidden layers to 768, which results in 188M parameters in total.\nIn the experiments, we used 32 queries, where each query has a 768-dimensional vector, which equals the hidden dimension of the Q-former.\nFor the long-context Q-former, we used 32 queries for each of the Q-former modules.\nThe transformer encoder to combine the contextual and current token embeddings had two transformer blocks, each of which had a self-attention layer and a full-connected layer with 768-dimensional hidden activations. These Q-formers and the transformer were jointly trained from scratch without any shared parameters.\nFor the text conditioning, speech subtitles annotated by YouTube and video descriptions obtained from VideoLLaMA3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite> were fed into the LLM decoder.</p>\n\n",
                "matched_terms": [
                    "videollama3",
                    "text",
                    "speech",
                    "descriptions",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance was evaluated using the BLEU-2 and METEOR scores computed between the generated and ground-truth sequences used in the robotics field <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Visualtranslating4robot2018</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2D/3D_RN_Robot2021</span>]</cite>.\nWe applied cross-validation, where one half of the validation set was used to select the best-epoch model, and the other half was used to measure model performance.\nThus, the shown scores are the average over the two subsets.</p>\n\n",
                "matched_terms": [
                    "groundtruth",
                    "generated",
                    "meteor",
                    "bleu2",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T3\" title=\"TABLE III &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> shows the quality of action sequences and descriptions generated by the long-context Q-former.\n&#8220;Baseline&#8221; denotes the Q-former model trained with pairs of multimodal features and their target sequences without contextual information.\nUnlike the method in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite> that trains a single model to generate both action sequences and descriptions in a multitask manner, our baseline model was trained to generate action sequences and descriptions separately. Although we can use the same technique to achieve slightly better quality, we omitted that process for simplicity. In addition, our baseline model already achieves a quality comparable to the result of multitask training in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>. This could be because there are some differences in the hyperparameter setting, where we tuned the batch size, the learning rate, and the number of epochs in preliminary experiments.</p>\n\n",
                "matched_terms": [
                    "action",
                    "generated",
                    "sequences",
                    "descriptions",
                    "baseline",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can see certain gains in BLEU and METEOR scores by using the long-context Q-former. The three rows after the baseline correspond to the results of left-context expansion, where introducing the previous two clips is sufficient, and we do not have to consider the full left context.\nThe next three rows display the results of introducing both left and right contexts, yielding further gains. Similar to the left-context expansion, it is sufficient to include two previous and two following clips to achieve the best quality.\nThese results demonstrate that the contextual information helps action and description generation and the long-context Q-former can effectively utilize that information.\nNote that the training time of the long-context Q-former with full-video context (*,0,*) took 8.2 hours on a single A40 GPU, compared to 5.5 hours for the baseline, when performing two-stage training for 30 epochs with batch size 16. The average inference time was 5.8 seconds compared to 4.1 seconds for the baseline. Thus, the computational cost for the long-context model is not crucial.</p>\n\n",
                "matched_terms": [
                    "action",
                    "description",
                    "meteor",
                    "baseline",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we combined the long-context Q-former with text conditioning.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T5\" title=\"TABLE V &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> shows the quality of the generated sequences using the combined approach.\nThe results indicate that the performance gains by long-context Q-former and text conditioning are additive, achieving the best BLEU and METEOR scores.\nThe relative gains of the scores from the baseline are 16.7% and 11.9% for action sequences and 17.9% and 15.1% for action descriptions, respectively.</p>\n\n",
                "matched_terms": [
                    "action",
                    "generated",
                    "meteor",
                    "text",
                    "sequences",
                    "descriptions",
                    "baseline",
                    "conditioning",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T2\" title=\"TABLE II &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> shows examples of generated action sequences and descriptions, where the rows contain the reference, the baseline result, the sequence generated by the proposed method, the speech subtitle, and the VideoLLaMA3 description. They also include generated descriptions for the previous and the following clips in the second example. The proposed method used both long-context expansion and text conditioning.\nIn the first example, the errors in the baseline result are corrected by the proposed method, where the word &#8220;spring roll&#8221; is substituted with the correct word &#8220;fish&#8221;. This shows that the VideoLLaMA description successfully supported the correct word through text conditioning.\nIn the second example, the object &#8220;potato&#8221; in the action sequence and &#8220;parmesan cheese&#8221; in the action description are corrected to &#8220;cabbage&#8221; using the proposed model. This correction could be made using information from the multimodal features of the previous and following clips, as the model predicted the word. Although the VideoLLaMA description also included &#8220;cabbage&#8221;, the error could not be fixed when the long-context features were not used. We also have several examples in which speech subtitles support correcting the action sequences and descriptions, but we omit them due to space limitations.\nAs in those examples, the long context and text conditioning provide helpful information to generate accurate action sequences and descriptions.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "descriptions",
                    "videollama3",
                    "generated",
                    "description",
                    "text",
                    "sequences",
                    "speech",
                    "subtitle",
                    "baseline",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we show the impact of text information on the feature side and the prompt side, i.e., text conditioning.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S7.T6\" title=\"TABLE VI &#8227; VII Analysis and discussions &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a> shows the quality of action sequences and descriptions in different feature combinations.\nThe first row shows the baseline result, where we used &#8220;base&#8221; features consisting of image, video, audio, and subtitle features.\nThe second row shows the result when we removed subtitles from the base features. Compared to the baseline, the BLEU and METEOR scores do not change significantly, meaning that the baseline model did not effectively utilize the subtitle feature.\nThe third row indicates that text conditioning with subtitles substantially improves the sequence quality.\nThe fourth row corresponds to the result when we use subtitles on both sides, showing that adding subtitles to the features does not help.\nWe also used VideoLLaMA descriptions as features and/or text conditioning, and obtained similar results to the case of subtitles.\nThe last two rows show the results when we removed the base features, where we used only subtitles or VideoLLaMA descriptions, because the text may already contain rich semantic information of the video, and therefore, we want to check if the base features are still necessary for this task.\nWe can see substantial degradation of the quality, which means that the multimodal features are still important for this task even though a subtitle or rich video description is provided.\nFurthermore, we found that not all videos have valid subtitles. They include transcripts of unrelated utterances, non-English subtitles, and background music alone, which may not contribute to the performance. Therefore, the multimodal features are essential to compensate for invalid subtitles.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "descriptions",
                    "description",
                    "meteor",
                    "text",
                    "sequences",
                    "subtitle",
                    "baseline",
                    "conditioning",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposed a method for robot action sequence and confirmation sentence generation that leverages (a) a long-context Q-former considering left and right context dependency in full videos and (b) a text-conditioned LLM decoder to retain the precise language information.\nWe trained the above proposed models, generating single-arm robot micro-step action sequences and robot action confirmation in natural language using the YouCook2 dataset.\nTo mitigate the sparseness of the human action descriptions, we leveraged the video descriptions obtained from VideoLLaMA3.\nExperimental results show that our proposed long-context Q-former outperformed the baseline model for all metrics.\nWe confirmed that the combination of all multimodal features and text-conditioning performed the best.\nFuture work includes (1) multitask training of robot action steps and action descriptions, (2) comparison with other multimodal fusion approaches such as simple sequence concatenation or pooling+MLP, (3) evaluation for tasks other than cooking, and (4) evaluation using a simulator or a real robot.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "videollama3",
                    "sequences",
                    "descriptions",
                    "baseline"
                ]
            }
        ]
    },
    "S6.T5": {
        "caption": "TABLE V: Quality of action sequences and descriptions generated by long-context Q-former and text conditioning.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Action sequence</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Action description</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Context</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Text conditioning</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">BLEU-2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">METEOR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">BLEU-2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">METEOR</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Baseline</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.370</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.260</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.229</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.159</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">VideoLLaMA3+subtitle</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.424</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.290</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.260</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.178</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-2,-1,0</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">VideoLLaMA3+subtitle</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.427</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.290</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.264</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.180</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-2,-1,0,+1,+2</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">VideoLLaMA3+subtitle</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.432</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.291</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.270</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.183</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "action",
            "conditioning",
            "sequence",
            "qformer",
            "context",
            "generated",
            "description",
            "meteor",
            "bleu2",
            "sequences",
            "videollama3subtitle",
            "longcontext",
            "descriptions",
            "baseline",
            "text",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Finally, we combined the long-context Q-former with text conditioning.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T5\" title=\"TABLE V &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> shows the quality of the generated sequences using the combined approach.\nThe results indicate that the performance gains by long-context Q-former and text conditioning are additive, achieving the best BLEU and METEOR scores.\nThe relative gains of the scores from the baseline are 16.7% and 11.9% for action sequences and 17.9% and 15.1% for action descriptions, respectively.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment.\nThis paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding.\nThe state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps.\nAlthough actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos.\nFurthermore, this paper proposes a text-conditioning approach\nto feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former.\nExperiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning.\nFurthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.</p>\n\n",
                "matched_terms": [
                    "action",
                    "qformer",
                    "context",
                    "longcontext",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are several works on robotic manipulation actions at a high level that have proposed how instructions can be stored and analyzed&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tenorth2013automated</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2014cognitive</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2015robot</span>]</cite>.\nInitial work utilized contrastive learning to learn a reward function to train reinforcement learning (RL) agents&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sermanet2018time</span>]</cite>.\nMore recently, there have been some works\nusing robot primitives and extraction of cost functions from task videos to enable imitation of demonstrated tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bahl2022human</span>]</cite>.\nThere has also been some work on training perception modules on large sets of manipulation data to simplify learning of manipulation tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">R3M_2022</span>]</cite>. Finally, there has been growing interest in using vision and language for learning diverse robot skills. There are some works on training visual language models using human instruction videos that are well aligned to robot actions to generate action sequences <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Visualtranslating4robot2018</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2D/3D_RN_Robot2021</span>]</cite>.\nThen, multimodal language models have been applied to robot action planning using various features, such as audio, visual, speech, and text, to expand knowledge acquisition of the tasks from human demonstration videos.\nInitially, multimodal scene-understanding-based robot action planning applied an audio-visual transformer trained from cooking videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori23_interspeech</span>]</cite>, where the action sequence was translated from human action descriptions.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "sequences",
                    "descriptions",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, neural language models such as Large Language Models (LLMs) have been applied to bridge the gaps between sensing information and abstract-level understanding for robot action planning.\nThis framework allows us to implement an end-to-end approach to build a human-robot collaboration system directly from multimodal scene understanding addressing all the steps (1) to (5).\nPromising use cases of LLMs were reported in the robotics research field, such as CLIPort <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cliport2021</span>]</cite> and SayCan <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SayCan2022</span>]</cite> in creating robotic agents that perform open-vocabulary tasks.\nPROGPROMPT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singh2023progprompt</span>]</cite> introduces a programmatic LLM prompt structure that facilitates the generation of plans in diverse environments, robot functionalities, and tasks.\nLLM-POP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Lingfeng_ICRA24</span>]</cite> targets partially observable task planning, where LLMs expand vocabulary and context considerations, while visual grounding LLMs enhance spatial reasoning capabilities. COWP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2023integrating</span>]</cite> introduces an LLM-based open-world task planning system for robots.\nSome works explore using LLMs to directly predict a dense sequence of end-effector poses for robot actions with vision models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kwon2024language</span>]</cite>.\nAnother study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">raman2022planning</span>]</cite> explores re-prompting strategies to enhance the executability and accuracy of LLM-generated plans but relies strictly on templated prompts.\nMultimodal scene understanding has significantly advanced with AV-transformers, and has been applied to robot action planning with human action videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori23_interspeech</span>]</cite>.\nDespite its success, its capability for multimodal reasoning is still limited because it was impractical to cover all combinations of multimodal inputs from the existing videos.</p>\n\n",
                "matched_terms": [
                    "action",
                    "context",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the discrepancy issues between the training and inference stages due to the data sparsity in multimodal fusion, BLIP2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BLIP2_ICML23</span>]</cite>, narrowing down the semantic space, was applied to train a multimodal large language model (AVBLIP) for robot action planning <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>]</cite>.\nVarious features are embedded into the semantic space of LLMs by training a Q-former (cross-token transformer),\nwhich is an effective method in terms of computational efficiency and information retention compared to conventional fusion methods\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">an2025llmcentricmultimodalfusionsurvey</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wadekar2024evolutionmultimodalmodelarchitectures</span>]</cite>,\nusing both contrastive loss and action generation loss in this approach.\nThe Q-former-based multimodal LLM contributes to enhancing the performance of robot action planning.</p>\n\n",
                "matched_terms": [
                    "action",
                    "qformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this research trend, this paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation.\nThe state-of-the-art approach leveraging AVBLIP generates robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although action sequence dependency can be captured through an entire video to achieve the goal of a long-horizon task, the current single-clip-based approaches do not apply such long context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, the conventional AVBLIP feeds text embeddings into multimodal Q-former and could unexpectedly be degraded due to the high abstraction of the language information in the multimodal semantic space, thereby losing rich information represented in concrete words. To retain the original rich language information, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder that generates robot action confirmation and robot action steps.\nFinally, we show that the long-context Q-former improves the confirmation and action planning by integrating\nVideoLLaMA3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "qformer",
                    "context",
                    "longcontext",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main contributions of this work consist of (a) proposing a long-context Q-former for robot action confirmation sentence generation,\n(b) introducing a text conditioning that feeds text embedding vectors to an LLM decoder to retain original language information, (c) demonstrating the effectiveness of the proposed approaches for robot confirmation sentence and robot action sequence generation in the cooking domain, and\n(d) examining the contribution of VideoLLaMA.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "qformer",
                    "text",
                    "longcontext",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">a) Robot Action Description:</span>\nYouCook2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>http://youcook2.eecs.umich.edu/</span></span></span>\nis already annotated with human instructions in natural language to describe human cooking action steps, as introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhou2018towards</span>]</cite>.\nThe cooking action steps for each video are annotated with start and end time stamps and English description. An example of the description is &#8220;Grill the tomatoes in a pan and then put them on a plate&#8221; starting from 00:21 and ending at 00:52.</p>\n\n",
                "matched_terms": [
                    "action",
                    "description"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">b) Robot Micro-step Action:</span>\nIn the work of <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>,\nthe human instructions were translated into\na micro-step action sequence such that a single-arm robot could achieve the same goals as humans demonstrated actions, as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nAlthough a humanoid robot is illustrated in this example, action steps are\ndesigned for comoditized single-arm robots.\nThe micro-step action sequences for single-arm robot action are represented by &#8220;single-arm action&#8221;, &#8220;target object&#8221;, &#8220;preposition&#8221;, and &#8220;place&#8221;, to achieve the same actions as humans.\nWe borrowed the same data conditions used in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>.\nSingle-arm actions were selected from the following 12 candidates:\nOpen, Close, Pick, Place, Pour, Stir, TurnOn, TurnOff, Wipe, Cut, Scoop, Squeeze. The target objects were selected as one of the nouns in the human action instruction as much as possible.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequences",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work employs AVBLIP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>]</cite>, where BLIP-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BLIP2_ICML23</span>]</cite>, a vision-language pre-training method, is extended to handle multimodal features.\nBLIP-2 bootstraps from a frozen image encoder and a frozen LLM, where a Querying Transformer (Q-former)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">carion2020end</span>]</cite> is trained to bridge the gap between the vision and text modalities.\nThe image encoder in BLIP-2 is replaced with audio-visual encoders for video, audio, and text feature sequences in AVBLIP.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates an architecture of AVBLIP consisting of a Q-former and an LLM decoder.\nThe Q-former is trained to extract a fixed number of embeddings from a multimodal encoder that outputs sequences with different lengths.\nThe self-attention layers are shared between two transformer submodules: (1) a multimodal transformer that interacts with the frozen audio-visual encoders and (2) a text transformer that works as a text encoder and a text decoder.\nA set of learnable query embeddings is input to the multimodal transformer.\nThe queries interact with each other through self-attention layers and interact with audio-visual features through cross-attention layers.\nThe queries can additionally interact with the text through the same cross-attention layers. Finally, the queries are converted to an output feature.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sequences",
                    "qformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LLM Decoder generates a sequence of micro-step actions for a single-arm robot from multimodal features aligned to language features obtained by the Q-former.\nThe LLM Decoder is constructed with a frozen LLM and a feed-forward layer. By using the LLM as a decoder, it leverages the LLM&#8217;s inference capabilities when generating action sequences. In this study, we use OPT-2.7B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022opt</span>]</cite> as the LLM.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequences",
                    "qformer",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of AVBLIP consists of two stages: (1) vision-language representation learning with frozen multimodal encoders and (2) vision-to-language generative learning with a frozen LLM.\nIn the second stage, we connect the Q-former to the frozen LLM Decoder and perform multimodal action sequence generation. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the extracted multimodal features from the video clip are converted to token embeddings using the Q-former. The embedding vectors are then projected to the LLM embedding space using a fully-connected layer. Then, the LLM Decoder generates action sequences and descriptions (confirmation sentences) for the given video clip. We use the cross-entropy loss function for the ground-truth sequences in this stage.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "sequences",
                    "qformer",
                    "descriptions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The AVBLIP model in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> can generate action sequences and confirmation sentences for short video clips of around 10-20 seconds. However, these clips are part of a longer instruction video, consisting of 5-10 sequential clips that aim at a single goal, e.g., &#8220;cooking meatloaf&#8221;.\nTherefore, incorporating contextual information from the previous and succeeding video clips is a promising extension of the model to generate more accurate sequences, because the video clips are interdependent: for example, some items cooked in a clip may be used in the next clip.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F3\" title=\"Figure 3 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows an extended AVBLIP architecture based on a long-context Q-former, which contains two Q-former modules, one processes multi-modal features from the current clip as originally used and the other processes the features from the surrounding clips to utilize them to enhance the output for the current clip, where the surrounding clips could contain the current one.\nBoth Q-formers output token embeddings, which are then combined using a transformer encoder.\nFinally, the LLM decoder receives the combined embeddings to generate micro-step actions for the robot and action description (as a confirmation) for the human.\nThe two Q-formers and the transformer encoder are jointly trained in the same manner as the original AVBLIP framework described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S3\" title=\"III Robot action generation using AVBLIP &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>.</p>\n\n",
                "matched_terms": [
                    "action",
                    "longcontext",
                    "description",
                    "qformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>, the AVBLIP model can generate both micro-step action sequences and action descriptions by feeding query token embeddings to a pre-trained LLM, where the LLM will not further be trained or finetuned.\nThus, the Q-former is trained to generate token embeddings that convey the semantic information of the video clip as well as instructions to the LLM to generate a micro-step action sequence or an action description.\nAlthough the set of embeddings is a compact and effective representation, it may not retain the detailed information necessary for the LLM to generate fine-grained action sequences.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "description",
                    "sequences",
                    "qformer",
                    "descriptions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In principle, directly feeding multi-modal features and a natural language instruction to the LLM could be more effective in generating accurate action sequences.\nHowever, it is challenging to fine-tune the LLM to understand the multimodal features using limited training data and further to manually create prompts that generate specialized sequences, such as robot micro-step actions. In other words, the Q-former-based approach remains beneficial for obtaining suitable token embeddings, which serve as prompts for the pre-trained LLM.\nHowever, there is a concern that precise semantic information could be lost. In particular, text information, such as speech subtitles obtained by automatic speech recognition, often contains exact keywords indicating specific ingredients and utensils. Therefore, it is reasonable that only the text information is fed to the LLM directly with minimal information loss.</p>\n\n",
                "matched_terms": [
                    "action",
                    "text",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work applies text conditioning to the LLM by prepending the text information to the Q-former token embeddings as in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F3\" title=\"Figure 3 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, where the text is tokenized and embedded by the same LLM.\nThe text can be subtitles in the video and/or automatic video descriptions generated by an external multimodal LLM such as VideoLLaMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2023videollama</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">videoSALMONN</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "conditioning",
                    "generated",
                    "qformer",
                    "descriptions",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We borrowed micro-step action sequences&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>.\nThe action set contains 2,790 unique phrases of 195 verbs, 2,229 objects, 33 prepositions, and 1002 places.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal features such as video, image, and audio were extracted using Omnivore <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">girdhar2022omnivore</span>]</cite>, Contrastive Language-Image Pre-Training (CLIP) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2021learning</span>]</cite>, and Audio Spectrogram Transformer (AST) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong21b_interspeech</span>]</cite>, respectively.\nThe image and video features were concatenated and projected to a single video feature sequence before feeding them to the encoder.\nIf a subtitle was available in the video, text features were extracted by Glove word embedding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pennington2014glove</span>]</cite>. Otherwise, we fed an embedding vector for the <span class=\"ltx_text ltx_font_typewriter\">&lt;unk&gt;</span> label.\nThe numbers of dimensions of the audio, visual, and text features are <math alttext=\"768\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mn>768</mn><annotation encoding=\"application/x-tex\">768</annotation></semantics></math>, <math alttext=\"1024\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m2\" intent=\":literal\"><semantics><mn>1024</mn><annotation encoding=\"application/x-tex\">1024</annotation></semantics></math>, and <math alttext=\"300\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m3\" intent=\":literal\"><semantics><mn>300</mn><annotation encoding=\"application/x-tex\">300</annotation></semantics></math>, respectively.</p>\n\n",
                "matched_terms": [
                    "text",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialized the Q-former with the pre-trained weights of BERTbase&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span>]</cite>, while the cross-attention layers were randomly initialized. We set the number of dimensions of the hidden layers to 768, which results in 188M parameters in total.\nIn the experiments, we used 32 queries, where each query has a 768-dimensional vector, which equals the hidden dimension of the Q-former.\nFor the long-context Q-former, we used 32 queries for each of the Q-former modules.\nThe transformer encoder to combine the contextual and current token embeddings had two transformer blocks, each of which had a self-attention layer and a full-connected layer with 768-dimensional hidden activations. These Q-formers and the transformer were jointly trained from scratch without any shared parameters.\nFor the text conditioning, speech subtitles annotated by YouTube and video descriptions obtained from VideoLLaMA3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite> were fed into the LLM decoder.</p>\n\n",
                "matched_terms": [
                    "conditioning",
                    "qformer",
                    "longcontext",
                    "descriptions",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance was evaluated using the BLEU-2 and METEOR scores computed between the generated and ground-truth sequences used in the robotics field <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Visualtranslating4robot2018</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2D/3D_RN_Robot2021</span>]</cite>.\nWe applied cross-validation, where one half of the validation set was used to select the best-epoch model, and the other half was used to measure model performance.\nThus, the shown scores are the average over the two subsets.</p>\n\n",
                "matched_terms": [
                    "meteor",
                    "bleu2",
                    "sequences",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T3\" title=\"TABLE III &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> shows the quality of action sequences and descriptions generated by the long-context Q-former.\n&#8220;Baseline&#8221; denotes the Q-former model trained with pairs of multimodal features and their target sequences without contextual information.\nUnlike the method in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite> that trains a single model to generate both action sequences and descriptions in a multitask manner, our baseline model was trained to generate action sequences and descriptions separately. Although we can use the same technique to achieve slightly better quality, we omitted that process for simplicity. In addition, our baseline model already achieves a quality comparable to the result of multitask training in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>. This could be because there are some differences in the hyperparameter setting, where we tuned the batch size, the learning rate, and the number of epochs in preliminary experiments.</p>\n\n",
                "matched_terms": [
                    "action",
                    "qformer",
                    "generated",
                    "sequences",
                    "longcontext",
                    "descriptions",
                    "baseline",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can see certain gains in BLEU and METEOR scores by using the long-context Q-former. The three rows after the baseline correspond to the results of left-context expansion, where introducing the previous two clips is sufficient, and we do not have to consider the full left context.\nThe next three rows display the results of introducing both left and right contexts, yielding further gains. Similar to the left-context expansion, it is sufficient to include two previous and two following clips to achieve the best quality.\nThese results demonstrate that the contextual information helps action and description generation and the long-context Q-former can effectively utilize that information.\nNote that the training time of the long-context Q-former with full-video context (*,0,*) took 8.2 hours on a single A40 GPU, compared to 5.5 hours for the baseline, when performing two-stage training for 30 epochs with batch size 16. The average inference time was 5.8 seconds compared to 4.1 seconds for the baseline. Thus, the computational cost for the long-context model is not crucial.</p>\n\n",
                "matched_terms": [
                    "action",
                    "longcontext",
                    "context",
                    "meteor",
                    "description",
                    "qformer",
                    "baseline",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T4\" title=\"TABLE IV &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> shows the quality of action sequences and descriptions generated by the Q-former with text conditioning.\nFirst, speech subtitles provided a substantial gain from the baseline in all metrics.\nThis demonstrates that text conditioning of the LLM decoder is very effective for this task.\nThen, we introduced a different text conditioning with video descriptions generated by the VideoLLaMA 7B model that achieves state-of-the-art performance in multiple video understanding benchmarks <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite>.\nTo ask VideoLLaMA3 to generate rich video descriptions for text conditioning, we used a simple prompt &#8220;Describe the cooking video in detail.&#8221;\nThis approach also improved the quality of the action sequences and descriptions, which are close to those with speech subtitles.\nThen, we concatenated two text sequences and fed them to the LLM (&#8220;VideoLLaMA3+subtitle&#8221;), and obtained further improvement.\nThis result shows that rich and precise information is very useful when it is provided as an LLM&#8217;s context.\nFurther, we tested the cases that use generated action descriptions and ground-truth descriptions.\nWith the descriptions generated by VideoLLaMA3+subtitle conditioning, the action sequence quality degraded. This is because the generated descriptions were much shorter than those of VideoLLaMA3+subtitle, and a certain level of information could be lost.\nThe ground-truth descriptions were concise, yet they provided accurate descriptions, achieving the highest quality, which is considered the upper bound of the text conditioning approach.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "context",
                    "generated",
                    "text",
                    "sequences",
                    "videollama3subtitle",
                    "qformer",
                    "descriptions",
                    "baseline",
                    "conditioning",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T2\" title=\"TABLE II &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> shows examples of generated action sequences and descriptions, where the rows contain the reference, the baseline result, the sequence generated by the proposed method, the speech subtitle, and the VideoLLaMA3 description. They also include generated descriptions for the previous and the following clips in the second example. The proposed method used both long-context expansion and text conditioning.\nIn the first example, the errors in the baseline result are corrected by the proposed method, where the word &#8220;spring roll&#8221; is substituted with the correct word &#8220;fish&#8221;. This shows that the VideoLLaMA description successfully supported the correct word through text conditioning.\nIn the second example, the object &#8220;potato&#8221; in the action sequence and &#8220;parmesan cheese&#8221; in the action description are corrected to &#8220;cabbage&#8221; using the proposed model. This correction could be made using information from the multimodal features of the previous and following clips, as the model predicted the word. Although the VideoLLaMA description also included &#8220;cabbage&#8221;, the error could not be fixed when the long-context features were not used. We also have several examples in which speech subtitles support correcting the action sequences and descriptions, but we omit them due to space limitations.\nAs in those examples, the long context and text conditioning provide helpful information to generate accurate action sequences and descriptions.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "context",
                    "generated",
                    "description",
                    "text",
                    "sequences",
                    "longcontext",
                    "descriptions",
                    "baseline",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we show the impact of text information on the feature side and the prompt side, i.e., text conditioning.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S7.T6\" title=\"TABLE VI &#8227; VII Analysis and discussions &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a> shows the quality of action sequences and descriptions in different feature combinations.\nThe first row shows the baseline result, where we used &#8220;base&#8221; features consisting of image, video, audio, and subtitle features.\nThe second row shows the result when we removed subtitles from the base features. Compared to the baseline, the BLEU and METEOR scores do not change significantly, meaning that the baseline model did not effectively utilize the subtitle feature.\nThe third row indicates that text conditioning with subtitles substantially improves the sequence quality.\nThe fourth row corresponds to the result when we use subtitles on both sides, showing that adding subtitles to the features does not help.\nWe also used VideoLLaMA descriptions as features and/or text conditioning, and obtained similar results to the case of subtitles.\nThe last two rows show the results when we removed the base features, where we used only subtitles or VideoLLaMA descriptions, because the text may already contain rich semantic information of the video, and therefore, we want to check if the base features are still necessary for this task.\nWe can see substantial degradation of the quality, which means that the multimodal features are still important for this task even though a subtitle or rich video description is provided.\nFurthermore, we found that not all videos have valid subtitles. They include transcripts of unrelated utterances, non-English subtitles, and background music alone, which may not contribute to the performance. Therefore, the multimodal features are essential to compensate for invalid subtitles.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "description",
                    "meteor",
                    "text",
                    "sequences",
                    "descriptions",
                    "baseline",
                    "conditioning",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposed a method for robot action sequence and confirmation sentence generation that leverages (a) a long-context Q-former considering left and right context dependency in full videos and (b) a text-conditioned LLM decoder to retain the precise language information.\nWe trained the above proposed models, generating single-arm robot micro-step action sequences and robot action confirmation in natural language using the YouCook2 dataset.\nTo mitigate the sparseness of the human action descriptions, we leveraged the video descriptions obtained from VideoLLaMA3.\nExperimental results show that our proposed long-context Q-former outperformed the baseline model for all metrics.\nWe confirmed that the combination of all multimodal features and text-conditioning performed the best.\nFuture work includes (1) multitask training of robot action steps and action descriptions, (2) comparison with other multimodal fusion approaches such as simple sequence concatenation or pooling+MLP, (3) evaluation for tasks other than cooking, and (4) evaluation using a simulator or a real robot.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "longcontext",
                    "context",
                    "sequences",
                    "qformer",
                    "descriptions",
                    "baseline"
                ]
            }
        ]
    },
    "S7.T6": {
        "caption": "TABLE VI: Impact of text information on the feature side and the prompt side. “Base” denotes basic multimodal features we used in this work, i.e., image, video, audio, and subtitle features.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Action sequence</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Action description</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Features</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Text conditioning</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BLEU-2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">METEOR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BLEU-2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">METEOR</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Base</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.370</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.260</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.229</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.159</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Base - subtitle</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.373</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.257</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.231</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.158</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Base</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Subtitle</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.411</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.281</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.257</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.170</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Base - subtitle</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Subtitle</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.409</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.281</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.258</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.170</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Base + VideoLLaMA3</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.377</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.262</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.233</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.160</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Base</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VideoLLaMA3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.401</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.275</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.255</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.168</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Base + VideoLLaMA3</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VideoLLaMA3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.400</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.276</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.256</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.170</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Subtitle only</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Subtitle</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.223</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.219</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.174</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.126</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VideoLLaMA3 only</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VideoLLaMA3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.353</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.252</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.211</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.151</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "side",
            "basic",
            "meteor",
            "denotes",
            "bleu2",
            "feature",
            "subtitle",
            "conditioning",
            "information",
            "base",
            "audio",
            "used",
            "image",
            "text",
            "action",
            "“base”",
            "only",
            "impact",
            "multimodal",
            "features",
            "sequence",
            "video",
            "videollama3",
            "prompt",
            "work",
            "description"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Finally, we show the impact of text information on the feature side and the prompt side, i.e., text conditioning.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S7.T6\" title=\"TABLE VI &#8227; VII Analysis and discussions &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">VI</span></a> shows the quality of action sequences and descriptions in different feature combinations.\nThe first row shows the baseline result, where we used &#8220;base&#8221; features consisting of image, video, audio, and subtitle features.\nThe second row shows the result when we removed subtitles from the base features. Compared to the baseline, the BLEU and METEOR scores do not change significantly, meaning that the baseline model did not effectively utilize the subtitle feature.\nThe third row indicates that text conditioning with subtitles substantially improves the sequence quality.\nThe fourth row corresponds to the result when we use subtitles on both sides, showing that adding subtitles to the features does not help.\nWe also used VideoLLaMA descriptions as features and/or text conditioning, and obtained similar results to the case of subtitles.\nThe last two rows show the results when we removed the base features, where we used only subtitles or VideoLLaMA descriptions, because the text may already contain rich semantic information of the video, and therefore, we want to check if the base features are still necessary for this task.\nWe can see substantial degradation of the quality, which means that the multimodal features are still important for this task even though a subtitle or rich video description is provided.\nFurthermore, we found that not all videos have valid subtitles. They include transcripts of unrelated utterances, non-English subtitles, and background music alone, which may not contribute to the performance. Therefore, the multimodal features are essential to compensate for invalid subtitles.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment.\nThis paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding.\nThe state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps.\nAlthough actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos.\nFurthermore, this paper proposes a text-conditioning approach\nto feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former.\nExperiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning.\nFurthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.</p>\n\n",
                "matched_terms": [
                    "action",
                    "multimodal",
                    "video",
                    "videollama3",
                    "text",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Human-robot interaction using natural language has the potential to be the most effective way for human-robot collaboration for shared tasks in daily life. Human-to-human collaboration is easily achieved because humans share the required knowledge about tasks and surrounding environments, and can understand other humans&#8217; behaviors.\nIf there are unknown things, humans can use natural language to confirm what to do and how to do it.\nSuch fundamental functions for human-robot collaboration can be built using multimodal scene understanding that enables robots to interpret their environment and interact with humans based on such understanding.\nThis goal lies at the intersection of multiple avenues of research in speech understanding, audio event detection, object and action recognition in computer vision, physical sensing/manipulation for robot control, and natural language generation to interact with humans.\nAchieving effective human-robot collaboration requires significant advances in the following areas: (1) multimodal scene understanding for human and robot environments,\n(2) robot action planning based on multimodal understanding and logical and physical affordance,\n(3) robot action generation/execution according to the planning,\n(4) robot action replanning if necessary, and (5) human-robot interaction via dialogue to achieve the goals efficiently.</p>\n\n",
                "matched_terms": [
                    "action",
                    "audio",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are several works on robotic manipulation actions at a high level that have proposed how instructions can be stored and analyzed&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tenorth2013automated</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2014cognitive</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2015robot</span>]</cite>.\nInitial work utilized contrastive learning to learn a reward function to train reinforcement learning (RL) agents&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sermanet2018time</span>]</cite>.\nMore recently, there have been some works\nusing robot primitives and extraction of cost functions from task videos to enable imitation of demonstrated tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bahl2022human</span>]</cite>.\nThere has also been some work on training perception modules on large sets of manipulation data to simplify learning of manipulation tasks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">R3M_2022</span>]</cite>. Finally, there has been growing interest in using vision and language for learning diverse robot skills. There are some works on training visual language models using human instruction videos that are well aligned to robot actions to generate action sequences <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Visualtranslating4robot2018</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2D/3D_RN_Robot2021</span>]</cite>.\nThen, multimodal language models have been applied to robot action planning using various features, such as audio, visual, speech, and text, to expand knowledge acquisition of the tasks from human demonstration videos.\nInitially, multimodal scene-understanding-based robot action planning applied an audio-visual transformer trained from cooking videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori23_interspeech</span>]</cite>, where the action sequence was translated from human action descriptions.</p>\n\n",
                "matched_terms": [
                    "action",
                    "multimodal",
                    "features",
                    "sequence",
                    "audio",
                    "work",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recently, neural language models such as Large Language Models (LLMs) have been applied to bridge the gaps between sensing information and abstract-level understanding for robot action planning.\nThis framework allows us to implement an end-to-end approach to build a human-robot collaboration system directly from multimodal scene understanding addressing all the steps (1) to (5).\nPromising use cases of LLMs were reported in the robotics research field, such as CLIPort <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cliport2021</span>]</cite> and SayCan <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SayCan2022</span>]</cite> in creating robotic agents that perform open-vocabulary tasks.\nPROGPROMPT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">singh2023progprompt</span>]</cite> introduces a programmatic LLM prompt structure that facilitates the generation of plans in diverse environments, robot functionalities, and tasks.\nLLM-POP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Lingfeng_ICRA24</span>]</cite> targets partially observable task planning, where LLMs expand vocabulary and context considerations, while visual grounding LLMs enhance spatial reasoning capabilities. COWP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ding2023integrating</span>]</cite> introduces an LLM-based open-world task planning system for robots.\nSome works explore using LLMs to directly predict a dense sequence of end-effector poses for robot actions with vision models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kwon2024language</span>]</cite>.\nAnother study&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">raman2022planning</span>]</cite> explores re-prompting strategies to enhance the executability and accuracy of LLM-generated plans but relies strictly on templated prompts.\nMultimodal scene understanding has significantly advanced with AV-transformers, and has been applied to robot action planning with human action videos&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori23_interspeech</span>]</cite>.\nDespite its success, its capability for multimodal reasoning is still limited because it was impractical to cover all combinations of multimodal inputs from the existing videos.</p>\n\n",
                "matched_terms": [
                    "action",
                    "multimodal",
                    "sequence",
                    "prompt",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate the discrepancy issues between the training and inference stages due to the data sparsity in multimodal fusion, BLIP2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BLIP2_ICML23</span>]</cite>, narrowing down the semantic space, was applied to train a multimodal large language model (AVBLIP) for robot action planning <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>]</cite>.\nVarious features are embedded into the semantic space of LLMs by training a Q-former (cross-token transformer),\nwhich is an effective method in terms of computational efficiency and information retention compared to conventional fusion methods\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">an2025llmcentricmultimodalfusionsurvey</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wadekar2024evolutionmultimodalmodelarchitectures</span>]</cite>,\nusing both contrastive loss and action generation loss in this approach.\nThe Q-former-based multimodal LLM contributes to enhancing the performance of robot action planning.</p>\n\n",
                "matched_terms": [
                    "action",
                    "multimodal",
                    "features",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this research trend, this paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation.\nThe state-of-the-art approach leveraging AVBLIP generates robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although action sequence dependency can be captured through an entire video to achieve the goal of a long-horizon task, the current single-clip-based approaches do not apply such long context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, the conventional AVBLIP feeds text embeddings into multimodal Q-former and could unexpectedly be degraded due to the high abstraction of the language information in the multimodal semantic space, thereby losing rich information represented in concrete words. To retain the original rich language information, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder that generates robot action confirmation and robot action steps.\nFinally, we show that the long-context Q-former improves the confirmation and action planning by integrating\nVideoLLaMA3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "action",
                    "multimodal",
                    "sequence",
                    "video",
                    "videollama3",
                    "text",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The main contributions of this work consist of (a) proposing a long-context Q-former for robot action confirmation sentence generation,\n(b) introducing a text conditioning that feeds text embedding vectors to an LLM decoder to retain original language information, (c) demonstrating the effectiveness of the proposed approaches for robot confirmation sentence and robot action sequence generation in the cooking domain, and\n(d) examining the contribution of VideoLLaMA.</p>\n\n",
                "matched_terms": [
                    "action",
                    "conditioning",
                    "sequence",
                    "work",
                    "text",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">a) Robot Action Description:</span>\nYouCook2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>http://youcook2.eecs.umich.edu/</span></span></span>\nis already annotated with human instructions in natural language to describe human cooking action steps, as introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhou2018towards</span>]</cite>.\nThe cooking action steps for each video are annotated with start and end time stamps and English description. An example of the description is &#8220;Grill the tomatoes in a pan and then put them on a plate&#8221; starting from 00:21 and ending at 00:52.</p>\n\n",
                "matched_terms": [
                    "action",
                    "video",
                    "description"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">b) Robot Micro-step Action:</span>\nIn the work of <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>,\nthe human instructions were translated into\na micro-step action sequence such that a single-arm robot could achieve the same goals as humans demonstrated actions, as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nAlthough a humanoid robot is illustrated in this example, action steps are\ndesigned for comoditized single-arm robots.\nThe micro-step action sequences for single-arm robot action are represented by &#8220;single-arm action&#8221;, &#8220;target object&#8221;, &#8220;preposition&#8221;, and &#8220;place&#8221;, to achieve the same actions as humans.\nWe borrowed the same data conditions used in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>.\nSingle-arm actions were selected from the following 12 candidates:\nOpen, Close, Pick, Place, Pour, Stir, TurnOn, TurnOff, Wipe, Cut, Scoop, Squeeze. The target objects were selected as one of the nouns in the human action instruction as much as possible.</p>\n\n",
                "matched_terms": [
                    "action",
                    "used",
                    "work",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work employs AVBLIP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>]</cite>, where BLIP-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">BLIP2_ICML23</span>]</cite>, a vision-language pre-training method, is extended to handle multimodal features.\nBLIP-2 bootstraps from a frozen image encoder and a frozen LLM, where a Querying Transformer (Q-former)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">carion2020end</span>]</cite> is trained to bridge the gap between the vision and text modalities.\nThe image encoder in BLIP-2 is replaced with audio-visual encoders for video, audio, and text feature sequences in AVBLIP.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates an architecture of AVBLIP consisting of a Q-former and an LLM decoder.\nThe Q-former is trained to extract a fixed number of embeddings from a multimodal encoder that outputs sequences with different lengths.\nThe self-attention layers are shared between two transformer submodules: (1) a multimodal transformer that interacts with the frozen audio-visual encoders and (2) a text transformer that works as a text encoder and a text decoder.\nA set of learnable query embeddings is input to the multimodal transformer.\nThe queries interact with each other through self-attention layers and interact with audio-visual features through cross-attention layers.\nThe queries can additionally interact with the text through the same cross-attention layers. Finally, the queries are converted to an output feature.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "features",
                    "audio",
                    "video",
                    "work",
                    "feature",
                    "image",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LLM Decoder generates a sequence of micro-step actions for a single-arm robot from multimodal features aligned to language features obtained by the Q-former.\nThe LLM Decoder is constructed with a frozen LLM and a feed-forward layer. By using the LLM as a decoder, it leverages the LLM&#8217;s inference capabilities when generating action sequences. In this study, we use OPT-2.7B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022opt</span>]</cite> as the LLM.</p>\n\n",
                "matched_terms": [
                    "action",
                    "multimodal",
                    "features",
                    "sequence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training of AVBLIP consists of two stages: (1) vision-language representation learning with frozen multimodal encoders and (2) vision-to-language generative learning with a frozen LLM.\nIn the second stage, we connect the Q-former to the frozen LLM Decoder and perform multimodal action sequence generation. As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the extracted multimodal features from the video clip are converted to token embeddings using the Q-former. The embedding vectors are then projected to the LLM embedding space using a fully-connected layer. Then, the LLM Decoder generates action sequences and descriptions (confirmation sentences) for the given video clip. We use the cross-entropy loss function for the ground-truth sequences in this stage.</p>\n\n",
                "matched_terms": [
                    "action",
                    "multimodal",
                    "features",
                    "sequence",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The AVBLIP model in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F2\" title=\"Figure 2 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> can generate action sequences and confirmation sentences for short video clips of around 10-20 seconds. However, these clips are part of a longer instruction video, consisting of 5-10 sequential clips that aim at a single goal, e.g., &#8220;cooking meatloaf&#8221;.\nTherefore, incorporating contextual information from the previous and succeeding video clips is a promising extension of the model to generate more accurate sequences, because the video clips are interdependent: for example, some items cooked in a clip may be used in the next clip.</p>\n\n",
                "matched_terms": [
                    "action",
                    "used",
                    "video",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F3\" title=\"Figure 3 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows an extended AVBLIP architecture based on a long-context Q-former, which contains two Q-former modules, one processes multi-modal features from the current clip as originally used and the other processes the features from the surrounding clips to utilize them to enhance the output for the current clip, where the surrounding clips could contain the current one.\nBoth Q-formers output token embeddings, which are then combined using a transformer encoder.\nFinally, the LLM decoder receives the combined embeddings to generate micro-step actions for the robot and action description (as a confirmation) for the human.\nThe two Q-formers and the transformer encoder are jointly trained in the same manner as the original AVBLIP framework described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S3\" title=\"III Robot action generation using AVBLIP &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>.</p>\n\n",
                "matched_terms": [
                    "action",
                    "multimodal",
                    "features",
                    "description",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As demonstrated in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Motonari_ICRA24_WS4Cooking</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>, the AVBLIP model can generate both micro-step action sequences and action descriptions by feeding query token embeddings to a pre-trained LLM, where the LLM will not further be trained or finetuned.\nThus, the Q-former is trained to generate token embeddings that convey the semantic information of the video clip as well as instructions to the LLM to generate a micro-step action sequence or an action description.\nAlthough the set of embeddings is a compact and effective representation, it may not retain the detailed information necessary for the LLM to generate fine-grained action sequences.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "video",
                    "description",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In principle, directly feeding multi-modal features and a natural language instruction to the LLM could be more effective in generating accurate action sequences.\nHowever, it is challenging to fine-tune the LLM to understand the multimodal features using limited training data and further to manually create prompts that generate specialized sequences, such as robot micro-step actions. In other words, the Q-former-based approach remains beneficial for obtaining suitable token embeddings, which serve as prompts for the pre-trained LLM.\nHowever, there is a concern that precise semantic information could be lost. In particular, text information, such as speech subtitles obtained by automatic speech recognition, often contains exact keywords indicating specific ingredients and utensils. Therefore, it is reasonable that only the text information is fed to the LLM directly with minimal information loss.</p>\n\n",
                "matched_terms": [
                    "action",
                    "multimodal",
                    "features",
                    "only",
                    "text",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work applies text conditioning to the LLM by prepending the text information to the Q-former token embeddings as in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S2.F3\" title=\"Figure 3 &#8227; II Action Planning Data &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, where the text is tokenized and embedded by the same LLM.\nThe text can be subtitles in the video and/or automatic video descriptions generated by an external multimodal LLM such as VideoLLaMA <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2023videollama</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">videoSALMONN</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "conditioning",
                    "video",
                    "work",
                    "text",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed methods were tested using YouCook2 consisting of cooking action video clips aligned with human action instruction in natural language. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T1\" title=\"TABLE I &#8227; VI-A Setup &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> shows the data statistics. In this work, we used the validation set for evaluation since the test set was not publicly available.</p>\n\n",
                "matched_terms": [
                    "action",
                    "used",
                    "work",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal features such as video, image, and audio were extracted using Omnivore <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">girdhar2022omnivore</span>]</cite>, Contrastive Language-Image Pre-Training (CLIP) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2021learning</span>]</cite>, and Audio Spectrogram Transformer (AST) <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gong21b_interspeech</span>]</cite>, respectively.\nThe image and video features were concatenated and projected to a single video feature sequence before feeding them to the encoder.\nIf a subtitle was available in the video, text features were extracted by Glove word embedding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pennington2014glove</span>]</cite>. Otherwise, we fed an embedding vector for the <span class=\"ltx_text ltx_font_typewriter\">&lt;unk&gt;</span> label.\nThe numbers of dimensions of the audio, visual, and text features are <math alttext=\"768\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mn>768</mn><annotation encoding=\"application/x-tex\">768</annotation></semantics></math>, <math alttext=\"1024\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m2\" intent=\":literal\"><semantics><mn>1024</mn><annotation encoding=\"application/x-tex\">1024</annotation></semantics></math>, and <math alttext=\"300\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m3\" intent=\":literal\"><semantics><mn>300</mn><annotation encoding=\"application/x-tex\">300</annotation></semantics></math>, respectively.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "features",
                    "sequence",
                    "audio",
                    "video",
                    "feature",
                    "subtitle",
                    "image",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialized the Q-former with the pre-trained weights of BERTbase&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span>]</cite>, while the cross-attention layers were randomly initialized. We set the number of dimensions of the hidden layers to 768, which results in 188M parameters in total.\nIn the experiments, we used 32 queries, where each query has a 768-dimensional vector, which equals the hidden dimension of the Q-former.\nFor the long-context Q-former, we used 32 queries for each of the Q-former modules.\nThe transformer encoder to combine the contextual and current token embeddings had two transformer blocks, each of which had a self-attention layer and a full-connected layer with 768-dimensional hidden activations. These Q-formers and the transformer were jointly trained from scratch without any shared parameters.\nFor the text conditioning, speech subtitles annotated by YouTube and video descriptions obtained from VideoLLaMA3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite> were fed into the LLM decoder.</p>\n\n",
                "matched_terms": [
                    "video",
                    "videollama3",
                    "text",
                    "used",
                    "conditioning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance was evaluated using the BLEU-2 and METEOR scores computed between the generated and ground-truth sequences used in the robotics field <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Visualtranslating4robot2018</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">2D/3D_RN_Robot2021</span>]</cite>.\nWe applied cross-validation, where one half of the validation set was used to select the best-epoch model, and the other half was used to measure model performance.\nThus, the shown scores are the average over the two subsets.</p>\n\n",
                "matched_terms": [
                    "meteor",
                    "used",
                    "bleu2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T3\" title=\"TABLE III &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">III</span></a> shows the quality of action sequences and descriptions generated by the long-context Q-former.\n&#8220;Baseline&#8221; denotes the Q-former model trained with pairs of multimodal features and their target sequences without contextual information.\nUnlike the method in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite> that trains a single model to generate both action sequences and descriptions in a multitask manner, our baseline model was trained to generate action sequences and descriptions separately. Although we can use the same technique to achieve slightly better quality, we omitted that process for simplicity. In addition, our baseline model already achieves a quality comparable to the result of multitask training in <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hori_2025_ICASSP</span>]</cite>. This could be because there are some differences in the hyperparameter setting, where we tuned the batch size, the learning rate, and the number of epochs in preliminary experiments.</p>\n\n",
                "matched_terms": [
                    "action",
                    "multimodal",
                    "features",
                    "denotes",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can see certain gains in BLEU and METEOR scores by using the long-context Q-former. The three rows after the baseline correspond to the results of left-context expansion, where introducing the previous two clips is sufficient, and we do not have to consider the full left context.\nThe next three rows display the results of introducing both left and right contexts, yielding further gains. Similar to the left-context expansion, it is sufficient to include two previous and two following clips to achieve the best quality.\nThese results demonstrate that the contextual information helps action and description generation and the long-context Q-former can effectively utilize that information.\nNote that the training time of the long-context Q-former with full-video context (*,0,*) took 8.2 hours on a single A40 GPU, compared to 5.5 hours for the baseline, when performing two-stage training for 30 epochs with batch size 16. The average inference time was 5.8 seconds compared to 4.1 seconds for the baseline. Thus, the computational cost for the long-context model is not crucial.</p>\n\n",
                "matched_terms": [
                    "meteor",
                    "description",
                    "information",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T4\" title=\"TABLE IV &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> shows the quality of action sequences and descriptions generated by the Q-former with text conditioning.\nFirst, speech subtitles provided a substantial gain from the baseline in all metrics.\nThis demonstrates that text conditioning of the LLM decoder is very effective for this task.\nThen, we introduced a different text conditioning with video descriptions generated by the VideoLLaMA 7B model that achieves state-of-the-art performance in multiple video understanding benchmarks <cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">damonlpsg2025videollama3</span>]</cite>.\nTo ask VideoLLaMA3 to generate rich video descriptions for text conditioning, we used a simple prompt &#8220;Describe the cooking video in detail.&#8221;\nThis approach also improved the quality of the action sequences and descriptions, which are close to those with speech subtitles.\nThen, we concatenated two text sequences and fed them to the LLM (&#8220;VideoLLaMA3+subtitle&#8221;), and obtained further improvement.\nThis result shows that rich and precise information is very useful when it is provided as an LLM&#8217;s context.\nFurther, we tested the cases that use generated action descriptions and ground-truth descriptions.\nWith the descriptions generated by VideoLLaMA3+subtitle conditioning, the action sequence quality degraded. This is because the generated descriptions were much shorter than those of VideoLLaMA3+subtitle, and a certain level of information could be lost.\nThe ground-truth descriptions were concise, yet they provided accurate descriptions, achieving the highest quality, which is considered the upper bound of the text conditioning approach.</p>\n\n",
                "matched_terms": [
                    "action",
                    "sequence",
                    "video",
                    "videollama3",
                    "prompt",
                    "text",
                    "used",
                    "conditioning",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we combined the long-context Q-former with text conditioning.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T5\" title=\"TABLE V &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> shows the quality of the generated sequences using the combined approach.\nThe results indicate that the performance gains by long-context Q-former and text conditioning are additive, achieving the best BLEU and METEOR scores.\nThe relative gains of the scores from the baseline are 16.7% and 11.9% for action sequences and 17.9% and 15.1% for action descriptions, respectively.</p>\n\n",
                "matched_terms": [
                    "meteor",
                    "text",
                    "conditioning",
                    "action"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.17335v1#S6.T2\" title=\"TABLE II &#8227; VI-B Results &#8227; VI Experiments &#8227; Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM\"><span class=\"ltx_text ltx_ref_tag\">II</span></a> shows examples of generated action sequences and descriptions, where the rows contain the reference, the baseline result, the sequence generated by the proposed method, the speech subtitle, and the VideoLLaMA3 description. They also include generated descriptions for the previous and the following clips in the second example. The proposed method used both long-context expansion and text conditioning.\nIn the first example, the errors in the baseline result are corrected by the proposed method, where the word &#8220;spring roll&#8221; is substituted with the correct word &#8220;fish&#8221;. This shows that the VideoLLaMA description successfully supported the correct word through text conditioning.\nIn the second example, the object &#8220;potato&#8221; in the action sequence and &#8220;parmesan cheese&#8221; in the action description are corrected to &#8220;cabbage&#8221; using the proposed model. This correction could be made using information from the multimodal features of the previous and following clips, as the model predicted the word. Although the VideoLLaMA description also included &#8220;cabbage&#8221;, the error could not be fixed when the long-context features were not used. We also have several examples in which speech subtitles support correcting the action sequences and descriptions, but we omit them due to space limitations.\nAs in those examples, the long context and text conditioning provide helpful information to generate accurate action sequences and descriptions.</p>\n\n",
                "matched_terms": [
                    "action",
                    "multimodal",
                    "subtitle",
                    "features",
                    "sequence",
                    "videollama3",
                    "description",
                    "text",
                    "used",
                    "conditioning",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposed a method for robot action sequence and confirmation sentence generation that leverages (a) a long-context Q-former considering left and right context dependency in full videos and (b) a text-conditioned LLM decoder to retain the precise language information.\nWe trained the above proposed models, generating single-arm robot micro-step action sequences and robot action confirmation in natural language using the YouCook2 dataset.\nTo mitigate the sparseness of the human action descriptions, we leveraged the video descriptions obtained from VideoLLaMA3.\nExperimental results show that our proposed long-context Q-former outperformed the baseline model for all metrics.\nWe confirmed that the combination of all multimodal features and text-conditioning performed the best.\nFuture work includes (1) multitask training of robot action steps and action descriptions, (2) comparison with other multimodal fusion approaches such as simple sequence concatenation or pooling+MLP, (3) evaluation for tasks other than cooking, and (4) evaluation using a simulator or a real robot.</p>\n\n",
                "matched_terms": [
                    "action",
                    "multimodal",
                    "features",
                    "sequence",
                    "video",
                    "videollama3",
                    "work",
                    "information"
                ]
            }
        ]
    }
}