{
    "S2.T1": {
        "source_file": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations",
        "caption": "Table 1: Comparison of spatial audio datasets, where T denotes speech transcripts, P represents sound source positions, N indicates natural language prompts, and C stands for sound class tags",
        "body": "Dataset\nAudio Format\nCollect\nHours\nType\nVisual\nLabel\n\n\nFOA\nMulti\nBinaural\nT\nP\nN\nC\n\n\n\n\nSpatial LibriSpeech\n✓\n✓\n✗\nSimulated\n650\nSpeech\n-\n✓\n✓\n✗\n✗\n\n\nYT-Ambigen\n✓\n✗\n✗\nCrawled\n142\nALL\nVideo\n✗\n✗\n✗\n✗\n\n\nBEWO-1M\n✗\n✗\n✓\nCraw+Sim\n2800\nALL\nImage\n✗\n✗\n✓\n✗\n\n\nFAIR-Play\n✗\n✗\n✓\nRecorded\n5.2\nMusic\nVideo\n✗\n✗\n✗\n✗\n\n\nSTARSS23\n✓\n✓\n✗\nRecorded\n7.5\nALL\nVideo\n✗\n✓\n✗\n✓\n\n\nBinauralMusic\n✗\n✗\n✓\nCrawled\n15.2\nMusic\nVideo\n✗\n✗\n✗\n✓\n\n\nRealMAN\n✗\n✓\n✗\nRecorded\n228.2\nSpeech\nImage\n✓\n✓\n✗\n✗\n\n\nSphere360\n✓\n✗\n✗\nCrawled\n288\nALL\nVideo\n✗\n✗\n✗\n✗\n\n\nMRSAudio (Ours)\n✓\n✗\n✓\nRecorded\n484\nALL\nVideo\n✓\n✓\n✓\n✓",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audio Format</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Collect</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Hours</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Visual</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Label</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">FOA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">Multi</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Binaural</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">T</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">P</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">N</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">C</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Spatial LibriSpeech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Simulated</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">650</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">YT-Ambigen</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Crawled</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">142</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ALL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Video</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">BEWO-1M</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Craw+Sim</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2800</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ALL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Image</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">FAIR-Play</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Recorded</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Music</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Video</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">STARSS23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Recorded</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ALL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Video</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">BinauralMusic</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Crawled</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">15.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Music</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Video</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">RealMAN</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Recorded</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">228.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Image</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Sphere360</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Crawled</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">288</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ALL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Video</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MRSAudio (Ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Recorded</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">484</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ALL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Video</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "type",
            "crawsim",
            "source",
            "visual",
            "librispeech",
            "ours",
            "collect",
            "video",
            "datasets",
            "format",
            "hours",
            "class",
            "multi",
            "recorded",
            "speech",
            "starss23",
            "positions",
            "label",
            "all",
            "tags",
            "where",
            "foa",
            "crawled",
            "simulated",
            "represents",
            "language",
            "spatial",
            "stands",
            "mrsaudio",
            "dataset",
            "transcripts",
            "denotes",
            "binaural",
            "image",
            "fairplay",
            "prompts",
            "indicates",
            "sphere360",
            "ytambigen",
            "binauralmusic",
            "sound",
            "music",
            "natural",
            "realman",
            "comparison",
            "audio",
            "bewo1m"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, existing datasets vary in their goals and modalities. For instance, Spatial LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Sarabia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib32\" title=\"\">2023</a>)</cite> simulates spatial speech using the LibriSpeech corpus and is mainly intended for binaural TTS applications. RealMAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib40\" title=\"\">2024</a>)</cite> is a large-scale real-recorded and annotated 32-channel microphone array dataset for multichannel speech enhancement and source localization. YT-Ambigen <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>)</cite> and Sphere360 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite> are derived from web-crawled video datasets, but lack explicit spatial annotations, making them suitable primarily for video-to-spatial audio generation. BinauralMusic focuses on musical content, offering instrument class tags. BEWO-1M <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>)</cite> combines crawled content and simulated binaural rendering, and provides image or GPT-generated prompts. STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> features real-world FOA and multi-channel audio alongside synchronized videos and includes sound class and position labels.\nIn contrast to prior datasets, MRSAudio offers a comprehensive, large-scale, and real-world spatial audio corpus, featuring 484 hours of recorded data in both FOA and binaural formats, covering all audio domains including general audio, speech, singing, and music. Uniquely, MRSAudio includes synchronized audio, video, and 3D positional geometry, along with fine-grained cross-modal annotations, such as: transcripts, word and phoneme boundaries and music scores.\nThese features make MRSAudio an ideal resource for a broad range of spatial audio generation and understanding tasks, including spatial speech, music, and singing voice synthesis.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive spatial environments, where auditory cues enable sound source localization in three-dimensional space.\nDespite the critical role of spatial audio in immersive technologies such as VR/AR, most existing multimodal datasets provide only monaural audio, which limits the development of spatial audio generation and understanding.\nTo address these challenges, we introduce MRSAudio, a large-scale multimodal spatial audio dataset designed to advance research in spatial audio understanding and generation.\nMRSAudio spans four distinct components: MRSLife, MRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios.\nThe dataset includes synchronized binaural and ambisonic audio, exocentric and egocentric video, motion trajectories, and fine-grained annotations such as transcripts, phoneme boundaries, lyrics, scores, and prompts.\nTo demonstrate the utility and versatility of MRSAudio, we establish five foundational tasks: audio spatialization, and spatial text to speech, spatial singing voice synthesis, spatial music generation and sound event localization and detection.\nResults show that MRSAudio enables high-quality spatial modeling and supports a broad range of spatial audio research.\nDemos and dataset access are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mrsaudio.github.io\" title=\"\">https://mrsaudio.github.io</a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial",
                    "source",
                    "mrsaudio",
                    "sound",
                    "dataset",
                    "where",
                    "transcripts",
                    "music",
                    "video",
                    "binaural",
                    "datasets",
                    "prompts",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive and interpret physical environments. With the rapid growth of film, virtual reality (VR), augmented reality (AR), and gaming applications, users increasingly expect not only precise audiovisual alignment but also highly immersive experiences. While recent advances in deep learning have enabled realistic generation of speech, music, and sound effects synchronized with text or video <cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib25\" title=\"\">2022</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib41\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib37\" title=\"\">2024</a>)</cite>, most models focus on monaural audio and neglect spatialized soundscapes that enhance immersion. The human binaural system uses interaural time differences (ITD) and interaural level differences (ILD) to localize sound in three-dimensional space, and spatial audio must remain consistent with visual cues <cite class=\"ltx_cite ltx_citemacro_citep\">(Yost, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib42\" title=\"\">1998</a>; Cohen &amp; Knudsen, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib12\" title=\"\">1999</a>; Grothe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib18\" title=\"\">2010</a>)</cite>. Any mismatch can disrupt immersion and weaken the sense of presence. For example, hearing a cat&#8217;s meow from the left immediately suggests its location, even if it is just off-screen.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial",
                    "visual",
                    "sound",
                    "music",
                    "video",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the growing importance of spatial audio in these immersive technologies<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib39\" title=\"\">2020</a>; Huiyu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib22\" title=\"\">2025</a>)</cite>, progress in machine learning for spatial audio understanding is limited by the fundamental spatial data constraints. Most existing audio datasets<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib17\" title=\"\">2017</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib9\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib3\" title=\"\">2023</a>)</cite> focus on monaural recordings, which discard vital spatial information, effectively \"flattening\" the soundscape and preventing models from learning key physical phenomena such as room reverberation, echo patterns, and sound propagation. Moreover, the scarcity of multimodal datasets that align spatial audio with synchronized visual, position geometric, and semantic annotations hinders the development of advanced auditory scene-analysis systems capable of human-like spatial perception.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "visual",
                    "sound",
                    "datasets",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these gaps, we present <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a 484-hour large-scale multimodal spatial audio dataset designed to support both spatial audio understanding and generation. It integrates high-fidelity spatial recordings with synchronized video, 3D pose tracking, and rich semantic annotations, enabling comprehensive modeling of real-world auditory scenes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S0.F1\" title=\"Figure 1 &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the dataset comprises four subsets, each targeting distinct tasks and scenarios.\n<span class=\"ltx_text ltx_font_bold\">MRSLife</span> (129 h) captures daily activities such as board games, cooking, and office work, using egocentric video and FOA audio annotated with sound events and speech transcripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSpeech</span> (206 h) includes binaural conversations from 44 speakers across diverse indoor environments, paired with video, 3D source positions, and complete scripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSing</span> (80 h) features high-quality solo singing performances in Chinese, English, German, and French by 20 vocalists, each aligned with time-stamped lyrics and corresponding musical scores.\n<span class=\"ltx_text ltx_font_bold\">MRSMusic</span> (69 h) offers spatial recordings of 23 Traditional Chinese, Western and Electronic instruments, with symbolic score annotations that support learning-based methods for symbolic-to-audio generation and fine-grained localization.\nTogether, these four subsets support a broad spectrum of spatial audio research problems, including event detection, sound localization, and binaural or ambisonic audio generation. By pairing spatial audio with synchronized exocentric and egocentric video, geometric tracking, and detailed semantic labels, MRSAudio enables new research directions in multimodal spatial understanding and cross-modal generation. Throughout this paper, unless stated otherwise, we use the term spatial audio to refer to binaural audio.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "positions",
                    "spatial",
                    "source",
                    "mrsaudio",
                    "sound",
                    "dataset",
                    "transcripts",
                    "foa",
                    "video",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a 484-hour, large-scale multimodal spatial audio dataset explicitly designed to push the boundaries of spatial audio understanding and generative modeling.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrsaudio",
                    "spatial",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assemble synchronized binaural and ambisonic recordings with exocentric and egocentric video, geometric source positions, transcripts, scores, lyrics, and event labels, providing one of the most richly annotated multimodal resources for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "positions",
                    "spatial",
                    "source",
                    "transcripts",
                    "video",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish and release evaluation protocols and baseline implementations for five benchmark tasks: audio spatialization generation, spatial text-to-speech, spatial singing voice synthesis, spatial music generation, and localization and detection of sound events, in order to demonstrate MRSAudio&#8217;s versatility and to foster reproducible research.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "spatial",
                    "audio",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2\" title=\"2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reviews existing spatial audio datasets.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3\" title=\"3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> describes the design, collection process, and key statistics of MRSAudio.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4\" title=\"4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents extensive benchmark experiments using state-of-the-art methods on five core spatial audio tasks: audio spatialization, spatial text-to-speech, spatial singing voice generation, spatial music synthesis, and sound event localization and detection.\nFinally, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S5\" title=\"5 Conclusion and Discussion &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> concludes the paper and discusses the limitations and potential risks associated with MRSAudio.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "mrsaudio",
                    "sound",
                    "music",
                    "datasets",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning has driven remarkable progress in both audio generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib20\" title=\"\">2023a</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib41\" title=\"\">2023</a>; Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib25\" title=\"\">2022</a>)</cite> and audio understanding <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib11\" title=\"\">2023</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib21\" title=\"\">2023b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib35\" title=\"\">2024</a>)</cite> tasks. However, the majority of these advances still rely heavily on monaural audio, which lacks the ability to represent or capture the rich spatial cues that naturally occur in real-world environments.\nThe rapid adoption of VR/AR technologies has concurrently driven growing demand for immersive spatial audio experiences. Researchers have focused on several key technologies including: sound event localization and detection <cite class=\"ltx_cite ltx_citemacro_citep\">(Adavanne et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib2\" title=\"\">2019</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib36\" title=\"\">2022</a>)</cite>, mono-to-spatial audio conversion <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao &amp; Grauman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib15\" title=\"\">2019</a>; Pedro&#160;Morgado &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib31\" title=\"\">2018</a>)</cite>, and end-to-end spatial audio generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite recent progress, spatial audio generation and understanding remain constrained by the paucity of high-quality datasets. Due to the difficulty and expense of collecting and annotating high-quality spatial audio datasets, most open-source datasets still primarily consist of simulated or web-crawled content.\nSimulated datasets offer precise annotations but lack perceptual realism, while crawled datasets often provide real-world diversity but are missing critical labels, such as listener and source positions and content-level annotations, thus limiting their utility.\nFor instance, spatial symbolic music generation demands accurate musical scores and corresponding positional metadata.\nTo better understand the current landscape, we survey existing spatial audio datasets. These datasets differ in terms of audio format, including First-Order Ambisonics (FOA), multi-channel arrays, and binaural microphones, as well as in their collection methods (simulated, crawled, recorded) and annotation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "positions",
                    "spatial",
                    "source",
                    "music",
                    "foa",
                    "crawled",
                    "simulated",
                    "binaural",
                    "datasets",
                    "format",
                    "recorded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a freely available multimodal spatial audio corpus with synchronized video, positional data, and fine-grained annotations, released under the CC BY-NC-SA 4.0 license. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our data processing pipeline, with detailed descriptions in subsequent subsections. We then summarize key statistics that demonstrate MRSAudio&#8217;s scale and diversity.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrsaudio",
                    "video",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure that MRSAudio comprehensively covers scenarios from daily life, speech, singing, and music, we conduct systematic and modular planning before recording as follows.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mrsaudio",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span> MRSSpeech targets clean, high-quality conversational recordings for TTS task. All spoken interactions are recorded in controlled indoor environments with minimal noise. We invite speakers to participate in content-driven conversations based on predefined scripts.</p>\n\n",
                "matched_terms": [
                    "all",
                    "recorded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> MRSMusic captures immersive instrumental performances suitable for spatial music generation and analysis.\nWe record solo performances from 45 professional musicians across 23 instruments, including Traditional Chinese, Western and Electronic instruments.\nEach performance is paired with its corresponding musical score to support score-based music generation.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the predefined planning, we conduct parallel data collection across all modules.\nTo ensure participant anonymity, masks are worn during recording when necessary. All participants sign an open-source data release agreement, allowing the dataset to be freely distributed for academic research purposes.\nThe recording details are summarized as follows:</p>\n\n",
                "matched_terms": [
                    "all",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span> In MRSDialogue, audio is recorded using a professional binaural recording head and high-resolution sound cards, while synchronized video is captured using industry-standard cameras. In MRSSound, in addition to binaural audio and exocentric video, we also captured FOA (Zoom H3-VR) and egocentric video (Gopro camera).To ensure the effectiveness of the egocentric video, participants are asked to remain within the frontal field of view of the binaural recording head.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sound",
                    "foa",
                    "binaural",
                    "video",
                    "recorded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span> To introduce spatial variability, we select four recording rooms that differ in size and acoustic material. Each speaker reads inflected emotions from the scripts while walking through the space, producing dynamic spatial cues. Recordings include spoken passages, binaural audio and exocentric video, as well as a clean mono audio by a lavalier microphone placed near the speaker.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "binaural",
                    "video",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span> The professional singers perform according to musical scores. To introduce variation in source&#8211;listener geometry, we adjust the position of the head-mounted binaural microphone relative to the singer. In addition to the binaural audio and synchronized video, each session includes a clean vocal track recorded with a studio-grade condenser microphone.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "binaural",
                    "video",
                    "recorded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> We record 23 Traditional Chinese, Western and Electronic instruments, performed by 45\nprofessional musicians.To capture rich spatial detail, we vary microphone placement around the instrument. Recordings include binaural audio, monaural audio, exocentric video, and synchronized video that records playing gestures. Full recording details are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS2\" title=\"A.2 Details of Recording &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "binaural",
                    "video",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To maximize MRSAudio&#8217;s utility across a wide range of tasks, we begin with event-level annotations for all vocal and acoustic content in MRSLife, MRSSpeech, MRSSing, and MRSMusic. However, coarse annotations alone are insufficient for fine-grained tasks such as singing voice modeling and music generation from scores. To bridge this gap, we design a comprehensive annotation pipeline. Full implementation details and detailed annotation guidelines are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS3\" title=\"A.3 Details of Annotation &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span>\nFor MRSDialogue, we apply WhisperX <cite class=\"ltx_cite ltx_citemacro_citep\">(Bain et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib6\" title=\"\">2023</a>)</cite> for automatic speech recognition and speaker diarization to generate initial transcripts and speaker turns. Human annotators correct recognition errors and speaker attribution mismatches. The audio is then segmented into utterances and the transcripts are converted into phoneme sequences (using pypinyin for Mandarin). A two-stage alignment process follows: we first apply the Montreal Forced Aligner (MFA) <cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib30\" title=\"\">2017</a>)</cite> for coarse word/phoneme mapping, then manually refine boundaries in Praat <cite class=\"ltx_cite ltx_citemacro_citep\">(Boersma, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib8\" title=\"\">2001</a>)</cite>.\nFor MRSSound, we annotate sound event categories and corresponding time intervals.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "sound",
                    "transcripts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nWe use voice activity detection (VAD) to segment recordings into singing regions, then align pre-existing lyrics using LyricFA&#8217;s ASR-based dynamic programming algorithm<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/wolfgitpr/LyricFA\" title=\"\">https://github.com/wolfgitpr/LyricFA</a></span></span></span>.\nPhoneme generation is language-dependent: pypinyin for Mandarin, ARPA for English, and MFA&#8217;s built-in phoneme sets for French and German. Alignment is conducted via MFA, followed by manual refinement. Melody and rhythm of singing are transcribed into MIDI format using ROSVOT <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib27\" title=\"\">2024</a>)</cite>. Annotators then label each excerpt with high-level style descriptors, such as emotional tone (happy, sad), tempo (slow, moderate, fast), and pitch range (low, medium, high).</p>\n\n",
                "matched_terms": [
                    "label",
                    "format"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span>\nWe use Audio Slicer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/flutydeer/audio-slicer\" title=\"\">https://github.com/flutydeer/audio-slicer</a></span></span></span> to segment the music recordings and generate initial symbolic annotations with basic-pitch <cite class=\"ltx_cite ltx_citemacro_citep\">(Bittner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib7\" title=\"\">2022</a>)</cite>, and then employ professional musicians to verify and adjust note onsets, offsets, and dynamics to match the performance accurately.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source Localization:</span>\nFor static sources, we manually record 3D positions relative to the capture space. For dynamic scenes, we use the Ultra-Wideband (UWB) system<cite class=\"ltx_cite ltx_citemacro_citep\">(Aiello &amp; Rogerson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib4\" title=\"\">2003</a>)</cite> to track the positions of sound sources in real time. Based on the recorded position trajectories, we generate natural language motion descriptions using GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib1\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "positions",
                    "source",
                    "sound",
                    "natural",
                    "recorded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-processing pipeline comprises three key steps to refine raw annotated data. First, segmentation splits continuous recordings into task-oriented clips: utterances for speech and singing are extracted via alignment timestamps, while MRSSound audio is uniformly divided into 10-second segments. Next, multimodal synchronization aligns each clip with auxiliary modalities (text, video, position metadata) with temporal anchors. Static sound sources are annotated with manually measured 3D coordinates, whereas dynamic sources leverage interpolated UWB tracking trajectories. For scenes where participants&#8217; faces are visible, anonymization is performed by adding half-face masks. Finally, quality assurance involves domain experts auditing 15% of clips across all modules to verify temporal alignment precision, cross-modal content consistency, and annotation accuracy.\nFull auditing protocols are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS4\" title=\"A.4 Details of Post-Processing &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>, ensuring reproducibility of this process.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "sound",
                    "where",
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To illustrate spatial diversity, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) shows the 3D distribution of sound source positions relative to the listener. The heatmap reveals near-uniform azimuthal coverage, with greater density in the frontal hemisphere due to the prevalence of egocentric video recordings. Elevation angles are concentrated between &#8211;40&#176; and 40&#176;, aligning with typical human sound perception patterns.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b) summarizes the annotations using a keyword cloud that captures the range of real-world activities recorded. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c) presents the proportions of recording spaces by room size: medium-sized rooms are the most common, accounting for approximately 40% of sessions, while small and large rooms each represent around 30%. Recording duration is evenly distributed across room types, ensuring diverse acoustic coverage.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(d) shows the distribution of segment durations after automatic and manual segmentation. Most audio clips are shorter than 10 seconds, which is suitable for modeling short-duration events and supports efficient downstream training and inference.\nOverall, MRSAudio offers comprehensive coverage across spatial positions, acoustic environments, scene types, and temporal structures, making it well-suited for a wide range of spatial audio generation and understanding tasks. A more detailed breakdown of per-scenario statistics is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1\" title=\"Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "positions",
                    "spatial",
                    "source",
                    "mrsaudio",
                    "sound",
                    "video",
                    "recorded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the quality and utility of MRSAudio in real-world scenarios, we evaluate it on five representative spatial audio tasks: (i) audio spatialization, (ii) spatial text-to-speech, (iii) spatial singing voice synthesis, (iv) spatial music generation, and (v) sound event localization and detection (SELD). These tasks cover both generation and understanding, and are critical for applications such as AR/VR, spatial media production, and perceptual scene analysis.\nAll experiments are conducted using state-of-the-art methods on a server equipped with eight NVIDIA RTX 4090 GPUs.\nFor different tasks, we employ distinct objective metrics. For generation tasks, we compute cosine similarity scores for direction (ANG Cos) and distance (DIS Cos) to quantify spatial alignment quality. Additionally, we utilize subjective MOS-Q (Mean Opinion Score for Quality) to evaluate the quality of generated audio and MOS-P (Mean Opinion Score for Position) to assess spatial perception. For implementation details of training and evaluation metrics, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2\" title=\"Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "mrsaudio",
                    "all",
                    "sound",
                    "music",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio spatialization aims to synthesize spatially immersive soundscapes from monaural inputs and source positional information. To evaluate MRSAudio on this task, we adopt BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite>, a diffusion-based generation model that predicts binaural waveforms conditioned on monaural audio and source coordinates. Since the downstream generation tasks can be formulated as predicting interaural (binaural) representations from mono audio, we employ a single-stage training scheme for all experiments. For comparison, we include a traditional signal processing baseline (DSP), which renders binaural audio using virtual source positions simulated via room impulse responses (RIRs) and head-related transfer functions (HRTFs).\nWe adopt the following objective metrics to evaluate audio quality:\n(1) W-L2: waveform L2 distance,\n(2) A-L2: amplitude envelope L2 distance,\n(3) P-L2: phase difference L2,\n(4) STFT: multi-resolution STFT loss,\n(5) PESQ: perceptual speech quality score.\nResults for the ground truth and DSP baseline are obtained by averaging across all test sets from the four MRSAudio subsets. Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS3\" title=\"B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "positions",
                    "source",
                    "all",
                    "mrsaudio",
                    "simulated",
                    "binaural",
                    "comparison",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Spatialization &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BinauralGrad achieves strong performance across all MRSAudio subsets, surpassing the classical DSP baseline on most objective and subjective metrics. The highest performance is observed on MRSLife, likely due to the relatively limited variation in sound sources within these scenes. These results demonstrate that MRSAudio&#8217;s rich spatial annotations and diverse acoustic environments provide a solid foundation for training and evaluating spatial audio generation models.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "mrsaudio",
                    "all",
                    "sound",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial TTS aims to produce high-quality speech enriched with spatial cues, thereby enhancing immersion and realism in AR/VR. Although recent advances in TTS have led to impressive improvements in speech quality, progress in spatialized speech generation remains limited due to the scarcity of spatially annotated recordings with rich, high-quality labels.\nTo evaluate the effectiveness of MRSAudio for this task, we train an end-to-end model following ISDrama <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib43\" title=\"\">2025</a>)</cite> to directly generate spatial speech from text and position data. Additionally, we compare with cascaded pipelines that combine monaural TTS models (CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib14\" title=\"\">2024</a>)</cite> and F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib10\" title=\"\">2024</a>)</cite>) with the audio spatialization module. This allows us to assess both speech generation quality and the spatial fidelity enabled by our dataset.\nFor content evaluation, we report Character Error Rate (CER) and Speaker Similarity (SIM). Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS4\" title=\"B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial",
                    "mrsaudio",
                    "dataset",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T3\" title=\"Table 3 &#8227; 4.2 Spatial Text to Speech &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the Mono+SP method achieves strong performance across most metrics. The CER remains low and comparable to the ground truth, indicating preserved linguistic accuracy after spatialization. A high SIM score reflects stable timbre learning. ANG Cos and DIS Cos show good spatial alignment with the ground truth, and subjective MOS scores confirm that the generated speech is both natural and spatially coherent. These results demonstrate that MRSSpeech provides high-quality, spatially annotated training data that enables effective and realistic spatial TTS.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "natural",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial SVS aims to produce expressive, high-quality singing voices enriched with accurate spatial cues, thereby enhancing listener immersion. While traditional SVS has advanced considerably, spatial SVS remains underexplored due to the lack of high-quality, spatially annotated datasets.\nTo evaluate MRSAudio for this task, we use the MRSSing subset to train and benchmark models. We adopt the ISDrama architecture to perform end-to-end spatial singing synthesis, incorporating note-level pitch control to enhance prosody accuracy. For comparison, we use the open-source SVS models Rmssinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib19\" title=\"\">2023</a>)</cite>. The outputs are then spatialized with BinauralGrad.\nWe employ objective metrics, including Mel-Cepstral Distortion (MCD) and F0 Frame Error (FFE), to evaluate spectral and pitch similarity between predicted and ground-truth. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS5\" title=\"B.5 Spatial Singing Voice Synthesis &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "mrsaudio",
                    "comparison",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T4\" title=\"Table 4 &#8227; 4.3 Spatial Singing Voice Synthesis &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the Mono + SP approach trained on MRSSing with pitch control achieves the best performance across most metrics. Its low MCD indicates strong spectral fidelity, and high ANG Cos and DIS Cos scores demonstrate effective spatial alignment. Subjective MOS results confirm that the generated singing voices are natural, high-quality, and spatially coherent.\nThese findings validate MRSSing as an effective resource for spatial singing voice generation.</p>\n\n",
                "matched_terms": [
                    "indicates",
                    "natural",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task aims to synthesize spatially immersive music conditioned on symbolic scores. While datasets like FAIR-Play offer high-quality instrument recordings, they lack aligned sheet music, limiting controllable generation. In contrast, our MRSMusic subset includes 23 instruments with aligned scores, enabling fine-grained, score-based spatial music synthesis.\nWe benchmark three systems on MRSMusic. First, we apply BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite> to spatialize mono recordings. Second, we utilize Make-An-Audio 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib13\" title=\"\">2023</a>)</cite> to generate mono music from MIDI scores, which is subsequently spatialized. Third, we adapt ISDrama to accept both score embeddings and spatial poses, enabling end-to-end spatial symbolic music generation.\nFor objective evaluation, we compute Fr&#233;chet Audio Distance (FAD) and FFE to evaluate the results. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS6\" title=\"B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "music",
                    "fairplay",
                    "datasets",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T5\" title=\"Table 5 &#8227; 4.4 Spatial Music Generation &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the Mono + SP pipeline achieves strong performance, benefiting from access to ground truth mono audio. The Make-An-Audio 2 + SP exhibits limitations in FAD and pitch accuracy. This suggests that general-purpose audio generation models may struggle to fully capture structured musical information from symbolic prompts. The ISDrama (Music) model generates music directly from the symbolic inputs and spatial cues, achieving better coherence and spatial alignment than the Make-An-Audio 2 + SP. These results highlight MRSMusic&#8217;s effectiveness in supporting spatially controllable music generation across diverse instruments and spatial conditions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "music",
                    "spatial",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task evaluates the ability to detect and localize sound events using MRSAudio&#8217;s spatial annotations. We follow STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> using both audio-only and audio-visual variants on the MRSSound subset.\nIn the audio-only condition, models receive either FOA or binaural waveforms as input and predict sound event classes along with 3D source coordinates. For the audio-visual condition, we extract bounding boxes of visible persons to serve as coarse visual priors, which are fused with the audio representations. We also explore architectural variations by replacing the original convolutional backbone with a Transformer encoder, allowing us to assess the impact of temporal modeling capacity on spatial prediction.\nWe evaluate model performance using four standard joint detection and localization metrics, including location-aware detection (<math alttext=\"F_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">F_{20^{\\circ}}</annotation></semantics></math>, <math alttext=\"ER_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub></mrow><annotation encoding=\"application/x-tex\">ER_{20^{\\circ}}</annotation></semantics></math>) and class-aware localization (<math alttext=\"LE_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>E</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LE_{CD}</annotation></semantics></math>, <math alttext=\"LR_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LR_{CD}</annotation></semantics></math>). Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS7\" title=\"B.7 Sound Event Localization and Detection &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.7</span></a>.</p>\n\n",
                "matched_terms": [
                    "starss23",
                    "spatial",
                    "source",
                    "visual",
                    "sound",
                    "foa",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T6\" title=\"Table 6 &#8227; 4.5 Sound Event Localization and Detection &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, model performance varies with architecture, input modality, and audio format. Transformer-based models generally outperform ConvNet baselines, particularly in reducing localization error. FOA input consistently yields better results than binaural audio, benefiting from richer spatial representation. For example, the Transformer with FOA and no visual input achieves the lowest error rate (ER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> 0.99) and highest localization recall (LR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub> 87.32). The addition of visual features improves performance in some ConvNet settings (e.g., F<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m3\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> rises from 9.33 to 13.00 with FOA), but offers limited or inconsistent gains in Transformer models, possibly due to modality mismatch or redundancy. These results highlight the value of MRSAudio&#8217;s spatial annotations and multimodal streams in supporting flexible evaluation under both unimodal and multimodal configurations.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "visual",
                    "foa",
                    "binaural",
                    "format",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce MRSAudio, a large-scale, multimodal spatial audio corpus designed to support a wide range of generation and understanding tasks. MRSAudio comprises four complementary modules, MRSLife, MRSSpeech , MRSSing, and MRSMusic, each captured with binaural/FOA audio, synchronized video, precise 3D pose metadata, and richly detailed annotations (event labels, transcripts, phoneme boundaries, lyrics, musical scores, and motion prompts). Through extensive benchmarks on audio spatialization, binaural speech, singing, and music generation , as well as sound event localization and detection, we demonstrate that MRSAudio&#8217;s scale, diversity, and annotation depth enable state&#8208;of&#8208;the&#8208;art performance and unlock new avenues for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial",
                    "mrsaudio",
                    "sound",
                    "transcripts",
                    "music",
                    "video",
                    "binaural",
                    "prompts",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Directions:</span>\nWhile MRSAudio offers broad multimodal coverage, two limitations remain. First, although synchronized video is provided for all recordings, current benchmarks only explore visual input in a limited subset of tasks. Second, while the dataset includes both binaural and First-Order Ambisonic (FOA) formats, the FOA subset is smaller in scale, and most tasks focus on binaural audio, limiting spatial modeling diversity.\nFuture work will expand the role of visual modalities in tasks such as sound localization and scene understanding. We also plan to increase FOA recordings to balance data distribution and support broader spatial audio research, and develop more FOA-specific benchmarks to better utilize ambisonic spatial cues.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "all",
                    "visual",
                    "mrsaudio",
                    "sound",
                    "dataset",
                    "foa",
                    "video",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Societal Impact:</span>\nAs with any large-scale audiovisual dataset, MRSAudio carries potential risks if misused. It could be exploited to generate highly realistic yet synthetic spatial audio for deepfakes or disinformation in AR and VR applications. To mitigate such risks, MRSAudio is released under a noncommercial license with clear usage guidelines. We encourage responsible use in accordance with ethical standards, including consent management, data governance, and transparency.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrsaudio",
                    "spatial",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the planning phase, we systematically analyze real-world application scenarios for spatial audio and divide them into four major categories. MRSLife focuses on daily environmental sound events, MRSSpeech targets high-quality conversational data, MRSSing captures solo vocal performances, and MRSMusic records immersive instrumental music. These four scenarios collectively cover a broad range of everyday acoustic contexts and are designed to support a wide spectrum of downstream tasks.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "spatial",
                    "audio",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on predefined scenario requirements, we recruit professionals from relevant fields to participate in the recording process. To ensure spatial diversity, we rent various venues tailored to each scenario type. Following the FAIR-Play protocol, we use a 3Dio Free Space XLR binaural microphone to capture spatial audio, GoPro HERO cameras and mobile phones to record exocentric and egocentric videos, and UWB-based tracking systems to capture motion trajectories.\nOnce personnel, locations, and equipment are secured, we prepare task-specific materials for each subset. For MRSLife, we further divide the content based on the proportion of speech involved, resulting in two subcategories: MRSDialogue (e.g., group games, board games) and MRSSound (e.g., kungfu, kitchens, offices, sports). We predefine common sound events for each, such as clattering dishes, keyboard typing, and table tennis.\nFor MRSSpeech, we compile a large corpus of scripts from movies, TV shows, and crosstalk performances and automatically extract dialogue passages for speaker delivery.\nFor MRSSing, we design lyrics in four languages (Chinese, English, German, and French) and recruit singers across a range of vocal types to maximize diversity.\nFor MRSMusic, we collect solo performances across 23 traditional and modern instruments, covering a wide array of timbres and playing techniques.\nSpecific sound categories for each subset are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.T7\" title=\"Table 7 &#8227; A.1 Details of Planning &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "type",
                    "spatial",
                    "sound",
                    "collect",
                    "binaural",
                    "fairplay",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit a large number of participants with professional backgrounds in singing, music, and language to contribute to the recording process. To protect their identity, participants are asked to wear masks in appropriate scenarios. Prior to participation, all individuals sign consent forms agreeing to the open-source release of their audio and video under the CC BY-NC-SA 4.0 license.</p>\n\n",
                "matched_terms": [
                    "language",
                    "all",
                    "music",
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio is recorded in WAV format at a sampling rate of 48 kHz. Video is recorded at a minimum resolution of 1080p and 24 frames per second, and is later standardized to this format during post-processing.</p>\n\n",
                "matched_terms": [
                    "all",
                    "recorded",
                    "video",
                    "format",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span>\nWe recruit 62 participants to perform daily activities including board games, cooking, exercise, and office work. In MRSDialogue, each participant is compensated $30 per recorded hour. Binaural audio is captured using head-mounted microphones, and third-person video is recorded. In MRSSound scenes such as kung fu or kitchen demonstrations, performers are paid $50 per hour. Both binaural and FOA (First-Order Ambisonics) recordings are collected, along with first-person and third-person video. The binaural dummy head is co-located with the egocentric camera and the Zoom H3-VR (Ambisonic recorder). The egocentric camera is rigidly aligned with the head&#8217;s gaze to capture first-person visuals, while the Ambisonic recorder is mounted 7.5 cm above the head. An exocentric camera is placed at a surveyed position in the scene to provide a third-person view of the environment and object relationships.\nParticipants receive brief scene descriptions and are asked to act naturally while maintaining a single active sound source when possible. The total duration for MRSLife recordings reaches approximately 150 hours.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "source",
                    "sound",
                    "foa",
                    "video",
                    "binaural",
                    "hours",
                    "recorded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span>\nWe employ 44 expressive speakers to read from scripted texts, each paid $30 per hour of recorded audio. The total recorded duration reaches 200 hours, with compensation totaling $40,000. During recording sessions, speakers alternate between standing and walking around the recording area to introduce spatial diversity while maintaining speech clarity. Binaural audio is captured using head-mounted microphones, and clean monaural speech is recorded using lavalier microphones. All sessions are filmed from a third-person perspective.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "spatial",
                    "all",
                    "binaural",
                    "hours",
                    "recorded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nEighteen professional singers participate in the recordings. Each singer is fluent in at least one of the following languages: Chinese, English, German, or French, and collectively they cover all vocal ranges including soprano, alto, tenor, and bass. Performers are paid $50 per hour, contributing a total of 80 hours of audio. Singing is recorded at a fixed position to ensure spatial consistency. Binaural recordings are captured with head-mounted microphones, and monaural audio is recorded with a studio-grade microphone. Third-person panoramic video is also captured.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatial",
                    "all",
                    "video",
                    "binaural",
                    "hours",
                    "recorded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span>\nWe engage 45 instrumentalists performing 23 Traditional Chinese, Western and Electronic instruments such as violin, erhu, pipa, electric guitar, and keyboard. Each performer receives $60 per recorded hour. A total of 69 hours of solo music performances are recorded. Audio is captured using both head-mounted binaural microphones and reference monaural microphones. Third-person video provides a full-scene view, while first-person cameras are used to capture detailed playing gestures.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "music",
                    "video",
                    "binaural",
                    "hours",
                    "recorded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a team of domain experts in singing, music performance, and linguistics to carry out and review all annotations, compensating each annotator at a rate of $15 per hour. Prior to beginning their work, every expert receives a clear explanation of how the annotations will be used and agrees to release their annotation results under an open&#8208;source license for academic research.\nFor all modules, we first synchronize each audio with its corresponding video, mono&#8208;channel reference track, and 3D positional metadata. Annotators then verify and correct this synchronization to ensure perfect alignment across modalities.</p>\n\n",
                "matched_terms": [
                    "all",
                    "video",
                    "audio",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife.</span>\nIn MRSDialogue scenes, we automatically generate initial transcripts and speaker clusters with WhisperX, extracting word&#8208;level timestamps and speaker IDs. Experts then load these results in Praat and assign each cluster to the correct speaker, correcting transcription errors as needed. Next, we run Montreal Forced Aligner (MFA) <cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib30\" title=\"\">2017</a>)</cite> to produce coarse phoneme&#8208;to&#8208;audio alignments (exported in TextGrid format), using pypinyin to convert Chinese text into phoneme sequences.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mozillazg/python-pinyin\" title=\"\">https://github.com/mozillazg/python-pinyin</a></span></span></span> Finally, annotators refine word and phoneme boundaries in Praat to achieve millisecond&#8208;level precision.\nIn MRSSound segments, annotators additionally label each time interval with the corresponding event category (e.g., &#8220;clattering,&#8221; &#8220;typing,&#8221; &#8220;pages turning&#8221;).</p>\n\n",
                "matched_terms": [
                    "label",
                    "format",
                    "transcripts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing.</span>\nEach segment contains a solo vocal performance with known lyrics. We first apply voice activity detection (VAD) to segment the recordings. Lyrics are aligned to each segment using LyricFN&#8217;s ASR-based dynamic programming method. English phonemes follow the ARPABET standard,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://en.wikipedia.org/wiki/ARPABET\" title=\"\">https://en.wikipedia.org/wiki/ARPABET</a></span></span></span> while German and French use MFA&#8217;s phoneme sets.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mfa-models.readthedocs.io/en/latest/dictionary/\" title=\"\">https://mfa-models.readthedocs.io/en/latest/dictionary/</a></span></span></span> We then apply MFA for initial alignment and refine it manually in Praat to obtain precise word and phoneme boundaries. Annotators assign fine-grained style tags, including emotion, genre, and tempo. To generate score annotations, we extract F0 contours using RMVPE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib38\" title=\"\">2023</a>)</cite> and convert them into MIDI format via ROSVOT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib27\" title=\"\">2024</a>)</cite>, followed by expert correction.</p>\n\n",
                "matched_terms": [
                    "tags",
                    "format"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation:</span>\nAfter annotation, we segment the raw recordings into shorter clips to support spatial audio generation and analysis tasks. For speech and singing, we use alignment timestamps to extract utterances. For MRSSound, each recording is divided into fixed 10-second segments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Synchronization:</span>\nIn addition to binaural audio, each clip is synchronized with the following modalities: audio, text, video, and position data. Using the segment timestamps, we align all streams temporally. For static sources, we attach manually recorded 3D coordinates; for dynamic sources, we interpolate Ultra-Wideband (UWB) tracking data over the segment duration.\nThis process yields fully synchronized multimodal clips ready for downstream spatial audio tasks.\nFurthermore, for segments where participants&#8217; faces are visible, we apply anonymization by using a face detection model to overlay digital masks during post-processing.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatial",
                    "all",
                    "where",
                    "video",
                    "binaural",
                    "recorded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Checking:</span>\nTo ensure the reliability of annotations, domain experts perform a random audit on 15% of the segmented clips across all four modules. The audit process involves the following steps:\n(1) Verifying the temporal synchronization across different modal;\n(2) Confirming that the assigned event labels accurately reflect the audiovisual content present in each clip;\n(3) Reviewing the speech segments to check the correctness of word- and phoneme-level alignments;\n(4) Evaluating singing clips for accurate alignment between lyrics and audio, consistency with musical scores, and correctness of assigned style labels;\n(5) Assessing MRSMusic excerpts to verify that musical properties, such as key, pitch, and note duration.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSDialogue.</span>\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F4\" title=\"Figure 4 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a) presents the 3D spatial distribution of sound sources in MRSDialogue. In this subset, which features frequent human conversations, most sources are located around the ear-level height of the listener. The azimuthal distribution covers nearly all directions surrounding the listener, offering diverse angular data for training spatial localization models with strong generalization.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "all",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F4\" title=\"Figure 4 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(b) shows the phoneme distribution across all speech segments. The most common phoneme is &#8216;i&#8217;, while the least frequent is &#8216;ueng&#8217;. This distribution aligns with real-world phonetic patterns and highlights the dataset&#8217;s linguistic richness. Such broad phoneme coverage is beneficial for downstream tasks in speech synthesis and recognition under spatial settings.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSound.</span>\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F5\" title=\"Figure 5 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(a) illustrates the spatial distribution of sound sources in MRSSound relative to the listener&#8217;s head-centered coordinate system. The listener&#8217;s facing direction is at 90 degrees, and most sound events are concentrated in the frontal hemisphere (azimuth from 0&#176; to 180&#176;), which is consistent with the first-person video capture setup. Some events also appear in the rear field (&#8211;180&#176; to 0&#176;). In elevation, the majority of sound sources are distributed between &#8211;90&#176; and 40&#176;, realistically reflecting everyday human perception in standing scenarios, where sound events typically originate below ear level. This broad spatial coverage supports the training of spatial audio models with strong generalization ability.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "sound",
                    "where",
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F5\" title=\"Figure 5 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(b) presents the duration distribution of recorded sound events. MRSSound covers a wide variety of everyday scenarios, including cooking in kitchens, working in office environments, and sports-related activities. The diversity of event types and durations makes the subset suitable for training and evaluating models in real-world spatial sound event understanding.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "sound",
                    "recorded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F6\" title=\"Figure 6 &#8227; A.5.2 Statistics of MRSSpeech &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>(a) illustrates the spatial distribution of speech sources with respect to the listener. The azimuth angles span the full 360&#176; around the listener, providing comprehensive coverage of spatial directions. Elevation angles are mostly concentrated between 0&#176; and 60&#176;. Smaller elevations reflect scenarios where both the speaker and listener are either standing or seated, while larger elevations (above 30&#176;) simulate common real-world speech situations such as meetings, where a standing speaker addresses a seated listener. This diverse spatial coverage supports generalization in spatial speech modeling.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F6\" title=\"Figure 6 &#8227; A.5.2 Statistics of MRSSpeech &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>(b) shows a word cloud representing the diversity of dialogue content. The transcripts are sourced from theatrical scripts, films, and other spoken-only scenarios, capturing a wide range of expressive and stylistic variation. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F6\" title=\"Figure 6 &#8227; A.5.2 Statistics of MRSSpeech &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>(c) presents the distribution of room sizes used for speech recordings. Most multi-speaker interactions take place in medium to large rooms, such as meeting or lecture spaces. We include three distinct environments with varying absorption properties and dimensions to simulate different acoustic conditions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "transcripts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F6\" title=\"Figure 6 &#8227; A.5.2 Statistics of MRSSpeech &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>(d) displays the distribution of audio segment durations. Most conversational turns are short, but the dataset also includes extended monologues exceeding 20 seconds, allowing models to capture both brief interactions and long-range motion or speaker dynamics.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F6\" title=\"Figure 6 &#8227; A.5.2 Statistics of MRSSpeech &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>(e) presents the phoneme distribution, which covers all common phonetic units in the dataset&#8217;s target language. This phonetic diversity ensures that MRSSpeech provides strong generalizability for phoneme-aware models in speech synthesis and recognition.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "language",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F7\" title=\"Figure 7 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(a) shows the 3D spatial distribution of sound sources with respect to the listener. The majority of sources are located in front of the listener, consistent with the setup of solo vocal recordings. However, the coverage also spans surrounding directions in both azimuth and elevation, ensuring spatial variability for training robust spatial audio models.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "sound",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, MRSSing provides extensive diversity in spatial positioning, language, emotion, vocal range, segment duration, and pitch. This makes it a strong foundation for research on spatial singing voice synthesis and expressive vocal modeling.</p>\n\n",
                "matched_terms": [
                    "language",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F8\" title=\"Figure 8 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(a) illustrates the spatial positions of musical instruments relative to the listener. Most sources are located in front of the listener, consistent with typical music listening scenarios. However, the coverage also includes surrounding directions, contributing to spatial diversity in training and evaluation.</p>\n\n",
                "matched_terms": [
                    "positions",
                    "spatial",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding generalization capacity, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F8\" title=\"Figure 8 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(d) shows that audio segment durations range from approximately 4 to 11 seconds, matching typical training input lengths. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F8\" title=\"Figure 8 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(e) demonstrates that the dataset covers a full range of musical pitch values, supporting tasks that require robust pitch generalization.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Team.</span> We employ 25 professional annotators (aged 20&#8211;25, gender ratio of about 3:2, male to female) to perform fine-grained annotations of transcripts, phoneme boundaries, music scores, and movement descriptors. All annotators were trained student researchers with relevant domain expertise.</p>\n\n",
                "matched_terms": [
                    "all",
                    "music",
                    "transcripts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct subjective evaluation of the generation tasks using Mean Opinion Score (MOS). For each task, we randomly sample 40 utterances from the test set. Each utterance is paired with a corresponding source-position prompt and is evaluated by at least five expert listeners.\nMOS-Q, Listeners rate each sample on a five-point Likert scale ranging from 1 (bad) to 5 (excellent).\nFor audio quality, we use MOS-Q, where listeners wear headphones and assess the clarity and naturalness of the generated audio.\nFor spatial perception, we use MOS-P, where listeners evaluate the realism of spatial cues and whether the perceived direction and distance of the sound source match the prompt description.\nAll participants are fairly compensated for their time at a rate of $15 per hour, resulting in a total cost of approximately $2000. Participants are informed that their evaluations will be used exclusively for academic research purposes.\nInstructions for audio evaluations are shown in Figure\n&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.F9\" title=\"Figure 9 &#8227; B.1 Subjective Evaluation Metrics &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "source",
                    "all",
                    "sound",
                    "where",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively assess spatial audio generation and understanding across multiple tasks, we adopt a set of objective metrics that evaluate signal fidelity, spatial consistency, intelligibility, and speaker or pitch accuracy. We randomly sample 400 data points as the test set.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure waveform similarity using Wave L2, the mean squared error (MSE) between the generated and reference binaural waveforms. Amplitude L2 and Phase L2 are computed after applying Short-Time Fourier Transform (STFT), reflecting errors in the magnitude and phase components respectively. MRSTFT loss<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/csteinmetz1/auraloss\" title=\"\">https://github.com/csteinmetz1/auraloss</a></span></span></span> is also used, combining spectral convergence, log-magnitude, and linear-magnitude terms for better spectral alignment.\nIn addition, we use the perceptual evaluation metric PESQ<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/aliutkus/speechmetrics\" title=\"\">https://github.com/aliutkus/speechmetrics</a></span></span></span> to assess audio quality for speech-related tasks. Since PESQ is designed for speech, it is omitted for MRSLife and MRSMusic. For all metrics except PESQ, lower values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate spatial consistency, we use Spatial-AST<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib44\" title=\"\">2024</a>)</cite> to extract angular and distance embeddings from the binaural audio. Since Spatial-AST predicts positions only for static sources, we compute the cosine similarity between the predicted and ground truth embeddings within 1-second segments and average the results to assess overall spatial fidelity.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "binaural",
                    "positions",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite>, we adopt four joint detection and localization metrics:\nF<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m1\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math>: location-aware F-score; a prediction is correct if the event class matches and angular error is below <math alttext=\"20^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m2\" intent=\":literal\"><semantics><msup><mn>20</mn><mo>&#8728;</mo></msup><annotation encoding=\"application/x-tex\">20^{\\circ}</annotation></semantics></math>.\nER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m3\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math>: error rate computed as the sum of insertions, deletions, and substitutions over reference events.\nLE<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub>: class-aware localization error, the mean angular difference between predicted and reference directions.\nLR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub>: class-aware localization recall, the percentage of correctly localized events among all instances of each class.\nWe compute all metrics in 1-second non-overlapping segments using macro-averaging across all event classes. Higher values of F<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m6\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> and LR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub>, and lower values of ER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m8\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> and LE<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub> indicate better performance.</p>\n\n",
                "matched_terms": [
                    "class",
                    "all",
                    "starss23"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build upon BinauralGrad&#8217;s two-stage diffusion-based framework to convert monaural audio into spatial audio. However, in our subsequent generation experiments, the synthesized monaural audio typically corresponds to a centrally positioned source between the ears, effectively serving as the first stage of BinauralGrad. Therefore, for the spatialization experiments presented here, we use only the second stage of BinauralGrad to validate spatial audio generation.\nSpecifically, we first convert binaural recordings to monaural input by averaging the two channels. Next, we apply a DSP-based method to produce a coarse spatial approximation of the binaural signal. This monaural input and its simulated binaural counterpart are then used as input to the BinauralGrad model, which is conditioned on the object&#8217;s motion trajectory to generate spatialized binaural audio.\nWe list the architecture and hyperparameters of BinauralGrad in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T8\" title=\"Table 8 &#8227; B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "source",
                    "binaural",
                    "simulated",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune two pre-trained models, CosyVoice and F5-TTS, using monaural audio obtained by averaging the binaural recordings from the MRSSpeech subset. This allows the models to learn the generation characteristics specific to our dataset. After monaural generation, we apply a pre-trained spatialization model trained on MRSSpeech to convert the output into spatialized binaural audio. For the ISDrama baseline, we adopt the model variant proposed in the original paper that conditions generation on predefined spatial paths to produce binaural spatial speech directly.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial",
                    "dataset",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ISDrama baseline, we adopt the spatial-path-conditioned generation framework and extend it by incorporating Rmssinger&#8217;s note encoder, allowing explicit pitch control through musical score input.\nWe use the single stage variation of Rmssinger, a standard singing voice synthesis model, and train it with monaural targets derived from the averaged binaural recordings in the MRSSing subset. This enables the model to generate pitch-accurate audio that reflects the acoustic characteristics. The synthesized monaural audio is then spatialized using the spatialization model trained on MRSSing.\nWe list the architecture and hyperparameters of Rmssinger in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T9\" title=\"Table 9 &#8227; B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For spatial music generation, in the ISDrama setting, we follow the path-conditioned generation pipeline but remove the phoneme encoder, using only the note encoder to process symbolic scores as the sole control condition for generating spatialized music.\nWe also adopt Make-An-Audio 2 as our base model and train it using monaural audio derived from averaged MRSMusic binaural recordings. We use the pretrained spectrogram autoencoder. Instrument category and symbolic score information are concatenated into the text prompt to guide the music generation process. The generated audio is subsequently spatialized using the MRSMusic-trained spatialization model. We list the hyper-parameters of Make-An-Audio 2 in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T10\" title=\"Table 10 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "binaural",
                    "spatial",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the STARSS23 framework for FOA-based sound event localization and detection, training on the event segments from the MRSSound subset under both audio-only and audio-visual conditions. To enable binaural input, we modify the model architecture to use three input channels and extract interaural phase difference features. This allows us to adapt the SELD model to binaural audio. Additionally, we experiment with replacing convolutional layers with Transformer encoders to explore the effect of different architectures on sound event localization and detection performance.\nWe list the hyper-parameters of SELD in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T11\" title=\"Table 11 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "starss23",
                    "binaural",
                    "sound"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations",
        "caption": "Table 2: Audio Spatialization Performance. For MRSlife, we only use the MRSSound subset.",
        "body": "Method\n\nW-L2 ×10−3\\times 10^{-3} ↓\\downarrow\n\n\nA-L2 ↓\\downarrow\n\n\nP-L2 ↓\\downarrow\n\n\nPESQ ↑\\uparrow\n\n\nSTFT ↓\\downarrow\n\n\nMOS-Q ↑\\uparrow\n\n\nMOS-P ↑\\uparrow\n\n\n\nGround Truth\n–\n–\n–\n–\n–\n\n4.69 ±\\pm 0.08\n\n\n4.56 ±\\pm 0.10\n\n\n\n\n\nDSP\n1.691\n0.048\n1.562\n2.830\n1.246\n\n3.89 ±\\pm 0.09\n\n\n3.75 ±\\pm 0.11\n\n\n\nMRSLife\n0.076\n0.025\n0.898\n-\n2.243\n\n3.91 ±\\pm 0.07\n\n\n3.87 ±\\pm 0.10\n\n\n\nMRSSpeech\n0.460\n0.061\n0.807\n1.929\n2.352\n\n3.88 ±\\pm 0.08\n\n\n3.84 ±\\pm 0.08\n\n\n\nMRSSing\n0.647\n0.093\n1.004\n1.723\n2.539\n\n3.84 ±\\pm 0.09\n\n\n3.91 ±\\pm 0.07\n\n\n\nMRSMusic\n0.705\n0.063\n0.835\n-\n1.724\n\n3.87 ±\\pm 0.07\n\n\n3.93 ±\\pm 0.09\n\n\n\nALL\n0.305\n0.041\n0.801\n2.352\n1.681\n3.93 ±\\pm 0.09\n3.94 ±\\pm 0.07",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">W-L2 <math alttext=\"\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\times 10^{-3}</annotation></semantics></math></span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A-L2</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">P-L2</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">PESQ</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">STFT</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MOS-Q</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MOS-P</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ground Truth</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.69 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.08</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.56 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.10</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">DSP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.691</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.048</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.562</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.830</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.246</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.89 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.75 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.11</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">MRSLife</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.076</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.025</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.898</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.243</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.91 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.07</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.87 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.10</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">MRSSpeech</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.460</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.061</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.807</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.929</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.352</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.88 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.84 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.08</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">MRSSing</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.647</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.093</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.004</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.723</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.539</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.84 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.91 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.07</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">MRSMusic</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.705</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.063</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.835</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.724</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.87 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.07</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.93 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ALL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.305</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.041</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.801</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.352</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.681</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.93 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m21\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.94 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m22\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.07</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "mosp",
            "subset",
            "mrssing",
            "↓downarrow",
            "±pm",
            "ground",
            "mrslife",
            "stft",
            "spatialization",
            "wl2",
            "×10−3times",
            "mrsspeech",
            "all",
            "pesq",
            "truth",
            "mrsmusic",
            "performance",
            "↑uparrow",
            "only",
            "mosq",
            "mrssound",
            "dsp",
            "pl2",
            "al2",
            "method",
            "use",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Spatialization &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BinauralGrad achieves strong performance across all MRSAudio subsets, surpassing the classical DSP baseline on most objective and subjective metrics. The highest performance is observed on MRSLife, likely due to the relatively limited variation in sound sources within these scenes. These results demonstrate that MRSAudio&#8217;s rich spatial annotations and diverse acoustic environments provide a solid foundation for training and evaluating spatial audio generation models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive spatial environments, where auditory cues enable sound source localization in three-dimensional space.\nDespite the critical role of spatial audio in immersive technologies such as VR/AR, most existing multimodal datasets provide only monaural audio, which limits the development of spatial audio generation and understanding.\nTo address these challenges, we introduce MRSAudio, a large-scale multimodal spatial audio dataset designed to advance research in spatial audio understanding and generation.\nMRSAudio spans four distinct components: MRSLife, MRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios.\nThe dataset includes synchronized binaural and ambisonic audio, exocentric and egocentric video, motion trajectories, and fine-grained annotations such as transcripts, phoneme boundaries, lyrics, scores, and prompts.\nTo demonstrate the utility and versatility of MRSAudio, we establish five foundational tasks: audio spatialization, and spatial text to speech, spatial singing voice synthesis, spatial music generation and sound event localization and detection.\nResults show that MRSAudio enables high-quality spatial modeling and supports a broad range of spatial audio research.\nDemos and dataset access are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mrsaudio.github.io\" title=\"\">https://mrsaudio.github.io</a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatialization",
                    "mrssing",
                    "mrsspeech",
                    "mrsmusic",
                    "only",
                    "mrslife"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive and interpret physical environments. With the rapid growth of film, virtual reality (VR), augmented reality (AR), and gaming applications, users increasingly expect not only precise audiovisual alignment but also highly immersive experiences. While recent advances in deep learning have enabled realistic generation of speech, music, and sound effects synchronized with text or video <cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib25\" title=\"\">2022</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib41\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib37\" title=\"\">2024</a>)</cite>, most models focus on monaural audio and neglect spatialized soundscapes that enhance immersion. The human binaural system uses interaural time differences (ITD) and interaural level differences (ILD) to localize sound in three-dimensional space, and spatial audio must remain consistent with visual cues <cite class=\"ltx_cite ltx_citemacro_citep\">(Yost, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib42\" title=\"\">1998</a>; Cohen &amp; Knudsen, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib12\" title=\"\">1999</a>; Grothe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib18\" title=\"\">2010</a>)</cite>. Any mismatch can disrupt immersion and weaken the sense of presence. For example, hearing a cat&#8217;s meow from the left immediately suggests its location, even if it is just off-screen.</p>\n\n",
                "matched_terms": [
                    "only",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these gaps, we present <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a 484-hour large-scale multimodal spatial audio dataset designed to support both spatial audio understanding and generation. It integrates high-fidelity spatial recordings with synchronized video, 3D pose tracking, and rich semantic annotations, enabling comprehensive modeling of real-world auditory scenes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S0.F1\" title=\"Figure 1 &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the dataset comprises four subsets, each targeting distinct tasks and scenarios.\n<span class=\"ltx_text ltx_font_bold\">MRSLife</span> (129 h) captures daily activities such as board games, cooking, and office work, using egocentric video and FOA audio annotated with sound events and speech transcripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSpeech</span> (206 h) includes binaural conversations from 44 speakers across diverse indoor environments, paired with video, 3D source positions, and complete scripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSing</span> (80 h) features high-quality solo singing performances in Chinese, English, German, and French by 20 vocalists, each aligned with time-stamped lyrics and corresponding musical scores.\n<span class=\"ltx_text ltx_font_bold\">MRSMusic</span> (69 h) offers spatial recordings of 23 Traditional Chinese, Western and Electronic instruments, with symbolic score annotations that support learning-based methods for symbolic-to-audio generation and fine-grained localization.\nTogether, these four subsets support a broad spectrum of spatial audio research problems, including event detection, sound localization, and binaural or ambisonic audio generation. By pairing spatial audio with synchronized exocentric and egocentric video, geometric tracking, and detailed semantic labels, MRSAudio enables new research directions in multimodal spatial understanding and cross-modal generation. Throughout this paper, unless stated otherwise, we use the term spatial audio to refer to binaural audio.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrssing",
                    "mrsspeech",
                    "use",
                    "mrsmusic",
                    "mrslife"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We organize the data into four complementary subsets (MRSLife, MRSSpeech, MRSSing, MRSMusic), each carefully tailored to different real-world acoustic scenarios and equipped with rich, scenario-specific annotations to facilitate downstream task development.</p>\n\n",
                "matched_terms": [
                    "mrsmusic",
                    "mrsspeech",
                    "mrssing",
                    "mrslife"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish and release evaluation protocols and baseline implementations for five benchmark tasks: audio spatialization generation, spatial text-to-speech, spatial singing voice synthesis, spatial music generation, and localization and detection of sound events, in order to demonstrate MRSAudio&#8217;s versatility and to foster reproducible research.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2\" title=\"2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reviews existing spatial audio datasets.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3\" title=\"3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> describes the design, collection process, and key statistics of MRSAudio.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4\" title=\"4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents extensive benchmark experiments using state-of-the-art methods on five core spatial audio tasks: audio spatialization, spatial text-to-speech, spatial singing voice generation, spatial music synthesis, and sound event localization and detection.\nFinally, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S5\" title=\"5 Conclusion and Discussion &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> concludes the paper and discusses the limitations and potential risks associated with MRSAudio.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, existing datasets vary in their goals and modalities. For instance, Spatial LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Sarabia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib32\" title=\"\">2023</a>)</cite> simulates spatial speech using the LibriSpeech corpus and is mainly intended for binaural TTS applications. RealMAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib40\" title=\"\">2024</a>)</cite> is a large-scale real-recorded and annotated 32-channel microphone array dataset for multichannel speech enhancement and source localization. YT-Ambigen <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>)</cite> and Sphere360 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite> are derived from web-crawled video datasets, but lack explicit spatial annotations, making them suitable primarily for video-to-spatial audio generation. BinauralMusic focuses on musical content, offering instrument class tags. BEWO-1M <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>)</cite> combines crawled content and simulated binaural rendering, and provides image or GPT-generated prompts. STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> features real-world FOA and multi-channel audio alongside synchronized videos and includes sound class and position labels.\nIn contrast to prior datasets, MRSAudio offers a comprehensive, large-scale, and real-world spatial audio corpus, featuring 484 hours of recorded data in both FOA and binaural formats, covering all audio domains including general audio, speech, singing, and music. Uniquely, MRSAudio includes synchronized audio, video, and 3D positional geometry, along with fine-grained cross-modal annotations, such as: transcripts, word and phoneme boundaries and music scores.\nThese features make MRSAudio an ideal resource for a broad range of spatial audio generation and understanding tasks, including spatial speech, music, and singing voice synthesis.</p>\n\n",
                "matched_terms": [
                    "all",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span> This subset focuses on everyday conversations and environmental sound events. Based on the degree of human vocal interaction, MRSLife is further divided into two parts: MRSDialogue, which captures unscripted conversations that naturally include spontaneous action sounds (e.g., footsteps, door movements), and MRSSound, which focuses on non-verbal sound events primarily caused by physical activities such as cooking, typing, or sports.</p>\n\n",
                "matched_terms": [
                    "subset",
                    "mrslife",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span> MRSSpeech targets clean, high-quality conversational recordings for TTS task. All spoken interactions are recorded in controlled indoor environments with minimal noise. We invite speakers to participate in content-driven conversations based on predefined scripts.</p>\n\n",
                "matched_terms": [
                    "all",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> MRSMusic captures immersive instrumental performances suitable for spatial music generation and analysis.\nWe record solo performances from 45 professional musicians across 23 instruments, including Traditional Chinese, Western and Electronic instruments.\nEach performance is paired with its corresponding musical score to support score-based music generation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span> In MRSDialogue, audio is recorded using a professional binaural recording head and high-resolution sound cards, while synchronized video is captured using industry-standard cameras. In MRSSound, in addition to binaural audio and exocentric video, we also captured FOA (Zoom H3-VR) and egocentric video (Gopro camera).To ensure the effectiveness of the egocentric video, participants are asked to remain within the frontal field of view of the binaural recording head.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrslife",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span> To introduce spatial variability, we select four recording rooms that differ in size and acoustic material. Each speaker reads inflected emotions from the scripts while walking through the space, producing dynamic spatial cues. Recordings include spoken passages, binaural audio and exocentric video, as well as a clean mono audio by a lavalier microphone placed near the speaker.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span> The professional singers perform according to musical scores. To introduce variation in source&#8211;listener geometry, we adjust the position of the head-mounted binaural microphone relative to the singer. In addition to the binaural audio and synchronized video, each session includes a clean vocal track recorded with a studio-grade condenser microphone.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> We record 23 Traditional Chinese, Western and Electronic instruments, performed by 45\nprofessional musicians.To capture rich spatial detail, we vary microphone placement around the instrument. Recordings include binaural audio, monaural audio, exocentric video, and synchronized video that records playing gestures. Full recording details are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS2\" title=\"A.2 Details of Recording &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To maximize MRSAudio&#8217;s utility across a wide range of tasks, we begin with event-level annotations for all vocal and acoustic content in MRSLife, MRSSpeech, MRSSing, and MRSMusic. However, coarse annotations alone are insufficient for fine-grained tasks such as singing voice modeling and music generation from scores. To bridge this gap, we design a comprehensive annotation pipeline. Full implementation details and detailed annotation guidelines are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS3\" title=\"A.3 Details of Annotation &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "mrssing",
                    "mrsspeech",
                    "all",
                    "mrsmusic",
                    "mrslife"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span>\nFor MRSDialogue, we apply WhisperX <cite class=\"ltx_cite ltx_citemacro_citep\">(Bain et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib6\" title=\"\">2023</a>)</cite> for automatic speech recognition and speaker diarization to generate initial transcripts and speaker turns. Human annotators correct recognition errors and speaker attribution mismatches. The audio is then segmented into utterances and the transcripts are converted into phoneme sequences (using pypinyin for Mandarin). A two-stage alignment process follows: we first apply the Montreal Forced Aligner (MFA) <cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib30\" title=\"\">2017</a>)</cite> for coarse word/phoneme mapping, then manually refine boundaries in Praat <cite class=\"ltx_cite ltx_citemacro_citep\">(Boersma, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib8\" title=\"\">2001</a>)</cite>.\nFor MRSSound, we annotate sound event categories and corresponding time intervals.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrslife",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span>\nGiven the availability of full scripts, we adapt WhisperX for long-form word-to-audio alignment of up to 30 minutes.\nEach script line is automatically matched to its corresponding audio segment (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS3\" title=\"A.3 Details of Annotation &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a> for more details).\nAnnotators then review these alignments, correcting any omissions or insertions caused by actors&#8217; deviations from the script. Finally, phoneme sequences are extracted and aligned with the audio using the same procedure as in MRSDialogue.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nWe use voice activity detection (VAD) to segment recordings into singing regions, then align pre-existing lyrics using LyricFA&#8217;s ASR-based dynamic programming algorithm<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/wolfgitpr/LyricFA\" title=\"\">https://github.com/wolfgitpr/LyricFA</a></span></span></span>.\nPhoneme generation is language-dependent: pypinyin for Mandarin, ARPA for English, and MFA&#8217;s built-in phoneme sets for French and German. Alignment is conducted via MFA, followed by manual refinement. Melody and rhythm of singing are transcribed into MIDI format using ROSVOT <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib27\" title=\"\">2024</a>)</cite>. Annotators then label each excerpt with high-level style descriptors, such as emotional tone (happy, sad), tempo (slow, moderate, fast), and pitch range (low, medium, high).</p>\n\n",
                "matched_terms": [
                    "use",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span>\nWe use Audio Slicer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/flutydeer/audio-slicer\" title=\"\">https://github.com/flutydeer/audio-slicer</a></span></span></span> to segment the music recordings and generate initial symbolic annotations with basic-pitch <cite class=\"ltx_cite ltx_citemacro_citep\">(Bittner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib7\" title=\"\">2022</a>)</cite>, and then employ professional musicians to verify and adjust note onsets, offsets, and dynamics to match the performance accurately.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "performance",
                    "use",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-processing pipeline comprises three key steps to refine raw annotated data. First, segmentation splits continuous recordings into task-oriented clips: utterances for speech and singing are extracted via alignment timestamps, while MRSSound audio is uniformly divided into 10-second segments. Next, multimodal synchronization aligns each clip with auxiliary modalities (text, video, position metadata) with temporal anchors. Static sound sources are annotated with manually measured 3D coordinates, whereas dynamic sources leverage interpolated UWB tracking trajectories. For scenes where participants&#8217; faces are visible, anonymization is performed by adding half-face masks. Finally, quality assurance involves domain experts auditing 15% of clips across all modules to verify temporal alignment precision, cross-modal content consistency, and annotation accuracy.\nFull auditing protocols are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS4\" title=\"A.4 Details of Post-Processing &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>, ensuring reproducibility of this process.</p>\n\n",
                "matched_terms": [
                    "all",
                    "audio",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the quality and utility of MRSAudio in real-world scenarios, we evaluate it on five representative spatial audio tasks: (i) audio spatialization, (ii) spatial text-to-speech, (iii) spatial singing voice synthesis, (iv) spatial music generation, and (v) sound event localization and detection (SELD). These tasks cover both generation and understanding, and are critical for applications such as AR/VR, spatial media production, and perceptual scene analysis.\nAll experiments are conducted using state-of-the-art methods on a server equipped with eight NVIDIA RTX 4090 GPUs.\nFor different tasks, we employ distinct objective metrics. For generation tasks, we compute cosine similarity scores for direction (ANG Cos) and distance (DIS Cos) to quantify spatial alignment quality. Additionally, we utilize subjective MOS-Q (Mean Opinion Score for Quality) to evaluate the quality of generated audio and MOS-P (Mean Opinion Score for Position) to assess spatial perception. For implementation details of training and evaluation metrics, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2\" title=\"Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "mosp",
                    "spatialization",
                    "all",
                    "audio",
                    "mosq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio spatialization aims to synthesize spatially immersive soundscapes from monaural inputs and source positional information. To evaluate MRSAudio on this task, we adopt BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite>, a diffusion-based generation model that predicts binaural waveforms conditioned on monaural audio and source coordinates. Since the downstream generation tasks can be formulated as predicting interaural (binaural) representations from mono audio, we employ a single-stage training scheme for all experiments. For comparison, we include a traditional signal processing baseline (DSP), which renders binaural audio using virtual source positions simulated via room impulse responses (RIRs) and head-related transfer functions (HRTFs).\nWe adopt the following objective metrics to evaluate audio quality:\n(1) W-L2: waveform L2 distance,\n(2) A-L2: amplitude envelope L2 distance,\n(3) P-L2: phase difference L2,\n(4) STFT: multi-resolution STFT loss,\n(5) PESQ: perceptual speech quality score.\nResults for the ground truth and DSP baseline are obtained by averaging across all test sets from the four MRSAudio subsets. Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS3\" title=\"B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "dsp",
                    "stft",
                    "spatialization",
                    "wl2",
                    "pl2",
                    "all",
                    "pesq",
                    "al2",
                    "truth",
                    "ground",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial TTS aims to produce high-quality speech enriched with spatial cues, thereby enhancing immersion and realism in AR/VR. Although recent advances in TTS have led to impressive improvements in speech quality, progress in spatialized speech generation remains limited due to the scarcity of spatially annotated recordings with rich, high-quality labels.\nTo evaluate the effectiveness of MRSAudio for this task, we train an end-to-end model following ISDrama <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib43\" title=\"\">2025</a>)</cite> to directly generate spatial speech from text and position data. Additionally, we compare with cascaded pipelines that combine monaural TTS models (CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib14\" title=\"\">2024</a>)</cite> and F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib10\" title=\"\">2024</a>)</cite>) with the audio spatialization module. This allows us to assess both speech generation quality and the spatial fidelity enabled by our dataset.\nFor content evaluation, we report Character Error Rate (CER) and Speaker Similarity (SIM). Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS4\" title=\"B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T3\" title=\"Table 3 &#8227; 4.2 Spatial Text to Speech &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the Mono+SP method achieves strong performance across most metrics. The CER remains low and comparable to the ground truth, indicating preserved linguistic accuracy after spatialization. A high SIM score reflects stable timbre learning. ANG Cos and DIS Cos show good spatial alignment with the ground truth, and subjective MOS scores confirm that the generated speech is both natural and spatially coherent. These results demonstrate that MRSSpeech provides high-quality, spatially annotated training data that enables effective and realistic spatial TTS.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "mrsspeech",
                    "method",
                    "truth",
                    "ground",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial SVS aims to produce expressive, high-quality singing voices enriched with accurate spatial cues, thereby enhancing listener immersion. While traditional SVS has advanced considerably, spatial SVS remains underexplored due to the lack of high-quality, spatially annotated datasets.\nTo evaluate MRSAudio for this task, we use the MRSSing subset to train and benchmark models. We adopt the ISDrama architecture to perform end-to-end spatial singing synthesis, incorporating note-level pitch control to enhance prosody accuracy. For comparison, we use the open-source SVS models Rmssinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib19\" title=\"\">2023</a>)</cite>. The outputs are then spatialized with BinauralGrad.\nWe employ objective metrics, including Mel-Cepstral Distortion (MCD) and F0 Frame Error (FFE), to evaluate spectral and pitch similarity between predicted and ground-truth. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS5\" title=\"B.5 Spatial Singing Voice Synthesis &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "use",
                    "mrssing",
                    "subset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T4\" title=\"Table 4 &#8227; 4.3 Spatial Singing Voice Synthesis &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the Mono + SP approach trained on MRSSing with pitch control achieves the best performance across most metrics. Its low MCD indicates strong spectral fidelity, and high ANG Cos and DIS Cos scores demonstrate effective spatial alignment. Subjective MOS results confirm that the generated singing voices are natural, high-quality, and spatially coherent.\nThese findings validate MRSSing as an effective resource for spatial singing voice generation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task aims to synthesize spatially immersive music conditioned on symbolic scores. While datasets like FAIR-Play offer high-quality instrument recordings, they lack aligned sheet music, limiting controllable generation. In contrast, our MRSMusic subset includes 23 instruments with aligned scores, enabling fine-grained, score-based spatial music synthesis.\nWe benchmark three systems on MRSMusic. First, we apply BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite> to spatialize mono recordings. Second, we utilize Make-An-Audio 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib13\" title=\"\">2023</a>)</cite> to generate mono music from MIDI scores, which is subsequently spatialized. Third, we adapt ISDrama to accept both score embeddings and spatial poses, enabling end-to-end spatial symbolic music generation.\nFor objective evaluation, we compute Fr&#233;chet Audio Distance (FAD) and FFE to evaluate the results. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS6\" title=\"B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "subset",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T5\" title=\"Table 5 &#8227; 4.4 Spatial Music Generation &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the Mono + SP pipeline achieves strong performance, benefiting from access to ground truth mono audio. The Make-An-Audio 2 + SP exhibits limitations in FAD and pitch accuracy. This suggests that general-purpose audio generation models may struggle to fully capture structured musical information from symbolic prompts. The ISDrama (Music) model generates music directly from the symbolic inputs and spatial cues, achieving better coherence and spatial alignment than the Make-An-Audio 2 + SP. These results highlight MRSMusic&#8217;s effectiveness in supporting spatially controllable music generation across diverse instruments and spatial conditions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "performance",
                    "truth",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task evaluates the ability to detect and localize sound events using MRSAudio&#8217;s spatial annotations. We follow STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> using both audio-only and audio-visual variants on the MRSSound subset.\nIn the audio-only condition, models receive either FOA or binaural waveforms as input and predict sound event classes along with 3D source coordinates. For the audio-visual condition, we extract bounding boxes of visible persons to serve as coarse visual priors, which are fused with the audio representations. We also explore architectural variations by replacing the original convolutional backbone with a Transformer encoder, allowing us to assess the impact of temporal modeling capacity on spatial prediction.\nWe evaluate model performance using four standard joint detection and localization metrics, including location-aware detection (<math alttext=\"F_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">F_{20^{\\circ}}</annotation></semantics></math>, <math alttext=\"ER_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub></mrow><annotation encoding=\"application/x-tex\">ER_{20^{\\circ}}</annotation></semantics></math>) and class-aware localization (<math alttext=\"LE_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>E</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LE_{CD}</annotation></semantics></math>, <math alttext=\"LR_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LR_{CD}</annotation></semantics></math>). Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS7\" title=\"B.7 Sound Event Localization and Detection &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.7</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "performance",
                    "subset",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T6\" title=\"Table 6 &#8227; 4.5 Sound Event Localization and Detection &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, model performance varies with architecture, input modality, and audio format. Transformer-based models generally outperform ConvNet baselines, particularly in reducing localization error. FOA input consistently yields better results than binaural audio, benefiting from richer spatial representation. For example, the Transformer with FOA and no visual input achieves the lowest error rate (ER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> 0.99) and highest localization recall (LR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub> 87.32). The addition of visual features improves performance in some ConvNet settings (e.g., F<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m3\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> rises from 9.33 to 13.00 with FOA), but offers limited or inconsistent gains in Transformer models, possibly due to modality mismatch or redundancy. These results highlight the value of MRSAudio&#8217;s spatial annotations and multimodal streams in supporting flexible evaluation under both unimodal and multimodal configurations.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce MRSAudio, a large-scale, multimodal spatial audio corpus designed to support a wide range of generation and understanding tasks. MRSAudio comprises four complementary modules, MRSLife, MRSSpeech , MRSSing, and MRSMusic, each captured with binaural/FOA audio, synchronized video, precise 3D pose metadata, and richly detailed annotations (event labels, transcripts, phoneme boundaries, lyrics, musical scores, and motion prompts). Through extensive benchmarks on audio spatialization, binaural speech, singing, and music generation , as well as sound event localization and detection, we demonstrate that MRSAudio&#8217;s scale, diversity, and annotation depth enable state&#8208;of&#8208;the&#8208;art performance and unlock new avenues for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatialization",
                    "mrssing",
                    "mrsspeech",
                    "mrsmusic",
                    "performance",
                    "mrslife"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Directions:</span>\nWhile MRSAudio offers broad multimodal coverage, two limitations remain. First, although synchronized video is provided for all recordings, current benchmarks only explore visual input in a limited subset of tasks. Second, while the dataset includes both binaural and First-Order Ambisonic (FOA) formats, the FOA subset is smaller in scale, and most tasks focus on binaural audio, limiting spatial modeling diversity.\nFuture work will expand the role of visual modalities in tasks such as sound localization and scene understanding. We also plan to increase FOA recordings to balance data distribution and support broader spatial audio research, and develop more FOA-specific benchmarks to better utilize ambisonic spatial cues.</p>\n\n",
                "matched_terms": [
                    "only",
                    "all",
                    "subset",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Societal Impact:</span>\nAs with any large-scale audiovisual dataset, MRSAudio carries potential risks if misused. It could be exploited to generate highly realistic yet synthetic spatial audio for deepfakes or disinformation in AR and VR applications. To mitigate such risks, MRSAudio is released under a noncommercial license with clear usage guidelines. We encourage responsible use in accordance with ethical standards, including consent management, data governance, and transparency.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the planning phase, we systematically analyze real-world application scenarios for spatial audio and divide them into four major categories. MRSLife focuses on daily environmental sound events, MRSSpeech targets high-quality conversational data, MRSSing captures solo vocal performances, and MRSMusic records immersive instrumental music. These four scenarios collectively cover a broad range of everyday acoustic contexts and are designed to support a wide spectrum of downstream tasks.</p>\n\n",
                "matched_terms": [
                    "mrssing",
                    "mrsspeech",
                    "mrslife",
                    "mrsmusic",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on predefined scenario requirements, we recruit professionals from relevant fields to participate in the recording process. To ensure spatial diversity, we rent various venues tailored to each scenario type. Following the FAIR-Play protocol, we use a 3Dio Free Space XLR binaural microphone to capture spatial audio, GoPro HERO cameras and mobile phones to record exocentric and egocentric videos, and UWB-based tracking systems to capture motion trajectories.\nOnce personnel, locations, and equipment are secured, we prepare task-specific materials for each subset. For MRSLife, we further divide the content based on the proportion of speech involved, resulting in two subcategories: MRSDialogue (e.g., group games, board games) and MRSSound (e.g., kungfu, kitchens, offices, sports). We predefine common sound events for each, such as clattering dishes, keyboard typing, and table tennis.\nFor MRSSpeech, we compile a large corpus of scripts from movies, TV shows, and crosstalk performances and automatically extract dialogue passages for speaker delivery.\nFor MRSSing, we design lyrics in four languages (Chinese, English, German, and French) and recruit singers across a range of vocal types to maximize diversity.\nFor MRSMusic, we collect solo performances across 23 traditional and modern instruments, covering a wide array of timbres and playing techniques.\nSpecific sound categories for each subset are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.T7\" title=\"Table 7 &#8227; A.1 Details of Planning &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrssing",
                    "mrsspeech",
                    "subset",
                    "use",
                    "mrsmusic",
                    "mrslife",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recruit a large number of participants with professional backgrounds in singing, music, and language to contribute to the recording process. To protect their identity, participants are asked to wear masks in appropriate scenarios. Prior to participation, all individuals sign consent forms agreeing to the open-source release of their audio and video under the CC BY-NC-SA 4.0 license.</p>\n\n",
                "matched_terms": [
                    "all",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio is recorded in WAV format at a sampling rate of 48 kHz. Video is recorded at a minimum resolution of 1080p and 24 frames per second, and is later standardized to this format during post-processing.</p>\n\n",
                "matched_terms": [
                    "all",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span>\nWe recruit 62 participants to perform daily activities including board games, cooking, exercise, and office work. In MRSDialogue, each participant is compensated $30 per recorded hour. Binaural audio is captured using head-mounted microphones, and third-person video is recorded. In MRSSound scenes such as kung fu or kitchen demonstrations, performers are paid $50 per hour. Both binaural and FOA (First-Order Ambisonics) recordings are collected, along with first-person and third-person video. The binaural dummy head is co-located with the egocentric camera and the Zoom H3-VR (Ambisonic recorder). The egocentric camera is rigidly aligned with the head&#8217;s gaze to capture first-person visuals, while the Ambisonic recorder is mounted 7.5 cm above the head. An exocentric camera is placed at a surveyed position in the scene to provide a third-person view of the environment and object relationships.\nParticipants receive brief scene descriptions and are asked to act naturally while maintaining a single active sound source when possible. The total duration for MRSLife recordings reaches approximately 150 hours.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrslife",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span>\nWe employ 44 expressive speakers to read from scripted texts, each paid $30 per hour of recorded audio. The total recorded duration reaches 200 hours, with compensation totaling $40,000. During recording sessions, speakers alternate between standing and walking around the recording area to introduce spatial diversity while maintaining speech clarity. Binaural audio is captured using head-mounted microphones, and clean monaural speech is recorded using lavalier microphones. All sessions are filmed from a third-person perspective.</p>\n\n",
                "matched_terms": [
                    "all",
                    "audio",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nEighteen professional singers participate in the recordings. Each singer is fluent in at least one of the following languages: Chinese, English, German, or French, and collectively they cover all vocal ranges including soprano, alto, tenor, and bass. Performers are paid $50 per hour, contributing a total of 80 hours of audio. Singing is recorded at a fixed position to ensure spatial consistency. Binaural recordings are captured with head-mounted microphones, and monaural audio is recorded with a studio-grade microphone. Third-person panoramic video is also captured.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "all",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span>\nWe engage 45 instrumentalists performing 23 Traditional Chinese, Western and Electronic instruments such as violin, erhu, pipa, electric guitar, and keyboard. Each performer receives $60 per recorded hour. A total of 69 hours of solo music performances are recorded. Audio is captured using both head-mounted binaural microphones and reference monaural microphones. Third-person video provides a full-scene view, while first-person cameras are used to capture detailed playing gestures.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a team of domain experts in singing, music performance, and linguistics to carry out and review all annotations, compensating each annotator at a rate of $15 per hour. Prior to beginning their work, every expert receives a clear explanation of how the annotations will be used and agrees to release their annotation results under an open&#8208;source license for academic research.\nFor all modules, we first synchronize each audio with its corresponding video, mono&#8208;channel reference track, and 3D positional metadata. Annotators then verify and correct this synchronization to ensure perfect alignment across modalities.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "all",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife.</span>\nIn MRSDialogue scenes, we automatically generate initial transcripts and speaker clusters with WhisperX, extracting word&#8208;level timestamps and speaker IDs. Experts then load these results in Praat and assign each cluster to the correct speaker, correcting transcription errors as needed. Next, we run Montreal Forced Aligner (MFA) <cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib30\" title=\"\">2017</a>)</cite> to produce coarse phoneme&#8208;to&#8208;audio alignments (exported in TextGrid format), using pypinyin to convert Chinese text into phoneme sequences.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mozillazg/python-pinyin\" title=\"\">https://github.com/mozillazg/python-pinyin</a></span></span></span> Finally, annotators refine word and phoneme boundaries in Praat to achieve millisecond&#8208;level precision.\nIn MRSSound segments, annotators additionally label each time interval with the corresponding event category (e.g., &#8220;clattering,&#8221; &#8220;typing,&#8221; &#8220;pages turning&#8221;).</p>\n\n",
                "matched_terms": [
                    "mrslife",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech.</span>\nGiven full dialogue scripts, we perform automatic alignment to 30-minute recordings using a chunk-based extension of WhisperX. This method divides each utterance-level audio segment into fixed-length chunks (e.g., 30 seconds), applies phoneme-level models (e.g., wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib5\" title=\"\">2020</a>)</cite> ) for emission prediction on each chunk, and then concatenates emissions to form a complete alignment matrix.\nWe compute alignments via a trellis-based dynamic programming algorithm with backtracking, followed by scaling to restore absolute timestamps. Word- and sentence-level timings are derived by grouping aligned characters using word indices and sentence tokenization. Missing or partial timings are interpolated using a specified method (e.g., nearest).\nThis pipeline enables efficient and accurate alignment of long-form recordings on GPU. We then refine sentence boundaries in Praat and apply the same MFA-plus-Praat workflow used in MRSLife for fine-grained phoneme and word-level alignment.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "method",
                    "mrslife",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing.</span>\nEach segment contains a solo vocal performance with known lyrics. We first apply voice activity detection (VAD) to segment the recordings. Lyrics are aligned to each segment using LyricFN&#8217;s ASR-based dynamic programming method. English phonemes follow the ARPABET standard,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://en.wikipedia.org/wiki/ARPABET\" title=\"\">https://en.wikipedia.org/wiki/ARPABET</a></span></span></span> while German and French use MFA&#8217;s phoneme sets.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mfa-models.readthedocs.io/en/latest/dictionary/\" title=\"\">https://mfa-models.readthedocs.io/en/latest/dictionary/</a></span></span></span> We then apply MFA for initial alignment and refine it manually in Praat to obtain precise word and phoneme boundaries. Annotators assign fine-grained style tags, including emotion, genre, and tempo. To generate score annotations, we extract F0 contours using RMVPE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib38\" title=\"\">2023</a>)</cite> and convert them into MIDI format via ROSVOT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib27\" title=\"\">2024</a>)</cite>, followed by expert correction.</p>\n\n",
                "matched_terms": [
                    "use",
                    "performance",
                    "method",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic.</span>\nWe segment the recordings using AutoSlicer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib29\" title=\"\">2022</a>)</cite> and generate preliminary symbolic annotations using basic-pitch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bittner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib7\" title=\"\">2022</a>)</cite>. Professional musicians then verify and refine the annotations, adjusting note onsets, offsets, dynamics, and articulations to ensure consistency between the score and performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation:</span>\nAfter annotation, we segment the raw recordings into shorter clips to support spatial audio generation and analysis tasks. For speech and singing, we use alignment timestamps to extract utterances. For MRSSound, each recording is divided into fixed 10-second segments.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "use",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Synchronization:</span>\nIn addition to binaural audio, each clip is synchronized with the following modalities: audio, text, video, and position data. Using the segment timestamps, we align all streams temporally. For static sources, we attach manually recorded 3D coordinates; for dynamic sources, we interpolate Ultra-Wideband (UWB) tracking data over the segment duration.\nThis process yields fully synchronized multimodal clips ready for downstream spatial audio tasks.\nFurthermore, for segments where participants&#8217; faces are visible, we apply anonymization by using a face detection model to overlay digital masks during post-processing.</p>\n\n",
                "matched_terms": [
                    "all",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Checking:</span>\nTo ensure the reliability of annotations, domain experts perform a random audit on 15% of the segmented clips across all four modules. The audit process involves the following steps:\n(1) Verifying the temporal synchronization across different modal;\n(2) Confirming that the assigned event labels accurately reflect the audiovisual content present in each clip;\n(3) Reviewing the speech segments to check the correctness of word- and phoneme-level alignments;\n(4) Evaluating singing clips for accurate alignment between lyrics and audio, consistency with musical scores, and correctness of assigned style labels;\n(5) Assessing MRSMusic excerpts to verify that musical properties, such as key, pitch, and note duration.</p>\n\n",
                "matched_terms": [
                    "all",
                    "audio",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSDialogue.</span>\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F4\" title=\"Figure 4 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a) presents the 3D spatial distribution of sound sources in MRSDialogue. In this subset, which features frequent human conversations, most sources are located around the ear-level height of the listener. The azimuthal distribution covers nearly all directions surrounding the listener, offering diverse angular data for training spatial localization models with strong generalization.</p>\n\n",
                "matched_terms": [
                    "all",
                    "subset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSound.</span>\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F5\" title=\"Figure 5 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(a) illustrates the spatial distribution of sound sources in MRSSound relative to the listener&#8217;s head-centered coordinate system. The listener&#8217;s facing direction is at 90 degrees, and most sound events are concentrated in the frontal hemisphere (azimuth from 0&#176; to 180&#176;), which is consistent with the first-person video capture setup. Some events also appear in the rear field (&#8211;180&#176; to 0&#176;). In elevation, the majority of sound sources are distributed between &#8211;90&#176; and 40&#176;, realistically reflecting everyday human perception in standing scenarios, where sound events typically originate below ear level. This broad spatial coverage supports the training of spatial audio models with strong generalization ability.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F5\" title=\"Figure 5 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(b) presents the duration distribution of recorded sound events. MRSSound covers a wide variety of everyday scenarios, including cooking in kitchens, working in office environments, and sports-related activities. The diversity of event types and durations makes the subset suitable for training and evaluating models in real-world spatial sound event understanding.</p>\n\n",
                "matched_terms": [
                    "subset",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F6\" title=\"Figure 6 &#8227; A.5.2 Statistics of MRSSpeech &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>(e) presents the phoneme distribution, which covers all common phonetic units in the dataset&#8217;s target language. This phonetic diversity ensures that MRSSpeech provides strong generalizability for phoneme-aware models in speech synthesis and recognition.</p>\n\n",
                "matched_terms": [
                    "all",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct subjective evaluation of the generation tasks using Mean Opinion Score (MOS). For each task, we randomly sample 40 utterances from the test set. Each utterance is paired with a corresponding source-position prompt and is evaluated by at least five expert listeners.\nMOS-Q, Listeners rate each sample on a five-point Likert scale ranging from 1 (bad) to 5 (excellent).\nFor audio quality, we use MOS-Q, where listeners wear headphones and assess the clarity and naturalness of the generated audio.\nFor spatial perception, we use MOS-P, where listeners evaluate the realism of spatial cues and whether the perceived direction and distance of the sound source match the prompt description.\nAll participants are fairly compensated for their time at a rate of $15 per hour, resulting in a total cost of approximately $2000. Participants are informed that their evaluations will be used exclusively for academic research purposes.\nInstructions for audio evaluations are shown in Figure\n&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.F9\" title=\"Figure 9 &#8227; B.1 Subjective Evaluation Metrics &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mosp",
                    "all",
                    "use",
                    "mosq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure waveform similarity using Wave L2, the mean squared error (MSE) between the generated and reference binaural waveforms. Amplitude L2 and Phase L2 are computed after applying Short-Time Fourier Transform (STFT), reflecting errors in the magnitude and phase components respectively. MRSTFT loss<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/csteinmetz1/auraloss\" title=\"\">https://github.com/csteinmetz1/auraloss</a></span></span></span> is also used, combining spectral convergence, log-magnitude, and linear-magnitude terms for better spectral alignment.\nIn addition, we use the perceptual evaluation metric PESQ<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/aliutkus/speechmetrics\" title=\"\">https://github.com/aliutkus/speechmetrics</a></span></span></span> to assess audio quality for speech-related tasks. Since PESQ is designed for speech, it is omitted for MRSLife and MRSMusic. For all metrics except PESQ, lower values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "stft",
                    "audio",
                    "all",
                    "pesq",
                    "use",
                    "mrsmusic",
                    "performance",
                    "mrslife"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate spatial consistency, we use Spatial-AST<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib44\" title=\"\">2024</a>)</cite> to extract angular and distance embeddings from the binaural audio. Since Spatial-AST predicts positions only for static sources, we compute the cosine similarity between the predicted and ground truth embeddings within 1-second segments and average the results to assess overall spatial fidelity.</p>\n\n",
                "matched_terms": [
                    "use",
                    "truth",
                    "ground",
                    "only",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"m_{t}(d)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{t}(d)</annotation></semantics></math> and <math alttext=\"\\hat{m}_{t}(d)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>m</mi><mo>^</mo></mover><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{m}_{t}(d)</annotation></semantics></math> represent the <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>-th Mel-frequency cepstral coefficient (MFCC) at frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> for the ground truth and synthesized signals, respectively, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the number of MFCC dimensions.</p>\n\n",
                "matched_terms": [
                    "truth",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we adopt F0 Frame Error (FFE) to evaluate pitch accuracy by comparing extracted F0 contours between the synthesized and ground truth audio.</p>\n\n",
                "matched_terms": [
                    "truth",
                    "audio",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use Fr&#233;chet Audio Distance (FAD)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib23\" title=\"\">2018</a>)</cite> to assess perceptual similarity between the feature distributions of generated and reference audio. In addition, F0 Frame Error (FFE) is used to evaluate pitch accuracy by comparing the extracted F0 contours against the reference musical scores.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite>, we adopt four joint detection and localization metrics:\nF<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m1\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math>: location-aware F-score; a prediction is correct if the event class matches and angular error is below <math alttext=\"20^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m2\" intent=\":literal\"><semantics><msup><mn>20</mn><mo>&#8728;</mo></msup><annotation encoding=\"application/x-tex\">20^{\\circ}</annotation></semantics></math>.\nER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m3\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math>: error rate computed as the sum of insertions, deletions, and substitutions over reference events.\nLE<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub>: class-aware localization error, the mean angular difference between predicted and reference directions.\nLR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub>: class-aware localization recall, the percentage of correctly localized events among all instances of each class.\nWe compute all metrics in 1-second non-overlapping segments using macro-averaging across all event classes. Higher values of F<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m6\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> and LR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub>, and lower values of ER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m8\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> and LE<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub> indicate better performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build upon BinauralGrad&#8217;s two-stage diffusion-based framework to convert monaural audio into spatial audio. However, in our subsequent generation experiments, the synthesized monaural audio typically corresponds to a centrally positioned source between the ears, effectively serving as the first stage of BinauralGrad. Therefore, for the spatialization experiments presented here, we use only the second stage of BinauralGrad to validate spatial audio generation.\nSpecifically, we first convert binaural recordings to monaural input by averaging the two channels. Next, we apply a DSP-based method to produce a coarse spatial approximation of the binaural signal. This monaural input and its simulated binaural counterpart are then used as input to the BinauralGrad model, which is conditioned on the object&#8217;s motion trajectory to generate spatialized binaural audio.\nWe list the architecture and hyperparameters of BinauralGrad in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T8\" title=\"Table 8 &#8227; B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "method",
                    "use",
                    "only",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune two pre-trained models, CosyVoice and F5-TTS, using monaural audio obtained by averaging the binaural recordings from the MRSSpeech subset. This allows the models to learn the generation characteristics specific to our dataset. After monaural generation, we apply a pre-trained spatialization model trained on MRSSpeech to convert the output into spatialized binaural audio. For the ISDrama baseline, we adopt the model variant proposed in the original paper that conditions generation on predefined spatial paths to produce binaural spatial speech directly.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatialization",
                    "subset",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ISDrama baseline, we adopt the spatial-path-conditioned generation framework and extend it by incorporating Rmssinger&#8217;s note encoder, allowing explicit pitch control through musical score input.\nWe use the single stage variation of Rmssinger, a standard singing voice synthesis model, and train it with monaural targets derived from the averaged binaural recordings in the MRSSing subset. This enables the model to generate pitch-accurate audio that reflects the acoustic characteristics. The synthesized monaural audio is then spatialized using the spatialization model trained on MRSSing.\nWe list the architecture and hyperparameters of Rmssinger in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T9\" title=\"Table 9 &#8227; B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "mrssing",
                    "subset",
                    "use",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For spatial music generation, in the ISDrama setting, we follow the path-conditioned generation pipeline but remove the phoneme encoder, using only the note encoder to process symbolic scores as the sole control condition for generating spatialized music.\nWe also adopt Make-An-Audio 2 as our base model and train it using monaural audio derived from averaged MRSMusic binaural recordings. We use the pretrained spectrogram autoencoder. Instrument category and symbolic score information are concatenated into the text prompt to guide the music generation process. The generated audio is subsequently spatialized using the MRSMusic-trained spatialization model. We list the hyper-parameters of Make-An-Audio 2 in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T10\" title=\"Table 10 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "use",
                    "mrsmusic",
                    "only",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the STARSS23 framework for FOA-based sound event localization and detection, training on the event segments from the MRSSound subset under both audio-only and audio-visual conditions. To enable binaural input, we modify the model architecture to use three input channels and extract interaural phase difference features. This allows us to adapt the SELD model to binaural audio. Additionally, we experiment with replacing convolutional layers with Transformer encoders to explore the effect of different architectures on sound event localization and detection performance.\nWe list the hyper-parameters of SELD in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T11\" title=\"Table 11 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "subset",
                    "use",
                    "performance",
                    "audio",
                    "mrssound"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations",
        "caption": "Table 3: Spatial TTS Performance on MRSSpeech. “SP” denotes the Audio Spatialization.",
        "body": "Method\nObjective\nSubjective\n\n\n\nCER ↓\\downarrow\n\n\nSIM ↑\\uparrow\n\n\nANG Cos ↑\\uparrow\n\n\nDIS Cos ↑\\uparrow\n\n\nMOS-Q ↑\\uparrow\n\n\nMOS-P ↑\\uparrow\n\n\n\nGround Truth\n2.54%\n–\n–\n–\n\n4.39 ±\\pm 0.08\n\n\n4.16 ±\\pm 0.10\n\n\n\nMono + SP\n2.56%\n0.98\n0.44\n0.68\n3.88 ±\\pm 0.08\n3.84 ±\\pm 0.08\n\n\nCosyVoice + SP\n3.89%\n0.96\n0.41\n0.63\n\n3.75 ±\\pm 0.12\n\n\n3.72 ±\\pm 0.09\n\n\n\nF5-TTS + SP\n3.15%\n0.97\n0.40\n0.62\n\n3.69 ±\\pm 0.13\n\n\n3.67 ±\\pm 0.14\n\n\n\nISDrama (speech)\n3.35%\n0.96\n0.48\n0.65\n\n3.85 ±\\pm 0.09\n\n\n3.82 ±\\pm 0.11",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Objective</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">SIM </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ANG Cos </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DIS Cos </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MOS-Q </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MOS-P </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ground Truth</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.54%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.39 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.16 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.10</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Mono + SP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.56%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.88 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.84 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.08</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice + SP</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.89%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.63</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.75 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.12</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.72 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">F5-TTS + SP</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.15%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.62</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.69 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.13</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.67 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.14</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ISDrama (speech)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.35%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.85 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.82 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.11</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "dis",
            "mosp",
            "isdrama",
            "subjective",
            "cosyvoice",
            "↓downarrow",
            "sim",
            "f5tts",
            "±pm",
            "ground",
            "objective",
            "speech",
            "audio",
            "spatialization",
            "mrsspeech",
            "mono",
            "tts",
            "cos",
            "spatial",
            "denotes",
            "truth",
            "cer",
            "↑uparrow",
            "performance",
            "method",
            "“sp”",
            "ang",
            "mosq"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T3\" title=\"Table 3 &#8227; 4.2 Spatial Text to Speech &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the Mono+SP method achieves strong performance across most metrics. The CER remains low and comparable to the ground truth, indicating preserved linguistic accuracy after spatialization. A high SIM score reflects stable timbre learning. ANG Cos and DIS Cos show good spatial alignment with the ground truth, and subjective MOS scores confirm that the generated speech is both natural and spatially coherent. These results demonstrate that MRSSpeech provides high-quality, spatially annotated training data that enables effective and realistic spatial TTS.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive spatial environments, where auditory cues enable sound source localization in three-dimensional space.\nDespite the critical role of spatial audio in immersive technologies such as VR/AR, most existing multimodal datasets provide only monaural audio, which limits the development of spatial audio generation and understanding.\nTo address these challenges, we introduce MRSAudio, a large-scale multimodal spatial audio dataset designed to advance research in spatial audio understanding and generation.\nMRSAudio spans four distinct components: MRSLife, MRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios.\nThe dataset includes synchronized binaural and ambisonic audio, exocentric and egocentric video, motion trajectories, and fine-grained annotations such as transcripts, phoneme boundaries, lyrics, scores, and prompts.\nTo demonstrate the utility and versatility of MRSAudio, we establish five foundational tasks: audio spatialization, and spatial text to speech, spatial singing voice synthesis, spatial music generation and sound event localization and detection.\nResults show that MRSAudio enables high-quality spatial modeling and supports a broad range of spatial audio research.\nDemos and dataset access are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mrsaudio.github.io\" title=\"\">https://mrsaudio.github.io</a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatialization",
                    "spatial",
                    "mrsspeech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive and interpret physical environments. With the rapid growth of film, virtual reality (VR), augmented reality (AR), and gaming applications, users increasingly expect not only precise audiovisual alignment but also highly immersive experiences. While recent advances in deep learning have enabled realistic generation of speech, music, and sound effects synchronized with text or video <cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib25\" title=\"\">2022</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib41\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib37\" title=\"\">2024</a>)</cite>, most models focus on monaural audio and neglect spatialized soundscapes that enhance immersion. The human binaural system uses interaural time differences (ITD) and interaural level differences (ILD) to localize sound in three-dimensional space, and spatial audio must remain consistent with visual cues <cite class=\"ltx_cite ltx_citemacro_citep\">(Yost, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib42\" title=\"\">1998</a>; Cohen &amp; Knudsen, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib12\" title=\"\">1999</a>; Grothe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib18\" title=\"\">2010</a>)</cite>. Any mismatch can disrupt immersion and weaken the sense of presence. For example, hearing a cat&#8217;s meow from the left immediately suggests its location, even if it is just off-screen.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the growing importance of spatial audio in these immersive technologies<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib39\" title=\"\">2020</a>; Huiyu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib22\" title=\"\">2025</a>)</cite>, progress in machine learning for spatial audio understanding is limited by the fundamental spatial data constraints. Most existing audio datasets<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib17\" title=\"\">2017</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib9\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib3\" title=\"\">2023</a>)</cite> focus on monaural recordings, which discard vital spatial information, effectively \"flattening\" the soundscape and preventing models from learning key physical phenomena such as room reverberation, echo patterns, and sound propagation. Moreover, the scarcity of multimodal datasets that align spatial audio with synchronized visual, position geometric, and semantic annotations hinders the development of advanced auditory scene-analysis systems capable of human-like spatial perception.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these gaps, we present <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a 484-hour large-scale multimodal spatial audio dataset designed to support both spatial audio understanding and generation. It integrates high-fidelity spatial recordings with synchronized video, 3D pose tracking, and rich semantic annotations, enabling comprehensive modeling of real-world auditory scenes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S0.F1\" title=\"Figure 1 &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the dataset comprises four subsets, each targeting distinct tasks and scenarios.\n<span class=\"ltx_text ltx_font_bold\">MRSLife</span> (129 h) captures daily activities such as board games, cooking, and office work, using egocentric video and FOA audio annotated with sound events and speech transcripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSpeech</span> (206 h) includes binaural conversations from 44 speakers across diverse indoor environments, paired with video, 3D source positions, and complete scripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSing</span> (80 h) features high-quality solo singing performances in Chinese, English, German, and French by 20 vocalists, each aligned with time-stamped lyrics and corresponding musical scores.\n<span class=\"ltx_text ltx_font_bold\">MRSMusic</span> (69 h) offers spatial recordings of 23 Traditional Chinese, Western and Electronic instruments, with symbolic score annotations that support learning-based methods for symbolic-to-audio generation and fine-grained localization.\nTogether, these four subsets support a broad spectrum of spatial audio research problems, including event detection, sound localization, and binaural or ambisonic audio generation. By pairing spatial audio with synchronized exocentric and egocentric video, geometric tracking, and detailed semantic labels, MRSAudio enables new research directions in multimodal spatial understanding and cross-modal generation. Throughout this paper, unless stated otherwise, we use the term spatial audio to refer to binaural audio.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial",
                    "audio",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a 484-hour, large-scale multimodal spatial audio dataset explicitly designed to push the boundaries of spatial audio understanding and generative modeling.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assemble synchronized binaural and ambisonic recordings with exocentric and egocentric video, geometric source positions, transcripts, scores, lyrics, and event labels, providing one of the most richly annotated multimodal resources for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish and release evaluation protocols and baseline implementations for five benchmark tasks: audio spatialization generation, spatial text-to-speech, spatial singing voice synthesis, spatial music generation, and localization and detection of sound events, in order to demonstrate MRSAudio&#8217;s versatility and to foster reproducible research.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2\" title=\"2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reviews existing spatial audio datasets.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3\" title=\"3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> describes the design, collection process, and key statistics of MRSAudio.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4\" title=\"4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents extensive benchmark experiments using state-of-the-art methods on five core spatial audio tasks: audio spatialization, spatial text-to-speech, spatial singing voice generation, spatial music synthesis, and sound event localization and detection.\nFinally, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S5\" title=\"5 Conclusion and Discussion &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> concludes the paper and discusses the limitations and potential risks associated with MRSAudio.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning has driven remarkable progress in both audio generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib20\" title=\"\">2023a</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib41\" title=\"\">2023</a>; Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib25\" title=\"\">2022</a>)</cite> and audio understanding <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib11\" title=\"\">2023</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib21\" title=\"\">2023b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib35\" title=\"\">2024</a>)</cite> tasks. However, the majority of these advances still rely heavily on monaural audio, which lacks the ability to represent or capture the rich spatial cues that naturally occur in real-world environments.\nThe rapid adoption of VR/AR technologies has concurrently driven growing demand for immersive spatial audio experiences. Researchers have focused on several key technologies including: sound event localization and detection <cite class=\"ltx_cite ltx_citemacro_citep\">(Adavanne et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib2\" title=\"\">2019</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib36\" title=\"\">2022</a>)</cite>, mono-to-spatial audio conversion <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao &amp; Grauman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib15\" title=\"\">2019</a>; Pedro&#160;Morgado &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib31\" title=\"\">2018</a>)</cite>, and end-to-end spatial audio generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite recent progress, spatial audio generation and understanding remain constrained by the paucity of high-quality datasets. Due to the difficulty and expense of collecting and annotating high-quality spatial audio datasets, most open-source datasets still primarily consist of simulated or web-crawled content.\nSimulated datasets offer precise annotations but lack perceptual realism, while crawled datasets often provide real-world diversity but are missing critical labels, such as listener and source positions and content-level annotations, thus limiting their utility.\nFor instance, spatial symbolic music generation demands accurate musical scores and corresponding positional metadata.\nTo better understand the current landscape, we survey existing spatial audio datasets. These datasets differ in terms of audio format, including First-Order Ambisonics (FOA), multi-channel arrays, and binaural microphones, as well as in their collection methods (simulated, crawled, recorded) and annotation.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, existing datasets vary in their goals and modalities. For instance, Spatial LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Sarabia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib32\" title=\"\">2023</a>)</cite> simulates spatial speech using the LibriSpeech corpus and is mainly intended for binaural TTS applications. RealMAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib40\" title=\"\">2024</a>)</cite> is a large-scale real-recorded and annotated 32-channel microphone array dataset for multichannel speech enhancement and source localization. YT-Ambigen <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>)</cite> and Sphere360 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite> are derived from web-crawled video datasets, but lack explicit spatial annotations, making them suitable primarily for video-to-spatial audio generation. BinauralMusic focuses on musical content, offering instrument class tags. BEWO-1M <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>)</cite> combines crawled content and simulated binaural rendering, and provides image or GPT-generated prompts. STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> features real-world FOA and multi-channel audio alongside synchronized videos and includes sound class and position labels.\nIn contrast to prior datasets, MRSAudio offers a comprehensive, large-scale, and real-world spatial audio corpus, featuring 484 hours of recorded data in both FOA and binaural formats, covering all audio domains including general audio, speech, singing, and music. Uniquely, MRSAudio includes synchronized audio, video, and 3D positional geometry, along with fine-grained cross-modal annotations, such as: transcripts, word and phoneme boundaries and music scores.\nThese features make MRSAudio an ideal resource for a broad range of spatial audio generation and understanding tasks, including spatial speech, music, and singing voice synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts",
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a freely available multimodal spatial audio corpus with synchronized video, positional data, and fine-grained annotations, released under the CC BY-NC-SA 4.0 license. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our data processing pipeline, with detailed descriptions in subsequent subsections. We then summarize key statistics that demonstrate MRSAudio&#8217;s scale and diversity.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span> MRSSpeech targets clean, high-quality conversational recordings for TTS task. All spoken interactions are recorded in controlled indoor environments with minimal noise. We invite speakers to participate in content-driven conversations based on predefined scripts.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> MRSMusic captures immersive instrumental performances suitable for spatial music generation and analysis.\nWe record solo performances from 45 professional musicians across 23 instruments, including Traditional Chinese, Western and Electronic instruments.\nEach performance is paired with its corresponding musical score to support score-based music generation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span> To introduce spatial variability, we select four recording rooms that differ in size and acoustic material. Each speaker reads inflected emotions from the scripts while walking through the space, producing dynamic spatial cues. Recordings include spoken passages, binaural audio and exocentric video, as well as a clean mono audio by a lavalier microphone placed near the speaker.</p>\n\n",
                "matched_terms": [
                    "mono",
                    "spatial",
                    "audio",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> We record 23 Traditional Chinese, Western and Electronic instruments, performed by 45\nprofessional musicians.To capture rich spatial detail, we vary microphone placement around the instrument. Recordings include binaural audio, monaural audio, exocentric video, and synchronized video that records playing gestures. Full recording details are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS2\" title=\"A.2 Details of Recording &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span>\nFor MRSDialogue, we apply WhisperX <cite class=\"ltx_cite ltx_citemacro_citep\">(Bain et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib6\" title=\"\">2023</a>)</cite> for automatic speech recognition and speaker diarization to generate initial transcripts and speaker turns. Human annotators correct recognition errors and speaker attribution mismatches. The audio is then segmented into utterances and the transcripts are converted into phoneme sequences (using pypinyin for Mandarin). A two-stage alignment process follows: we first apply the Montreal Forced Aligner (MFA) <cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib30\" title=\"\">2017</a>)</cite> for coarse word/phoneme mapping, then manually refine boundaries in Praat <cite class=\"ltx_cite ltx_citemacro_citep\">(Boersma, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib8\" title=\"\">2001</a>)</cite>.\nFor MRSSound, we annotate sound event categories and corresponding time intervals.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span>\nGiven the availability of full scripts, we adapt WhisperX for long-form word-to-audio alignment of up to 30 minutes.\nEach script line is automatically matched to its corresponding audio segment (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS3\" title=\"A.3 Details of Annotation &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a> for more details).\nAnnotators then review these alignments, correcting any omissions or insertions caused by actors&#8217; deviations from the script. Finally, phoneme sequences are extracted and aligned with the audio using the same procedure as in MRSDialogue.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span>\nWe use Audio Slicer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/flutydeer/audio-slicer\" title=\"\">https://github.com/flutydeer/audio-slicer</a></span></span></span> to segment the music recordings and generate initial symbolic annotations with basic-pitch <cite class=\"ltx_cite ltx_citemacro_citep\">(Bittner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib7\" title=\"\">2022</a>)</cite>, and then employ professional musicians to verify and adjust note onsets, offsets, and dynamics to match the performance accurately.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-processing pipeline comprises three key steps to refine raw annotated data. First, segmentation splits continuous recordings into task-oriented clips: utterances for speech and singing are extracted via alignment timestamps, while MRSSound audio is uniformly divided into 10-second segments. Next, multimodal synchronization aligns each clip with auxiliary modalities (text, video, position metadata) with temporal anchors. Static sound sources are annotated with manually measured 3D coordinates, whereas dynamic sources leverage interpolated UWB tracking trajectories. For scenes where participants&#8217; faces are visible, anonymization is performed by adding half-face masks. Finally, quality assurance involves domain experts auditing 15% of clips across all modules to verify temporal alignment precision, cross-modal content consistency, and annotation accuracy.\nFull auditing protocols are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS4\" title=\"A.4 Details of Post-Processing &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>, ensuring reproducibility of this process.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To illustrate spatial diversity, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) shows the 3D distribution of sound source positions relative to the listener. The heatmap reveals near-uniform azimuthal coverage, with greater density in the frontal hemisphere due to the prevalence of egocentric video recordings. Elevation angles are concentrated between &#8211;40&#176; and 40&#176;, aligning with typical human sound perception patterns.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b) summarizes the annotations using a keyword cloud that captures the range of real-world activities recorded. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c) presents the proportions of recording spaces by room size: medium-sized rooms are the most common, accounting for approximately 40% of sessions, while small and large rooms each represent around 30%. Recording duration is evenly distributed across room types, ensuring diverse acoustic coverage.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(d) shows the distribution of segment durations after automatic and manual segmentation. Most audio clips are shorter than 10 seconds, which is suitable for modeling short-duration events and supports efficient downstream training and inference.\nOverall, MRSAudio offers comprehensive coverage across spatial positions, acoustic environments, scene types, and temporal structures, making it well-suited for a wide range of spatial audio generation and understanding tasks. A more detailed breakdown of per-scenario statistics is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1\" title=\"Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the quality and utility of MRSAudio in real-world scenarios, we evaluate it on five representative spatial audio tasks: (i) audio spatialization, (ii) spatial text-to-speech, (iii) spatial singing voice synthesis, (iv) spatial music generation, and (v) sound event localization and detection (SELD). These tasks cover both generation and understanding, and are critical for applications such as AR/VR, spatial media production, and perceptual scene analysis.\nAll experiments are conducted using state-of-the-art methods on a server equipped with eight NVIDIA RTX 4090 GPUs.\nFor different tasks, we employ distinct objective metrics. For generation tasks, we compute cosine similarity scores for direction (ANG Cos) and distance (DIS Cos) to quantify spatial alignment quality. Additionally, we utilize subjective MOS-Q (Mean Opinion Score for Quality) to evaluate the quality of generated audio and MOS-P (Mean Opinion Score for Position) to assess spatial perception. For implementation details of training and evaluation metrics, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2\" title=\"Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "dis",
                    "mosp",
                    "spatialization",
                    "spatial",
                    "mosq",
                    "subjective",
                    "cos",
                    "ang",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio spatialization aims to synthesize spatially immersive soundscapes from monaural inputs and source positional information. To evaluate MRSAudio on this task, we adopt BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite>, a diffusion-based generation model that predicts binaural waveforms conditioned on monaural audio and source coordinates. Since the downstream generation tasks can be formulated as predicting interaural (binaural) representations from mono audio, we employ a single-stage training scheme for all experiments. For comparison, we include a traditional signal processing baseline (DSP), which renders binaural audio using virtual source positions simulated via room impulse responses (RIRs) and head-related transfer functions (HRTFs).\nWe adopt the following objective metrics to evaluate audio quality:\n(1) W-L2: waveform L2 distance,\n(2) A-L2: amplitude envelope L2 distance,\n(3) P-L2: phase difference L2,\n(4) STFT: multi-resolution STFT loss,\n(5) PESQ: perceptual speech quality score.\nResults for the ground truth and DSP baseline are obtained by averaging across all test sets from the four MRSAudio subsets. Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS3\" title=\"B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatialization",
                    "mono",
                    "truth",
                    "ground",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Spatialization &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BinauralGrad achieves strong performance across all MRSAudio subsets, surpassing the classical DSP baseline on most objective and subjective metrics. The highest performance is observed on MRSLife, likely due to the relatively limited variation in sound sources within these scenes. These results demonstrate that MRSAudio&#8217;s rich spatial annotations and diverse acoustic environments provide a solid foundation for training and evaluating spatial audio generation models.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "subjective",
                    "performance",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial TTS aims to produce high-quality speech enriched with spatial cues, thereby enhancing immersion and realism in AR/VR. Although recent advances in TTS have led to impressive improvements in speech quality, progress in spatialized speech generation remains limited due to the scarcity of spatially annotated recordings with rich, high-quality labels.\nTo evaluate the effectiveness of MRSAudio for this task, we train an end-to-end model following ISDrama <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib43\" title=\"\">2025</a>)</cite> to directly generate spatial speech from text and position data. Additionally, we compare with cascaded pipelines that combine monaural TTS models (CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib14\" title=\"\">2024</a>)</cite> and F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib10\" title=\"\">2024</a>)</cite>) with the audio spatialization module. This allows us to assess both speech generation quality and the spatial fidelity enabled by our dataset.\nFor content evaluation, we report Character Error Rate (CER) and Speaker Similarity (SIM). Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS4\" title=\"B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "isdrama",
                    "spatialization",
                    "spatial",
                    "cosyvoice",
                    "tts",
                    "sim",
                    "f5tts",
                    "cer",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial SVS aims to produce expressive, high-quality singing voices enriched with accurate spatial cues, thereby enhancing listener immersion. While traditional SVS has advanced considerably, spatial SVS remains underexplored due to the lack of high-quality, spatially annotated datasets.\nTo evaluate MRSAudio for this task, we use the MRSSing subset to train and benchmark models. We adopt the ISDrama architecture to perform end-to-end spatial singing synthesis, incorporating note-level pitch control to enhance prosody accuracy. For comparison, we use the open-source SVS models Rmssinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib19\" title=\"\">2023</a>)</cite>. The outputs are then spatialized with BinauralGrad.\nWe employ objective metrics, including Mel-Cepstral Distortion (MCD) and F0 Frame Error (FFE), to evaluate spectral and pitch similarity between predicted and ground-truth. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS5\" title=\"B.5 Spatial Singing Voice Synthesis &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "isdrama",
                    "spatial",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T4\" title=\"Table 4 &#8227; 4.3 Spatial Singing Voice Synthesis &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the Mono + SP approach trained on MRSSing with pitch control achieves the best performance across most metrics. Its low MCD indicates strong spectral fidelity, and high ANG Cos and DIS Cos scores demonstrate effective spatial alignment. Subjective MOS results confirm that the generated singing voices are natural, high-quality, and spatially coherent.\nThese findings validate MRSSing as an effective resource for spatial singing voice generation.</p>\n\n",
                "matched_terms": [
                    "dis",
                    "spatial",
                    "subjective",
                    "mono",
                    "cos",
                    "ang",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task aims to synthesize spatially immersive music conditioned on symbolic scores. While datasets like FAIR-Play offer high-quality instrument recordings, they lack aligned sheet music, limiting controllable generation. In contrast, our MRSMusic subset includes 23 instruments with aligned scores, enabling fine-grained, score-based spatial music synthesis.\nWe benchmark three systems on MRSMusic. First, we apply BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite> to spatialize mono recordings. Second, we utilize Make-An-Audio 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib13\" title=\"\">2023</a>)</cite> to generate mono music from MIDI scores, which is subsequently spatialized. Third, we adapt ISDrama to accept both score embeddings and spatial poses, enabling end-to-end spatial symbolic music generation.\nFor objective evaluation, we compute Fr&#233;chet Audio Distance (FAD) and FFE to evaluate the results. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS6\" title=\"B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "isdrama",
                    "spatial",
                    "mono",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T5\" title=\"Table 5 &#8227; 4.4 Spatial Music Generation &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the Mono + SP pipeline achieves strong performance, benefiting from access to ground truth mono audio. The Make-An-Audio 2 + SP exhibits limitations in FAD and pitch accuracy. This suggests that general-purpose audio generation models may struggle to fully capture structured musical information from symbolic prompts. The ISDrama (Music) model generates music directly from the symbolic inputs and spatial cues, achieving better coherence and spatial alignment than the Make-An-Audio 2 + SP. These results highlight MRSMusic&#8217;s effectiveness in supporting spatially controllable music generation across diverse instruments and spatial conditions.</p>\n\n",
                "matched_terms": [
                    "isdrama",
                    "spatial",
                    "mono",
                    "truth",
                    "ground",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task evaluates the ability to detect and localize sound events using MRSAudio&#8217;s spatial annotations. We follow STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> using both audio-only and audio-visual variants on the MRSSound subset.\nIn the audio-only condition, models receive either FOA or binaural waveforms as input and predict sound event classes along with 3D source coordinates. For the audio-visual condition, we extract bounding boxes of visible persons to serve as coarse visual priors, which are fused with the audio representations. We also explore architectural variations by replacing the original convolutional backbone with a Transformer encoder, allowing us to assess the impact of temporal modeling capacity on spatial prediction.\nWe evaluate model performance using four standard joint detection and localization metrics, including location-aware detection (<math alttext=\"F_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">F_{20^{\\circ}}</annotation></semantics></math>, <math alttext=\"ER_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub></mrow><annotation encoding=\"application/x-tex\">ER_{20^{\\circ}}</annotation></semantics></math>) and class-aware localization (<math alttext=\"LE_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>E</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LE_{CD}</annotation></semantics></math>, <math alttext=\"LR_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LR_{CD}</annotation></semantics></math>). Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS7\" title=\"B.7 Sound Event Localization and Detection &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.7</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T6\" title=\"Table 6 &#8227; 4.5 Sound Event Localization and Detection &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, model performance varies with architecture, input modality, and audio format. Transformer-based models generally outperform ConvNet baselines, particularly in reducing localization error. FOA input consistently yields better results than binaural audio, benefiting from richer spatial representation. For example, the Transformer with FOA and no visual input achieves the lowest error rate (ER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> 0.99) and highest localization recall (LR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub> 87.32). The addition of visual features improves performance in some ConvNet settings (e.g., F<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m3\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> rises from 9.33 to 13.00 with FOA), but offers limited or inconsistent gains in Transformer models, possibly due to modality mismatch or redundancy. These results highlight the value of MRSAudio&#8217;s spatial annotations and multimodal streams in supporting flexible evaluation under both unimodal and multimodal configurations.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce MRSAudio, a large-scale, multimodal spatial audio corpus designed to support a wide range of generation and understanding tasks. MRSAudio comprises four complementary modules, MRSLife, MRSSpeech , MRSSing, and MRSMusic, each captured with binaural/FOA audio, synchronized video, precise 3D pose metadata, and richly detailed annotations (event labels, transcripts, phoneme boundaries, lyrics, musical scores, and motion prompts). Through extensive benchmarks on audio spatialization, binaural speech, singing, and music generation , as well as sound event localization and detection, we demonstrate that MRSAudio&#8217;s scale, diversity, and annotation depth enable state&#8208;of&#8208;the&#8208;art performance and unlock new avenues for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatialization",
                    "spatial",
                    "mrsspeech",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Directions:</span>\nWhile MRSAudio offers broad multimodal coverage, two limitations remain. First, although synchronized video is provided for all recordings, current benchmarks only explore visual input in a limited subset of tasks. Second, while the dataset includes both binaural and First-Order Ambisonic (FOA) formats, the FOA subset is smaller in scale, and most tasks focus on binaural audio, limiting spatial modeling diversity.\nFuture work will expand the role of visual modalities in tasks such as sound localization and scene understanding. We also plan to increase FOA recordings to balance data distribution and support broader spatial audio research, and develop more FOA-specific benchmarks to better utilize ambisonic spatial cues.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Societal Impact:</span>\nAs with any large-scale audiovisual dataset, MRSAudio carries potential risks if misused. It could be exploited to generate highly realistic yet synthetic spatial audio for deepfakes or disinformation in AR and VR applications. To mitigate such risks, MRSAudio is released under a noncommercial license with clear usage guidelines. We encourage responsible use in accordance with ethical standards, including consent management, data governance, and transparency.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the planning phase, we systematically analyze real-world application scenarios for spatial audio and divide them into four major categories. MRSLife focuses on daily environmental sound events, MRSSpeech targets high-quality conversational data, MRSSing captures solo vocal performances, and MRSMusic records immersive instrumental music. These four scenarios collectively cover a broad range of everyday acoustic contexts and are designed to support a wide spectrum of downstream tasks.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on predefined scenario requirements, we recruit professionals from relevant fields to participate in the recording process. To ensure spatial diversity, we rent various venues tailored to each scenario type. Following the FAIR-Play protocol, we use a 3Dio Free Space XLR binaural microphone to capture spatial audio, GoPro HERO cameras and mobile phones to record exocentric and egocentric videos, and UWB-based tracking systems to capture motion trajectories.\nOnce personnel, locations, and equipment are secured, we prepare task-specific materials for each subset. For MRSLife, we further divide the content based on the proportion of speech involved, resulting in two subcategories: MRSDialogue (e.g., group games, board games) and MRSSound (e.g., kungfu, kitchens, offices, sports). We predefine common sound events for each, such as clattering dishes, keyboard typing, and table tennis.\nFor MRSSpeech, we compile a large corpus of scripts from movies, TV shows, and crosstalk performances and automatically extract dialogue passages for speaker delivery.\nFor MRSSing, we design lyrics in four languages (Chinese, English, German, and French) and recruit singers across a range of vocal types to maximize diversity.\nFor MRSMusic, we collect solo performances across 23 traditional and modern instruments, covering a wide array of timbres and playing techniques.\nSpecific sound categories for each subset are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.T7\" title=\"Table 7 &#8227; A.1 Details of Planning &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial",
                    "audio",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span>\nWe employ 44 expressive speakers to read from scripted texts, each paid $30 per hour of recorded audio. The total recorded duration reaches 200 hours, with compensation totaling $40,000. During recording sessions, speakers alternate between standing and walking around the recording area to introduce spatial diversity while maintaining speech clarity. Binaural audio is captured using head-mounted microphones, and clean monaural speech is recorded using lavalier microphones. All sessions are filmed from a third-person perspective.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial",
                    "audio",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nEighteen professional singers participate in the recordings. Each singer is fluent in at least one of the following languages: Chinese, English, German, or French, and collectively they cover all vocal ranges including soprano, alto, tenor, and bass. Performers are paid $50 per hour, contributing a total of 80 hours of audio. Singing is recorded at a fixed position to ensure spatial consistency. Binaural recordings are captured with head-mounted microphones, and monaural audio is recorded with a studio-grade microphone. Third-person panoramic video is also captured.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a team of domain experts in singing, music performance, and linguistics to carry out and review all annotations, compensating each annotator at a rate of $15 per hour. Prior to beginning their work, every expert receives a clear explanation of how the annotations will be used and agrees to release their annotation results under an open&#8208;source license for academic research.\nFor all modules, we first synchronize each audio with its corresponding video, mono&#8208;channel reference track, and 3D positional metadata. Annotators then verify and correct this synchronization to ensure perfect alignment across modalities.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech.</span>\nGiven full dialogue scripts, we perform automatic alignment to 30-minute recordings using a chunk-based extension of WhisperX. This method divides each utterance-level audio segment into fixed-length chunks (e.g., 30 seconds), applies phoneme-level models (e.g., wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib5\" title=\"\">2020</a>)</cite> ) for emission prediction on each chunk, and then concatenates emissions to form a complete alignment matrix.\nWe compute alignments via a trellis-based dynamic programming algorithm with backtracking, followed by scaling to restore absolute timestamps. Word- and sentence-level timings are derived by grouping aligned characters using word indices and sentence tokenization. Missing or partial timings are interpolated using a specified method (e.g., nearest).\nThis pipeline enables efficient and accurate alignment of long-form recordings on GPU. We then refine sentence boundaries in Praat and apply the same MFA-plus-Praat workflow used in MRSLife for fine-grained phoneme and word-level alignment.</p>\n\n",
                "matched_terms": [
                    "method",
                    "audio",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing.</span>\nEach segment contains a solo vocal performance with known lyrics. We first apply voice activity detection (VAD) to segment the recordings. Lyrics are aligned to each segment using LyricFN&#8217;s ASR-based dynamic programming method. English phonemes follow the ARPABET standard,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://en.wikipedia.org/wiki/ARPABET\" title=\"\">https://en.wikipedia.org/wiki/ARPABET</a></span></span></span> while German and French use MFA&#8217;s phoneme sets.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mfa-models.readthedocs.io/en/latest/dictionary/\" title=\"\">https://mfa-models.readthedocs.io/en/latest/dictionary/</a></span></span></span> We then apply MFA for initial alignment and refine it manually in Praat to obtain precise word and phoneme boundaries. Annotators assign fine-grained style tags, including emotion, genre, and tempo. To generate score annotations, we extract F0 contours using RMVPE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib38\" title=\"\">2023</a>)</cite> and convert them into MIDI format via ROSVOT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib27\" title=\"\">2024</a>)</cite>, followed by expert correction.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation:</span>\nAfter annotation, we segment the raw recordings into shorter clips to support spatial audio generation and analysis tasks. For speech and singing, we use alignment timestamps to extract utterances. For MRSSound, each recording is divided into fixed 10-second segments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Synchronization:</span>\nIn addition to binaural audio, each clip is synchronized with the following modalities: audio, text, video, and position data. Using the segment timestamps, we align all streams temporally. For static sources, we attach manually recorded 3D coordinates; for dynamic sources, we interpolate Ultra-Wideband (UWB) tracking data over the segment duration.\nThis process yields fully synchronized multimodal clips ready for downstream spatial audio tasks.\nFurthermore, for segments where participants&#8217; faces are visible, we apply anonymization by using a face detection model to overlay digital masks during post-processing.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Checking:</span>\nTo ensure the reliability of annotations, domain experts perform a random audit on 15% of the segmented clips across all four modules. The audit process involves the following steps:\n(1) Verifying the temporal synchronization across different modal;\n(2) Confirming that the assigned event labels accurately reflect the audiovisual content present in each clip;\n(3) Reviewing the speech segments to check the correctness of word- and phoneme-level alignments;\n(4) Evaluating singing clips for accurate alignment between lyrics and audio, consistency with musical scores, and correctness of assigned style labels;\n(5) Assessing MRSMusic excerpts to verify that musical properties, such as key, pitch, and note duration.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F4\" title=\"Figure 4 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(b) shows the phoneme distribution across all speech segments. The most common phoneme is &#8216;i&#8217;, while the least frequent is &#8216;ueng&#8217;. This distribution aligns with real-world phonetic patterns and highlights the dataset&#8217;s linguistic richness. Such broad phoneme coverage is beneficial for downstream tasks in speech synthesis and recognition under spatial settings.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSound.</span>\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F5\" title=\"Figure 5 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(a) illustrates the spatial distribution of sound sources in MRSSound relative to the listener&#8217;s head-centered coordinate system. The listener&#8217;s facing direction is at 90 degrees, and most sound events are concentrated in the frontal hemisphere (azimuth from 0&#176; to 180&#176;), which is consistent with the first-person video capture setup. Some events also appear in the rear field (&#8211;180&#176; to 0&#176;). In elevation, the majority of sound sources are distributed between &#8211;90&#176; and 40&#176;, realistically reflecting everyday human perception in standing scenarios, where sound events typically originate below ear level. This broad spatial coverage supports the training of spatial audio models with strong generalization ability.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F6\" title=\"Figure 6 &#8227; A.5.2 Statistics of MRSSpeech &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>(a) illustrates the spatial distribution of speech sources with respect to the listener. The azimuth angles span the full 360&#176; around the listener, providing comprehensive coverage of spatial directions. Elevation angles are mostly concentrated between 0&#176; and 60&#176;. Smaller elevations reflect scenarios where both the speaker and listener are either standing or seated, while larger elevations (above 30&#176;) simulate common real-world speech situations such as meetings, where a standing speaker addresses a seated listener. This diverse spatial coverage supports generalization in spatial speech modeling.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F6\" title=\"Figure 6 &#8227; A.5.2 Statistics of MRSSpeech &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>(e) presents the phoneme distribution, which covers all common phonetic units in the dataset&#8217;s target language. This phonetic diversity ensures that MRSSpeech provides strong generalizability for phoneme-aware models in speech synthesis and recognition.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F7\" title=\"Figure 7 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(a) shows the 3D spatial distribution of sound sources with respect to the listener. The majority of sources are located in front of the listener, consistent with the setup of solo vocal recordings. However, the coverage also spans surrounding directions in both azimuth and elevation, ensuring spatial variability for training robust spatial audio models.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct subjective evaluation of the generation tasks using Mean Opinion Score (MOS). For each task, we randomly sample 40 utterances from the test set. Each utterance is paired with a corresponding source-position prompt and is evaluated by at least five expert listeners.\nMOS-Q, Listeners rate each sample on a five-point Likert scale ranging from 1 (bad) to 5 (excellent).\nFor audio quality, we use MOS-Q, where listeners wear headphones and assess the clarity and naturalness of the generated audio.\nFor spatial perception, we use MOS-P, where listeners evaluate the realism of spatial cues and whether the perceived direction and distance of the sound source match the prompt description.\nAll participants are fairly compensated for their time at a rate of $15 per hour, resulting in a total cost of approximately $2000. Participants are informed that their evaluations will be used exclusively for academic research purposes.\nInstructions for audio evaluations are shown in Figure\n&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.F9\" title=\"Figure 9 &#8227; B.1 Subjective Evaluation Metrics &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "mosp",
                    "spatial",
                    "subjective",
                    "audio",
                    "mosq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively assess spatial audio generation and understanding across multiple tasks, we adopt a set of objective metrics that evaluate signal fidelity, spatial consistency, intelligibility, and speaker or pitch accuracy. We randomly sample 400 data points as the test set.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatial",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure waveform similarity using Wave L2, the mean squared error (MSE) between the generated and reference binaural waveforms. Amplitude L2 and Phase L2 are computed after applying Short-Time Fourier Transform (STFT), reflecting errors in the magnitude and phase components respectively. MRSTFT loss<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/csteinmetz1/auraloss\" title=\"\">https://github.com/csteinmetz1/auraloss</a></span></span></span> is also used, combining spectral convergence, log-magnitude, and linear-magnitude terms for better spectral alignment.\nIn addition, we use the perceptual evaluation metric PESQ<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/aliutkus/speechmetrics\" title=\"\">https://github.com/aliutkus/speechmetrics</a></span></span></span> to assess audio quality for speech-related tasks. Since PESQ is designed for speech, it is omitted for MRSLife and MRSMusic. For all metrics except PESQ, lower values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate spatial consistency, we use Spatial-AST<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib44\" title=\"\">2024</a>)</cite> to extract angular and distance embeddings from the binaural audio. Since Spatial-AST predicts positions only for static sources, we compute the cosine similarity between the predicted and ground truth embeddings within 1-second segments and average the results to assess overall spatial fidelity.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "truth",
                    "audio",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate speech intelligibility using Character Error Rate (CER), which measures the proportion of character-level differences between ASR transcriptions and reference texts. Transcriptions are generated using the Paraformer-zh model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib16\" title=\"\">2023</a>)</cite>. To assess speaker consistency, we compute Speaker Identity Matching (SIM) as the cosine similarity between speaker embeddings extracted using a WavLM-based speaker verification model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/pyannote/speaker-diarization\" title=\"\">https://huggingface.co/pyannote/speaker-diarization</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "cer",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"m_{t}(d)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{t}(d)</annotation></semantics></math> and <math alttext=\"\\hat{m}_{t}(d)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>m</mi><mo>^</mo></mover><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{m}_{t}(d)</annotation></semantics></math> represent the <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>-th Mel-frequency cepstral coefficient (MFCC) at frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> for the ground truth and synthesized signals, respectively, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the number of MFCC dimensions.</p>\n\n",
                "matched_terms": [
                    "truth",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we adopt F0 Frame Error (FFE) to evaluate pitch accuracy by comparing extracted F0 contours between the synthesized and ground truth audio.</p>\n\n",
                "matched_terms": [
                    "truth",
                    "audio",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build upon BinauralGrad&#8217;s two-stage diffusion-based framework to convert monaural audio into spatial audio. However, in our subsequent generation experiments, the synthesized monaural audio typically corresponds to a centrally positioned source between the ears, effectively serving as the first stage of BinauralGrad. Therefore, for the spatialization experiments presented here, we use only the second stage of BinauralGrad to validate spatial audio generation.\nSpecifically, we first convert binaural recordings to monaural input by averaging the two channels. Next, we apply a DSP-based method to produce a coarse spatial approximation of the binaural signal. This monaural input and its simulated binaural counterpart are then used as input to the BinauralGrad model, which is conditioned on the object&#8217;s motion trajectory to generate spatialized binaural audio.\nWe list the architecture and hyperparameters of BinauralGrad in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T8\" title=\"Table 8 &#8227; B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "method",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune two pre-trained models, CosyVoice and F5-TTS, using monaural audio obtained by averaging the binaural recordings from the MRSSpeech subset. This allows the models to learn the generation characteristics specific to our dataset. After monaural generation, we apply a pre-trained spatialization model trained on MRSSpeech to convert the output into spatialized binaural audio. For the ISDrama baseline, we adopt the model variant proposed in the original paper that conditions generation on predefined spatial paths to produce binaural spatial speech directly.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "isdrama",
                    "spatialization",
                    "spatial",
                    "mrsspeech",
                    "cosyvoice",
                    "f5tts",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ISDrama baseline, we adopt the spatial-path-conditioned generation framework and extend it by incorporating Rmssinger&#8217;s note encoder, allowing explicit pitch control through musical score input.\nWe use the single stage variation of Rmssinger, a standard singing voice synthesis model, and train it with monaural targets derived from the averaged binaural recordings in the MRSSing subset. This enables the model to generate pitch-accurate audio that reflects the acoustic characteristics. The synthesized monaural audio is then spatialized using the spatialization model trained on MRSSing.\nWe list the architecture and hyperparameters of Rmssinger in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T9\" title=\"Table 9 &#8227; B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "isdrama",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For spatial music generation, in the ISDrama setting, we follow the path-conditioned generation pipeline but remove the phoneme encoder, using only the note encoder to process symbolic scores as the sole control condition for generating spatialized music.\nWe also adopt Make-An-Audio 2 as our base model and train it using monaural audio derived from averaged MRSMusic binaural recordings. We use the pretrained spectrogram autoencoder. Instrument category and symbolic score information are concatenated into the text prompt to guide the music generation process. The generated audio is subsequently spatialized using the MRSMusic-trained spatialization model. We list the hyper-parameters of Make-An-Audio 2 in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T10\" title=\"Table 10 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "isdrama",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the STARSS23 framework for FOA-based sound event localization and detection, training on the event segments from the MRSSound subset under both audio-only and audio-visual conditions. To enable binaural input, we modify the model architecture to use three input channels and extract interaural phase difference features. This allows us to adapt the SELD model to binaural audio. Additionally, we experiment with replacing convolutional layers with Transformer encoders to explore the effect of different architectures on sound event localization and detection performance.\nWe list the hyper-parameters of SELD in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T11\" title=\"Table 11 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations",
        "caption": "Table 4: Spatial SVS Performance on MRSSing. “SP” denotes the Audio Spatialization.",
        "body": "Method\nObjective\nSubjective\n\n\n\nMCD ↓\\downarrow\n\n\nFFE ↓\\downarrow\n\n\nANG Cos ↑\\uparrow\n\n\nDIS Cos ↑\\uparrow\n\n\nMOS-Q ↑\\uparrow\n\n\nMOS-P ↑\\uparrow\n\n\n\nGT (Ground Truth)\n-\n-\n-\n-\n\n4.45 ±\\pm 0.10\n\n\n4.30 ±\\pm 0.12\n\n\n\nMono+SP\n3.19\n0.17\n0.51\n0.71\n\n3.84 ±\\pm 0.09\n\n3.91 ±\\pm 0.07\n\n\nRmssinger+SP\n3.85\n0.23\n0.45\n0.65\n\n3.65 ±\\pm 0.08\n\n\n3.81 ±\\pm 0.11\n\n\n\nISDrama(sing)\n3.71\n0.21\n0.47\n0.70\n3.86 ±\\pm 0.13\n\n3.88 ±\\pm 0.09",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Objective</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MCD </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FFE </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ANG Cos </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DIS Cos </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MOS-Q </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MOS-P </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">GT (Ground Truth)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.45 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.10</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.30 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.12</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Mono+SP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.84 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.91 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Rmssinger+SP</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.85</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.65</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.65 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.81 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.11</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ISDrama(sing)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.86 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.88 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "dis",
            "mosp",
            "mrssing",
            "subjective",
            "mcd",
            "↓downarrow",
            "±pm",
            "ground",
            "objective",
            "monosp",
            "audio",
            "spatialization",
            "svs",
            "cos",
            "isdramasing",
            "spatial",
            "ffe",
            "denotes",
            "truth",
            "rmssingersp",
            "↑uparrow",
            "performance",
            "method",
            "“sp”",
            "ang",
            "mosq"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T4\" title=\"Table 4 &#8227; 4.3 Spatial Singing Voice Synthesis &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the Mono + SP approach trained on MRSSing with pitch control achieves the best performance across most metrics. Its low MCD indicates strong spectral fidelity, and high ANG Cos and DIS Cos scores demonstrate effective spatial alignment. Subjective MOS results confirm that the generated singing voices are natural, high-quality, and spatially coherent.\nThese findings validate MRSSing as an effective resource for spatial singing voice generation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive spatial environments, where auditory cues enable sound source localization in three-dimensional space.\nDespite the critical role of spatial audio in immersive technologies such as VR/AR, most existing multimodal datasets provide only monaural audio, which limits the development of spatial audio generation and understanding.\nTo address these challenges, we introduce MRSAudio, a large-scale multimodal spatial audio dataset designed to advance research in spatial audio understanding and generation.\nMRSAudio spans four distinct components: MRSLife, MRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios.\nThe dataset includes synchronized binaural and ambisonic audio, exocentric and egocentric video, motion trajectories, and fine-grained annotations such as transcripts, phoneme boundaries, lyrics, scores, and prompts.\nTo demonstrate the utility and versatility of MRSAudio, we establish five foundational tasks: audio spatialization, and spatial text to speech, spatial singing voice synthesis, spatial music generation and sound event localization and detection.\nResults show that MRSAudio enables high-quality spatial modeling and supports a broad range of spatial audio research.\nDemos and dataset access are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mrsaudio.github.io\" title=\"\">https://mrsaudio.github.io</a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "spatialization",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive and interpret physical environments. With the rapid growth of film, virtual reality (VR), augmented reality (AR), and gaming applications, users increasingly expect not only precise audiovisual alignment but also highly immersive experiences. While recent advances in deep learning have enabled realistic generation of speech, music, and sound effects synchronized with text or video <cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib25\" title=\"\">2022</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib41\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib37\" title=\"\">2024</a>)</cite>, most models focus on monaural audio and neglect spatialized soundscapes that enhance immersion. The human binaural system uses interaural time differences (ITD) and interaural level differences (ILD) to localize sound in three-dimensional space, and spatial audio must remain consistent with visual cues <cite class=\"ltx_cite ltx_citemacro_citep\">(Yost, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib42\" title=\"\">1998</a>; Cohen &amp; Knudsen, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib12\" title=\"\">1999</a>; Grothe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib18\" title=\"\">2010</a>)</cite>. Any mismatch can disrupt immersion and weaken the sense of presence. For example, hearing a cat&#8217;s meow from the left immediately suggests its location, even if it is just off-screen.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the growing importance of spatial audio in these immersive technologies<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib39\" title=\"\">2020</a>; Huiyu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib22\" title=\"\">2025</a>)</cite>, progress in machine learning for spatial audio understanding is limited by the fundamental spatial data constraints. Most existing audio datasets<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib17\" title=\"\">2017</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib9\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib3\" title=\"\">2023</a>)</cite> focus on monaural recordings, which discard vital spatial information, effectively \"flattening\" the soundscape and preventing models from learning key physical phenomena such as room reverberation, echo patterns, and sound propagation. Moreover, the scarcity of multimodal datasets that align spatial audio with synchronized visual, position geometric, and semantic annotations hinders the development of advanced auditory scene-analysis systems capable of human-like spatial perception.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these gaps, we present <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a 484-hour large-scale multimodal spatial audio dataset designed to support both spatial audio understanding and generation. It integrates high-fidelity spatial recordings with synchronized video, 3D pose tracking, and rich semantic annotations, enabling comprehensive modeling of real-world auditory scenes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S0.F1\" title=\"Figure 1 &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the dataset comprises four subsets, each targeting distinct tasks and scenarios.\n<span class=\"ltx_text ltx_font_bold\">MRSLife</span> (129 h) captures daily activities such as board games, cooking, and office work, using egocentric video and FOA audio annotated with sound events and speech transcripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSpeech</span> (206 h) includes binaural conversations from 44 speakers across diverse indoor environments, paired with video, 3D source positions, and complete scripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSing</span> (80 h) features high-quality solo singing performances in Chinese, English, German, and French by 20 vocalists, each aligned with time-stamped lyrics and corresponding musical scores.\n<span class=\"ltx_text ltx_font_bold\">MRSMusic</span> (69 h) offers spatial recordings of 23 Traditional Chinese, Western and Electronic instruments, with symbolic score annotations that support learning-based methods for symbolic-to-audio generation and fine-grained localization.\nTogether, these four subsets support a broad spectrum of spatial audio research problems, including event detection, sound localization, and binaural or ambisonic audio generation. By pairing spatial audio with synchronized exocentric and egocentric video, geometric tracking, and detailed semantic labels, MRSAudio enables new research directions in multimodal spatial understanding and cross-modal generation. Throughout this paper, unless stated otherwise, we use the term spatial audio to refer to binaural audio.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a 484-hour, large-scale multimodal spatial audio dataset explicitly designed to push the boundaries of spatial audio understanding and generative modeling.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assemble synchronized binaural and ambisonic recordings with exocentric and egocentric video, geometric source positions, transcripts, scores, lyrics, and event labels, providing one of the most richly annotated multimodal resources for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish and release evaluation protocols and baseline implementations for five benchmark tasks: audio spatialization generation, spatial text-to-speech, spatial singing voice synthesis, spatial music generation, and localization and detection of sound events, in order to demonstrate MRSAudio&#8217;s versatility and to foster reproducible research.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2\" title=\"2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reviews existing spatial audio datasets.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3\" title=\"3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> describes the design, collection process, and key statistics of MRSAudio.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4\" title=\"4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents extensive benchmark experiments using state-of-the-art methods on five core spatial audio tasks: audio spatialization, spatial text-to-speech, spatial singing voice generation, spatial music synthesis, and sound event localization and detection.\nFinally, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S5\" title=\"5 Conclusion and Discussion &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> concludes the paper and discusses the limitations and potential risks associated with MRSAudio.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning has driven remarkable progress in both audio generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib20\" title=\"\">2023a</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib41\" title=\"\">2023</a>; Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib25\" title=\"\">2022</a>)</cite> and audio understanding <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib11\" title=\"\">2023</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib21\" title=\"\">2023b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib35\" title=\"\">2024</a>)</cite> tasks. However, the majority of these advances still rely heavily on monaural audio, which lacks the ability to represent or capture the rich spatial cues that naturally occur in real-world environments.\nThe rapid adoption of VR/AR technologies has concurrently driven growing demand for immersive spatial audio experiences. Researchers have focused on several key technologies including: sound event localization and detection <cite class=\"ltx_cite ltx_citemacro_citep\">(Adavanne et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib2\" title=\"\">2019</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib36\" title=\"\">2022</a>)</cite>, mono-to-spatial audio conversion <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao &amp; Grauman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib15\" title=\"\">2019</a>; Pedro&#160;Morgado &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib31\" title=\"\">2018</a>)</cite>, and end-to-end spatial audio generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite recent progress, spatial audio generation and understanding remain constrained by the paucity of high-quality datasets. Due to the difficulty and expense of collecting and annotating high-quality spatial audio datasets, most open-source datasets still primarily consist of simulated or web-crawled content.\nSimulated datasets offer precise annotations but lack perceptual realism, while crawled datasets often provide real-world diversity but are missing critical labels, such as listener and source positions and content-level annotations, thus limiting their utility.\nFor instance, spatial symbolic music generation demands accurate musical scores and corresponding positional metadata.\nTo better understand the current landscape, we survey existing spatial audio datasets. These datasets differ in terms of audio format, including First-Order Ambisonics (FOA), multi-channel arrays, and binaural microphones, as well as in their collection methods (simulated, crawled, recorded) and annotation.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, existing datasets vary in their goals and modalities. For instance, Spatial LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Sarabia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib32\" title=\"\">2023</a>)</cite> simulates spatial speech using the LibriSpeech corpus and is mainly intended for binaural TTS applications. RealMAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib40\" title=\"\">2024</a>)</cite> is a large-scale real-recorded and annotated 32-channel microphone array dataset for multichannel speech enhancement and source localization. YT-Ambigen <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>)</cite> and Sphere360 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite> are derived from web-crawled video datasets, but lack explicit spatial annotations, making them suitable primarily for video-to-spatial audio generation. BinauralMusic focuses on musical content, offering instrument class tags. BEWO-1M <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>)</cite> combines crawled content and simulated binaural rendering, and provides image or GPT-generated prompts. STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> features real-world FOA and multi-channel audio alongside synchronized videos and includes sound class and position labels.\nIn contrast to prior datasets, MRSAudio offers a comprehensive, large-scale, and real-world spatial audio corpus, featuring 484 hours of recorded data in both FOA and binaural formats, covering all audio domains including general audio, speech, singing, and music. Uniquely, MRSAudio includes synchronized audio, video, and 3D positional geometry, along with fine-grained cross-modal annotations, such as: transcripts, word and phoneme boundaries and music scores.\nThese features make MRSAudio an ideal resource for a broad range of spatial audio generation and understanding tasks, including spatial speech, music, and singing voice synthesis.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a freely available multimodal spatial audio corpus with synchronized video, positional data, and fine-grained annotations, released under the CC BY-NC-SA 4.0 license. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our data processing pipeline, with detailed descriptions in subsequent subsections. We then summarize key statistics that demonstrate MRSAudio&#8217;s scale and diversity.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> MRSMusic captures immersive instrumental performances suitable for spatial music generation and analysis.\nWe record solo performances from 45 professional musicians across 23 instruments, including Traditional Chinese, Western and Electronic instruments.\nEach performance is paired with its corresponding musical score to support score-based music generation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span> To introduce spatial variability, we select four recording rooms that differ in size and acoustic material. Each speaker reads inflected emotions from the scripts while walking through the space, producing dynamic spatial cues. Recordings include spoken passages, binaural audio and exocentric video, as well as a clean mono audio by a lavalier microphone placed near the speaker.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span> The professional singers perform according to musical scores. To introduce variation in source&#8211;listener geometry, we adjust the position of the head-mounted binaural microphone relative to the singer. In addition to the binaural audio and synchronized video, each session includes a clean vocal track recorded with a studio-grade condenser microphone.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> We record 23 Traditional Chinese, Western and Electronic instruments, performed by 45\nprofessional musicians.To capture rich spatial detail, we vary microphone placement around the instrument. Recordings include binaural audio, monaural audio, exocentric video, and synchronized video that records playing gestures. Full recording details are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS2\" title=\"A.2 Details of Recording &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span>\nWe use Audio Slicer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/flutydeer/audio-slicer\" title=\"\">https://github.com/flutydeer/audio-slicer</a></span></span></span> to segment the music recordings and generate initial symbolic annotations with basic-pitch <cite class=\"ltx_cite ltx_citemacro_citep\">(Bittner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib7\" title=\"\">2022</a>)</cite>, and then employ professional musicians to verify and adjust note onsets, offsets, and dynamics to match the performance accurately.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To illustrate spatial diversity, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) shows the 3D distribution of sound source positions relative to the listener. The heatmap reveals near-uniform azimuthal coverage, with greater density in the frontal hemisphere due to the prevalence of egocentric video recordings. Elevation angles are concentrated between &#8211;40&#176; and 40&#176;, aligning with typical human sound perception patterns.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b) summarizes the annotations using a keyword cloud that captures the range of real-world activities recorded. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c) presents the proportions of recording spaces by room size: medium-sized rooms are the most common, accounting for approximately 40% of sessions, while small and large rooms each represent around 30%. Recording duration is evenly distributed across room types, ensuring diverse acoustic coverage.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(d) shows the distribution of segment durations after automatic and manual segmentation. Most audio clips are shorter than 10 seconds, which is suitable for modeling short-duration events and supports efficient downstream training and inference.\nOverall, MRSAudio offers comprehensive coverage across spatial positions, acoustic environments, scene types, and temporal structures, making it well-suited for a wide range of spatial audio generation and understanding tasks. A more detailed breakdown of per-scenario statistics is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1\" title=\"Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the quality and utility of MRSAudio in real-world scenarios, we evaluate it on five representative spatial audio tasks: (i) audio spatialization, (ii) spatial text-to-speech, (iii) spatial singing voice synthesis, (iv) spatial music generation, and (v) sound event localization and detection (SELD). These tasks cover both generation and understanding, and are critical for applications such as AR/VR, spatial media production, and perceptual scene analysis.\nAll experiments are conducted using state-of-the-art methods on a server equipped with eight NVIDIA RTX 4090 GPUs.\nFor different tasks, we employ distinct objective metrics. For generation tasks, we compute cosine similarity scores for direction (ANG Cos) and distance (DIS Cos) to quantify spatial alignment quality. Additionally, we utilize subjective MOS-Q (Mean Opinion Score for Quality) to evaluate the quality of generated audio and MOS-P (Mean Opinion Score for Position) to assess spatial perception. For implementation details of training and evaluation metrics, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2\" title=\"Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "dis",
                    "mosp",
                    "spatialization",
                    "spatial",
                    "mosq",
                    "subjective",
                    "cos",
                    "ang",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio spatialization aims to synthesize spatially immersive soundscapes from monaural inputs and source positional information. To evaluate MRSAudio on this task, we adopt BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite>, a diffusion-based generation model that predicts binaural waveforms conditioned on monaural audio and source coordinates. Since the downstream generation tasks can be formulated as predicting interaural (binaural) representations from mono audio, we employ a single-stage training scheme for all experiments. For comparison, we include a traditional signal processing baseline (DSP), which renders binaural audio using virtual source positions simulated via room impulse responses (RIRs) and head-related transfer functions (HRTFs).\nWe adopt the following objective metrics to evaluate audio quality:\n(1) W-L2: waveform L2 distance,\n(2) A-L2: amplitude envelope L2 distance,\n(3) P-L2: phase difference L2,\n(4) STFT: multi-resolution STFT loss,\n(5) PESQ: perceptual speech quality score.\nResults for the ground truth and DSP baseline are obtained by averaging across all test sets from the four MRSAudio subsets. Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS3\" title=\"B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "truth",
                    "ground",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Spatialization &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BinauralGrad achieves strong performance across all MRSAudio subsets, surpassing the classical DSP baseline on most objective and subjective metrics. The highest performance is observed on MRSLife, likely due to the relatively limited variation in sound sources within these scenes. These results demonstrate that MRSAudio&#8217;s rich spatial annotations and diverse acoustic environments provide a solid foundation for training and evaluating spatial audio generation models.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "subjective",
                    "performance",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial TTS aims to produce high-quality speech enriched with spatial cues, thereby enhancing immersion and realism in AR/VR. Although recent advances in TTS have led to impressive improvements in speech quality, progress in spatialized speech generation remains limited due to the scarcity of spatially annotated recordings with rich, high-quality labels.\nTo evaluate the effectiveness of MRSAudio for this task, we train an end-to-end model following ISDrama <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib43\" title=\"\">2025</a>)</cite> to directly generate spatial speech from text and position data. Additionally, we compare with cascaded pipelines that combine monaural TTS models (CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib14\" title=\"\">2024</a>)</cite> and F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib10\" title=\"\">2024</a>)</cite>) with the audio spatialization module. This allows us to assess both speech generation quality and the spatial fidelity enabled by our dataset.\nFor content evaluation, we report Character Error Rate (CER) and Speaker Similarity (SIM). Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS4\" title=\"B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T3\" title=\"Table 3 &#8227; 4.2 Spatial Text to Speech &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the Mono+SP method achieves strong performance across most metrics. The CER remains low and comparable to the ground truth, indicating preserved linguistic accuracy after spatialization. A high SIM score reflects stable timbre learning. ANG Cos and DIS Cos show good spatial alignment with the ground truth, and subjective MOS scores confirm that the generated speech is both natural and spatially coherent. These results demonstrate that MRSSpeech provides high-quality, spatially annotated training data that enables effective and realistic spatial TTS.</p>\n\n",
                "matched_terms": [
                    "dis",
                    "spatialization",
                    "spatial",
                    "subjective",
                    "method",
                    "cos",
                    "truth",
                    "ground",
                    "ang",
                    "performance",
                    "monosp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial SVS aims to produce expressive, high-quality singing voices enriched with accurate spatial cues, thereby enhancing listener immersion. While traditional SVS has advanced considerably, spatial SVS remains underexplored due to the lack of high-quality, spatially annotated datasets.\nTo evaluate MRSAudio for this task, we use the MRSSing subset to train and benchmark models. We adopt the ISDrama architecture to perform end-to-end spatial singing synthesis, incorporating note-level pitch control to enhance prosody accuracy. For comparison, we use the open-source SVS models Rmssinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib19\" title=\"\">2023</a>)</cite>. The outputs are then spatialized with BinauralGrad.\nWe employ objective metrics, including Mel-Cepstral Distortion (MCD) and F0 Frame Error (FFE), to evaluate spectral and pitch similarity between predicted and ground-truth. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS5\" title=\"B.5 Spatial Singing Voice Synthesis &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "mrssing",
                    "mcd",
                    "ffe",
                    "svs",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task aims to synthesize spatially immersive music conditioned on symbolic scores. While datasets like FAIR-Play offer high-quality instrument recordings, they lack aligned sheet music, limiting controllable generation. In contrast, our MRSMusic subset includes 23 instruments with aligned scores, enabling fine-grained, score-based spatial music synthesis.\nWe benchmark three systems on MRSMusic. First, we apply BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite> to spatialize mono recordings. Second, we utilize Make-An-Audio 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib13\" title=\"\">2023</a>)</cite> to generate mono music from MIDI scores, which is subsequently spatialized. Third, we adapt ISDrama to accept both score embeddings and spatial poses, enabling end-to-end spatial symbolic music generation.\nFor objective evaluation, we compute Fr&#233;chet Audio Distance (FAD) and FFE to evaluate the results. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS6\" title=\"B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "objective",
                    "ffe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T5\" title=\"Table 5 &#8227; 4.4 Spatial Music Generation &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the Mono + SP pipeline achieves strong performance, benefiting from access to ground truth mono audio. The Make-An-Audio 2 + SP exhibits limitations in FAD and pitch accuracy. This suggests that general-purpose audio generation models may struggle to fully capture structured musical information from symbolic prompts. The ISDrama (Music) model generates music directly from the symbolic inputs and spatial cues, achieving better coherence and spatial alignment than the Make-An-Audio 2 + SP. These results highlight MRSMusic&#8217;s effectiveness in supporting spatially controllable music generation across diverse instruments and spatial conditions.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "truth",
                    "ground",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task evaluates the ability to detect and localize sound events using MRSAudio&#8217;s spatial annotations. We follow STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> using both audio-only and audio-visual variants on the MRSSound subset.\nIn the audio-only condition, models receive either FOA or binaural waveforms as input and predict sound event classes along with 3D source coordinates. For the audio-visual condition, we extract bounding boxes of visible persons to serve as coarse visual priors, which are fused with the audio representations. We also explore architectural variations by replacing the original convolutional backbone with a Transformer encoder, allowing us to assess the impact of temporal modeling capacity on spatial prediction.\nWe evaluate model performance using four standard joint detection and localization metrics, including location-aware detection (<math alttext=\"F_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">F_{20^{\\circ}}</annotation></semantics></math>, <math alttext=\"ER_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub></mrow><annotation encoding=\"application/x-tex\">ER_{20^{\\circ}}</annotation></semantics></math>) and class-aware localization (<math alttext=\"LE_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>E</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LE_{CD}</annotation></semantics></math>, <math alttext=\"LR_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LR_{CD}</annotation></semantics></math>). Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS7\" title=\"B.7 Sound Event Localization and Detection &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.7</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T6\" title=\"Table 6 &#8227; 4.5 Sound Event Localization and Detection &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, model performance varies with architecture, input modality, and audio format. Transformer-based models generally outperform ConvNet baselines, particularly in reducing localization error. FOA input consistently yields better results than binaural audio, benefiting from richer spatial representation. For example, the Transformer with FOA and no visual input achieves the lowest error rate (ER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> 0.99) and highest localization recall (LR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub> 87.32). The addition of visual features improves performance in some ConvNet settings (e.g., F<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m3\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> rises from 9.33 to 13.00 with FOA), but offers limited or inconsistent gains in Transformer models, possibly due to modality mismatch or redundancy. These results highlight the value of MRSAudio&#8217;s spatial annotations and multimodal streams in supporting flexible evaluation under both unimodal and multimodal configurations.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce MRSAudio, a large-scale, multimodal spatial audio corpus designed to support a wide range of generation and understanding tasks. MRSAudio comprises four complementary modules, MRSLife, MRSSpeech , MRSSing, and MRSMusic, each captured with binaural/FOA audio, synchronized video, precise 3D pose metadata, and richly detailed annotations (event labels, transcripts, phoneme boundaries, lyrics, musical scores, and motion prompts). Through extensive benchmarks on audio spatialization, binaural speech, singing, and music generation , as well as sound event localization and detection, we demonstrate that MRSAudio&#8217;s scale, diversity, and annotation depth enable state&#8208;of&#8208;the&#8208;art performance and unlock new avenues for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "spatial",
                    "mrssing",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Directions:</span>\nWhile MRSAudio offers broad multimodal coverage, two limitations remain. First, although synchronized video is provided for all recordings, current benchmarks only explore visual input in a limited subset of tasks. Second, while the dataset includes both binaural and First-Order Ambisonic (FOA) formats, the FOA subset is smaller in scale, and most tasks focus on binaural audio, limiting spatial modeling diversity.\nFuture work will expand the role of visual modalities in tasks such as sound localization and scene understanding. We also plan to increase FOA recordings to balance data distribution and support broader spatial audio research, and develop more FOA-specific benchmarks to better utilize ambisonic spatial cues.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Societal Impact:</span>\nAs with any large-scale audiovisual dataset, MRSAudio carries potential risks if misused. It could be exploited to generate highly realistic yet synthetic spatial audio for deepfakes or disinformation in AR and VR applications. To mitigate such risks, MRSAudio is released under a noncommercial license with clear usage guidelines. We encourage responsible use in accordance with ethical standards, including consent management, data governance, and transparency.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the planning phase, we systematically analyze real-world application scenarios for spatial audio and divide them into four major categories. MRSLife focuses on daily environmental sound events, MRSSpeech targets high-quality conversational data, MRSSing captures solo vocal performances, and MRSMusic records immersive instrumental music. These four scenarios collectively cover a broad range of everyday acoustic contexts and are designed to support a wide spectrum of downstream tasks.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on predefined scenario requirements, we recruit professionals from relevant fields to participate in the recording process. To ensure spatial diversity, we rent various venues tailored to each scenario type. Following the FAIR-Play protocol, we use a 3Dio Free Space XLR binaural microphone to capture spatial audio, GoPro HERO cameras and mobile phones to record exocentric and egocentric videos, and UWB-based tracking systems to capture motion trajectories.\nOnce personnel, locations, and equipment are secured, we prepare task-specific materials for each subset. For MRSLife, we further divide the content based on the proportion of speech involved, resulting in two subcategories: MRSDialogue (e.g., group games, board games) and MRSSound (e.g., kungfu, kitchens, offices, sports). We predefine common sound events for each, such as clattering dishes, keyboard typing, and table tennis.\nFor MRSSpeech, we compile a large corpus of scripts from movies, TV shows, and crosstalk performances and automatically extract dialogue passages for speaker delivery.\nFor MRSSing, we design lyrics in four languages (Chinese, English, German, and French) and recruit singers across a range of vocal types to maximize diversity.\nFor MRSMusic, we collect solo performances across 23 traditional and modern instruments, covering a wide array of timbres and playing techniques.\nSpecific sound categories for each subset are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.T7\" title=\"Table 7 &#8227; A.1 Details of Planning &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span>\nWe employ 44 expressive speakers to read from scripted texts, each paid $30 per hour of recorded audio. The total recorded duration reaches 200 hours, with compensation totaling $40,000. During recording sessions, speakers alternate between standing and walking around the recording area to introduce spatial diversity while maintaining speech clarity. Binaural audio is captured using head-mounted microphones, and clean monaural speech is recorded using lavalier microphones. All sessions are filmed from a third-person perspective.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nEighteen professional singers participate in the recordings. Each singer is fluent in at least one of the following languages: Chinese, English, German, or French, and collectively they cover all vocal ranges including soprano, alto, tenor, and bass. Performers are paid $50 per hour, contributing a total of 80 hours of audio. Singing is recorded at a fixed position to ensure spatial consistency. Binaural recordings are captured with head-mounted microphones, and monaural audio is recorded with a studio-grade microphone. Third-person panoramic video is also captured.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a team of domain experts in singing, music performance, and linguistics to carry out and review all annotations, compensating each annotator at a rate of $15 per hour. Prior to beginning their work, every expert receives a clear explanation of how the annotations will be used and agrees to release their annotation results under an open&#8208;source license for academic research.\nFor all modules, we first synchronize each audio with its corresponding video, mono&#8208;channel reference track, and 3D positional metadata. Annotators then verify and correct this synchronization to ensure perfect alignment across modalities.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech.</span>\nGiven full dialogue scripts, we perform automatic alignment to 30-minute recordings using a chunk-based extension of WhisperX. This method divides each utterance-level audio segment into fixed-length chunks (e.g., 30 seconds), applies phoneme-level models (e.g., wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib5\" title=\"\">2020</a>)</cite> ) for emission prediction on each chunk, and then concatenates emissions to form a complete alignment matrix.\nWe compute alignments via a trellis-based dynamic programming algorithm with backtracking, followed by scaling to restore absolute timestamps. Word- and sentence-level timings are derived by grouping aligned characters using word indices and sentence tokenization. Missing or partial timings are interpolated using a specified method (e.g., nearest).\nThis pipeline enables efficient and accurate alignment of long-form recordings on GPU. We then refine sentence boundaries in Praat and apply the same MFA-plus-Praat workflow used in MRSLife for fine-grained phoneme and word-level alignment.</p>\n\n",
                "matched_terms": [
                    "method",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing.</span>\nEach segment contains a solo vocal performance with known lyrics. We first apply voice activity detection (VAD) to segment the recordings. Lyrics are aligned to each segment using LyricFN&#8217;s ASR-based dynamic programming method. English phonemes follow the ARPABET standard,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://en.wikipedia.org/wiki/ARPABET\" title=\"\">https://en.wikipedia.org/wiki/ARPABET</a></span></span></span> while German and French use MFA&#8217;s phoneme sets.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mfa-models.readthedocs.io/en/latest/dictionary/\" title=\"\">https://mfa-models.readthedocs.io/en/latest/dictionary/</a></span></span></span> We then apply MFA for initial alignment and refine it manually in Praat to obtain precise word and phoneme boundaries. Annotators assign fine-grained style tags, including emotion, genre, and tempo. To generate score annotations, we extract F0 contours using RMVPE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib38\" title=\"\">2023</a>)</cite> and convert them into MIDI format via ROSVOT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib27\" title=\"\">2024</a>)</cite>, followed by expert correction.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "method",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation:</span>\nAfter annotation, we segment the raw recordings into shorter clips to support spatial audio generation and analysis tasks. For speech and singing, we use alignment timestamps to extract utterances. For MRSSound, each recording is divided into fixed 10-second segments.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Synchronization:</span>\nIn addition to binaural audio, each clip is synchronized with the following modalities: audio, text, video, and position data. Using the segment timestamps, we align all streams temporally. For static sources, we attach manually recorded 3D coordinates; for dynamic sources, we interpolate Ultra-Wideband (UWB) tracking data over the segment duration.\nThis process yields fully synchronized multimodal clips ready for downstream spatial audio tasks.\nFurthermore, for segments where participants&#8217; faces are visible, we apply anonymization by using a face detection model to overlay digital masks during post-processing.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSound.</span>\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F5\" title=\"Figure 5 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(a) illustrates the spatial distribution of sound sources in MRSSound relative to the listener&#8217;s head-centered coordinate system. The listener&#8217;s facing direction is at 90 degrees, and most sound events are concentrated in the frontal hemisphere (azimuth from 0&#176; to 180&#176;), which is consistent with the first-person video capture setup. Some events also appear in the rear field (&#8211;180&#176; to 0&#176;). In elevation, the majority of sound sources are distributed between &#8211;90&#176; and 40&#176;, realistically reflecting everyday human perception in standing scenarios, where sound events typically originate below ear level. This broad spatial coverage supports the training of spatial audio models with strong generalization ability.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F7\" title=\"Figure 7 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(a) shows the 3D spatial distribution of sound sources with respect to the listener. The majority of sources are located in front of the listener, consistent with the setup of solo vocal recordings. However, the coverage also spans surrounding directions in both azimuth and elevation, ensuring spatial variability for training robust spatial audio models.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, MRSSing provides extensive diversity in spatial positioning, language, emotion, vocal range, segment duration, and pitch. This makes it a strong foundation for research on spatial singing voice synthesis and expressive vocal modeling.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct subjective evaluation of the generation tasks using Mean Opinion Score (MOS). For each task, we randomly sample 40 utterances from the test set. Each utterance is paired with a corresponding source-position prompt and is evaluated by at least five expert listeners.\nMOS-Q, Listeners rate each sample on a five-point Likert scale ranging from 1 (bad) to 5 (excellent).\nFor audio quality, we use MOS-Q, where listeners wear headphones and assess the clarity and naturalness of the generated audio.\nFor spatial perception, we use MOS-P, where listeners evaluate the realism of spatial cues and whether the perceived direction and distance of the sound source match the prompt description.\nAll participants are fairly compensated for their time at a rate of $15 per hour, resulting in a total cost of approximately $2000. Participants are informed that their evaluations will be used exclusively for academic research purposes.\nInstructions for audio evaluations are shown in Figure\n&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.F9\" title=\"Figure 9 &#8227; B.1 Subjective Evaluation Metrics &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "mosp",
                    "spatial",
                    "subjective",
                    "audio",
                    "mosq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively assess spatial audio generation and understanding across multiple tasks, we adopt a set of objective metrics that evaluate signal fidelity, spatial consistency, intelligibility, and speaker or pitch accuracy. We randomly sample 400 data points as the test set.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatial",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure waveform similarity using Wave L2, the mean squared error (MSE) between the generated and reference binaural waveforms. Amplitude L2 and Phase L2 are computed after applying Short-Time Fourier Transform (STFT), reflecting errors in the magnitude and phase components respectively. MRSTFT loss<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/csteinmetz1/auraloss\" title=\"\">https://github.com/csteinmetz1/auraloss</a></span></span></span> is also used, combining spectral convergence, log-magnitude, and linear-magnitude terms for better spectral alignment.\nIn addition, we use the perceptual evaluation metric PESQ<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/aliutkus/speechmetrics\" title=\"\">https://github.com/aliutkus/speechmetrics</a></span></span></span> to assess audio quality for speech-related tasks. Since PESQ is designed for speech, it is omitted for MRSLife and MRSMusic. For all metrics except PESQ, lower values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate spatial consistency, we use Spatial-AST<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib44\" title=\"\">2024</a>)</cite> to extract angular and distance embeddings from the binaural audio. Since Spatial-AST predicts positions only for static sources, we compute the cosine similarity between the predicted and ground truth embeddings within 1-second segments and average the results to assess overall spatial fidelity.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "truth",
                    "audio",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"m_{t}(d)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{t}(d)</annotation></semantics></math> and <math alttext=\"\\hat{m}_{t}(d)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>m</mi><mo>^</mo></mover><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{m}_{t}(d)</annotation></semantics></math> represent the <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>-th Mel-frequency cepstral coefficient (MFCC) at frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> for the ground truth and synthesized signals, respectively, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the number of MFCC dimensions.</p>\n\n",
                "matched_terms": [
                    "truth",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we adopt F0 Frame Error (FFE) to evaluate pitch accuracy by comparing extracted F0 contours between the synthesized and ground truth audio.</p>\n\n",
                "matched_terms": [
                    "ffe",
                    "ground",
                    "audio",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use Fr&#233;chet Audio Distance (FAD)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib23\" title=\"\">2018</a>)</cite> to assess perceptual similarity between the feature distributions of generated and reference audio. In addition, F0 Frame Error (FFE) is used to evaluate pitch accuracy by comparing the extracted F0 contours against the reference musical scores.</p>\n\n",
                "matched_terms": [
                    "ffe",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build upon BinauralGrad&#8217;s two-stage diffusion-based framework to convert monaural audio into spatial audio. However, in our subsequent generation experiments, the synthesized monaural audio typically corresponds to a centrally positioned source between the ears, effectively serving as the first stage of BinauralGrad. Therefore, for the spatialization experiments presented here, we use only the second stage of BinauralGrad to validate spatial audio generation.\nSpecifically, we first convert binaural recordings to monaural input by averaging the two channels. Next, we apply a DSP-based method to produce a coarse spatial approximation of the binaural signal. This monaural input and its simulated binaural counterpart are then used as input to the BinauralGrad model, which is conditioned on the object&#8217;s motion trajectory to generate spatialized binaural audio.\nWe list the architecture and hyperparameters of BinauralGrad in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T8\" title=\"Table 8 &#8227; B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "method",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune two pre-trained models, CosyVoice and F5-TTS, using monaural audio obtained by averaging the binaural recordings from the MRSSpeech subset. This allows the models to learn the generation characteristics specific to our dataset. After monaural generation, we apply a pre-trained spatialization model trained on MRSSpeech to convert the output into spatialized binaural audio. For the ISDrama baseline, we adopt the model variant proposed in the original paper that conditions generation on predefined spatial paths to produce binaural spatial speech directly.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ISDrama baseline, we adopt the spatial-path-conditioned generation framework and extend it by incorporating Rmssinger&#8217;s note encoder, allowing explicit pitch control through musical score input.\nWe use the single stage variation of Rmssinger, a standard singing voice synthesis model, and train it with monaural targets derived from the averaged binaural recordings in the MRSSing subset. This enables the model to generate pitch-accurate audio that reflects the acoustic characteristics. The synthesized monaural audio is then spatialized using the spatialization model trained on MRSSing.\nWe list the architecture and hyperparameters of Rmssinger in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T9\" title=\"Table 9 &#8227; B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatialization",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For spatial music generation, in the ISDrama setting, we follow the path-conditioned generation pipeline but remove the phoneme encoder, using only the note encoder to process symbolic scores as the sole control condition for generating spatialized music.\nWe also adopt Make-An-Audio 2 as our base model and train it using monaural audio derived from averaged MRSMusic binaural recordings. We use the pretrained spectrogram autoencoder. Instrument category and symbolic score information are concatenated into the text prompt to guide the music generation process. The generated audio is subsequently spatialized using the MRSMusic-trained spatialization model. We list the hyper-parameters of Make-An-Audio 2 in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T10\" title=\"Table 10 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the STARSS23 framework for FOA-based sound event localization and detection, training on the event segments from the MRSSound subset under both audio-only and audio-visual conditions. To enable binaural input, we modify the model architecture to use three input channels and extract interaural phase difference features. This allows us to adapt the SELD model to binaural audio. Additionally, we experiment with replacing convolutional layers with Transformer encoders to explore the effect of different architectures on sound event localization and detection performance.\nWe list the hyper-parameters of SELD in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T11\" title=\"Table 11 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            }
        ]
    },
    "S4.T5": {
        "source_file": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations",
        "caption": "Table 5: Spatial MG Performance on MRSMusic. “SP” denotes the Audio Spatialization.",
        "body": "Method\nObjective\nSubjective\n\n\n\nFAD ↓\\downarrow\n\n\nFFE ↓\\downarrow\n\n\nANG Cos ↑\\uparrow\n\n\nDIS Cos ↑\\uparrow\n\n\nMOS-Q ↑\\uparrow\n\n\nMOS-P ↑\\uparrow\n\n\n\nGT (Ground Truth)\n–\n–\n–\n–\n\n4.49 ±\\pm 0.09\n\n\n4.34 ±\\pm 0.11\n\n\n\nMono+SP\n2.88\n0.14\n0.48\n0.74\n\n3.87 ±\\pm 0.07\n\n3.93 ±\\pm 0.09\n\n\nMake-An-Audio 2+SP\n4.39\n0.49\n0.49\n0.41\n\n3.74 ±\\pm 0.10\n\n\n3.73 ±\\pm 0.13\n\n\n\nISDrama(music)\n2.45\n0.21\n0.53\n0.68\n3.89 ±\\pm 0.12\n\n3.88 ±\\pm 0.10",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Objective</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FAD </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FFE </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ANG Cos </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DIS Cos </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MOS-Q </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MOS-P </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">GT (Ground Truth)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.49 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.34 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.11</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Mono+SP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.87 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.07</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.93 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.09</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Make-An-Audio 2+SP</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.39</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.41</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.74 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.10</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.73 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.13</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ISDrama(music)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.89 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.88 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.10</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "dis",
            "mosp",
            "subjective",
            "↓downarrow",
            "makeanaudio",
            "±pm",
            "ground",
            "objective",
            "monosp",
            "audio",
            "spatialization",
            "cos",
            "spatial",
            "ffe",
            "fad",
            "denotes",
            "truth",
            "mrsmusic",
            "↑uparrow",
            "performance",
            "2sp",
            "isdramamusic",
            "method",
            "“sp”",
            "ang",
            "mosq"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T5\" title=\"Table 5 &#8227; 4.4 Spatial Music Generation &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the Mono + SP pipeline achieves strong performance, benefiting from access to ground truth mono audio. The Make-An-Audio 2 + SP exhibits limitations in FAD and pitch accuracy. This suggests that general-purpose audio generation models may struggle to fully capture structured musical information from symbolic prompts. The ISDrama (Music) model generates music directly from the symbolic inputs and spatial cues, achieving better coherence and spatial alignment than the Make-An-Audio 2 + SP. These results highlight MRSMusic&#8217;s effectiveness in supporting spatially controllable music generation across diverse instruments and spatial conditions.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive spatial environments, where auditory cues enable sound source localization in three-dimensional space.\nDespite the critical role of spatial audio in immersive technologies such as VR/AR, most existing multimodal datasets provide only monaural audio, which limits the development of spatial audio generation and understanding.\nTo address these challenges, we introduce MRSAudio, a large-scale multimodal spatial audio dataset designed to advance research in spatial audio understanding and generation.\nMRSAudio spans four distinct components: MRSLife, MRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios.\nThe dataset includes synchronized binaural and ambisonic audio, exocentric and egocentric video, motion trajectories, and fine-grained annotations such as transcripts, phoneme boundaries, lyrics, scores, and prompts.\nTo demonstrate the utility and versatility of MRSAudio, we establish five foundational tasks: audio spatialization, and spatial text to speech, spatial singing voice synthesis, spatial music generation and sound event localization and detection.\nResults show that MRSAudio enables high-quality spatial modeling and supports a broad range of spatial audio research.\nDemos and dataset access are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mrsaudio.github.io\" title=\"\">https://mrsaudio.github.io</a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "spatialization",
                    "audio",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive and interpret physical environments. With the rapid growth of film, virtual reality (VR), augmented reality (AR), and gaming applications, users increasingly expect not only precise audiovisual alignment but also highly immersive experiences. While recent advances in deep learning have enabled realistic generation of speech, music, and sound effects synchronized with text or video <cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib25\" title=\"\">2022</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib41\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib37\" title=\"\">2024</a>)</cite>, most models focus on monaural audio and neglect spatialized soundscapes that enhance immersion. The human binaural system uses interaural time differences (ITD) and interaural level differences (ILD) to localize sound in three-dimensional space, and spatial audio must remain consistent with visual cues <cite class=\"ltx_cite ltx_citemacro_citep\">(Yost, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib42\" title=\"\">1998</a>; Cohen &amp; Knudsen, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib12\" title=\"\">1999</a>; Grothe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib18\" title=\"\">2010</a>)</cite>. Any mismatch can disrupt immersion and weaken the sense of presence. For example, hearing a cat&#8217;s meow from the left immediately suggests its location, even if it is just off-screen.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the growing importance of spatial audio in these immersive technologies<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib39\" title=\"\">2020</a>; Huiyu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib22\" title=\"\">2025</a>)</cite>, progress in machine learning for spatial audio understanding is limited by the fundamental spatial data constraints. Most existing audio datasets<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib17\" title=\"\">2017</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib9\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib3\" title=\"\">2023</a>)</cite> focus on monaural recordings, which discard vital spatial information, effectively \"flattening\" the soundscape and preventing models from learning key physical phenomena such as room reverberation, echo patterns, and sound propagation. Moreover, the scarcity of multimodal datasets that align spatial audio with synchronized visual, position geometric, and semantic annotations hinders the development of advanced auditory scene-analysis systems capable of human-like spatial perception.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these gaps, we present <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a 484-hour large-scale multimodal spatial audio dataset designed to support both spatial audio understanding and generation. It integrates high-fidelity spatial recordings with synchronized video, 3D pose tracking, and rich semantic annotations, enabling comprehensive modeling of real-world auditory scenes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S0.F1\" title=\"Figure 1 &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the dataset comprises four subsets, each targeting distinct tasks and scenarios.\n<span class=\"ltx_text ltx_font_bold\">MRSLife</span> (129 h) captures daily activities such as board games, cooking, and office work, using egocentric video and FOA audio annotated with sound events and speech transcripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSpeech</span> (206 h) includes binaural conversations from 44 speakers across diverse indoor environments, paired with video, 3D source positions, and complete scripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSing</span> (80 h) features high-quality solo singing performances in Chinese, English, German, and French by 20 vocalists, each aligned with time-stamped lyrics and corresponding musical scores.\n<span class=\"ltx_text ltx_font_bold\">MRSMusic</span> (69 h) offers spatial recordings of 23 Traditional Chinese, Western and Electronic instruments, with symbolic score annotations that support learning-based methods for symbolic-to-audio generation and fine-grained localization.\nTogether, these four subsets support a broad spectrum of spatial audio research problems, including event detection, sound localization, and binaural or ambisonic audio generation. By pairing spatial audio with synchronized exocentric and egocentric video, geometric tracking, and detailed semantic labels, MRSAudio enables new research directions in multimodal spatial understanding and cross-modal generation. Throughout this paper, unless stated otherwise, we use the term spatial audio to refer to binaural audio.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a 484-hour, large-scale multimodal spatial audio dataset explicitly designed to push the boundaries of spatial audio understanding and generative modeling.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assemble synchronized binaural and ambisonic recordings with exocentric and egocentric video, geometric source positions, transcripts, scores, lyrics, and event labels, providing one of the most richly annotated multimodal resources for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish and release evaluation protocols and baseline implementations for five benchmark tasks: audio spatialization generation, spatial text-to-speech, spatial singing voice synthesis, spatial music generation, and localization and detection of sound events, in order to demonstrate MRSAudio&#8217;s versatility and to foster reproducible research.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2\" title=\"2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reviews existing spatial audio datasets.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3\" title=\"3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> describes the design, collection process, and key statistics of MRSAudio.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4\" title=\"4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents extensive benchmark experiments using state-of-the-art methods on five core spatial audio tasks: audio spatialization, spatial text-to-speech, spatial singing voice generation, spatial music synthesis, and sound event localization and detection.\nFinally, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S5\" title=\"5 Conclusion and Discussion &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> concludes the paper and discusses the limitations and potential risks associated with MRSAudio.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning has driven remarkable progress in both audio generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib20\" title=\"\">2023a</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib41\" title=\"\">2023</a>; Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib25\" title=\"\">2022</a>)</cite> and audio understanding <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib11\" title=\"\">2023</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib21\" title=\"\">2023b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib35\" title=\"\">2024</a>)</cite> tasks. However, the majority of these advances still rely heavily on monaural audio, which lacks the ability to represent or capture the rich spatial cues that naturally occur in real-world environments.\nThe rapid adoption of VR/AR technologies has concurrently driven growing demand for immersive spatial audio experiences. Researchers have focused on several key technologies including: sound event localization and detection <cite class=\"ltx_cite ltx_citemacro_citep\">(Adavanne et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib2\" title=\"\">2019</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib36\" title=\"\">2022</a>)</cite>, mono-to-spatial audio conversion <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao &amp; Grauman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib15\" title=\"\">2019</a>; Pedro&#160;Morgado &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib31\" title=\"\">2018</a>)</cite>, and end-to-end spatial audio generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite recent progress, spatial audio generation and understanding remain constrained by the paucity of high-quality datasets. Due to the difficulty and expense of collecting and annotating high-quality spatial audio datasets, most open-source datasets still primarily consist of simulated or web-crawled content.\nSimulated datasets offer precise annotations but lack perceptual realism, while crawled datasets often provide real-world diversity but are missing critical labels, such as listener and source positions and content-level annotations, thus limiting their utility.\nFor instance, spatial symbolic music generation demands accurate musical scores and corresponding positional metadata.\nTo better understand the current landscape, we survey existing spatial audio datasets. These datasets differ in terms of audio format, including First-Order Ambisonics (FOA), multi-channel arrays, and binaural microphones, as well as in their collection methods (simulated, crawled, recorded) and annotation.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, existing datasets vary in their goals and modalities. For instance, Spatial LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Sarabia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib32\" title=\"\">2023</a>)</cite> simulates spatial speech using the LibriSpeech corpus and is mainly intended for binaural TTS applications. RealMAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib40\" title=\"\">2024</a>)</cite> is a large-scale real-recorded and annotated 32-channel microphone array dataset for multichannel speech enhancement and source localization. YT-Ambigen <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>)</cite> and Sphere360 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite> are derived from web-crawled video datasets, but lack explicit spatial annotations, making them suitable primarily for video-to-spatial audio generation. BinauralMusic focuses on musical content, offering instrument class tags. BEWO-1M <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>)</cite> combines crawled content and simulated binaural rendering, and provides image or GPT-generated prompts. STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> features real-world FOA and multi-channel audio alongside synchronized videos and includes sound class and position labels.\nIn contrast to prior datasets, MRSAudio offers a comprehensive, large-scale, and real-world spatial audio corpus, featuring 484 hours of recorded data in both FOA and binaural formats, covering all audio domains including general audio, speech, singing, and music. Uniquely, MRSAudio includes synchronized audio, video, and 3D positional geometry, along with fine-grained cross-modal annotations, such as: transcripts, word and phoneme boundaries and music scores.\nThese features make MRSAudio an ideal resource for a broad range of spatial audio generation and understanding tasks, including spatial speech, music, and singing voice synthesis.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a freely available multimodal spatial audio corpus with synchronized video, positional data, and fine-grained annotations, released under the CC BY-NC-SA 4.0 license. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F2\" title=\"Figure 2 &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our data processing pipeline, with detailed descriptions in subsequent subsections. We then summarize key statistics that demonstrate MRSAudio&#8217;s scale and diversity.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> MRSMusic captures immersive instrumental performances suitable for spatial music generation and analysis.\nWe record solo performances from 45 professional musicians across 23 instruments, including Traditional Chinese, Western and Electronic instruments.\nEach performance is paired with its corresponding musical score to support score-based music generation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "spatial",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span> To introduce spatial variability, we select four recording rooms that differ in size and acoustic material. Each speaker reads inflected emotions from the scripts while walking through the space, producing dynamic spatial cues. Recordings include spoken passages, binaural audio and exocentric video, as well as a clean mono audio by a lavalier microphone placed near the speaker.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> We record 23 Traditional Chinese, Western and Electronic instruments, performed by 45\nprofessional musicians.To capture rich spatial detail, we vary microphone placement around the instrument. Recordings include binaural audio, monaural audio, exocentric video, and synchronized video that records playing gestures. Full recording details are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS2\" title=\"A.2 Details of Recording &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span>\nWe use Audio Slicer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/flutydeer/audio-slicer\" title=\"\">https://github.com/flutydeer/audio-slicer</a></span></span></span> to segment the music recordings and generate initial symbolic annotations with basic-pitch <cite class=\"ltx_cite ltx_citemacro_citep\">(Bittner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib7\" title=\"\">2022</a>)</cite>, and then employ professional musicians to verify and adjust note onsets, offsets, and dynamics to match the performance accurately.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To illustrate spatial diversity, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) shows the 3D distribution of sound source positions relative to the listener. The heatmap reveals near-uniform azimuthal coverage, with greater density in the frontal hemisphere due to the prevalence of egocentric video recordings. Elevation angles are concentrated between &#8211;40&#176; and 40&#176;, aligning with typical human sound perception patterns.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b) summarizes the annotations using a keyword cloud that captures the range of real-world activities recorded. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c) presents the proportions of recording spaces by room size: medium-sized rooms are the most common, accounting for approximately 40% of sessions, while small and large rooms each represent around 30%. Recording duration is evenly distributed across room types, ensuring diverse acoustic coverage.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(d) shows the distribution of segment durations after automatic and manual segmentation. Most audio clips are shorter than 10 seconds, which is suitable for modeling short-duration events and supports efficient downstream training and inference.\nOverall, MRSAudio offers comprehensive coverage across spatial positions, acoustic environments, scene types, and temporal structures, making it well-suited for a wide range of spatial audio generation and understanding tasks. A more detailed breakdown of per-scenario statistics is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1\" title=\"Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the quality and utility of MRSAudio in real-world scenarios, we evaluate it on five representative spatial audio tasks: (i) audio spatialization, (ii) spatial text-to-speech, (iii) spatial singing voice synthesis, (iv) spatial music generation, and (v) sound event localization and detection (SELD). These tasks cover both generation and understanding, and are critical for applications such as AR/VR, spatial media production, and perceptual scene analysis.\nAll experiments are conducted using state-of-the-art methods on a server equipped with eight NVIDIA RTX 4090 GPUs.\nFor different tasks, we employ distinct objective metrics. For generation tasks, we compute cosine similarity scores for direction (ANG Cos) and distance (DIS Cos) to quantify spatial alignment quality. Additionally, we utilize subjective MOS-Q (Mean Opinion Score for Quality) to evaluate the quality of generated audio and MOS-P (Mean Opinion Score for Position) to assess spatial perception. For implementation details of training and evaluation metrics, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2\" title=\"Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "dis",
                    "mosp",
                    "spatialization",
                    "spatial",
                    "mosq",
                    "subjective",
                    "cos",
                    "ang",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio spatialization aims to synthesize spatially immersive soundscapes from monaural inputs and source positional information. To evaluate MRSAudio on this task, we adopt BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite>, a diffusion-based generation model that predicts binaural waveforms conditioned on monaural audio and source coordinates. Since the downstream generation tasks can be formulated as predicting interaural (binaural) representations from mono audio, we employ a single-stage training scheme for all experiments. For comparison, we include a traditional signal processing baseline (DSP), which renders binaural audio using virtual source positions simulated via room impulse responses (RIRs) and head-related transfer functions (HRTFs).\nWe adopt the following objective metrics to evaluate audio quality:\n(1) W-L2: waveform L2 distance,\n(2) A-L2: amplitude envelope L2 distance,\n(3) P-L2: phase difference L2,\n(4) STFT: multi-resolution STFT loss,\n(5) PESQ: perceptual speech quality score.\nResults for the ground truth and DSP baseline are obtained by averaging across all test sets from the four MRSAudio subsets. Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS3\" title=\"B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "truth",
                    "ground",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Spatialization &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BinauralGrad achieves strong performance across all MRSAudio subsets, surpassing the classical DSP baseline on most objective and subjective metrics. The highest performance is observed on MRSLife, likely due to the relatively limited variation in sound sources within these scenes. These results demonstrate that MRSAudio&#8217;s rich spatial annotations and diverse acoustic environments provide a solid foundation for training and evaluating spatial audio generation models.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "subjective",
                    "performance",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial TTS aims to produce high-quality speech enriched with spatial cues, thereby enhancing immersion and realism in AR/VR. Although recent advances in TTS have led to impressive improvements in speech quality, progress in spatialized speech generation remains limited due to the scarcity of spatially annotated recordings with rich, high-quality labels.\nTo evaluate the effectiveness of MRSAudio for this task, we train an end-to-end model following ISDrama <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib43\" title=\"\">2025</a>)</cite> to directly generate spatial speech from text and position data. Additionally, we compare with cascaded pipelines that combine monaural TTS models (CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib14\" title=\"\">2024</a>)</cite> and F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib10\" title=\"\">2024</a>)</cite>) with the audio spatialization module. This allows us to assess both speech generation quality and the spatial fidelity enabled by our dataset.\nFor content evaluation, we report Character Error Rate (CER) and Speaker Similarity (SIM). Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS4\" title=\"B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T3\" title=\"Table 3 &#8227; 4.2 Spatial Text to Speech &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the Mono+SP method achieves strong performance across most metrics. The CER remains low and comparable to the ground truth, indicating preserved linguistic accuracy after spatialization. A high SIM score reflects stable timbre learning. ANG Cos and DIS Cos show good spatial alignment with the ground truth, and subjective MOS scores confirm that the generated speech is both natural and spatially coherent. These results demonstrate that MRSSpeech provides high-quality, spatially annotated training data that enables effective and realistic spatial TTS.</p>\n\n",
                "matched_terms": [
                    "dis",
                    "spatialization",
                    "spatial",
                    "subjective",
                    "method",
                    "cos",
                    "truth",
                    "ground",
                    "ang",
                    "performance",
                    "monosp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial SVS aims to produce expressive, high-quality singing voices enriched with accurate spatial cues, thereby enhancing listener immersion. While traditional SVS has advanced considerably, spatial SVS remains underexplored due to the lack of high-quality, spatially annotated datasets.\nTo evaluate MRSAudio for this task, we use the MRSSing subset to train and benchmark models. We adopt the ISDrama architecture to perform end-to-end spatial singing synthesis, incorporating note-level pitch control to enhance prosody accuracy. For comparison, we use the open-source SVS models Rmssinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib19\" title=\"\">2023</a>)</cite>. The outputs are then spatialized with BinauralGrad.\nWe employ objective metrics, including Mel-Cepstral Distortion (MCD) and F0 Frame Error (FFE), to evaluate spectral and pitch similarity between predicted and ground-truth. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS5\" title=\"B.5 Spatial Singing Voice Synthesis &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "ffe",
                    "spatial",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T4\" title=\"Table 4 &#8227; 4.3 Spatial Singing Voice Synthesis &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the Mono + SP approach trained on MRSSing with pitch control achieves the best performance across most metrics. Its low MCD indicates strong spectral fidelity, and high ANG Cos and DIS Cos scores demonstrate effective spatial alignment. Subjective MOS results confirm that the generated singing voices are natural, high-quality, and spatially coherent.\nThese findings validate MRSSing as an effective resource for spatial singing voice generation.</p>\n\n",
                "matched_terms": [
                    "dis",
                    "spatial",
                    "subjective",
                    "cos",
                    "ang",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task aims to synthesize spatially immersive music conditioned on symbolic scores. While datasets like FAIR-Play offer high-quality instrument recordings, they lack aligned sheet music, limiting controllable generation. In contrast, our MRSMusic subset includes 23 instruments with aligned scores, enabling fine-grained, score-based spatial music synthesis.\nWe benchmark three systems on MRSMusic. First, we apply BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite> to spatialize mono recordings. Second, we utilize Make-An-Audio 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib13\" title=\"\">2023</a>)</cite> to generate mono music from MIDI scores, which is subsequently spatialized. Third, we adapt ISDrama to accept both score embeddings and spatial poses, enabling end-to-end spatial symbolic music generation.\nFor objective evaluation, we compute Fr&#233;chet Audio Distance (FAD) and FFE to evaluate the results. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS6\" title=\"B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "ffe",
                    "fad",
                    "makeanaudio",
                    "mrsmusic",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task evaluates the ability to detect and localize sound events using MRSAudio&#8217;s spatial annotations. We follow STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> using both audio-only and audio-visual variants on the MRSSound subset.\nIn the audio-only condition, models receive either FOA or binaural waveforms as input and predict sound event classes along with 3D source coordinates. For the audio-visual condition, we extract bounding boxes of visible persons to serve as coarse visual priors, which are fused with the audio representations. We also explore architectural variations by replacing the original convolutional backbone with a Transformer encoder, allowing us to assess the impact of temporal modeling capacity on spatial prediction.\nWe evaluate model performance using four standard joint detection and localization metrics, including location-aware detection (<math alttext=\"F_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">F_{20^{\\circ}}</annotation></semantics></math>, <math alttext=\"ER_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub></mrow><annotation encoding=\"application/x-tex\">ER_{20^{\\circ}}</annotation></semantics></math>) and class-aware localization (<math alttext=\"LE_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>E</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LE_{CD}</annotation></semantics></math>, <math alttext=\"LR_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LR_{CD}</annotation></semantics></math>). Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS7\" title=\"B.7 Sound Event Localization and Detection &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.7</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T6\" title=\"Table 6 &#8227; 4.5 Sound Event Localization and Detection &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, model performance varies with architecture, input modality, and audio format. Transformer-based models generally outperform ConvNet baselines, particularly in reducing localization error. FOA input consistently yields better results than binaural audio, benefiting from richer spatial representation. For example, the Transformer with FOA and no visual input achieves the lowest error rate (ER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> 0.99) and highest localization recall (LR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub> 87.32). The addition of visual features improves performance in some ConvNet settings (e.g., F<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m3\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> rises from 9.33 to 13.00 with FOA), but offers limited or inconsistent gains in Transformer models, possibly due to modality mismatch or redundancy. These results highlight the value of MRSAudio&#8217;s spatial annotations and multimodal streams in supporting flexible evaluation under both unimodal and multimodal configurations.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce MRSAudio, a large-scale, multimodal spatial audio corpus designed to support a wide range of generation and understanding tasks. MRSAudio comprises four complementary modules, MRSLife, MRSSpeech , MRSSing, and MRSMusic, each captured with binaural/FOA audio, synchronized video, precise 3D pose metadata, and richly detailed annotations (event labels, transcripts, phoneme boundaries, lyrics, musical scores, and motion prompts). Through extensive benchmarks on audio spatialization, binaural speech, singing, and music generation , as well as sound event localization and detection, we demonstrate that MRSAudio&#8217;s scale, diversity, and annotation depth enable state&#8208;of&#8208;the&#8208;art performance and unlock new avenues for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "spatial",
                    "mrsmusic",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Directions:</span>\nWhile MRSAudio offers broad multimodal coverage, two limitations remain. First, although synchronized video is provided for all recordings, current benchmarks only explore visual input in a limited subset of tasks. Second, while the dataset includes both binaural and First-Order Ambisonic (FOA) formats, the FOA subset is smaller in scale, and most tasks focus on binaural audio, limiting spatial modeling diversity.\nFuture work will expand the role of visual modalities in tasks such as sound localization and scene understanding. We also plan to increase FOA recordings to balance data distribution and support broader spatial audio research, and develop more FOA-specific benchmarks to better utilize ambisonic spatial cues.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Societal Impact:</span>\nAs with any large-scale audiovisual dataset, MRSAudio carries potential risks if misused. It could be exploited to generate highly realistic yet synthetic spatial audio for deepfakes or disinformation in AR and VR applications. To mitigate such risks, MRSAudio is released under a noncommercial license with clear usage guidelines. We encourage responsible use in accordance with ethical standards, including consent management, data governance, and transparency.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the planning phase, we systematically analyze real-world application scenarios for spatial audio and divide them into four major categories. MRSLife focuses on daily environmental sound events, MRSSpeech targets high-quality conversational data, MRSSing captures solo vocal performances, and MRSMusic records immersive instrumental music. These four scenarios collectively cover a broad range of everyday acoustic contexts and are designed to support a wide spectrum of downstream tasks.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on predefined scenario requirements, we recruit professionals from relevant fields to participate in the recording process. To ensure spatial diversity, we rent various venues tailored to each scenario type. Following the FAIR-Play protocol, we use a 3Dio Free Space XLR binaural microphone to capture spatial audio, GoPro HERO cameras and mobile phones to record exocentric and egocentric videos, and UWB-based tracking systems to capture motion trajectories.\nOnce personnel, locations, and equipment are secured, we prepare task-specific materials for each subset. For MRSLife, we further divide the content based on the proportion of speech involved, resulting in two subcategories: MRSDialogue (e.g., group games, board games) and MRSSound (e.g., kungfu, kitchens, offices, sports). We predefine common sound events for each, such as clattering dishes, keyboard typing, and table tennis.\nFor MRSSpeech, we compile a large corpus of scripts from movies, TV shows, and crosstalk performances and automatically extract dialogue passages for speaker delivery.\nFor MRSSing, we design lyrics in four languages (Chinese, English, German, and French) and recruit singers across a range of vocal types to maximize diversity.\nFor MRSMusic, we collect solo performances across 23 traditional and modern instruments, covering a wide array of timbres and playing techniques.\nSpecific sound categories for each subset are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.T7\" title=\"Table 7 &#8227; A.1 Details of Planning &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span>\nWe employ 44 expressive speakers to read from scripted texts, each paid $30 per hour of recorded audio. The total recorded duration reaches 200 hours, with compensation totaling $40,000. During recording sessions, speakers alternate between standing and walking around the recording area to introduce spatial diversity while maintaining speech clarity. Binaural audio is captured using head-mounted microphones, and clean monaural speech is recorded using lavalier microphones. All sessions are filmed from a third-person perspective.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nEighteen professional singers participate in the recordings. Each singer is fluent in at least one of the following languages: Chinese, English, German, or French, and collectively they cover all vocal ranges including soprano, alto, tenor, and bass. Performers are paid $50 per hour, contributing a total of 80 hours of audio. Singing is recorded at a fixed position to ensure spatial consistency. Binaural recordings are captured with head-mounted microphones, and monaural audio is recorded with a studio-grade microphone. Third-person panoramic video is also captured.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span>\nWe engage 45 instrumentalists performing 23 Traditional Chinese, Western and Electronic instruments such as violin, erhu, pipa, electric guitar, and keyboard. Each performer receives $60 per recorded hour. A total of 69 hours of solo music performances are recorded. Audio is captured using both head-mounted binaural microphones and reference monaural microphones. Third-person video provides a full-scene view, while first-person cameras are used to capture detailed playing gestures.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a team of domain experts in singing, music performance, and linguistics to carry out and review all annotations, compensating each annotator at a rate of $15 per hour. Prior to beginning their work, every expert receives a clear explanation of how the annotations will be used and agrees to release their annotation results under an open&#8208;source license for academic research.\nFor all modules, we first synchronize each audio with its corresponding video, mono&#8208;channel reference track, and 3D positional metadata. Annotators then verify and correct this synchronization to ensure perfect alignment across modalities.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech.</span>\nGiven full dialogue scripts, we perform automatic alignment to 30-minute recordings using a chunk-based extension of WhisperX. This method divides each utterance-level audio segment into fixed-length chunks (e.g., 30 seconds), applies phoneme-level models (e.g., wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib5\" title=\"\">2020</a>)</cite> ) for emission prediction on each chunk, and then concatenates emissions to form a complete alignment matrix.\nWe compute alignments via a trellis-based dynamic programming algorithm with backtracking, followed by scaling to restore absolute timestamps. Word- and sentence-level timings are derived by grouping aligned characters using word indices and sentence tokenization. Missing or partial timings are interpolated using a specified method (e.g., nearest).\nThis pipeline enables efficient and accurate alignment of long-form recordings on GPU. We then refine sentence boundaries in Praat and apply the same MFA-plus-Praat workflow used in MRSLife for fine-grained phoneme and word-level alignment.</p>\n\n",
                "matched_terms": [
                    "method",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing.</span>\nEach segment contains a solo vocal performance with known lyrics. We first apply voice activity detection (VAD) to segment the recordings. Lyrics are aligned to each segment using LyricFN&#8217;s ASR-based dynamic programming method. English phonemes follow the ARPABET standard,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://en.wikipedia.org/wiki/ARPABET\" title=\"\">https://en.wikipedia.org/wiki/ARPABET</a></span></span></span> while German and French use MFA&#8217;s phoneme sets.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mfa-models.readthedocs.io/en/latest/dictionary/\" title=\"\">https://mfa-models.readthedocs.io/en/latest/dictionary/</a></span></span></span> We then apply MFA for initial alignment and refine it manually in Praat to obtain precise word and phoneme boundaries. Annotators assign fine-grained style tags, including emotion, genre, and tempo. To generate score annotations, we extract F0 contours using RMVPE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib38\" title=\"\">2023</a>)</cite> and convert them into MIDI format via ROSVOT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib27\" title=\"\">2024</a>)</cite>, followed by expert correction.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic.</span>\nWe segment the recordings using AutoSlicer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib29\" title=\"\">2022</a>)</cite> and generate preliminary symbolic annotations using basic-pitch&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bittner et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib7\" title=\"\">2022</a>)</cite>. Professional musicians then verify and refine the annotations, adjusting note onsets, offsets, dynamics, and articulations to ensure consistency between the score and performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation:</span>\nAfter annotation, we segment the raw recordings into shorter clips to support spatial audio generation and analysis tasks. For speech and singing, we use alignment timestamps to extract utterances. For MRSSound, each recording is divided into fixed 10-second segments.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Synchronization:</span>\nIn addition to binaural audio, each clip is synchronized with the following modalities: audio, text, video, and position data. Using the segment timestamps, we align all streams temporally. For static sources, we attach manually recorded 3D coordinates; for dynamic sources, we interpolate Ultra-Wideband (UWB) tracking data over the segment duration.\nThis process yields fully synchronized multimodal clips ready for downstream spatial audio tasks.\nFurthermore, for segments where participants&#8217; faces are visible, we apply anonymization by using a face detection model to overlay digital masks during post-processing.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Checking:</span>\nTo ensure the reliability of annotations, domain experts perform a random audit on 15% of the segmented clips across all four modules. The audit process involves the following steps:\n(1) Verifying the temporal synchronization across different modal;\n(2) Confirming that the assigned event labels accurately reflect the audiovisual content present in each clip;\n(3) Reviewing the speech segments to check the correctness of word- and phoneme-level alignments;\n(4) Evaluating singing clips for accurate alignment between lyrics and audio, consistency with musical scores, and correctness of assigned style labels;\n(5) Assessing MRSMusic excerpts to verify that musical properties, such as key, pitch, and note duration.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSound.</span>\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F5\" title=\"Figure 5 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(a) illustrates the spatial distribution of sound sources in MRSSound relative to the listener&#8217;s head-centered coordinate system. The listener&#8217;s facing direction is at 90 degrees, and most sound events are concentrated in the frontal hemisphere (azimuth from 0&#176; to 180&#176;), which is consistent with the first-person video capture setup. Some events also appear in the rear field (&#8211;180&#176; to 0&#176;). In elevation, the majority of sound sources are distributed between &#8211;90&#176; and 40&#176;, realistically reflecting everyday human perception in standing scenarios, where sound events typically originate below ear level. This broad spatial coverage supports the training of spatial audio models with strong generalization ability.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F7\" title=\"Figure 7 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(a) shows the 3D spatial distribution of sound sources with respect to the listener. The majority of sources are located in front of the listener, consistent with the setup of solo vocal recordings. However, the coverage also spans surrounding directions in both azimuth and elevation, ensuring spatial variability for training robust spatial audio models.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct subjective evaluation of the generation tasks using Mean Opinion Score (MOS). For each task, we randomly sample 40 utterances from the test set. Each utterance is paired with a corresponding source-position prompt and is evaluated by at least five expert listeners.\nMOS-Q, Listeners rate each sample on a five-point Likert scale ranging from 1 (bad) to 5 (excellent).\nFor audio quality, we use MOS-Q, where listeners wear headphones and assess the clarity and naturalness of the generated audio.\nFor spatial perception, we use MOS-P, where listeners evaluate the realism of spatial cues and whether the perceived direction and distance of the sound source match the prompt description.\nAll participants are fairly compensated for their time at a rate of $15 per hour, resulting in a total cost of approximately $2000. Participants are informed that their evaluations will be used exclusively for academic research purposes.\nInstructions for audio evaluations are shown in Figure\n&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.F9\" title=\"Figure 9 &#8227; B.1 Subjective Evaluation Metrics &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "mosp",
                    "spatial",
                    "subjective",
                    "audio",
                    "mosq"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively assess spatial audio generation and understanding across multiple tasks, we adopt a set of objective metrics that evaluate signal fidelity, spatial consistency, intelligibility, and speaker or pitch accuracy. We randomly sample 400 data points as the test set.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "spatial",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure waveform similarity using Wave L2, the mean squared error (MSE) between the generated and reference binaural waveforms. Amplitude L2 and Phase L2 are computed after applying Short-Time Fourier Transform (STFT), reflecting errors in the magnitude and phase components respectively. MRSTFT loss<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/csteinmetz1/auraloss\" title=\"\">https://github.com/csteinmetz1/auraloss</a></span></span></span> is also used, combining spectral convergence, log-magnitude, and linear-magnitude terms for better spectral alignment.\nIn addition, we use the perceptual evaluation metric PESQ<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/aliutkus/speechmetrics\" title=\"\">https://github.com/aliutkus/speechmetrics</a></span></span></span> to assess audio quality for speech-related tasks. Since PESQ is designed for speech, it is omitted for MRSLife and MRSMusic. For all metrics except PESQ, lower values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate spatial consistency, we use Spatial-AST<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib44\" title=\"\">2024</a>)</cite> to extract angular and distance embeddings from the binaural audio. Since Spatial-AST predicts positions only for static sources, we compute the cosine similarity between the predicted and ground truth embeddings within 1-second segments and average the results to assess overall spatial fidelity.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "truth",
                    "audio",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"m_{t}(d)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>m</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m_{t}(d)</annotation></semantics></math> and <math alttext=\"\\hat{m}_{t}(d)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m2\" intent=\":literal\"><semantics><mrow><msub><mover accent=\"true\"><mi>m</mi><mo>^</mo></mover><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>d</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{m}_{t}(d)</annotation></semantics></math> represent the <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m3\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>-th Mel-frequency cepstral coefficient (MFCC) at frame <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> for the ground truth and synthesized signals, respectively, and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px3.p3.m5\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the number of MFCC dimensions.</p>\n\n",
                "matched_terms": [
                    "truth",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we adopt F0 Frame Error (FFE) to evaluate pitch accuracy by comparing extracted F0 contours between the synthesized and ground truth audio.</p>\n\n",
                "matched_terms": [
                    "ffe",
                    "ground",
                    "audio",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use Fr&#233;chet Audio Distance (FAD)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib23\" title=\"\">2018</a>)</cite> to assess perceptual similarity between the feature distributions of generated and reference audio. In addition, F0 Frame Error (FFE) is used to evaluate pitch accuracy by comparing the extracted F0 contours against the reference musical scores.</p>\n\n",
                "matched_terms": [
                    "ffe",
                    "audio",
                    "fad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build upon BinauralGrad&#8217;s two-stage diffusion-based framework to convert monaural audio into spatial audio. However, in our subsequent generation experiments, the synthesized monaural audio typically corresponds to a centrally positioned source between the ears, effectively serving as the first stage of BinauralGrad. Therefore, for the spatialization experiments presented here, we use only the second stage of BinauralGrad to validate spatial audio generation.\nSpecifically, we first convert binaural recordings to monaural input by averaging the two channels. Next, we apply a DSP-based method to produce a coarse spatial approximation of the binaural signal. This monaural input and its simulated binaural counterpart are then used as input to the BinauralGrad model, which is conditioned on the object&#8217;s motion trajectory to generate spatialized binaural audio.\nWe list the architecture and hyperparameters of BinauralGrad in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T8\" title=\"Table 8 &#8227; B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "method",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune two pre-trained models, CosyVoice and F5-TTS, using monaural audio obtained by averaging the binaural recordings from the MRSSpeech subset. This allows the models to learn the generation characteristics specific to our dataset. After monaural generation, we apply a pre-trained spatialization model trained on MRSSpeech to convert the output into spatialized binaural audio. For the ISDrama baseline, we adopt the model variant proposed in the original paper that conditions generation on predefined spatial paths to produce binaural spatial speech directly.</p>\n\n",
                "matched_terms": [
                    "spatial",
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ISDrama baseline, we adopt the spatial-path-conditioned generation framework and extend it by incorporating Rmssinger&#8217;s note encoder, allowing explicit pitch control through musical score input.\nWe use the single stage variation of Rmssinger, a standard singing voice synthesis model, and train it with monaural targets derived from the averaged binaural recordings in the MRSSing subset. This enables the model to generate pitch-accurate audio that reflects the acoustic characteristics. The synthesized monaural audio is then spatialized using the spatialization model trained on MRSSing.\nWe list the architecture and hyperparameters of Rmssinger in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T9\" title=\"Table 9 &#8227; B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For spatial music generation, in the ISDrama setting, we follow the path-conditioned generation pipeline but remove the phoneme encoder, using only the note encoder to process symbolic scores as the sole control condition for generating spatialized music.\nWe also adopt Make-An-Audio 2 as our base model and train it using monaural audio derived from averaged MRSMusic binaural recordings. We use the pretrained spectrogram autoencoder. Instrument category and symbolic score information are concatenated into the text prompt to guide the music generation process. The generated audio is subsequently spatialized using the MRSMusic-trained spatialization model. We list the hyper-parameters of Make-An-Audio 2 in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T10\" title=\"Table 10 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "spatialization",
                    "spatial",
                    "makeanaudio",
                    "mrsmusic",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the STARSS23 framework for FOA-based sound event localization and detection, training on the event segments from the MRSSound subset under both audio-only and audio-visual conditions. To enable binaural input, we modify the model architecture to use three input channels and extract interaural phase difference features. This allows us to adapt the SELD model to binaural audio. Additionally, we experiment with replacing convolutional layers with Transformer encoders to explore the effect of different architectures on sound event localization and detection performance.\nWe list the hyper-parameters of SELD in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T11\" title=\"Table 11 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "audio"
                ]
            }
        ]
    },
    "S4.T6": {
        "source_file": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations",
        "caption": "Table 6: Sound Event Localization and Detection on MRSSound.",
        "body": "Model\nAudio Type\nVisual\n\nER20∘{}_{20^{\\circ}} ↓\\downarrow\n\n\nF20∘{}_{20^{\\circ}} ↑\\uparrow\n\n\nLECD ↓\\downarrow\n\n\nLRCD ↑\\uparrow\n\n\n\n\n\nConvNet\nFOA\n✓\n\n1.17 ±\\pm 0.02\n\n13.00 ±\\pm 2.72\n\n42.44 ±\\pm 8.91\n\n\n82.76 ±\\pm 5.19\n\n\n\nConvNet\nFOA\n✗\n\n1.12 ±\\pm 0.02\n\n\n9.33 ±\\pm 0.36\n\n\n46.47 ±\\pm 4.46\n\n\n85.35 ±\\pm 3.80\n\n\n\nConvNet\nBinaural\n✓\n\n1.11 ±\\pm 0.03\n\n\n5.90 ±\\pm 3.86\n\n\n41.95 ±\\pm 9.85\n\n\n45.81 ±\\pm 11.29\n\n\n\nConvNet\nBinaural\n✗\n\n1.11 ±\\pm 0.02\n\n\n6.00 ±\\pm 0.34\n\n\n45.17 ±\\pm 7.92\n\n\n70.14 ±\\pm 10.38\n\n\n\nTransformer\nFOA\n✓\n\n1.01 ±\\pm 0.01\n\n\n7.52 ±\\pm 0.42\n\n35.69 ±\\pm 7.47\n\n46.78 ±\\pm 7.62\n\n\n\nTransformer\nFOA\n✗\n0.99 ±\\pm 0.05\n\n7.95 ±\\pm 0.15\n\n\n48.76 ±\\pm 2.66\n\n87.32 ±\\pm 2.14\n\n\nTransformer\nBinaural\n✓\n\n1.01 ±\\pm 0.02\n\n\n8.47 ±\\pm 0.65\n\n\n36.18 ±\\pm 7.29\n\n\n41.33 ±\\pm 12.73\n\n\n\nTransformer\nBinaural\n✗\n\n1.11 ±\\pm 0.04\n\n\n7.37 ±\\pm 0.97\n\n\n46.74 ±\\pm 7.91\n\n\n43.87 ±\\pm 10.74",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audio Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Visual</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ER</span><math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m1\" intent=\":literal\"><semantics><msub><mi/><msup><mn mathsize=\"0.900em\">20</mn><mo mathsize=\"0.900em\">&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">F</span><math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m3\" intent=\":literal\"><semantics><msub><mi/><msup><mn mathsize=\"0.900em\">20</mn><mo mathsize=\"0.900em\">&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">LE</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CD</span></sub><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">LR</span><sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">CD</span></sub><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ConvNet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FOA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.17 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">13.00 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 2.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">42.44 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 8.91</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">82.76 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 5.19</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ConvNet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">FOA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.12 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">9.33 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.36</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">46.47 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 4.46</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">85.35 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 3.80</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ConvNet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Binaural</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.11 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.03</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">5.90 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 3.86</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">41.95 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 9.85</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">45.81 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 11.29</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ConvNet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Binaural</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.11 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">6.00 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m22\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.34</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">45.17 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m23\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 7.92</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">70.14 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m24\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 10.38</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Transformer</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">FOA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.01 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m25\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.01</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">7.52 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m26\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.42</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">35.69 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m27\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 7.47</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">46.78 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m28\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 7.62</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Transformer</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">FOA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.99 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m29\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.05</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">7.95 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m30\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.15</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">48.76 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m31\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 2.66</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">87.32 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m32\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 2.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Transformer</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Binaural</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.01 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m33\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">8.47 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m34\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.65</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">36.18 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m35\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 7.29</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">41.33 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m36\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 12.73</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Transformer</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Binaural</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">1.11 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m37\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.04</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">7.37 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m38\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.97</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">46.74 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m39\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 7.91</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">43.87 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m40\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 10.74</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "detection",
            "type",
            "lecd",
            "visual",
            "↓downarrow",
            "±pm",
            "transformer",
            "localization",
            "f20∘20circ",
            "foa",
            "event",
            "model",
            "binaural",
            "convnet",
            "↑uparrow",
            "er20∘20circ",
            "lrcd",
            "mrssound",
            "sound",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T6\" title=\"Table 6 &#8227; 4.5 Sound Event Localization and Detection &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, model performance varies with architecture, input modality, and audio format. Transformer-based models generally outperform ConvNet baselines, particularly in reducing localization error. FOA input consistently yields better results than binaural audio, benefiting from richer spatial representation. For example, the Transformer with FOA and no visual input achieves the lowest error rate (ER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> 0.99) and highest localization recall (LR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub> 87.32). The addition of visual features improves performance in some ConvNet settings (e.g., F<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m3\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> rises from 9.33 to 13.00 with FOA), but offers limited or inconsistent gains in Transformer models, possibly due to modality mismatch or redundancy. These results highlight the value of MRSAudio&#8217;s spatial annotations and multimodal streams in supporting flexible evaluation under both unimodal and multimodal configurations.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive spatial environments, where auditory cues enable sound source localization in three-dimensional space.\nDespite the critical role of spatial audio in immersive technologies such as VR/AR, most existing multimodal datasets provide only monaural audio, which limits the development of spatial audio generation and understanding.\nTo address these challenges, we introduce MRSAudio, a large-scale multimodal spatial audio dataset designed to advance research in spatial audio understanding and generation.\nMRSAudio spans four distinct components: MRSLife, MRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios.\nThe dataset includes synchronized binaural and ambisonic audio, exocentric and egocentric video, motion trajectories, and fine-grained annotations such as transcripts, phoneme boundaries, lyrics, scores, and prompts.\nTo demonstrate the utility and versatility of MRSAudio, we establish five foundational tasks: audio spatialization, and spatial text to speech, spatial singing voice synthesis, spatial music generation and sound event localization and detection.\nResults show that MRSAudio enables high-quality spatial modeling and supports a broad range of spatial audio research.\nDemos and dataset access are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mrsaudio.github.io\" title=\"\">https://mrsaudio.github.io</a>.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "binaural",
                    "localization",
                    "audio",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive and interpret physical environments. With the rapid growth of film, virtual reality (VR), augmented reality (AR), and gaming applications, users increasingly expect not only precise audiovisual alignment but also highly immersive experiences. While recent advances in deep learning have enabled realistic generation of speech, music, and sound effects synchronized with text or video <cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib25\" title=\"\">2022</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib41\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib37\" title=\"\">2024</a>)</cite>, most models focus on monaural audio and neglect spatialized soundscapes that enhance immersion. The human binaural system uses interaural time differences (ITD) and interaural level differences (ILD) to localize sound in three-dimensional space, and spatial audio must remain consistent with visual cues <cite class=\"ltx_cite ltx_citemacro_citep\">(Yost, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib42\" title=\"\">1998</a>; Cohen &amp; Knudsen, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib12\" title=\"\">1999</a>; Grothe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib18\" title=\"\">2010</a>)</cite>. Any mismatch can disrupt immersion and weaken the sense of presence. For example, hearing a cat&#8217;s meow from the left immediately suggests its location, even if it is just off-screen.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "binaural",
                    "visual",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the growing importance of spatial audio in these immersive technologies<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib39\" title=\"\">2020</a>; Huiyu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib22\" title=\"\">2025</a>)</cite>, progress in machine learning for spatial audio understanding is limited by the fundamental spatial data constraints. Most existing audio datasets<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib17\" title=\"\">2017</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib9\" title=\"\">2020</a>; Agostinelli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib3\" title=\"\">2023</a>)</cite> focus on monaural recordings, which discard vital spatial information, effectively \"flattening\" the soundscape and preventing models from learning key physical phenomena such as room reverberation, echo patterns, and sound propagation. Moreover, the scarcity of multimodal datasets that align spatial audio with synchronized visual, position geometric, and semantic annotations hinders the development of advanced auditory scene-analysis systems capable of human-like spatial perception.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "visual",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these gaps, we present <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a 484-hour large-scale multimodal spatial audio dataset designed to support both spatial audio understanding and generation. It integrates high-fidelity spatial recordings with synchronized video, 3D pose tracking, and rich semantic annotations, enabling comprehensive modeling of real-world auditory scenes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S0.F1\" title=\"Figure 1 &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the dataset comprises four subsets, each targeting distinct tasks and scenarios.\n<span class=\"ltx_text ltx_font_bold\">MRSLife</span> (129 h) captures daily activities such as board games, cooking, and office work, using egocentric video and FOA audio annotated with sound events and speech transcripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSpeech</span> (206 h) includes binaural conversations from 44 speakers across diverse indoor environments, paired with video, 3D source positions, and complete scripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSing</span> (80 h) features high-quality solo singing performances in Chinese, English, German, and French by 20 vocalists, each aligned with time-stamped lyrics and corresponding musical scores.\n<span class=\"ltx_text ltx_font_bold\">MRSMusic</span> (69 h) offers spatial recordings of 23 Traditional Chinese, Western and Electronic instruments, with symbolic score annotations that support learning-based methods for symbolic-to-audio generation and fine-grained localization.\nTogether, these four subsets support a broad spectrum of spatial audio research problems, including event detection, sound localization, and binaural or ambisonic audio generation. By pairing spatial audio with synchronized exocentric and egocentric video, geometric tracking, and detailed semantic labels, MRSAudio enables new research directions in multimodal spatial understanding and cross-modal generation. Throughout this paper, unless stated otherwise, we use the term spatial audio to refer to binaural audio.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "foa",
                    "binaural",
                    "localization",
                    "audio",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assemble synchronized binaural and ambisonic recordings with exocentric and egocentric video, geometric source positions, transcripts, scores, lyrics, and event labels, providing one of the most richly annotated multimodal resources for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish and release evaluation protocols and baseline implementations for five benchmark tasks: audio spatialization generation, spatial text-to-speech, spatial singing voice synthesis, spatial music generation, and localization and detection of sound events, in order to demonstrate MRSAudio&#8217;s versatility and to foster reproducible research.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "localization",
                    "sound",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2\" title=\"2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reviews existing spatial audio datasets.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3\" title=\"3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> describes the design, collection process, and key statistics of MRSAudio.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4\" title=\"4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents extensive benchmark experiments using state-of-the-art methods on five core spatial audio tasks: audio spatialization, spatial text-to-speech, spatial singing voice generation, spatial music synthesis, and sound event localization and detection.\nFinally, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S5\" title=\"5 Conclusion and Discussion &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> concludes the paper and discusses the limitations and potential risks associated with MRSAudio.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "localization",
                    "audio",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deep learning has driven remarkable progress in both audio generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib20\" title=\"\">2023a</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib41\" title=\"\">2023</a>; Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib25\" title=\"\">2022</a>)</cite> and audio understanding <cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib11\" title=\"\">2023</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib21\" title=\"\">2023b</a>; Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib35\" title=\"\">2024</a>)</cite> tasks. However, the majority of these advances still rely heavily on monaural audio, which lacks the ability to represent or capture the rich spatial cues that naturally occur in real-world environments.\nThe rapid adoption of VR/AR technologies has concurrently driven growing demand for immersive spatial audio experiences. Researchers have focused on several key technologies including: sound event localization and detection <cite class=\"ltx_cite ltx_citemacro_citep\">(Adavanne et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib2\" title=\"\">2019</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib36\" title=\"\">2022</a>)</cite>, mono-to-spatial audio conversion <cite class=\"ltx_cite ltx_citemacro_citep\">(Gao &amp; Grauman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib15\" title=\"\">2019</a>; Pedro&#160;Morgado &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib31\" title=\"\">2018</a>)</cite>, and end-to-end spatial audio generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>; Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>; Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "localization",
                    "audio",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite recent progress, spatial audio generation and understanding remain constrained by the paucity of high-quality datasets. Due to the difficulty and expense of collecting and annotating high-quality spatial audio datasets, most open-source datasets still primarily consist of simulated or web-crawled content.\nSimulated datasets offer precise annotations but lack perceptual realism, while crawled datasets often provide real-world diversity but are missing critical labels, such as listener and source positions and content-level annotations, thus limiting their utility.\nFor instance, spatial symbolic music generation demands accurate musical scores and corresponding positional metadata.\nTo better understand the current landscape, we survey existing spatial audio datasets. These datasets differ in terms of audio format, including First-Order Ambisonics (FOA), multi-channel arrays, and binaural microphones, as well as in their collection methods (simulated, crawled, recorded) and annotation.</p>\n\n",
                "matched_terms": [
                    "foa",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, existing datasets vary in their goals and modalities. For instance, Spatial LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Sarabia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib32\" title=\"\">2023</a>)</cite> simulates spatial speech using the LibriSpeech corpus and is mainly intended for binaural TTS applications. RealMAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib40\" title=\"\">2024</a>)</cite> is a large-scale real-recorded and annotated 32-channel microphone array dataset for multichannel speech enhancement and source localization. YT-Ambigen <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>)</cite> and Sphere360 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite> are derived from web-crawled video datasets, but lack explicit spatial annotations, making them suitable primarily for video-to-spatial audio generation. BinauralMusic focuses on musical content, offering instrument class tags. BEWO-1M <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>)</cite> combines crawled content and simulated binaural rendering, and provides image or GPT-generated prompts. STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> features real-world FOA and multi-channel audio alongside synchronized videos and includes sound class and position labels.\nIn contrast to prior datasets, MRSAudio offers a comprehensive, large-scale, and real-world spatial audio corpus, featuring 484 hours of recorded data in both FOA and binaural formats, covering all audio domains including general audio, speech, singing, and music. Uniquely, MRSAudio includes synchronized audio, video, and 3D positional geometry, along with fine-grained cross-modal annotations, such as: transcripts, word and phoneme boundaries and music scores.\nThese features make MRSAudio an ideal resource for a broad range of spatial audio generation and understanding tasks, including spatial speech, music, and singing voice synthesis.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "foa",
                    "binaural",
                    "localization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span> This subset focuses on everyday conversations and environmental sound events. Based on the degree of human vocal interaction, MRSLife is further divided into two parts: MRSDialogue, which captures unscripted conversations that naturally include spontaneous action sounds (e.g., footsteps, door movements), and MRSSound, which focuses on non-verbal sound events primarily caused by physical activities such as cooking, typing, or sports.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span> In MRSDialogue, audio is recorded using a professional binaural recording head and high-resolution sound cards, while synchronized video is captured using industry-standard cameras. In MRSSound, in addition to binaural audio and exocentric video, we also captured FOA (Zoom H3-VR) and egocentric video (Gopro camera).To ensure the effectiveness of the egocentric video, participants are asked to remain within the frontal field of view of the binaural recording head.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "foa",
                    "binaural",
                    "audio",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span> To introduce spatial variability, we select four recording rooms that differ in size and acoustic material. Each speaker reads inflected emotions from the scripts while walking through the space, producing dynamic spatial cues. Recordings include spoken passages, binaural audio and exocentric video, as well as a clean mono audio by a lavalier microphone placed near the speaker.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span> The professional singers perform according to musical scores. To introduce variation in source&#8211;listener geometry, we adjust the position of the head-mounted binaural microphone relative to the singer. In addition to the binaural audio and synchronized video, each session includes a clean vocal track recorded with a studio-grade condenser microphone.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> We record 23 Traditional Chinese, Western and Electronic instruments, performed by 45\nprofessional musicians.To capture rich spatial detail, we vary microphone placement around the instrument. Recordings include binaural audio, monaural audio, exocentric video, and synchronized video that records playing gestures. Full recording details are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS2\" title=\"A.2 Details of Recording &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span>\nFor MRSDialogue, we apply WhisperX <cite class=\"ltx_cite ltx_citemacro_citep\">(Bain et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib6\" title=\"\">2023</a>)</cite> for automatic speech recognition and speaker diarization to generate initial transcripts and speaker turns. Human annotators correct recognition errors and speaker attribution mismatches. The audio is then segmented into utterances and the transcripts are converted into phoneme sequences (using pypinyin for Mandarin). A two-stage alignment process follows: we first apply the Montreal Forced Aligner (MFA) <cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib30\" title=\"\">2017</a>)</cite> for coarse word/phoneme mapping, then manually refine boundaries in Praat <cite class=\"ltx_cite ltx_citemacro_citep\">(Boersma, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib8\" title=\"\">2001</a>)</cite>.\nFor MRSSound, we annotate sound event categories and corresponding time intervals.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sound",
                    "event",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source Localization:</span>\nFor static sources, we manually record 3D positions relative to the capture space. For dynamic scenes, we use the Ultra-Wideband (UWB) system<cite class=\"ltx_cite ltx_citemacro_citep\">(Aiello &amp; Rogerson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib4\" title=\"\">2003</a>)</cite> to track the positions of sound sources in real time. Based on the recorded position trajectories, we generate natural language motion descriptions using GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib1\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "localization",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-processing pipeline comprises three key steps to refine raw annotated data. First, segmentation splits continuous recordings into task-oriented clips: utterances for speech and singing are extracted via alignment timestamps, while MRSSound audio is uniformly divided into 10-second segments. Next, multimodal synchronization aligns each clip with auxiliary modalities (text, video, position metadata) with temporal anchors. Static sound sources are annotated with manually measured 3D coordinates, whereas dynamic sources leverage interpolated UWB tracking trajectories. For scenes where participants&#8217; faces are visible, anonymization is performed by adding half-face masks. Finally, quality assurance involves domain experts auditing 15% of clips across all modules to verify temporal alignment precision, cross-modal content consistency, and annotation accuracy.\nFull auditing protocols are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS4\" title=\"A.4 Details of Post-Processing &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>, ensuring reproducibility of this process.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sound",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To illustrate spatial diversity, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) shows the 3D distribution of sound source positions relative to the listener. The heatmap reveals near-uniform azimuthal coverage, with greater density in the frontal hemisphere due to the prevalence of egocentric video recordings. Elevation angles are concentrated between &#8211;40&#176; and 40&#176;, aligning with typical human sound perception patterns.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b) summarizes the annotations using a keyword cloud that captures the range of real-world activities recorded. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c) presents the proportions of recording spaces by room size: medium-sized rooms are the most common, accounting for approximately 40% of sessions, while small and large rooms each represent around 30%. Recording duration is evenly distributed across room types, ensuring diverse acoustic coverage.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(d) shows the distribution of segment durations after automatic and manual segmentation. Most audio clips are shorter than 10 seconds, which is suitable for modeling short-duration events and supports efficient downstream training and inference.\nOverall, MRSAudio offers comprehensive coverage across spatial positions, acoustic environments, scene types, and temporal structures, making it well-suited for a wide range of spatial audio generation and understanding tasks. A more detailed breakdown of per-scenario statistics is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1\" title=\"Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the quality and utility of MRSAudio in real-world scenarios, we evaluate it on five representative spatial audio tasks: (i) audio spatialization, (ii) spatial text-to-speech, (iii) spatial singing voice synthesis, (iv) spatial music generation, and (v) sound event localization and detection (SELD). These tasks cover both generation and understanding, and are critical for applications such as AR/VR, spatial media production, and perceptual scene analysis.\nAll experiments are conducted using state-of-the-art methods on a server equipped with eight NVIDIA RTX 4090 GPUs.\nFor different tasks, we employ distinct objective metrics. For generation tasks, we compute cosine similarity scores for direction (ANG Cos) and distance (DIS Cos) to quantify spatial alignment quality. Additionally, we utilize subjective MOS-Q (Mean Opinion Score for Quality) to evaluate the quality of generated audio and MOS-P (Mean Opinion Score for Position) to assess spatial perception. For implementation details of training and evaluation metrics, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2\" title=\"Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "localization",
                    "audio",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio spatialization aims to synthesize spatially immersive soundscapes from monaural inputs and source positional information. To evaluate MRSAudio on this task, we adopt BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite>, a diffusion-based generation model that predicts binaural waveforms conditioned on monaural audio and source coordinates. Since the downstream generation tasks can be formulated as predicting interaural (binaural) representations from mono audio, we employ a single-stage training scheme for all experiments. For comparison, we include a traditional signal processing baseline (DSP), which renders binaural audio using virtual source positions simulated via room impulse responses (RIRs) and head-related transfer functions (HRTFs).\nWe adopt the following objective metrics to evaluate audio quality:\n(1) W-L2: waveform L2 distance,\n(2) A-L2: amplitude envelope L2 distance,\n(3) P-L2: phase difference L2,\n(4) STFT: multi-resolution STFT loss,\n(5) PESQ: perceptual speech quality score.\nResults for the ground truth and DSP baseline are obtained by averaging across all test sets from the four MRSAudio subsets. Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS3\" title=\"B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Spatialization &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BinauralGrad achieves strong performance across all MRSAudio subsets, surpassing the classical DSP baseline on most objective and subjective metrics. The highest performance is observed on MRSLife, likely due to the relatively limited variation in sound sources within these scenes. These results demonstrate that MRSAudio&#8217;s rich spatial annotations and diverse acoustic environments provide a solid foundation for training and evaluating spatial audio generation models.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial TTS aims to produce high-quality speech enriched with spatial cues, thereby enhancing immersion and realism in AR/VR. Although recent advances in TTS have led to impressive improvements in speech quality, progress in spatialized speech generation remains limited due to the scarcity of spatially annotated recordings with rich, high-quality labels.\nTo evaluate the effectiveness of MRSAudio for this task, we train an end-to-end model following ISDrama <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib43\" title=\"\">2025</a>)</cite> to directly generate spatial speech from text and position data. Additionally, we compare with cascaded pipelines that combine monaural TTS models (CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib14\" title=\"\">2024</a>)</cite> and F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib10\" title=\"\">2024</a>)</cite>) with the audio spatialization module. This allows us to assess both speech generation quality and the spatial fidelity enabled by our dataset.\nFor content evaluation, we report Character Error Rate (CER) and Speaker Similarity (SIM). Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS4\" title=\"B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T5\" title=\"Table 5 &#8227; 4.4 Spatial Music Generation &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, the Mono + SP pipeline achieves strong performance, benefiting from access to ground truth mono audio. The Make-An-Audio 2 + SP exhibits limitations in FAD and pitch accuracy. This suggests that general-purpose audio generation models may struggle to fully capture structured musical information from symbolic prompts. The ISDrama (Music) model generates music directly from the symbolic inputs and spatial cues, achieving better coherence and spatial alignment than the Make-An-Audio 2 + SP. These results highlight MRSMusic&#8217;s effectiveness in supporting spatially controllable music generation across diverse instruments and spatial conditions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task evaluates the ability to detect and localize sound events using MRSAudio&#8217;s spatial annotations. We follow STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> using both audio-only and audio-visual variants on the MRSSound subset.\nIn the audio-only condition, models receive either FOA or binaural waveforms as input and predict sound event classes along with 3D source coordinates. For the audio-visual condition, we extract bounding boxes of visible persons to serve as coarse visual priors, which are fused with the audio representations. We also explore architectural variations by replacing the original convolutional backbone with a Transformer encoder, allowing us to assess the impact of temporal modeling capacity on spatial prediction.\nWe evaluate model performance using four standard joint detection and localization metrics, including location-aware detection (<math alttext=\"F_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">F_{20^{\\circ}}</annotation></semantics></math>, <math alttext=\"ER_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub></mrow><annotation encoding=\"application/x-tex\">ER_{20^{\\circ}}</annotation></semantics></math>) and class-aware localization (<math alttext=\"LE_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>E</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LE_{CD}</annotation></semantics></math>, <math alttext=\"LR_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LR_{CD}</annotation></semantics></math>). Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS7\" title=\"B.7 Sound Event Localization and Detection &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.7</span></a>.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "model",
                    "visual",
                    "sound",
                    "foa",
                    "binaural",
                    "transformer",
                    "localization",
                    "audio",
                    "event",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce MRSAudio, a large-scale, multimodal spatial audio corpus designed to support a wide range of generation and understanding tasks. MRSAudio comprises four complementary modules, MRSLife, MRSSpeech , MRSSing, and MRSMusic, each captured with binaural/FOA audio, synchronized video, precise 3D pose metadata, and richly detailed annotations (event labels, transcripts, phoneme boundaries, lyrics, musical scores, and motion prompts). Through extensive benchmarks on audio spatialization, binaural speech, singing, and music generation , as well as sound event localization and detection, we demonstrate that MRSAudio&#8217;s scale, diversity, and annotation depth enable state&#8208;of&#8208;the&#8208;art performance and unlock new avenues for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "sound",
                    "binaural",
                    "localization",
                    "audio",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Directions:</span>\nWhile MRSAudio offers broad multimodal coverage, two limitations remain. First, although synchronized video is provided for all recordings, current benchmarks only explore visual input in a limited subset of tasks. Second, while the dataset includes both binaural and First-Order Ambisonic (FOA) formats, the FOA subset is smaller in scale, and most tasks focus on binaural audio, limiting spatial modeling diversity.\nFuture work will expand the role of visual modalities in tasks such as sound localization and scene understanding. We also plan to increase FOA recordings to balance data distribution and support broader spatial audio research, and develop more FOA-specific benchmarks to better utilize ambisonic spatial cues.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "sound",
                    "foa",
                    "binaural",
                    "localization",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the planning phase, we systematically analyze real-world application scenarios for spatial audio and divide them into four major categories. MRSLife focuses on daily environmental sound events, MRSSpeech targets high-quality conversational data, MRSSing captures solo vocal performances, and MRSMusic records immersive instrumental music. These four scenarios collectively cover a broad range of everyday acoustic contexts and are designed to support a wide spectrum of downstream tasks.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on predefined scenario requirements, we recruit professionals from relevant fields to participate in the recording process. To ensure spatial diversity, we rent various venues tailored to each scenario type. Following the FAIR-Play protocol, we use a 3Dio Free Space XLR binaural microphone to capture spatial audio, GoPro HERO cameras and mobile phones to record exocentric and egocentric videos, and UWB-based tracking systems to capture motion trajectories.\nOnce personnel, locations, and equipment are secured, we prepare task-specific materials for each subset. For MRSLife, we further divide the content based on the proportion of speech involved, resulting in two subcategories: MRSDialogue (e.g., group games, board games) and MRSSound (e.g., kungfu, kitchens, offices, sports). We predefine common sound events for each, such as clattering dishes, keyboard typing, and table tennis.\nFor MRSSpeech, we compile a large corpus of scripts from movies, TV shows, and crosstalk performances and automatically extract dialogue passages for speaker delivery.\nFor MRSSing, we design lyrics in four languages (Chinese, English, German, and French) and recruit singers across a range of vocal types to maximize diversity.\nFor MRSMusic, we collect solo performances across 23 traditional and modern instruments, covering a wide array of timbres and playing techniques.\nSpecific sound categories for each subset are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.T7\" title=\"Table 7 &#8227; A.1 Details of Planning &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "type",
                    "sound",
                    "binaural",
                    "audio",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span>\nWe recruit 62 participants to perform daily activities including board games, cooking, exercise, and office work. In MRSDialogue, each participant is compensated $30 per recorded hour. Binaural audio is captured using head-mounted microphones, and third-person video is recorded. In MRSSound scenes such as kung fu or kitchen demonstrations, performers are paid $50 per hour. Both binaural and FOA (First-Order Ambisonics) recordings are collected, along with first-person and third-person video. The binaural dummy head is co-located with the egocentric camera and the Zoom H3-VR (Ambisonic recorder). The egocentric camera is rigidly aligned with the head&#8217;s gaze to capture first-person visuals, while the Ambisonic recorder is mounted 7.5 cm above the head. An exocentric camera is placed at a surveyed position in the scene to provide a third-person view of the environment and object relationships.\nParticipants receive brief scene descriptions and are asked to act naturally while maintaining a single active sound source when possible. The total duration for MRSLife recordings reaches approximately 150 hours.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "foa",
                    "binaural",
                    "audio",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span>\nWe employ 44 expressive speakers to read from scripted texts, each paid $30 per hour of recorded audio. The total recorded duration reaches 200 hours, with compensation totaling $40,000. During recording sessions, speakers alternate between standing and walking around the recording area to introduce spatial diversity while maintaining speech clarity. Binaural audio is captured using head-mounted microphones, and clean monaural speech is recorded using lavalier microphones. All sessions are filmed from a third-person perspective.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nEighteen professional singers participate in the recordings. Each singer is fluent in at least one of the following languages: Chinese, English, German, or French, and collectively they cover all vocal ranges including soprano, alto, tenor, and bass. Performers are paid $50 per hour, contributing a total of 80 hours of audio. Singing is recorded at a fixed position to ensure spatial consistency. Binaural recordings are captured with head-mounted microphones, and monaural audio is recorded with a studio-grade microphone. Third-person panoramic video is also captured.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span>\nWe engage 45 instrumentalists performing 23 Traditional Chinese, Western and Electronic instruments such as violin, erhu, pipa, electric guitar, and keyboard. Each performer receives $60 per recorded hour. A total of 69 hours of solo music performances are recorded. Audio is captured using both head-mounted binaural microphones and reference monaural microphones. Third-person video provides a full-scene view, while first-person cameras are used to capture detailed playing gestures.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife.</span>\nIn MRSDialogue scenes, we automatically generate initial transcripts and speaker clusters with WhisperX, extracting word&#8208;level timestamps and speaker IDs. Experts then load these results in Praat and assign each cluster to the correct speaker, correcting transcription errors as needed. Next, we run Montreal Forced Aligner (MFA) <cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib30\" title=\"\">2017</a>)</cite> to produce coarse phoneme&#8208;to&#8208;audio alignments (exported in TextGrid format), using pypinyin to convert Chinese text into phoneme sequences.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mozillazg/python-pinyin\" title=\"\">https://github.com/mozillazg/python-pinyin</a></span></span></span> Finally, annotators refine word and phoneme boundaries in Praat to achieve millisecond&#8208;level precision.\nIn MRSSound segments, annotators additionally label each time interval with the corresponding event category (e.g., &#8220;clattering,&#8221; &#8220;typing,&#8221; &#8220;pages turning&#8221;).</p>\n\n",
                "matched_terms": [
                    "event",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation:</span>\nAfter annotation, we segment the raw recordings into shorter clips to support spatial audio generation and analysis tasks. For speech and singing, we use alignment timestamps to extract utterances. For MRSSound, each recording is divided into fixed 10-second segments.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Synchronization:</span>\nIn addition to binaural audio, each clip is synchronized with the following modalities: audio, text, video, and position data. Using the segment timestamps, we align all streams temporally. For static sources, we attach manually recorded 3D coordinates; for dynamic sources, we interpolate Ultra-Wideband (UWB) tracking data over the segment duration.\nThis process yields fully synchronized multimodal clips ready for downstream spatial audio tasks.\nFurthermore, for segments where participants&#8217; faces are visible, we apply anonymization by using a face detection model to overlay digital masks during post-processing.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "binaural",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Checking:</span>\nTo ensure the reliability of annotations, domain experts perform a random audit on 15% of the segmented clips across all four modules. The audit process involves the following steps:\n(1) Verifying the temporal synchronization across different modal;\n(2) Confirming that the assigned event labels accurately reflect the audiovisual content present in each clip;\n(3) Reviewing the speech segments to check the correctness of word- and phoneme-level alignments;\n(4) Evaluating singing clips for accurate alignment between lyrics and audio, consistency with musical scores, and correctness of assigned style labels;\n(5) Assessing MRSMusic excerpts to verify that musical properties, such as key, pitch, and note duration.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSDialogue.</span>\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F4\" title=\"Figure 4 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a) presents the 3D spatial distribution of sound sources in MRSDialogue. In this subset, which features frequent human conversations, most sources are located around the ear-level height of the listener. The azimuthal distribution covers nearly all directions surrounding the listener, offering diverse angular data for training spatial localization models with strong generalization.</p>\n\n",
                "matched_terms": [
                    "localization",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSound.</span>\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F5\" title=\"Figure 5 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(a) illustrates the spatial distribution of sound sources in MRSSound relative to the listener&#8217;s head-centered coordinate system. The listener&#8217;s facing direction is at 90 degrees, and most sound events are concentrated in the frontal hemisphere (azimuth from 0&#176; to 180&#176;), which is consistent with the first-person video capture setup. Some events also appear in the rear field (&#8211;180&#176; to 0&#176;). In elevation, the majority of sound sources are distributed between &#8211;90&#176; and 40&#176;, realistically reflecting everyday human perception in standing scenarios, where sound events typically originate below ear level. This broad spatial coverage supports the training of spatial audio models with strong generalization ability.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sound",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F5\" title=\"Figure 5 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(b) presents the duration distribution of recorded sound events. MRSSound covers a wide variety of everyday scenarios, including cooking in kitchens, working in office environments, and sports-related activities. The diversity of event types and durations makes the subset suitable for training and evaluating models in real-world spatial sound event understanding.</p>\n\n",
                "matched_terms": [
                    "sound",
                    "event",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F7\" title=\"Figure 7 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(a) shows the 3D spatial distribution of sound sources with respect to the listener. The majority of sources are located in front of the listener, consistent with the setup of solo vocal recordings. However, the coverage also spans surrounding directions in both azimuth and elevation, ensuring spatial variability for training robust spatial audio models.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct subjective evaluation of the generation tasks using Mean Opinion Score (MOS). For each task, we randomly sample 40 utterances from the test set. Each utterance is paired with a corresponding source-position prompt and is evaluated by at least five expert listeners.\nMOS-Q, Listeners rate each sample on a five-point Likert scale ranging from 1 (bad) to 5 (excellent).\nFor audio quality, we use MOS-Q, where listeners wear headphones and assess the clarity and naturalness of the generated audio.\nFor spatial perception, we use MOS-P, where listeners evaluate the realism of spatial cues and whether the perceived direction and distance of the sound source match the prompt description.\nAll participants are fairly compensated for their time at a rate of $15 per hour, resulting in a total cost of approximately $2000. Participants are informed that their evaluations will be used exclusively for academic research purposes.\nInstructions for audio evaluations are shown in Figure\n&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.F9\" title=\"Figure 9 &#8227; B.1 Subjective Evaluation Metrics &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "sound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure waveform similarity using Wave L2, the mean squared error (MSE) between the generated and reference binaural waveforms. Amplitude L2 and Phase L2 are computed after applying Short-Time Fourier Transform (STFT), reflecting errors in the magnitude and phase components respectively. MRSTFT loss<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/csteinmetz1/auraloss\" title=\"\">https://github.com/csteinmetz1/auraloss</a></span></span></span> is also used, combining spectral convergence, log-magnitude, and linear-magnitude terms for better spectral alignment.\nIn addition, we use the perceptual evaluation metric PESQ<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/aliutkus/speechmetrics\" title=\"\">https://github.com/aliutkus/speechmetrics</a></span></span></span> to assess audio quality for speech-related tasks. Since PESQ is designed for speech, it is omitted for MRSLife and MRSMusic. For all metrics except PESQ, lower values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate spatial consistency, we use Spatial-AST<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib44\" title=\"\">2024</a>)</cite> to extract angular and distance embeddings from the binaural audio. Since Spatial-AST predicts positions only for static sources, we compute the cosine similarity between the predicted and ground truth embeddings within 1-second segments and average the results to assess overall spatial fidelity.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite>, we adopt four joint detection and localization metrics:\nF<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m1\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math>: location-aware F-score; a prediction is correct if the event class matches and angular error is below <math alttext=\"20^{\\circ}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m2\" intent=\":literal\"><semantics><msup><mn>20</mn><mo>&#8728;</mo></msup><annotation encoding=\"application/x-tex\">20^{\\circ}</annotation></semantics></math>.\nER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m3\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math>: error rate computed as the sum of insertions, deletions, and substitutions over reference events.\nLE<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub>: class-aware localization error, the mean angular difference between predicted and reference directions.\nLR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub>: class-aware localization recall, the percentage of correctly localized events among all instances of each class.\nWe compute all metrics in 1-second non-overlapping segments using macro-averaging across all event classes. Higher values of F<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m6\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> and LR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub>, and lower values of ER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS0.Px5.p1.m8\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> and LE<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub> indicate better performance.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "lecd",
                    "f20∘20circ",
                    "lrcd",
                    "localization",
                    "er20∘20circ",
                    "event"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build upon BinauralGrad&#8217;s two-stage diffusion-based framework to convert monaural audio into spatial audio. However, in our subsequent generation experiments, the synthesized monaural audio typically corresponds to a centrally positioned source between the ears, effectively serving as the first stage of BinauralGrad. Therefore, for the spatialization experiments presented here, we use only the second stage of BinauralGrad to validate spatial audio generation.\nSpecifically, we first convert binaural recordings to monaural input by averaging the two channels. Next, we apply a DSP-based method to produce a coarse spatial approximation of the binaural signal. This monaural input and its simulated binaural counterpart are then used as input to the BinauralGrad model, which is conditioned on the object&#8217;s motion trajectory to generate spatialized binaural audio.\nWe list the architecture and hyperparameters of BinauralGrad in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T8\" title=\"Table 8 &#8227; B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune two pre-trained models, CosyVoice and F5-TTS, using monaural audio obtained by averaging the binaural recordings from the MRSSpeech subset. This allows the models to learn the generation characteristics specific to our dataset. After monaural generation, we apply a pre-trained spatialization model trained on MRSSpeech to convert the output into spatialized binaural audio. For the ISDrama baseline, we adopt the model variant proposed in the original paper that conditions generation on predefined spatial paths to produce binaural spatial speech directly.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ISDrama baseline, we adopt the spatial-path-conditioned generation framework and extend it by incorporating Rmssinger&#8217;s note encoder, allowing explicit pitch control through musical score input.\nWe use the single stage variation of Rmssinger, a standard singing voice synthesis model, and train it with monaural targets derived from the averaged binaural recordings in the MRSSing subset. This enables the model to generate pitch-accurate audio that reflects the acoustic characteristics. The synthesized monaural audio is then spatialized using the spatialization model trained on MRSSing.\nWe list the architecture and hyperparameters of Rmssinger in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T9\" title=\"Table 9 &#8227; B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For spatial music generation, in the ISDrama setting, we follow the path-conditioned generation pipeline but remove the phoneme encoder, using only the note encoder to process symbolic scores as the sole control condition for generating spatialized music.\nWe also adopt Make-An-Audio 2 as our base model and train it using monaural audio derived from averaged MRSMusic binaural recordings. We use the pretrained spectrogram autoencoder. Instrument category and symbolic score information are concatenated into the text prompt to guide the music generation process. The generated audio is subsequently spatialized using the MRSMusic-trained spatialization model. We list the hyper-parameters of Make-An-Audio 2 in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T10\" title=\"Table 10 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the STARSS23 framework for FOA-based sound event localization and detection, training on the event segments from the MRSSound subset under both audio-only and audio-visual conditions. To enable binaural input, we modify the model architecture to use three input channels and extract interaural phase difference features. This allows us to adapt the SELD model to binaural audio. Additionally, we experiment with replacing convolutional layers with Transformer encoders to explore the effect of different architectures on sound event localization and detection performance.\nWe list the hyper-parameters of SELD in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T11\" title=\"Table 11 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "model",
                    "sound",
                    "binaural",
                    "transformer",
                    "localization",
                    "audio",
                    "event",
                    "mrssound"
                ]
            }
        ]
    },
    "A1.T7": {
        "source_file": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations",
        "caption": "Table 7: Examples of predefined content types for each MRSAudio subset.",
        "body": "Subset\n\n\nSamples of Categories or Keywords\n\n\n\n\n\n\nMRSLife\n\n\nMRSDialogue: board games, card games, collaborative tasks\n\nMRSSound: kungfu, office, maracas, typing, whistle, gong\n\n\n\n\nMRSSpeech\n\n\nMovie scripts, crosstalk, scripted TV dialogue, multi-speaker conversations\n\n\n\n\nMRSSing\n\n\nChinese, English, German, French; soprano, alto, tenor, bass\n\n\n\n\nMRSMusic\n\n\nviolin, electronic keyboard, cello, viola, double bass, trumpet, trombone, euphonium, erhu, pipa, xiao, bawu, trumpet, trombone, and others (23 instruments in total)",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subset</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:313.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Samples of Categories or Keywords</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MRSLife</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:313.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MRSDialogue</span><span class=\"ltx_text\" style=\"font-size:90%;\">: board games, card games, collaborative tasks\n</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MRSSound</span><span class=\"ltx_text\" style=\"font-size:90%;\">: kungfu, office, maracas, typing, whistle, gong</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MRSSpeech</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:313.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Movie scripts, crosstalk, scripted TV dialogue, multi-speaker conversations</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MRSSing</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:313.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Chinese, English, German, French; soprano, alto, tenor, bass</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MRSMusic</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:313.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">violin, electronic keyboard, cello, viola, double bass, trumpet, trombone, euphonium, erhu, pipa, xiao, bawu, trumpet, trombone, and others (23 instruments in total)</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "board",
            "soprano",
            "kungfu",
            "subset",
            "gong",
            "tasks",
            "mrssing",
            "categories",
            "french",
            "maracas",
            "pipa",
            "typing",
            "german",
            "multispeaker",
            "alto",
            "content",
            "types",
            "erhu",
            "keyboard",
            "conversations",
            "mrslife",
            "office",
            "each",
            "euphonium",
            "mrsspeech",
            "mrsdialogue",
            "english",
            "instruments",
            "tenor",
            "collaborative",
            "bawu",
            "scripts",
            "bass",
            "electronic",
            "viola",
            "whistle",
            "movie",
            "dialogue",
            "trombone",
            "mrsaudio",
            "trumpet",
            "examples",
            "chinese",
            "predefined",
            "keywords",
            "crosstalk",
            "mrsmusic",
            "cello",
            "samples",
            "mrssound",
            "games",
            "violin",
            "double",
            "xiao",
            "total",
            "scripted",
            "others",
            "card"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Based on predefined scenario requirements, we recruit professionals from relevant fields to participate in the recording process. To ensure spatial diversity, we rent various venues tailored to each scenario type. Following the FAIR-Play protocol, we use a 3Dio Free Space XLR binaural microphone to capture spatial audio, GoPro HERO cameras and mobile phones to record exocentric and egocentric videos, and UWB-based tracking systems to capture motion trajectories.\nOnce personnel, locations, and equipment are secured, we prepare task-specific materials for each subset. For MRSLife, we further divide the content based on the proportion of speech involved, resulting in two subcategories: MRSDialogue (e.g., group games, board games) and MRSSound (e.g., kungfu, kitchens, offices, sports). We predefine common sound events for each, such as clattering dishes, keyboard typing, and table tennis.\nFor MRSSpeech, we compile a large corpus of scripts from movies, TV shows, and crosstalk performances and automatically extract dialogue passages for speaker delivery.\nFor MRSSing, we design lyrics in four languages (Chinese, English, German, and French) and recruit singers across a range of vocal types to maximize diversity.\nFor MRSMusic, we collect solo performances across 23 traditional and modern instruments, covering a wide array of timbres and playing techniques.\nSpecific sound categories for each subset are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.T7\" title=\"Table 7 &#8227; A.1 Details of Planning &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive spatial environments, where auditory cues enable sound source localization in three-dimensional space.\nDespite the critical role of spatial audio in immersive technologies such as VR/AR, most existing multimodal datasets provide only monaural audio, which limits the development of spatial audio generation and understanding.\nTo address these challenges, we introduce MRSAudio, a large-scale multimodal spatial audio dataset designed to advance research in spatial audio understanding and generation.\nMRSAudio spans four distinct components: MRSLife, MRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios.\nThe dataset includes synchronized binaural and ambisonic audio, exocentric and egocentric video, motion trajectories, and fine-grained annotations such as transcripts, phoneme boundaries, lyrics, scores, and prompts.\nTo demonstrate the utility and versatility of MRSAudio, we establish five foundational tasks: audio spatialization, and spatial text to speech, spatial singing voice synthesis, spatial music generation and sound event localization and detection.\nResults show that MRSAudio enables high-quality spatial modeling and supports a broad range of spatial audio research.\nDemos and dataset access are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mrsaudio.github.io\" title=\"\">https://mrsaudio.github.io</a>.</p>\n\n",
                "matched_terms": [
                    "mrssing",
                    "mrsspeech",
                    "tasks",
                    "mrsaudio",
                    "mrsmusic",
                    "mrslife"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these gaps, we present <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a 484-hour large-scale multimodal spatial audio dataset designed to support both spatial audio understanding and generation. It integrates high-fidelity spatial recordings with synchronized video, 3D pose tracking, and rich semantic annotations, enabling comprehensive modeling of real-world auditory scenes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S0.F1\" title=\"Figure 1 &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the dataset comprises four subsets, each targeting distinct tasks and scenarios.\n<span class=\"ltx_text ltx_font_bold\">MRSLife</span> (129 h) captures daily activities such as board games, cooking, and office work, using egocentric video and FOA audio annotated with sound events and speech transcripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSpeech</span> (206 h) includes binaural conversations from 44 speakers across diverse indoor environments, paired with video, 3D source positions, and complete scripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSing</span> (80 h) features high-quality solo singing performances in Chinese, English, German, and French by 20 vocalists, each aligned with time-stamped lyrics and corresponding musical scores.\n<span class=\"ltx_text ltx_font_bold\">MRSMusic</span> (69 h) offers spatial recordings of 23 Traditional Chinese, Western and Electronic instruments, with symbolic score annotations that support learning-based methods for symbolic-to-audio generation and fine-grained localization.\nTogether, these four subsets support a broad spectrum of spatial audio research problems, including event detection, sound localization, and binaural or ambisonic audio generation. By pairing spatial audio with synchronized exocentric and egocentric video, geometric tracking, and detailed semantic labels, MRSAudio enables new research directions in multimodal spatial understanding and cross-modal generation. Throughout this paper, unless stated otherwise, we use the term spatial audio to refer to binaural audio.</p>\n\n",
                "matched_terms": [
                    "board",
                    "office",
                    "each",
                    "games",
                    "mrssing",
                    "mrsspeech",
                    "tasks",
                    "french",
                    "electronic",
                    "mrsaudio",
                    "english",
                    "instruments",
                    "chinese",
                    "german",
                    "mrsmusic",
                    "conversations",
                    "mrslife",
                    "scripts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We organize the data into four complementary subsets (MRSLife, MRSSpeech, MRSSing, MRSMusic), each carefully tailored to different real-world acoustic scenarios and equipped with rich, scenario-specific annotations to facilitate downstream task development.</p>\n\n",
                "matched_terms": [
                    "each",
                    "mrssing",
                    "mrsspeech",
                    "mrsmusic",
                    "mrslife"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The remainder of this paper is organized as follows.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2\" title=\"2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reviews existing spatial audio datasets.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3\" title=\"3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> describes the design, collection process, and key statistics of MRSAudio.\nSection&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4\" title=\"4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents extensive benchmark experiments using state-of-the-art methods on five core spatial audio tasks: audio spatialization, spatial text-to-speech, spatial singing voice generation, spatial music synthesis, and sound event localization and detection.\nFinally, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S5\" title=\"5 Conclusion and Discussion &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> concludes the paper and discusses the limitations and potential risks associated with MRSAudio.</p>\n\n",
                "matched_terms": [
                    "mrsaudio",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, existing datasets vary in their goals and modalities. For instance, Spatial LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Sarabia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib32\" title=\"\">2023</a>)</cite> simulates spatial speech using the LibriSpeech corpus and is mainly intended for binaural TTS applications. RealMAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib40\" title=\"\">2024</a>)</cite> is a large-scale real-recorded and annotated 32-channel microphone array dataset for multichannel speech enhancement and source localization. YT-Ambigen <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>)</cite> and Sphere360 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite> are derived from web-crawled video datasets, but lack explicit spatial annotations, making them suitable primarily for video-to-spatial audio generation. BinauralMusic focuses on musical content, offering instrument class tags. BEWO-1M <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>)</cite> combines crawled content and simulated binaural rendering, and provides image or GPT-generated prompts. STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> features real-world FOA and multi-channel audio alongside synchronized videos and includes sound class and position labels.\nIn contrast to prior datasets, MRSAudio offers a comprehensive, large-scale, and real-world spatial audio corpus, featuring 484 hours of recorded data in both FOA and binaural formats, covering all audio domains including general audio, speech, singing, and music. Uniquely, MRSAudio includes synchronized audio, video, and 3D positional geometry, along with fine-grained cross-modal annotations, such as: transcripts, word and phoneme boundaries and music scores.\nThese features make MRSAudio an ideal resource for a broad range of spatial audio generation and understanding tasks, including spatial speech, music, and singing voice synthesis.</p>\n\n",
                "matched_terms": [
                    "mrsaudio",
                    "content",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span> This subset focuses on everyday conversations and environmental sound events. Based on the degree of human vocal interaction, MRSLife is further divided into two parts: MRSDialogue, which captures unscripted conversations that naturally include spontaneous action sounds (e.g., footsteps, door movements), and MRSSound, which focuses on non-verbal sound events primarily caused by physical activities such as cooking, typing, or sports.</p>\n\n",
                "matched_terms": [
                    "subset",
                    "mrsdialogue",
                    "typing",
                    "conversations",
                    "mrslife",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span> MRSSpeech targets clean, high-quality conversational recordings for TTS task. All spoken interactions are recorded in controlled indoor environments with minimal noise. We invite speakers to participate in content-driven conversations based on predefined scripts.</p>\n\n",
                "matched_terms": [
                    "predefined",
                    "conversations",
                    "mrsspeech",
                    "scripts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span> MRSSing captures solo vocal performances for singing voice synthesis tasks. It includes professional solo vocal recordings in four languages: Mandarin, English, German, and French, performed by singers covering the full vocal range including soprano, alto, tenor, and bass.</p>\n\n",
                "matched_terms": [
                    "soprano",
                    "bass",
                    "mrssing",
                    "tasks",
                    "french",
                    "english",
                    "tenor",
                    "german",
                    "alto"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> MRSMusic captures immersive instrumental performances suitable for spatial music generation and analysis.\nWe record solo performances from 45 professional musicians across 23 instruments, including Traditional Chinese, Western and Electronic instruments.\nEach performance is paired with its corresponding musical score to support score-based music generation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "instruments",
                    "chinese",
                    "mrsmusic",
                    "electronic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Details regarding personnel recruitment, venue selection, equipment configuration, and material preparation for each subset are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS1\" title=\"A.1 Details of Planning &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "subset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span> In MRSDialogue, audio is recorded using a professional binaural recording head and high-resolution sound cards, while synchronized video is captured using industry-standard cameras. In MRSSound, in addition to binaural audio and exocentric video, we also captured FOA (Zoom H3-VR) and egocentric video (Gopro camera).To ensure the effectiveness of the egocentric video, participants are asked to remain within the frontal field of view of the binaural recording head.</p>\n\n",
                "matched_terms": [
                    "mrsdialogue",
                    "mrslife",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span> To introduce spatial variability, we select four recording rooms that differ in size and acoustic material. Each speaker reads inflected emotions from the scripts while walking through the space, producing dynamic spatial cues. Recordings include spoken passages, binaural audio and exocentric video, as well as a clean mono audio by a lavalier microphone placed near the speaker.</p>\n\n",
                "matched_terms": [
                    "each",
                    "mrsspeech",
                    "scripts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span> The professional singers perform according to musical scores. To introduce variation in source&#8211;listener geometry, we adjust the position of the head-mounted binaural microphone relative to the singer. In addition to the binaural audio and synchronized video, each session includes a clean vocal track recorded with a studio-grade condenser microphone.</p>\n\n",
                "matched_terms": [
                    "each",
                    "mrssing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> We record 23 Traditional Chinese, Western and Electronic instruments, performed by 45\nprofessional musicians.To capture rich spatial detail, we vary microphone placement around the instrument. Recordings include binaural audio, monaural audio, exocentric video, and synchronized video that records playing gestures. Full recording details are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS2\" title=\"A.2 Details of Recording &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "instruments",
                    "mrsmusic",
                    "chinese",
                    "electronic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To maximize MRSAudio&#8217;s utility across a wide range of tasks, we begin with event-level annotations for all vocal and acoustic content in MRSLife, MRSSpeech, MRSSing, and MRSMusic. However, coarse annotations alone are insufficient for fine-grained tasks such as singing voice modeling and music generation from scores. To bridge this gap, we design a comprehensive annotation pipeline. Full implementation details and detailed annotation guidelines are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS3\" title=\"A.3 Details of Annotation &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "mrssing",
                    "mrsspeech",
                    "tasks",
                    "content",
                    "mrsmusic",
                    "mrslife"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span>\nFor MRSDialogue, we apply WhisperX <cite class=\"ltx_cite ltx_citemacro_citep\">(Bain et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib6\" title=\"\">2023</a>)</cite> for automatic speech recognition and speaker diarization to generate initial transcripts and speaker turns. Human annotators correct recognition errors and speaker attribution mismatches. The audio is then segmented into utterances and the transcripts are converted into phoneme sequences (using pypinyin for Mandarin). A two-stage alignment process follows: we first apply the Montreal Forced Aligner (MFA) <cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib30\" title=\"\">2017</a>)</cite> for coarse word/phoneme mapping, then manually refine boundaries in Praat <cite class=\"ltx_cite ltx_citemacro_citep\">(Boersma, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib8\" title=\"\">2001</a>)</cite>.\nFor MRSSound, we annotate sound event categories and corresponding time intervals.</p>\n\n",
                "matched_terms": [
                    "categories",
                    "mrsdialogue",
                    "mrslife",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span>\nGiven the availability of full scripts, we adapt WhisperX for long-form word-to-audio alignment of up to 30 minutes.\nEach script line is automatically matched to its corresponding audio segment (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS3\" title=\"A.3 Details of Annotation &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a> for more details).\nAnnotators then review these alignments, correcting any omissions or insertions caused by actors&#8217; deviations from the script. Finally, phoneme sequences are extracted and aligned with the audio using the same procedure as in MRSDialogue.</p>\n\n",
                "matched_terms": [
                    "each",
                    "mrsdialogue",
                    "mrsspeech",
                    "scripts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nWe use voice activity detection (VAD) to segment recordings into singing regions, then align pre-existing lyrics using LyricFA&#8217;s ASR-based dynamic programming algorithm<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/wolfgitpr/LyricFA\" title=\"\">https://github.com/wolfgitpr/LyricFA</a></span></span></span>.\nPhoneme generation is language-dependent: pypinyin for Mandarin, ARPA for English, and MFA&#8217;s built-in phoneme sets for French and German. Alignment is conducted via MFA, followed by manual refinement. Melody and rhythm of singing are transcribed into MIDI format using ROSVOT <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib27\" title=\"\">2024</a>)</cite>. Annotators then label each excerpt with high-level style descriptors, such as emotional tone (happy, sad), tempo (slow, moderate, fast), and pitch range (low, medium, high).</p>\n\n",
                "matched_terms": [
                    "each",
                    "mrssing",
                    "french",
                    "english",
                    "german"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-processing pipeline comprises three key steps to refine raw annotated data. First, segmentation splits continuous recordings into task-oriented clips: utterances for speech and singing are extracted via alignment timestamps, while MRSSound audio is uniformly divided into 10-second segments. Next, multimodal synchronization aligns each clip with auxiliary modalities (text, video, position metadata) with temporal anchors. Static sound sources are annotated with manually measured 3D coordinates, whereas dynamic sources leverage interpolated UWB tracking trajectories. For scenes where participants&#8217; faces are visible, anonymization is performed by adding half-face masks. Finally, quality assurance involves domain experts auditing 15% of clips across all modules to verify temporal alignment precision, cross-modal content consistency, and annotation accuracy.\nFull auditing protocols are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS4\" title=\"A.4 Details of Post-Processing &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>, ensuring reproducibility of this process.</p>\n\n",
                "matched_terms": [
                    "each",
                    "content",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To illustrate spatial diversity, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) shows the 3D distribution of sound source positions relative to the listener. The heatmap reveals near-uniform azimuthal coverage, with greater density in the frontal hemisphere due to the prevalence of egocentric video recordings. Elevation angles are concentrated between &#8211;40&#176; and 40&#176;, aligning with typical human sound perception patterns.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b) summarizes the annotations using a keyword cloud that captures the range of real-world activities recorded. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c) presents the proportions of recording spaces by room size: medium-sized rooms are the most common, accounting for approximately 40% of sessions, while small and large rooms each represent around 30%. Recording duration is evenly distributed across room types, ensuring diverse acoustic coverage.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(d) shows the distribution of segment durations after automatic and manual segmentation. Most audio clips are shorter than 10 seconds, which is suitable for modeling short-duration events and supports efficient downstream training and inference.\nOverall, MRSAudio offers comprehensive coverage across spatial positions, acoustic environments, scene types, and temporal structures, making it well-suited for a wide range of spatial audio generation and understanding tasks. A more detailed breakdown of per-scenario statistics is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1\" title=\"Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "mrsaudio",
                    "types",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the quality and utility of MRSAudio in real-world scenarios, we evaluate it on five representative spatial audio tasks: (i) audio spatialization, (ii) spatial text-to-speech, (iii) spatial singing voice synthesis, (iv) spatial music generation, and (v) sound event localization and detection (SELD). These tasks cover both generation and understanding, and are critical for applications such as AR/VR, spatial media production, and perceptual scene analysis.\nAll experiments are conducted using state-of-the-art methods on a server equipped with eight NVIDIA RTX 4090 GPUs.\nFor different tasks, we employ distinct objective metrics. For generation tasks, we compute cosine similarity scores for direction (ANG Cos) and distance (DIS Cos) to quantify spatial alignment quality. Additionally, we utilize subjective MOS-Q (Mean Opinion Score for Quality) to evaluate the quality of generated audio and MOS-P (Mean Opinion Score for Position) to assess spatial perception. For implementation details of training and evaluation metrics, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2\" title=\"Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "mrsaudio",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio spatialization aims to synthesize spatially immersive soundscapes from monaural inputs and source positional information. To evaluate MRSAudio on this task, we adopt BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite>, a diffusion-based generation model that predicts binaural waveforms conditioned on monaural audio and source coordinates. Since the downstream generation tasks can be formulated as predicting interaural (binaural) representations from mono audio, we employ a single-stage training scheme for all experiments. For comparison, we include a traditional signal processing baseline (DSP), which renders binaural audio using virtual source positions simulated via room impulse responses (RIRs) and head-related transfer functions (HRTFs).\nWe adopt the following objective metrics to evaluate audio quality:\n(1) W-L2: waveform L2 distance,\n(2) A-L2: amplitude envelope L2 distance,\n(3) P-L2: phase difference L2,\n(4) STFT: multi-resolution STFT loss,\n(5) PESQ: perceptual speech quality score.\nResults for the ground truth and DSP baseline are obtained by averaging across all test sets from the four MRSAudio subsets. Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS3\" title=\"B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "mrsaudio",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Spatialization &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BinauralGrad achieves strong performance across all MRSAudio subsets, surpassing the classical DSP baseline on most objective and subjective metrics. The highest performance is observed on MRSLife, likely due to the relatively limited variation in sound sources within these scenes. These results demonstrate that MRSAudio&#8217;s rich spatial annotations and diverse acoustic environments provide a solid foundation for training and evaluating spatial audio generation models.</p>\n\n",
                "matched_terms": [
                    "mrsaudio",
                    "mrslife"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial TTS aims to produce high-quality speech enriched with spatial cues, thereby enhancing immersion and realism in AR/VR. Although recent advances in TTS have led to impressive improvements in speech quality, progress in spatialized speech generation remains limited due to the scarcity of spatially annotated recordings with rich, high-quality labels.\nTo evaluate the effectiveness of MRSAudio for this task, we train an end-to-end model following ISDrama <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib43\" title=\"\">2025</a>)</cite> to directly generate spatial speech from text and position data. Additionally, we compare with cascaded pipelines that combine monaural TTS models (CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib14\" title=\"\">2024</a>)</cite> and F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib10\" title=\"\">2024</a>)</cite>) with the audio spatialization module. This allows us to assess both speech generation quality and the spatial fidelity enabled by our dataset.\nFor content evaluation, we report Character Error Rate (CER) and Speaker Similarity (SIM). Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS4\" title=\"B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "mrsaudio",
                    "content"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial SVS aims to produce expressive, high-quality singing voices enriched with accurate spatial cues, thereby enhancing listener immersion. While traditional SVS has advanced considerably, spatial SVS remains underexplored due to the lack of high-quality, spatially annotated datasets.\nTo evaluate MRSAudio for this task, we use the MRSSing subset to train and benchmark models. We adopt the ISDrama architecture to perform end-to-end spatial singing synthesis, incorporating note-level pitch control to enhance prosody accuracy. For comparison, we use the open-source SVS models Rmssinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib19\" title=\"\">2023</a>)</cite>. The outputs are then spatialized with BinauralGrad.\nWe employ objective metrics, including Mel-Cepstral Distortion (MCD) and F0 Frame Error (FFE), to evaluate spectral and pitch similarity between predicted and ground-truth. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS5\" title=\"B.5 Spatial Singing Voice Synthesis &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "mrsaudio",
                    "mrssing",
                    "subset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task aims to synthesize spatially immersive music conditioned on symbolic scores. While datasets like FAIR-Play offer high-quality instrument recordings, they lack aligned sheet music, limiting controllable generation. In contrast, our MRSMusic subset includes 23 instruments with aligned scores, enabling fine-grained, score-based spatial music synthesis.\nWe benchmark three systems on MRSMusic. First, we apply BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite> to spatialize mono recordings. Second, we utilize Make-An-Audio 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib13\" title=\"\">2023</a>)</cite> to generate mono music from MIDI scores, which is subsequently spatialized. Third, we adapt ISDrama to accept both score embeddings and spatial poses, enabling end-to-end spatial symbolic music generation.\nFor objective evaluation, we compute Fr&#233;chet Audio Distance (FAD) and FFE to evaluate the results. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS6\" title=\"B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "instruments",
                    "subset",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task evaluates the ability to detect and localize sound events using MRSAudio&#8217;s spatial annotations. We follow STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> using both audio-only and audio-visual variants on the MRSSound subset.\nIn the audio-only condition, models receive either FOA or binaural waveforms as input and predict sound event classes along with 3D source coordinates. For the audio-visual condition, we extract bounding boxes of visible persons to serve as coarse visual priors, which are fused with the audio representations. We also explore architectural variations by replacing the original convolutional backbone with a Transformer encoder, allowing us to assess the impact of temporal modeling capacity on spatial prediction.\nWe evaluate model performance using four standard joint detection and localization metrics, including location-aware detection (<math alttext=\"F_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">F_{20^{\\circ}}</annotation></semantics></math>, <math alttext=\"ER_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub></mrow><annotation encoding=\"application/x-tex\">ER_{20^{\\circ}}</annotation></semantics></math>) and class-aware localization (<math alttext=\"LE_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>E</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LE_{CD}</annotation></semantics></math>, <math alttext=\"LR_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LR_{CD}</annotation></semantics></math>). Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS7\" title=\"B.7 Sound Event Localization and Detection &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.7</span></a>.</p>\n\n",
                "matched_terms": [
                    "subset",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce MRSAudio, a large-scale, multimodal spatial audio corpus designed to support a wide range of generation and understanding tasks. MRSAudio comprises four complementary modules, MRSLife, MRSSpeech , MRSSing, and MRSMusic, each captured with binaural/FOA audio, synchronized video, precise 3D pose metadata, and richly detailed annotations (event labels, transcripts, phoneme boundaries, lyrics, musical scores, and motion prompts). Through extensive benchmarks on audio spatialization, binaural speech, singing, and music generation , as well as sound event localization and detection, we demonstrate that MRSAudio&#8217;s scale, diversity, and annotation depth enable state&#8208;of&#8208;the&#8208;art performance and unlock new avenues for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "each",
                    "mrssing",
                    "mrsspeech",
                    "tasks",
                    "mrsaudio",
                    "mrsmusic",
                    "mrslife"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Directions:</span>\nWhile MRSAudio offers broad multimodal coverage, two limitations remain. First, although synchronized video is provided for all recordings, current benchmarks only explore visual input in a limited subset of tasks. Second, while the dataset includes both binaural and First-Order Ambisonic (FOA) formats, the FOA subset is smaller in scale, and most tasks focus on binaural audio, limiting spatial modeling diversity.\nFuture work will expand the role of visual modalities in tasks such as sound localization and scene understanding. We also plan to increase FOA recordings to balance data distribution and support broader spatial audio research, and develop more FOA-specific benchmarks to better utilize ambisonic spatial cues.</p>\n\n",
                "matched_terms": [
                    "mrsaudio",
                    "subset",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the planning phase, we systematically analyze real-world application scenarios for spatial audio and divide them into four major categories. MRSLife focuses on daily environmental sound events, MRSSpeech targets high-quality conversational data, MRSSing captures solo vocal performances, and MRSMusic records immersive instrumental music. These four scenarios collectively cover a broad range of everyday acoustic contexts and are designed to support a wide spectrum of downstream tasks.</p>\n\n",
                "matched_terms": [
                    "mrssing",
                    "mrsspeech",
                    "tasks",
                    "categories",
                    "mrsmusic",
                    "mrslife"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span>\nWe recruit 62 participants to perform daily activities including board games, cooking, exercise, and office work. In MRSDialogue, each participant is compensated $30 per recorded hour. Binaural audio is captured using head-mounted microphones, and third-person video is recorded. In MRSSound scenes such as kung fu or kitchen demonstrations, performers are paid $50 per hour. Both binaural and FOA (First-Order Ambisonics) recordings are collected, along with first-person and third-person video. The binaural dummy head is co-located with the egocentric camera and the Zoom H3-VR (Ambisonic recorder). The egocentric camera is rigidly aligned with the head&#8217;s gaze to capture first-person visuals, while the Ambisonic recorder is mounted 7.5 cm above the head. An exocentric camera is placed at a surveyed position in the scene to provide a third-person view of the environment and object relationships.\nParticipants receive brief scene descriptions and are asked to act naturally while maintaining a single active sound source when possible. The total duration for MRSLife recordings reaches approximately 150 hours.</p>\n\n",
                "matched_terms": [
                    "board",
                    "each",
                    "office",
                    "games",
                    "mrsdialogue",
                    "total",
                    "mrslife",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span>\nWe employ 44 expressive speakers to read from scripted texts, each paid $30 per hour of recorded audio. The total recorded duration reaches 200 hours, with compensation totaling $40,000. During recording sessions, speakers alternate between standing and walking around the recording area to introduce spatial diversity while maintaining speech clarity. Binaural audio is captured using head-mounted microphones, and clean monaural speech is recorded using lavalier microphones. All sessions are filmed from a third-person perspective.</p>\n\n",
                "matched_terms": [
                    "total",
                    "each",
                    "scripted",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nEighteen professional singers participate in the recordings. Each singer is fluent in at least one of the following languages: Chinese, English, German, or French, and collectively they cover all vocal ranges including soprano, alto, tenor, and bass. Performers are paid $50 per hour, contributing a total of 80 hours of audio. Singing is recorded at a fixed position to ensure spatial consistency. Binaural recordings are captured with head-mounted microphones, and monaural audio is recorded with a studio-grade microphone. Third-person panoramic video is also captured.</p>\n\n",
                "matched_terms": [
                    "soprano",
                    "each",
                    "bass",
                    "mrssing",
                    "french",
                    "english",
                    "tenor",
                    "chinese",
                    "german",
                    "alto",
                    "total"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span>\nWe engage 45 instrumentalists performing 23 Traditional Chinese, Western and Electronic instruments such as violin, erhu, pipa, electric guitar, and keyboard. Each performer receives $60 per recorded hour. A total of 69 hours of solo music performances are recorded. Audio is captured using both head-mounted binaural microphones and reference monaural microphones. Third-person video provides a full-scene view, while first-person cameras are used to capture detailed playing gestures.</p>\n\n",
                "matched_terms": [
                    "each",
                    "violin",
                    "instruments",
                    "pipa",
                    "chinese",
                    "total",
                    "erhu",
                    "mrsmusic",
                    "keyboard",
                    "electronic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife.</span>\nIn MRSDialogue scenes, we automatically generate initial transcripts and speaker clusters with WhisperX, extracting word&#8208;level timestamps and speaker IDs. Experts then load these results in Praat and assign each cluster to the correct speaker, correcting transcription errors as needed. Next, we run Montreal Forced Aligner (MFA) <cite class=\"ltx_cite ltx_citemacro_citep\">(McAuliffe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib30\" title=\"\">2017</a>)</cite> to produce coarse phoneme&#8208;to&#8208;audio alignments (exported in TextGrid format), using pypinyin to convert Chinese text into phoneme sequences.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mozillazg/python-pinyin\" title=\"\">https://github.com/mozillazg/python-pinyin</a></span></span></span> Finally, annotators refine word and phoneme boundaries in Praat to achieve millisecond&#8208;level precision.\nIn MRSSound segments, annotators additionally label each time interval with the corresponding event category (e.g., &#8220;clattering,&#8221; &#8220;typing,&#8221; &#8220;pages turning&#8221;).</p>\n\n",
                "matched_terms": [
                    "each",
                    "mrsdialogue",
                    "chinese",
                    "mrslife",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech.</span>\nGiven full dialogue scripts, we perform automatic alignment to 30-minute recordings using a chunk-based extension of WhisperX. This method divides each utterance-level audio segment into fixed-length chunks (e.g., 30 seconds), applies phoneme-level models (e.g., wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib5\" title=\"\">2020</a>)</cite> ) for emission prediction on each chunk, and then concatenates emissions to form a complete alignment matrix.\nWe compute alignments via a trellis-based dynamic programming algorithm with backtracking, followed by scaling to restore absolute timestamps. Word- and sentence-level timings are derived by grouping aligned characters using word indices and sentence tokenization. Missing or partial timings are interpolated using a specified method (e.g., nearest).\nThis pipeline enables efficient and accurate alignment of long-form recordings on GPU. We then refine sentence boundaries in Praat and apply the same MFA-plus-Praat workflow used in MRSLife for fine-grained phoneme and word-level alignment.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dialogue",
                    "mrsspeech",
                    "mrslife",
                    "scripts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing.</span>\nEach segment contains a solo vocal performance with known lyrics. We first apply voice activity detection (VAD) to segment the recordings. Lyrics are aligned to each segment using LyricFN&#8217;s ASR-based dynamic programming method. English phonemes follow the ARPABET standard,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://en.wikipedia.org/wiki/ARPABET\" title=\"\">https://en.wikipedia.org/wiki/ARPABET</a></span></span></span> while German and French use MFA&#8217;s phoneme sets.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mfa-models.readthedocs.io/en/latest/dictionary/\" title=\"\">https://mfa-models.readthedocs.io/en/latest/dictionary/</a></span></span></span> We then apply MFA for initial alignment and refine it manually in Praat to obtain precise word and phoneme boundaries. Annotators assign fine-grained style tags, including emotion, genre, and tempo. To generate score annotations, we extract F0 contours using RMVPE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib38\" title=\"\">2023</a>)</cite> and convert them into MIDI format via ROSVOT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib27\" title=\"\">2024</a>)</cite>, followed by expert correction.</p>\n\n",
                "matched_terms": [
                    "each",
                    "mrssing",
                    "french",
                    "english",
                    "german"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation:</span>\nAfter annotation, we segment the raw recordings into shorter clips to support spatial audio generation and analysis tasks. For speech and singing, we use alignment timestamps to extract utterances. For MRSSound, each recording is divided into fixed 10-second segments.</p>\n\n",
                "matched_terms": [
                    "each",
                    "mrssound",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Synchronization:</span>\nIn addition to binaural audio, each clip is synchronized with the following modalities: audio, text, video, and position data. Using the segment timestamps, we align all streams temporally. For static sources, we attach manually recorded 3D coordinates; for dynamic sources, we interpolate Ultra-Wideband (UWB) tracking data over the segment duration.\nThis process yields fully synchronized multimodal clips ready for downstream spatial audio tasks.\nFurthermore, for segments where participants&#8217; faces are visible, we apply anonymization by using a face detection model to overlay digital masks during post-processing.</p>\n\n",
                "matched_terms": [
                    "each",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Checking:</span>\nTo ensure the reliability of annotations, domain experts perform a random audit on 15% of the segmented clips across all four modules. The audit process involves the following steps:\n(1) Verifying the temporal synchronization across different modal;\n(2) Confirming that the assigned event labels accurately reflect the audiovisual content present in each clip;\n(3) Reviewing the speech segments to check the correctness of word- and phoneme-level alignments;\n(4) Evaluating singing clips for accurate alignment between lyrics and audio, consistency with musical scores, and correctness of assigned style labels;\n(5) Assessing MRSMusic excerpts to verify that musical properties, such as key, pitch, and note duration.</p>\n\n",
                "matched_terms": [
                    "each",
                    "content",
                    "mrsmusic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSDialogue.</span>\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F4\" title=\"Figure 4 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>(a) presents the 3D spatial distribution of sound sources in MRSDialogue. In this subset, which features frequent human conversations, most sources are located around the ear-level height of the listener. The azimuthal distribution covers nearly all directions surrounding the listener, offering diverse angular data for training spatial localization models with strong generalization.</p>\n\n",
                "matched_terms": [
                    "mrsdialogue",
                    "conversations",
                    "subset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F5\" title=\"Figure 5 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(b) presents the duration distribution of recorded sound events. MRSSound covers a wide variety of everyday scenarios, including cooking in kitchens, working in office environments, and sports-related activities. The diversity of event types and durations makes the subset suitable for training and evaluating models in real-world spatial sound event understanding.</p>\n\n",
                "matched_terms": [
                    "office",
                    "types",
                    "subset",
                    "mrssound"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F6\" title=\"Figure 6 &#8227; A.5.2 Statistics of MRSSpeech &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>(b) shows a word cloud representing the diversity of dialogue content. The transcripts are sourced from theatrical scripts, films, and other spoken-only scenarios, capturing a wide range of expressive and stylistic variation. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F6\" title=\"Figure 6 &#8227; A.5.2 Statistics of MRSSpeech &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>(c) presents the distribution of room sizes used for speech recordings. Most multi-speaker interactions take place in medium to large rooms, such as meeting or lecture spaces. We include three distinct environments with varying absorption properties and dimensions to simulate different acoustic conditions.</p>\n\n",
                "matched_terms": [
                    "multispeaker",
                    "content",
                    "dialogue",
                    "scripts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F7\" title=\"Figure 7 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> (b) and (c) highlight the diversity in style and content. Emotion annotations in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F7\" title=\"Figure 7 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(b) include expressive labels such as happy and sad. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F7\" title=\"Figure 7 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(c) displays the coverage across vocal ranges, including soprano, alto, tenor, and bass, ensuring a wide span of pitch and timbral variation.</p>\n\n",
                "matched_terms": [
                    "soprano",
                    "bass",
                    "tenor",
                    "alto",
                    "content"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F8\" title=\"Figure 8 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(b) and (c) highlight the diversity of instrument types and musical genres. The instrument set spans Western, Traditional Chinese, and Electronic categories, while the genre annotations include folk, pop, and classical music. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F8\" title=\"Figure 8 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(f) further details the recording durations across 23 instruments, showing a relatively balanced distribution that facilitates downstream learning for different instrument types.</p>\n\n",
                "matched_terms": [
                    "categories",
                    "instruments",
                    "chinese",
                    "types",
                    "electronic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct subjective evaluation of the generation tasks using Mean Opinion Score (MOS). For each task, we randomly sample 40 utterances from the test set. Each utterance is paired with a corresponding source-position prompt and is evaluated by at least five expert listeners.\nMOS-Q, Listeners rate each sample on a five-point Likert scale ranging from 1 (bad) to 5 (excellent).\nFor audio quality, we use MOS-Q, where listeners wear headphones and assess the clarity and naturalness of the generated audio.\nFor spatial perception, we use MOS-P, where listeners evaluate the realism of spatial cues and whether the perceived direction and distance of the sound source match the prompt description.\nAll participants are fairly compensated for their time at a rate of $15 per hour, resulting in a total cost of approximately $2000. Participants are informed that their evaluations will be used exclusively for academic research purposes.\nInstructions for audio evaluations are shown in Figure\n&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.F9\" title=\"Figure 9 &#8227; B.1 Subjective Evaluation Metrics &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "total",
                    "each",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure waveform similarity using Wave L2, the mean squared error (MSE) between the generated and reference binaural waveforms. Amplitude L2 and Phase L2 are computed after applying Short-Time Fourier Transform (STFT), reflecting errors in the magnitude and phase components respectively. MRSTFT loss<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/csteinmetz1/auraloss\" title=\"\">https://github.com/csteinmetz1/auraloss</a></span></span></span> is also used, combining spectral convergence, log-magnitude, and linear-magnitude terms for better spectral alignment.\nIn addition, we use the perceptual evaluation metric PESQ<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/aliutkus/speechmetrics\" title=\"\">https://github.com/aliutkus/speechmetrics</a></span></span></span> to assess audio quality for speech-related tasks. Since PESQ is designed for speech, it is omitted for MRSLife and MRSMusic. For all metrics except PESQ, lower values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "mrslife",
                    "mrsmusic",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune two pre-trained models, CosyVoice and F5-TTS, using monaural audio obtained by averaging the binaural recordings from the MRSSpeech subset. This allows the models to learn the generation characteristics specific to our dataset. After monaural generation, we apply a pre-trained spatialization model trained on MRSSpeech to convert the output into spatialized binaural audio. For the ISDrama baseline, we adopt the model variant proposed in the original paper that conditions generation on predefined spatial paths to produce binaural spatial speech directly.</p>\n\n",
                "matched_terms": [
                    "predefined",
                    "subset",
                    "mrsspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ISDrama baseline, we adopt the spatial-path-conditioned generation framework and extend it by incorporating Rmssinger&#8217;s note encoder, allowing explicit pitch control through musical score input.\nWe use the single stage variation of Rmssinger, a standard singing voice synthesis model, and train it with monaural targets derived from the averaged binaural recordings in the MRSSing subset. This enables the model to generate pitch-accurate audio that reflects the acoustic characteristics. The synthesized monaural audio is then spatialized using the spatialization model trained on MRSSing.\nWe list the architecture and hyperparameters of Rmssinger in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T9\" title=\"Table 9 &#8227; B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "mrssing",
                    "subset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the STARSS23 framework for FOA-based sound event localization and detection, training on the event segments from the MRSSound subset under both audio-only and audio-visual conditions. To enable binaural input, we modify the model architecture to use three input channels and extract interaural phase difference features. This allows us to adapt the SELD model to binaural audio. Additionally, we experiment with replacing convolutional layers with Transformer encoders to explore the effect of different architectures on sound event localization and detection performance.\nWe list the hyper-parameters of SELD in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T11\" title=\"Table 11 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "subset",
                    "mrssound"
                ]
            }
        ]
    },
    "A2.T8": {
        "source_file": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations",
        "caption": "Table 8: Hyper-parameters of BinauralGrad modules.",
        "body": "Hyperparameter\nBinauralGrad\n\n\n\n\nBinaural\nEncoder\n\n\nWave Encoder Layers\n2\n\n\nPosition Encoder Layers\n2\n\n\nEncoder Conv1D Kernel\n3\n\n\nEncoder Dropout\n0.4\n\n\n\n\nMel Predictor\n\n\nResidual Blocks\n3\n\n\nBidirectional Layers\n3\n\n\nHidden_size\n128\n\n\nTraining Steps\n200\n\n\nSampling Steps\n6",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Hyperparameter</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BinauralGrad</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"4\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">Binaural</span>\n<span class=\"ltx_p\">Encoder</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Wave Encoder Layers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Position Encoder Layers</td>\n<td class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Encoder Conv1D Kernel</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Encoder Dropout</td>\n<td class=\"ltx_td ltx_align_center\">0.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"5\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">Mel Predictor</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Residual Blocks</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Bidirectional Layers</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Hidden_size</td>\n<td class=\"ltx_td ltx_align_center\">128</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Training Steps</td>\n<td class=\"ltx_td ltx_align_center\">200</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">Sampling Steps</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">6</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "hyperparameter",
            "blocks",
            "encoder",
            "hyperparameters",
            "bidirectional",
            "kernel",
            "position",
            "predictor",
            "layers",
            "mel",
            "binauralgrad",
            "modules",
            "conv1d",
            "residual",
            "binaural",
            "dropout",
            "hiddensize",
            "steps",
            "sampling",
            "wave"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We build upon BinauralGrad&#8217;s two-stage diffusion-based framework to convert monaural audio into spatial audio. However, in our subsequent generation experiments, the synthesized monaural audio typically corresponds to a centrally positioned source between the ears, effectively serving as the first stage of BinauralGrad. Therefore, for the spatialization experiments presented here, we use only the second stage of BinauralGrad to validate spatial audio generation.\nSpecifically, we first convert binaural recordings to monaural input by averaging the two channels. Next, we apply a DSP-based method to produce a coarse spatial approximation of the binaural signal. This monaural input and its simulated binaural counterpart are then used as input to the BinauralGrad model, which is conditioned on the object&#8217;s motion trajectory to generate spatialized binaural audio.\nWe list the architecture and hyperparameters of BinauralGrad in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T8\" title=\"Table 8 &#8227; B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, existing datasets vary in their goals and modalities. For instance, Spatial LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Sarabia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib32\" title=\"\">2023</a>)</cite> simulates spatial speech using the LibriSpeech corpus and is mainly intended for binaural TTS applications. RealMAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib40\" title=\"\">2024</a>)</cite> is a large-scale real-recorded and annotated 32-channel microphone array dataset for multichannel speech enhancement and source localization. YT-Ambigen <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>)</cite> and Sphere360 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite> are derived from web-crawled video datasets, but lack explicit spatial annotations, making them suitable primarily for video-to-spatial audio generation. BinauralMusic focuses on musical content, offering instrument class tags. BEWO-1M <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>)</cite> combines crawled content and simulated binaural rendering, and provides image or GPT-generated prompts. STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> features real-world FOA and multi-channel audio alongside synchronized videos and includes sound class and position labels.\nIn contrast to prior datasets, MRSAudio offers a comprehensive, large-scale, and real-world spatial audio corpus, featuring 484 hours of recorded data in both FOA and binaural formats, covering all audio domains including general audio, speech, singing, and music. Uniquely, MRSAudio includes synchronized audio, video, and 3D positional geometry, along with fine-grained cross-modal annotations, such as: transcripts, word and phoneme boundaries and music scores.\nThese features make MRSAudio an ideal resource for a broad range of spatial audio generation and understanding tasks, including spatial speech, music, and singing voice synthesis.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "position"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span> The professional singers perform according to musical scores. To introduce variation in source&#8211;listener geometry, we adjust the position of the head-mounted binaural microphone relative to the singer. In addition to the binaural audio and synchronized video, each session includes a clean vocal track recorded with a studio-grade condenser microphone.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "position"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-processing pipeline comprises three key steps to refine raw annotated data. First, segmentation splits continuous recordings into task-oriented clips: utterances for speech and singing are extracted via alignment timestamps, while MRSSound audio is uniformly divided into 10-second segments. Next, multimodal synchronization aligns each clip with auxiliary modalities (text, video, position metadata) with temporal anchors. Static sound sources are annotated with manually measured 3D coordinates, whereas dynamic sources leverage interpolated UWB tracking trajectories. For scenes where participants&#8217; faces are visible, anonymization is performed by adding half-face masks. Finally, quality assurance involves domain experts auditing 15% of clips across all modules to verify temporal alignment precision, cross-modal content consistency, and annotation accuracy.\nFull auditing protocols are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS4\" title=\"A.4 Details of Post-Processing &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>, ensuring reproducibility of this process.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "modules",
                    "position"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the quality and utility of MRSAudio in real-world scenarios, we evaluate it on five representative spatial audio tasks: (i) audio spatialization, (ii) spatial text-to-speech, (iii) spatial singing voice synthesis, (iv) spatial music generation, and (v) sound event localization and detection (SELD). These tasks cover both generation and understanding, and are critical for applications such as AR/VR, spatial media production, and perceptual scene analysis.\nAll experiments are conducted using state-of-the-art methods on a server equipped with eight NVIDIA RTX 4090 GPUs.\nFor different tasks, we employ distinct objective metrics. For generation tasks, we compute cosine similarity scores for direction (ANG Cos) and distance (DIS Cos) to quantify spatial alignment quality. Additionally, we utilize subjective MOS-Q (Mean Opinion Score for Quality) to evaluate the quality of generated audio and MOS-P (Mean Opinion Score for Position) to assess spatial perception. For implementation details of training and evaluation metrics, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2\" title=\"Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "position"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio spatialization aims to synthesize spatially immersive soundscapes from monaural inputs and source positional information. To evaluate MRSAudio on this task, we adopt BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite>, a diffusion-based generation model that predicts binaural waveforms conditioned on monaural audio and source coordinates. Since the downstream generation tasks can be formulated as predicting interaural (binaural) representations from mono audio, we employ a single-stage training scheme for all experiments. For comparison, we include a traditional signal processing baseline (DSP), which renders binaural audio using virtual source positions simulated via room impulse responses (RIRs) and head-related transfer functions (HRTFs).\nWe adopt the following objective metrics to evaluate audio quality:\n(1) W-L2: waveform L2 distance,\n(2) A-L2: amplitude envelope L2 distance,\n(3) P-L2: phase difference L2,\n(4) STFT: multi-resolution STFT loss,\n(5) PESQ: perceptual speech quality score.\nResults for the ground truth and DSP baseline are obtained by averaging across all test sets from the four MRSAudio subsets. Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS3\" title=\"B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "binauralgrad",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T2\" title=\"Table 2 &#8227; 4.1 Audio Spatialization &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, BinauralGrad achieves strong performance across all MRSAudio subsets, surpassing the classical DSP baseline on most objective and subjective metrics. The highest performance is observed on MRSLife, likely due to the relatively limited variation in sound sources within these scenes. These results demonstrate that MRSAudio&#8217;s rich spatial annotations and diverse acoustic environments provide a solid foundation for training and evaluating spatial audio generation models.</p>\n\n",
                "matched_terms": [
                    "binauralgrad",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task evaluates the ability to detect and localize sound events using MRSAudio&#8217;s spatial annotations. We follow STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> using both audio-only and audio-visual variants on the MRSSound subset.\nIn the audio-only condition, models receive either FOA or binaural waveforms as input and predict sound event classes along with 3D source coordinates. For the audio-visual condition, we extract bounding boxes of visible persons to serve as coarse visual priors, which are fused with the audio representations. We also explore architectural variations by replacing the original convolutional backbone with a Transformer encoder, allowing us to assess the impact of temporal modeling capacity on spatial prediction.\nWe evaluate model performance using four standard joint detection and localization metrics, including location-aware detection (<math alttext=\"F_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">F_{20^{\\circ}}</annotation></semantics></math>, <math alttext=\"ER_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub></mrow><annotation encoding=\"application/x-tex\">ER_{20^{\\circ}}</annotation></semantics></math>) and class-aware localization (<math alttext=\"LE_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>E</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LE_{CD}</annotation></semantics></math>, <math alttext=\"LR_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LR_{CD}</annotation></semantics></math>). Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS7\" title=\"B.7 Sound Event Localization and Detection &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.7</span></a>.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce MRSAudio, a large-scale, multimodal spatial audio corpus designed to support a wide range of generation and understanding tasks. MRSAudio comprises four complementary modules, MRSLife, MRSSpeech , MRSSing, and MRSMusic, each captured with binaural/FOA audio, synchronized video, precise 3D pose metadata, and richly detailed annotations (event labels, transcripts, phoneme boundaries, lyrics, musical scores, and motion prompts). Through extensive benchmarks on audio spatialization, binaural speech, singing, and music generation , as well as sound event localization and detection, we demonstrate that MRSAudio&#8217;s scale, diversity, and annotation depth enable state&#8208;of&#8208;the&#8208;art performance and unlock new avenues for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "modules"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span>\nWe recruit 62 participants to perform daily activities including board games, cooking, exercise, and office work. In MRSDialogue, each participant is compensated $30 per recorded hour. Binaural audio is captured using head-mounted microphones, and third-person video is recorded. In MRSSound scenes such as kung fu or kitchen demonstrations, performers are paid $50 per hour. Both binaural and FOA (First-Order Ambisonics) recordings are collected, along with first-person and third-person video. The binaural dummy head is co-located with the egocentric camera and the Zoom H3-VR (Ambisonic recorder). The egocentric camera is rigidly aligned with the head&#8217;s gaze to capture first-person visuals, while the Ambisonic recorder is mounted 7.5 cm above the head. An exocentric camera is placed at a surveyed position in the scene to provide a third-person view of the environment and object relationships.\nParticipants receive brief scene descriptions and are asked to act naturally while maintaining a single active sound source when possible. The total duration for MRSLife recordings reaches approximately 150 hours.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "position"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nEighteen professional singers participate in the recordings. Each singer is fluent in at least one of the following languages: Chinese, English, German, or French, and collectively they cover all vocal ranges including soprano, alto, tenor, and bass. Performers are paid $50 per hour, contributing a total of 80 hours of audio. Singing is recorded at a fixed position to ensure spatial consistency. Binaural recordings are captured with head-mounted microphones, and monaural audio is recorded with a studio-grade microphone. Third-person panoramic video is also captured.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "position"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Synchronization:</span>\nIn addition to binaural audio, each clip is synchronized with the following modalities: audio, text, video, and position data. Using the segment timestamps, we align all streams temporally. For static sources, we attach manually recorded 3D coordinates; for dynamic sources, we interpolate Ultra-Wideband (UWB) tracking data over the segment duration.\nThis process yields fully synchronized multimodal clips ready for downstream spatial audio tasks.\nFurthermore, for segments where participants&#8217; faces are visible, we apply anonymization by using a face detection model to overlay digital masks during post-processing.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "position"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Checking:</span>\nTo ensure the reliability of annotations, domain experts perform a random audit on 15% of the segmented clips across all four modules. The audit process involves the following steps:\n(1) Verifying the temporal synchronization across different modal;\n(2) Confirming that the assigned event labels accurately reflect the audiovisual content present in each clip;\n(3) Reviewing the speech segments to check the correctness of word- and phoneme-level alignments;\n(4) Evaluating singing clips for accurate alignment between lyrics and audio, consistency with musical scores, and correctness of assigned style labels;\n(5) Assessing MRSMusic excerpts to verify that musical properties, such as key, pitch, and note duration.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "modules"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure waveform similarity using Wave L2, the mean squared error (MSE) between the generated and reference binaural waveforms. Amplitude L2 and Phase L2 are computed after applying Short-Time Fourier Transform (STFT), reflecting errors in the magnitude and phase components respectively. MRSTFT loss<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/csteinmetz1/auraloss\" title=\"\">https://github.com/csteinmetz1/auraloss</a></span></span></span> is also used, combining spectral convergence, log-magnitude, and linear-magnitude terms for better spectral alignment.\nIn addition, we use the perceptual evaluation metric PESQ<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/aliutkus/speechmetrics\" title=\"\">https://github.com/aliutkus/speechmetrics</a></span></span></span> to assess audio quality for speech-related tasks. Since PESQ is designed for speech, it is omitted for MRSLife and MRSMusic. For all metrics except PESQ, lower values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "wave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ISDrama baseline, we adopt the spatial-path-conditioned generation framework and extend it by incorporating Rmssinger&#8217;s note encoder, allowing explicit pitch control through musical score input.\nWe use the single stage variation of Rmssinger, a standard singing voice synthesis model, and train it with monaural targets derived from the averaged binaural recordings in the MRSSing subset. This enables the model to generate pitch-accurate audio that reflects the acoustic characteristics. The synthesized monaural audio is then spatialized using the spatialization model trained on MRSSing.\nWe list the architecture and hyperparameters of Rmssinger in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T9\" title=\"Table 9 &#8227; B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "binaural",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For spatial music generation, in the ISDrama setting, we follow the path-conditioned generation pipeline but remove the phoneme encoder, using only the note encoder to process symbolic scores as the sole control condition for generating spatialized music.\nWe also adopt Make-An-Audio 2 as our base model and train it using monaural audio derived from averaged MRSMusic binaural recordings. We use the pretrained spectrogram autoencoder. Instrument category and symbolic score information are concatenated into the text prompt to guide the music generation process. The generated audio is subsequently spatialized using the MRSMusic-trained spatialization model. We list the hyper-parameters of Make-An-Audio 2 in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T10\" title=\"Table 10 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "binaural",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the STARSS23 framework for FOA-based sound event localization and detection, training on the event segments from the MRSSound subset under both audio-only and audio-visual conditions. To enable binaural input, we modify the model architecture to use three input channels and extract interaural phase difference features. This allows us to adapt the SELD model to binaural audio. Additionally, we experiment with replacing convolutional layers with Transformer encoders to explore the effect of different architectures on sound event localization and detection performance.\nWe list the hyper-parameters of SELD in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T11\" title=\"Table 11 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "layers",
                    "binaural",
                    "training"
                ]
            }
        ]
    },
    "A2.T9": {
        "source_file": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations",
        "caption": "Table 9: Hyper-parameters of Rmssinger modules.",
        "body": "Hyperparameter\nRmssinger\n\n\n\n\n\n\nPhoneme\nEncoder\n\n\nPhoneme Embedding\n256\n\n\nEncoder Layers\n4\n\n\nEncoder Hidden\n256\n\n\nEncoder Conv1D Kernel\n9\n\n\nEncoder Conv1D Filter Size\n1024\n\n\nEncoder Attention Heads\n2\n\n\nEncoder Dropout\n0.1\n\n\n\n\nNote\nEncoder\n\n\nPitches Embedding\n256\n\n\nType Embedding\n256\n\n\nDuration Hidden\n256\n\n\n\n\nPitch\nPredictor\n\n\nConv Layers\n12\n\n\nKernel Size\n3\n\n\nResidual Channel\n192\n\n\nHidden Channel\n256\n\n\nTraining Steps\n100\n\n\n\n\nMel Predictor\n\n\nConv Layers\n20\n\n\nKernel Size\n3\n\n\nResidual Channel\n256\n\n\nHidden Channel\n256\n\n\nTraining Steps\n100",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Hyperparameter</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Rmssinger</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"7\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">Phoneme</span>\n<span class=\"ltx_p\">Encoder</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Phoneme Embedding</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">256</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Encoder Layers</td>\n<td class=\"ltx_td ltx_align_center\">4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Encoder Hidden</td>\n<td class=\"ltx_td ltx_align_center\">256</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Encoder Conv1D Kernel</td>\n<td class=\"ltx_td ltx_align_center\">9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Encoder Conv1D Filter Size</td>\n<td class=\"ltx_td ltx_align_center\">1024</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Encoder Attention Heads</td>\n<td class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Encoder Dropout</td>\n<td class=\"ltx_td ltx_align_center\">0.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"3\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">Note</span>\n<span class=\"ltx_p\">Encoder</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Pitches Embedding</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">256</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Type Embedding</td>\n<td class=\"ltx_td ltx_align_center\">256</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Duration Hidden</td>\n<td class=\"ltx_td ltx_align_center\">256</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"5\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">Pitch</span>\n<span class=\"ltx_p\">Predictor</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Conv Layers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Kernel Size</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Residual Channel</td>\n<td class=\"ltx_td ltx_align_center\">192</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Hidden Channel</td>\n<td class=\"ltx_td ltx_align_center\">256</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Training Steps</td>\n<td class=\"ltx_td ltx_align_center\">100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"5\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\">Mel Predictor</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Conv Layers</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Kernel Size</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Residual Channel</td>\n<td class=\"ltx_td ltx_align_center\">256</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Hidden Channel</td>\n<td class=\"ltx_td ltx_align_center\">256</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">Training Steps</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">100</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "type",
            "training",
            "size",
            "hyperparameter",
            "embedding",
            "encoder",
            "hyperparameters",
            "kernel",
            "filter",
            "pitch",
            "predictor",
            "rmssinger",
            "layers",
            "duration",
            "mel",
            "modules",
            "conv1d",
            "residual",
            "hidden",
            "dropout",
            "conv",
            "phoneme",
            "steps",
            "channel",
            "attention",
            "heads",
            "note",
            "pitches"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For the ISDrama baseline, we adopt the spatial-path-conditioned generation framework and extend it by incorporating Rmssinger&#8217;s note encoder, allowing explicit pitch control through musical score input.\nWe use the single stage variation of Rmssinger, a standard singing voice synthesis model, and train it with monaural targets derived from the averaged binaural recordings in the MRSSing subset. This enables the model to generate pitch-accurate audio that reflects the acoustic characteristics. The synthesized monaural audio is then spatialized using the spatialization model trained on MRSSing.\nWe list the architecture and hyperparameters of Rmssinger in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T9\" title=\"Table 9 &#8227; B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nWe use voice activity detection (VAD) to segment recordings into singing regions, then align pre-existing lyrics using LyricFA&#8217;s ASR-based dynamic programming algorithm<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/wolfgitpr/LyricFA\" title=\"\">https://github.com/wolfgitpr/LyricFA</a></span></span></span>.\nPhoneme generation is language-dependent: pypinyin for Mandarin, ARPA for English, and MFA&#8217;s built-in phoneme sets for French and German. Alignment is conducted via MFA, followed by manual refinement. Melody and rhythm of singing are transcribed into MIDI format using ROSVOT <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib27\" title=\"\">2024</a>)</cite>. Annotators then label each excerpt with high-level style descriptors, such as emotional tone (happy, sad), tempo (slow, moderate, fast), and pitch range (low, medium, high).</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The post-processing pipeline comprises three key steps to refine raw annotated data. First, segmentation splits continuous recordings into task-oriented clips: utterances for speech and singing are extracted via alignment timestamps, while MRSSound audio is uniformly divided into 10-second segments. Next, multimodal synchronization aligns each clip with auxiliary modalities (text, video, position metadata) with temporal anchors. Static sound sources are annotated with manually measured 3D coordinates, whereas dynamic sources leverage interpolated UWB tracking trajectories. For scenes where participants&#8217; faces are visible, anonymization is performed by adding half-face masks. Finally, quality assurance involves domain experts auditing 15% of clips across all modules to verify temporal alignment precision, cross-modal content consistency, and annotation accuracy.\nFull auditing protocols are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS4\" title=\"A.4 Details of Post-Processing &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>, ensuring reproducibility of this process.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "modules"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To illustrate spatial diversity, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) shows the 3D distribution of sound source positions relative to the listener. The heatmap reveals near-uniform azimuthal coverage, with greater density in the frontal hemisphere due to the prevalence of egocentric video recordings. Elevation angles are concentrated between &#8211;40&#176; and 40&#176;, aligning with typical human sound perception patterns.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b) summarizes the annotations using a keyword cloud that captures the range of real-world activities recorded. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c) presents the proportions of recording spaces by room size: medium-sized rooms are the most common, accounting for approximately 40% of sessions, while small and large rooms each represent around 30%. Recording duration is evenly distributed across room types, ensuring diverse acoustic coverage.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(d) shows the distribution of segment durations after automatic and manual segmentation. Most audio clips are shorter than 10 seconds, which is suitable for modeling short-duration events and supports efficient downstream training and inference.\nOverall, MRSAudio offers comprehensive coverage across spatial positions, acoustic environments, scene types, and temporal structures, making it well-suited for a wide range of spatial audio generation and understanding tasks. A more detailed breakdown of per-scenario statistics is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1\" title=\"Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "training",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Spatial SVS aims to produce expressive, high-quality singing voices enriched with accurate spatial cues, thereby enhancing listener immersion. While traditional SVS has advanced considerably, spatial SVS remains underexplored due to the lack of high-quality, spatially annotated datasets.\nTo evaluate MRSAudio for this task, we use the MRSSing subset to train and benchmark models. We adopt the ISDrama architecture to perform end-to-end spatial singing synthesis, incorporating note-level pitch control to enhance prosody accuracy. For comparison, we use the open-source SVS models Rmssinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib19\" title=\"\">2023</a>)</cite>. The outputs are then spatialized with BinauralGrad.\nWe employ objective metrics, including Mel-Cepstral Distortion (MCD) and F0 Frame Error (FFE), to evaluate spectral and pitch similarity between predicted and ground-truth. Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS5\" title=\"B.5 Spatial Singing Voice Synthesis &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "rmssinger",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce MRSAudio, a large-scale, multimodal spatial audio corpus designed to support a wide range of generation and understanding tasks. MRSAudio comprises four complementary modules, MRSLife, MRSSpeech , MRSSing, and MRSMusic, each captured with binaural/FOA audio, synchronized video, precise 3D pose metadata, and richly detailed annotations (event labels, transcripts, phoneme boundaries, lyrics, musical scores, and motion prompts). Through extensive benchmarks on audio spatialization, binaural speech, singing, and music generation , as well as sound event localization and detection, we demonstrate that MRSAudio&#8217;s scale, diversity, and annotation depth enable state&#8208;of&#8208;the&#8208;art performance and unlock new avenues for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "modules"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Checking:</span>\nTo ensure the reliability of annotations, domain experts perform a random audit on 15% of the segmented clips across all four modules. The audit process involves the following steps:\n(1) Verifying the temporal synchronization across different modal;\n(2) Confirming that the assigned event labels accurately reflect the audiovisual content present in each clip;\n(3) Reviewing the speech segments to check the correctness of word- and phoneme-level alignments;\n(4) Evaluating singing clips for accurate alignment between lyrics and audio, consistency with musical scores, and correctness of assigned style labels;\n(5) Assessing MRSMusic excerpts to verify that musical properties, such as key, pitch, and note duration.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "steps",
                    "modules",
                    "note",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F5\" title=\"Figure 5 &#8227; A.5.1 Statistics of MRSLife &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>(b) presents the duration distribution of recorded sound events. MRSSound covers a wide variety of everyday scenarios, including cooking in kitchens, working in office environments, and sports-related activities. The diversity of event types and durations makes the subset suitable for training and evaluating models in real-world spatial sound event understanding.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F7\" title=\"Figure 7 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(d) presents the duration distribution of singing segments, which ranges from approximately 4 to 10 seconds. This aligns with typical input lengths used in training singing voice synthesis models. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F7\" title=\"Figure 7 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(e) illustrates the distribution of note pitches. The full range of musical notes is well represented, and a clear difference in pitch range is observed between male and female singers, with females generally singing at higher pitches. This confirms that MRSSing captures realistic vocal and musical variability.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "training",
                    "note",
                    "pitch",
                    "pitches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, MRSSing provides extensive diversity in spatial positioning, language, emotion, vocal range, segment duration, and pitch. This makes it a strong foundation for research on spatial singing voice synthesis and expressive vocal modeling.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding generalization capacity, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F8\" title=\"Figure 8 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(d) shows that audio segment durations range from approximately 4 to 11 seconds, matching typical training input lengths. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F8\" title=\"Figure 8 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(e) demonstrates that the dataset covers a full range of musical pitch values, supporting tasks that require robust pitch generalization.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For spatial music generation, in the ISDrama setting, we follow the path-conditioned generation pipeline but remove the phoneme encoder, using only the note encoder to process symbolic scores as the sole control condition for generating spatialized music.\nWe also adopt Make-An-Audio 2 as our base model and train it using monaural audio derived from averaged MRSMusic binaural recordings. We use the pretrained spectrogram autoencoder. Instrument category and symbolic score information are concatenated into the text prompt to guide the music generation process. The generated audio is subsequently spatialized using the MRSMusic-trained spatialization model. We list the hyper-parameters of Make-An-Audio 2 in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T10\" title=\"Table 10 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "encoder",
                    "phoneme",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the STARSS23 framework for FOA-based sound event localization and detection, training on the event segments from the MRSSound subset under both audio-only and audio-visual conditions. To enable binaural input, we modify the model architecture to use three input channels and extract interaural phase difference features. This allows us to adapt the SELD model to binaural audio. Additionally, we experiment with replacing convolutional layers with Transformer encoders to explore the effect of different architectures on sound event localization and detection performance.\nWe list the hyper-parameters of SELD in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T11\" title=\"Table 11 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "layers",
                    "training"
                ]
            }
        ]
    },
    "A2.T10": {
        "source_file": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations",
        "caption": "Table 10: Hyperparameters of Make-An-Audio 2.",
        "body": "Hyperparameter\nMake-An-Audio 2\n\n\n\n\nAutoencoders\nInput/Output Channels\n80\n\n\nHidden Channels\n20\n\n\nResidual Blocks\n2\n\n\nSpectrogram Shape\n(80, 624)\n\n\nChannel Multipiler\n[1,2,4][1,2,4]\n\n\nTransformer Backbone\nInput shape\n(20, T)\n\n\nCondition_embedding Size\n1024\n\n\nFeed-forward Hidden_size\n576\n\n\nNum of Transformer Heads\n8\n\n\nTransformer Blocks\n8\n\n\nTraining Steps\n1000\n\n\n\nSampling Steps\n100\n\n\nCLAP Text Encoder\nTransformer Embed Channels\n768\n\n\nOutput Project Channels\n1024\n\n\nToken Length\n77",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">Hyperparameter</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Make-An-Audio 2</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"5\">Autoencoders</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Input/Output Channels</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">80</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Hidden Channels</td>\n<td class=\"ltx_td ltx_align_center\">20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Residual Blocks</td>\n<td class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Spectrogram Shape</td>\n<td class=\"ltx_td ltx_align_center\">(80, 624)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Channel Multipiler</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"[1,2,4]\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T10.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>4</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[1,2,4]</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"6\">Transformer Backbone</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Input shape</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">(20, T)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Condition_embedding Size</td>\n<td class=\"ltx_td ltx_align_center\">1024</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Feed-forward Hidden_size</td>\n<td class=\"ltx_td ltx_align_center\">576</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Num of Transformer Heads</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Transformer Blocks</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Training Steps</td>\n<td class=\"ltx_td ltx_align_center\">1000</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Sampling Steps</td>\n<td class=\"ltx_td ltx_align_center\">100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"3\">CLAP Text Encoder</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Transformer Embed Channels</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">768</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Output Project Channels</td>\n<td class=\"ltx_td ltx_align_center\">1024</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">Token Length</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">77</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "size",
            "hyperparameter",
            "blocks",
            "autoencoders",
            "text",
            "conditionembedding",
            "input",
            "output",
            "inputoutput",
            "makeanaudio",
            "length",
            "transformer",
            "encoder",
            "shape",
            "hyperparameters",
            "feedforward",
            "clap",
            "spectrogram",
            "project",
            "channels",
            "backbone",
            "residual",
            "hidden",
            "token",
            "hiddensize",
            "embed",
            "steps",
            "num",
            "sampling",
            "channel",
            "multipiler",
            "heads"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For spatial music generation, in the ISDrama setting, we follow the path-conditioned generation pipeline but remove the phoneme encoder, using only the note encoder to process symbolic scores as the sole control condition for generating spatialized music.\nWe also adopt Make-An-Audio 2 as our base model and train it using monaural audio derived from averaged MRSMusic binaural recordings. We use the pretrained spectrogram autoencoder. Instrument category and symbolic score information are concatenated into the text prompt to guide the music generation process. The generated audio is subsequently spatialized using the MRSMusic-trained spatialization model. We list the hyper-parameters of Make-An-Audio 2 in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T10\" title=\"Table 10 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The post-processing pipeline comprises three key steps to refine raw annotated data. First, segmentation splits continuous recordings into task-oriented clips: utterances for speech and singing are extracted via alignment timestamps, while MRSSound audio is uniformly divided into 10-second segments. Next, multimodal synchronization aligns each clip with auxiliary modalities (text, video, position metadata) with temporal anchors. Static sound sources are annotated with manually measured 3D coordinates, whereas dynamic sources leverage interpolated UWB tracking trajectories. For scenes where participants&#8217; faces are visible, anonymization is performed by adding half-face masks. Finally, quality assurance involves domain experts auditing 15% of clips across all modules to verify temporal alignment precision, cross-modal content consistency, and annotation accuracy.\nFull auditing protocols are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS4\" title=\"A.4 Details of Post-Processing &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>, ensuring reproducibility of this process.</p>\n\n",
                "matched_terms": [
                    "text",
                    "steps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To illustrate spatial diversity, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) shows the 3D distribution of sound source positions relative to the listener. The heatmap reveals near-uniform azimuthal coverage, with greater density in the frontal hemisphere due to the prevalence of egocentric video recordings. Elevation angles are concentrated between &#8211;40&#176; and 40&#176;, aligning with typical human sound perception patterns.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b) summarizes the annotations using a keyword cloud that captures the range of real-world activities recorded. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(c) presents the proportions of recording spaces by room size: medium-sized rooms are the most common, accounting for approximately 40% of sessions, while small and large rooms each represent around 30%. Recording duration is evenly distributed across room types, ensuring diverse acoustic coverage.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S3.F3\" title=\"Figure 3 &#8227; 3.5 Statistics &#8227; 3 Dataset Description &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(d) shows the distribution of segment durations after automatic and manual segmentation. Most audio clips are shorter than 10 seconds, which is suitable for modeling short-duration events and supports efficient downstream training and inference.\nOverall, MRSAudio offers comprehensive coverage across spatial positions, acoustic environments, scene types, and temporal structures, making it well-suited for a wide range of spatial audio generation and understanding tasks. A more detailed breakdown of per-scenario statistics is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1\" title=\"Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task evaluates the ability to detect and localize sound events using MRSAudio&#8217;s spatial annotations. We follow STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> using both audio-only and audio-visual variants on the MRSSound subset.\nIn the audio-only condition, models receive either FOA or binaural waveforms as input and predict sound event classes along with 3D source coordinates. For the audio-visual condition, we extract bounding boxes of visible persons to serve as coarse visual priors, which are fused with the audio representations. We also explore architectural variations by replacing the original convolutional backbone with a Transformer encoder, allowing us to assess the impact of temporal modeling capacity on spatial prediction.\nWe evaluate model performance using four standard joint detection and localization metrics, including location-aware detection (<math alttext=\"F_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">F_{20^{\\circ}}</annotation></semantics></math>, <math alttext=\"ER_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub></mrow><annotation encoding=\"application/x-tex\">ER_{20^{\\circ}}</annotation></semantics></math>) and class-aware localization (<math alttext=\"LE_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>E</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LE_{CD}</annotation></semantics></math>, <math alttext=\"LR_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LR_{CD}</annotation></semantics></math>). Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS7\" title=\"B.7 Sound Event Localization and Detection &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.7</span></a>.</p>\n\n",
                "matched_terms": [
                    "backbone",
                    "encoder",
                    "input",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T6\" title=\"Table 6 &#8227; 4.5 Sound Event Localization and Detection &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, model performance varies with architecture, input modality, and audio format. Transformer-based models generally outperform ConvNet baselines, particularly in reducing localization error. FOA input consistently yields better results than binaural audio, benefiting from richer spatial representation. For example, the Transformer with FOA and no visual input achieves the lowest error rate (ER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> 0.99) and highest localization recall (LR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub> 87.32). The addition of visual features improves performance in some ConvNet settings (e.g., F<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m3\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> rises from 9.33 to 13.00 with FOA), but offers limited or inconsistent gains in Transformer models, possibly due to modality mismatch or redundancy. These results highlight the value of MRSAudio&#8217;s spatial annotations and multimodal streams in supporting flexible evaluation under both unimodal and multimodal configurations.</p>\n\n",
                "matched_terms": [
                    "input",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F7\" title=\"Figure 7 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(d) presents the duration distribution of singing segments, which ranges from approximately 4 to 10 seconds. This aligns with typical input lengths used in training singing voice synthesis models. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F7\" title=\"Figure 7 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>(e) illustrates the distribution of note pitches. The full range of musical notes is well represented, and a clear difference in pitch range is observed between male and female singers, with females generally singing at higher pitches. This confirms that MRSSing captures realistic vocal and musical variability.</p>\n\n",
                "matched_terms": [
                    "input",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding generalization capacity, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F8\" title=\"Figure 8 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(d) shows that audio segment durations range from approximately 4 to 11 seconds, matching typical training input lengths. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F8\" title=\"Figure 8 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(e) demonstrates that the dataset covers a full range of musical pitch values, supporting tasks that require robust pitch generalization.</p>\n\n",
                "matched_terms": [
                    "input",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build upon BinauralGrad&#8217;s two-stage diffusion-based framework to convert monaural audio into spatial audio. However, in our subsequent generation experiments, the synthesized monaural audio typically corresponds to a centrally positioned source between the ears, effectively serving as the first stage of BinauralGrad. Therefore, for the spatialization experiments presented here, we use only the second stage of BinauralGrad to validate spatial audio generation.\nSpecifically, we first convert binaural recordings to monaural input by averaging the two channels. Next, we apply a DSP-based method to produce a coarse spatial approximation of the binaural signal. This monaural input and its simulated binaural counterpart are then used as input to the BinauralGrad model, which is conditioned on the object&#8217;s motion trajectory to generate spatialized binaural audio.\nWe list the architecture and hyperparameters of BinauralGrad in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T8\" title=\"Table 8 &#8227; B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "input",
                    "channels"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ISDrama baseline, we adopt the spatial-path-conditioned generation framework and extend it by incorporating Rmssinger&#8217;s note encoder, allowing explicit pitch control through musical score input.\nWe use the single stage variation of Rmssinger, a standard singing voice synthesis model, and train it with monaural targets derived from the averaged binaural recordings in the MRSSing subset. This enables the model to generate pitch-accurate audio that reflects the acoustic characteristics. The synthesized monaural audio is then spatialized using the spatialization model trained on MRSSing.\nWe list the architecture and hyperparameters of Rmssinger in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T9\" title=\"Table 9 &#8227; B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "encoder",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We follow the STARSS23 framework for FOA-based sound event localization and detection, training on the event segments from the MRSSound subset under both audio-only and audio-visual conditions. To enable binaural input, we modify the model architecture to use three input channels and extract interaural phase difference features. This allows us to adapt the SELD model to binaural audio. Additionally, we experiment with replacing convolutional layers with Transformer encoders to explore the effect of different architectures on sound event localization and detection performance.\nWe list the hyper-parameters of SELD in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T11\" title=\"Table 11 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "training",
                    "channels",
                    "input",
                    "transformer"
                ]
            }
        ]
    },
    "A2.T11": {
        "source_file": "MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations",
        "caption": "Table 11: Hyperparameters of SELD.",
        "body": "Hyperparameter\nSELD\n\n\n\n\nInput\nBinaural Channels\n3\n\n\nFOA Channels\n7\n\n\nFrequency Bins\n128\n\n\nFrames\n120\n\n\nAudio Encoder(CNN)\nHidden_size\n64\n\n\nConv Blocks\n3\n\n\nAudio Encoder(Vit)\nHidden_size\n128\n\n\nConv Blocks\n1\n\n\nTransformer Blocks\n2\n\n\n\nNum of Transformer Heads\n4",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\">Hyperparameter</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SELD</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"4\">Input</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Binaural Channels</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">FOA Channels</td>\n<td class=\"ltx_td ltx_align_center\">7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Frequency Bins</td>\n<td class=\"ltx_td ltx_align_center\">128</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Frames</td>\n<td class=\"ltx_td ltx_align_center\">120</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"2\">Audio Encoder(CNN)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Hidden_size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">64</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Conv Blocks</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"3\">Audio Encoder(Vit)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Hidden_size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">128</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Conv Blocks</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Transformer Blocks</td>\n<td class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_bb ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">Num of Transformer Heads</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "hyperparameter",
            "blocks",
            "input",
            "seld",
            "encodercnn",
            "transformer",
            "hyperparameters",
            "encodervit",
            "foa",
            "frequency",
            "frames",
            "channels",
            "binaural",
            "conv",
            "hiddensize",
            "num",
            "bins",
            "heads",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We follow the STARSS23 framework for FOA-based sound event localization and detection, training on the event segments from the MRSSound subset under both audio-only and audio-visual conditions. To enable binaural input, we modify the model architecture to use three input channels and extract interaural phase difference features. This allows us to adapt the SELD model to binaural audio. Additionally, we experiment with replacing convolutional layers with Transformer encoders to explore the effect of different architectures on sound event localization and detection performance.\nWe list the hyper-parameters of SELD in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T11\" title=\"Table 11 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive spatial environments, where auditory cues enable sound source localization in three-dimensional space.\nDespite the critical role of spatial audio in immersive technologies such as VR/AR, most existing multimodal datasets provide only monaural audio, which limits the development of spatial audio generation and understanding.\nTo address these challenges, we introduce MRSAudio, a large-scale multimodal spatial audio dataset designed to advance research in spatial audio understanding and generation.\nMRSAudio spans four distinct components: MRSLife, MRSSpeech, MRSMusic, and MRSSing, covering diverse real-world scenarios.\nThe dataset includes synchronized binaural and ambisonic audio, exocentric and egocentric video, motion trajectories, and fine-grained annotations such as transcripts, phoneme boundaries, lyrics, scores, and prompts.\nTo demonstrate the utility and versatility of MRSAudio, we establish five foundational tasks: audio spatialization, and spatial text to speech, spatial singing voice synthesis, spatial music generation and sound event localization and detection.\nResults show that MRSAudio enables high-quality spatial modeling and supports a broad range of spatial audio research.\nDemos and dataset access are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mrsaudio.github.io\" title=\"\">https://mrsaudio.github.io</a>.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Humans rely on multisensory integration to perceive and interpret physical environments. With the rapid growth of film, virtual reality (VR), augmented reality (AR), and gaming applications, users increasingly expect not only precise audiovisual alignment but also highly immersive experiences. While recent advances in deep learning have enabled realistic generation of speech, music, and sound effects synchronized with text or video <cite class=\"ltx_cite ltx_citemacro_citep\">(Kreuk et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib25\" title=\"\">2022</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib41\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib37\" title=\"\">2024</a>)</cite>, most models focus on monaural audio and neglect spatialized soundscapes that enhance immersion. The human binaural system uses interaural time differences (ITD) and interaural level differences (ILD) to localize sound in three-dimensional space, and spatial audio must remain consistent with visual cues <cite class=\"ltx_cite ltx_citemacro_citep\">(Yost, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib42\" title=\"\">1998</a>; Cohen &amp; Knudsen, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib12\" title=\"\">1999</a>; Grothe et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib18\" title=\"\">2010</a>)</cite>. Any mismatch can disrupt immersion and weaken the sense of presence. For example, hearing a cat&#8217;s meow from the left immediately suggests its location, even if it is just off-screen.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these gaps, we present <span class=\"ltx_text ltx_font_bold\">MRSAudio</span>, a 484-hour large-scale multimodal spatial audio dataset designed to support both spatial audio understanding and generation. It integrates high-fidelity spatial recordings with synchronized video, 3D pose tracking, and rich semantic annotations, enabling comprehensive modeling of real-world auditory scenes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S0.F1\" title=\"Figure 1 &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the dataset comprises four subsets, each targeting distinct tasks and scenarios.\n<span class=\"ltx_text ltx_font_bold\">MRSLife</span> (129 h) captures daily activities such as board games, cooking, and office work, using egocentric video and FOA audio annotated with sound events and speech transcripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSpeech</span> (206 h) includes binaural conversations from 44 speakers across diverse indoor environments, paired with video, 3D source positions, and complete scripts.\n<span class=\"ltx_text ltx_font_bold\">MRSSing</span> (80 h) features high-quality solo singing performances in Chinese, English, German, and French by 20 vocalists, each aligned with time-stamped lyrics and corresponding musical scores.\n<span class=\"ltx_text ltx_font_bold\">MRSMusic</span> (69 h) offers spatial recordings of 23 Traditional Chinese, Western and Electronic instruments, with symbolic score annotations that support learning-based methods for symbolic-to-audio generation and fine-grained localization.\nTogether, these four subsets support a broad spectrum of spatial audio research problems, including event detection, sound localization, and binaural or ambisonic audio generation. By pairing spatial audio with synchronized exocentric and egocentric video, geometric tracking, and detailed semantic labels, MRSAudio enables new research directions in multimodal spatial understanding and cross-modal generation. Throughout this paper, unless stated otherwise, we use the term spatial audio to refer to binaural audio.</p>\n\n",
                "matched_terms": [
                    "foa",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assemble synchronized binaural and ambisonic recordings with exocentric and egocentric video, geometric source positions, transcripts, scores, lyrics, and event labels, providing one of the most richly annotated multimodal resources for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite recent progress, spatial audio generation and understanding remain constrained by the paucity of high-quality datasets. Due to the difficulty and expense of collecting and annotating high-quality spatial audio datasets, most open-source datasets still primarily consist of simulated or web-crawled content.\nSimulated datasets offer precise annotations but lack perceptual realism, while crawled datasets often provide real-world diversity but are missing critical labels, such as listener and source positions and content-level annotations, thus limiting their utility.\nFor instance, spatial symbolic music generation demands accurate musical scores and corresponding positional metadata.\nTo better understand the current landscape, we survey existing spatial audio datasets. These datasets differ in terms of audio format, including First-Order Ambisonics (FOA), multi-channel arrays, and binaural microphones, as well as in their collection methods (simulated, crawled, recorded) and annotation.</p>\n\n",
                "matched_terms": [
                    "foa",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, existing datasets vary in their goals and modalities. For instance, Spatial LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Sarabia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib32\" title=\"\">2023</a>)</cite> simulates spatial speech using the LibriSpeech corpus and is mainly intended for binaural TTS applications. RealMAN <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib40\" title=\"\">2024</a>)</cite> is a large-scale real-recorded and annotated 32-channel microphone array dataset for multichannel speech enhancement and source localization. YT-Ambigen <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib24\" title=\"\">2025</a>)</cite> and Sphere360 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib28\" title=\"\">2025</a>)</cite> are derived from web-crawled video datasets, but lack explicit spatial annotations, making them suitable primarily for video-to-spatial audio generation. BinauralMusic focuses on musical content, offering instrument class tags. BEWO-1M <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib34\" title=\"\">2024</a>)</cite> combines crawled content and simulated binaural rendering, and provides image or GPT-generated prompts. STARSS23 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> features real-world FOA and multi-channel audio alongside synchronized videos and includes sound class and position labels.\nIn contrast to prior datasets, MRSAudio offers a comprehensive, large-scale, and real-world spatial audio corpus, featuring 484 hours of recorded data in both FOA and binaural formats, covering all audio domains including general audio, speech, singing, and music. Uniquely, MRSAudio includes synchronized audio, video, and 3D positional geometry, along with fine-grained cross-modal annotations, such as: transcripts, word and phoneme boundaries and music scores.\nThese features make MRSAudio an ideal resource for a broad range of spatial audio generation and understanding tasks, including spatial speech, music, and singing voice synthesis.</p>\n\n",
                "matched_terms": [
                    "foa",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span> In MRSDialogue, audio is recorded using a professional binaural recording head and high-resolution sound cards, while synchronized video is captured using industry-standard cameras. In MRSSound, in addition to binaural audio and exocentric video, we also captured FOA (Zoom H3-VR) and egocentric video (Gopro camera).To ensure the effectiveness of the egocentric video, participants are asked to remain within the frontal field of view of the binaural recording head.</p>\n\n",
                "matched_terms": [
                    "foa",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span> To introduce spatial variability, we select four recording rooms that differ in size and acoustic material. Each speaker reads inflected emotions from the scripts while walking through the space, producing dynamic spatial cues. Recordings include spoken passages, binaural audio and exocentric video, as well as a clean mono audio by a lavalier microphone placed near the speaker.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span> The professional singers perform according to musical scores. To introduce variation in source&#8211;listener geometry, we adjust the position of the head-mounted binaural microphone relative to the singer. In addition to the binaural audio and synchronized video, each session includes a clean vocal track recorded with a studio-grade condenser microphone.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span> We record 23 Traditional Chinese, Western and Electronic instruments, performed by 45\nprofessional musicians.To capture rich spatial detail, we vary microphone placement around the instrument. Recordings include binaural audio, monaural audio, exocentric video, and synchronized video that records playing gestures. Full recording details are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.SS2\" title=\"A.2 Details of Recording &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To demonstrate the quality and utility of MRSAudio in real-world scenarios, we evaluate it on five representative spatial audio tasks: (i) audio spatialization, (ii) spatial text-to-speech, (iii) spatial singing voice synthesis, (iv) spatial music generation, and (v) sound event localization and detection (SELD). These tasks cover both generation and understanding, and are critical for applications such as AR/VR, spatial media production, and perceptual scene analysis.\nAll experiments are conducted using state-of-the-art methods on a server equipped with eight NVIDIA RTX 4090 GPUs.\nFor different tasks, we employ distinct objective metrics. For generation tasks, we compute cosine similarity scores for direction (ANG Cos) and distance (DIS Cos) to quantify spatial alignment quality. Additionally, we utilize subjective MOS-Q (Mean Opinion Score for Quality) to evaluate the quality of generated audio and MOS-P (Mean Opinion Score for Position) to assess spatial perception. For implementation details of training and evaluation metrics, please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2\" title=\"Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "seld",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio spatialization aims to synthesize spatially immersive soundscapes from monaural inputs and source positional information. To evaluate MRSAudio on this task, we adopt BinauralGrad&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Leng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib26\" title=\"\">2022</a>)</cite>, a diffusion-based generation model that predicts binaural waveforms conditioned on monaural audio and source coordinates. Since the downstream generation tasks can be formulated as predicting interaural (binaural) representations from mono audio, we employ a single-stage training scheme for all experiments. For comparison, we include a traditional signal processing baseline (DSP), which renders binaural audio using virtual source positions simulated via room impulse responses (RIRs) and head-related transfer functions (HRTFs).\nWe adopt the following objective metrics to evaluate audio quality:\n(1) W-L2: waveform L2 distance,\n(2) A-L2: amplitude envelope L2 distance,\n(3) P-L2: phase difference L2,\n(4) STFT: multi-resolution STFT loss,\n(5) PESQ: perceptual speech quality score.\nResults for the ground truth and DSP baseline are obtained by averaging across all test sets from the four MRSAudio subsets. Further details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS3\" title=\"B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This task evaluates the ability to detect and localize sound events using MRSAudio&#8217;s spatial annotations. We follow STARSS23&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shimada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib33\" title=\"\">2023</a>)</cite> using both audio-only and audio-visual variants on the MRSSound subset.\nIn the audio-only condition, models receive either FOA or binaural waveforms as input and predict sound event classes along with 3D source coordinates. For the audio-visual condition, we extract bounding boxes of visible persons to serve as coarse visual priors, which are fused with the audio representations. We also explore architectural variations by replacing the original convolutional backbone with a Transformer encoder, allowing us to assess the impact of temporal modeling capacity on spatial prediction.\nWe evaluate model performance using four standard joint detection and localization metrics, including location-aware detection (<math alttext=\"F_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">F_{20^{\\circ}}</annotation></semantics></math>, <math alttext=\"ER_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><msup><mn>20</mn><mo>&#8728;</mo></msup></msub></mrow><annotation encoding=\"application/x-tex\">ER_{20^{\\circ}}</annotation></semantics></math>) and class-aware localization (<math alttext=\"LE_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m3\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>E</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LE_{CD}</annotation></semantics></math>, <math alttext=\"LR_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>R</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">LR_{CD}</annotation></semantics></math>). Details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.SS7\" title=\"B.7 Sound Event Localization and Detection &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">B.7</span></a>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "foa",
                    "binaural",
                    "transformer",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#S4.T6\" title=\"Table 6 &#8227; 4.5 Sound Event Localization and Detection &#8227; 4 Benchmarks &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, model performance varies with architecture, input modality, and audio format. Transformer-based models generally outperform ConvNet baselines, particularly in reducing localization error. FOA input consistently yields better results than binaural audio, benefiting from richer spatial representation. For example, the Transformer with FOA and no visual input achieves the lowest error rate (ER<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m1\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> 0.99) and highest localization recall (LR<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">CD</span></sub> 87.32). The addition of visual features improves performance in some ConvNet settings (e.g., F<math alttext=\"{}_{20^{\\circ}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p2.m3\" intent=\":literal\"><semantics><msub><mi/><msup><mn>20</mn><mo>&#8728;</mo></msup></msub><annotation encoding=\"application/x-tex\">{}_{20^{\\circ}}</annotation></semantics></math> rises from 9.33 to 13.00 with FOA), but offers limited or inconsistent gains in Transformer models, possibly due to modality mismatch or redundancy. These results highlight the value of MRSAudio&#8217;s spatial annotations and multimodal streams in supporting flexible evaluation under both unimodal and multimodal configurations.</p>\n\n",
                "matched_terms": [
                    "input",
                    "foa",
                    "binaural",
                    "transformer",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce MRSAudio, a large-scale, multimodal spatial audio corpus designed to support a wide range of generation and understanding tasks. MRSAudio comprises four complementary modules, MRSLife, MRSSpeech , MRSSing, and MRSMusic, each captured with binaural/FOA audio, synchronized video, precise 3D pose metadata, and richly detailed annotations (event labels, transcripts, phoneme boundaries, lyrics, musical scores, and motion prompts). Through extensive benchmarks on audio spatialization, binaural speech, singing, and music generation , as well as sound event localization and detection, we demonstrate that MRSAudio&#8217;s scale, diversity, and annotation depth enable state&#8208;of&#8208;the&#8208;art performance and unlock new avenues for spatial audio research.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Directions:</span>\nWhile MRSAudio offers broad multimodal coverage, two limitations remain. First, although synchronized video is provided for all recordings, current benchmarks only explore visual input in a limited subset of tasks. Second, while the dataset includes both binaural and First-Order Ambisonic (FOA) formats, the FOA subset is smaller in scale, and most tasks focus on binaural audio, limiting spatial modeling diversity.\nFuture work will expand the role of visual modalities in tasks such as sound localization and scene understanding. We also plan to increase FOA recordings to balance data distribution and support broader spatial audio research, and develop more FOA-specific benchmarks to better utilize ambisonic spatial cues.</p>\n\n",
                "matched_terms": [
                    "foa",
                    "binaural",
                    "input",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on predefined scenario requirements, we recruit professionals from relevant fields to participate in the recording process. To ensure spatial diversity, we rent various venues tailored to each scenario type. Following the FAIR-Play protocol, we use a 3Dio Free Space XLR binaural microphone to capture spatial audio, GoPro HERO cameras and mobile phones to record exocentric and egocentric videos, and UWB-based tracking systems to capture motion trajectories.\nOnce personnel, locations, and equipment are secured, we prepare task-specific materials for each subset. For MRSLife, we further divide the content based on the proportion of speech involved, resulting in two subcategories: MRSDialogue (e.g., group games, board games) and MRSSound (e.g., kungfu, kitchens, offices, sports). We predefine common sound events for each, such as clattering dishes, keyboard typing, and table tennis.\nFor MRSSpeech, we compile a large corpus of scripts from movies, TV shows, and crosstalk performances and automatically extract dialogue passages for speaker delivery.\nFor MRSSing, we design lyrics in four languages (Chinese, English, German, and French) and recruit singers across a range of vocal types to maximize diversity.\nFor MRSMusic, we collect solo performances across 23 traditional and modern instruments, covering a wide array of timbres and playing techniques.\nSpecific sound categories for each subset are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.T7\" title=\"Table 7 &#8227; A.1 Details of Planning &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio is recorded in WAV format at a sampling rate of 48 kHz. Video is recorded at a minimum resolution of 1080p and 24 frames per second, and is later standardized to this format during post-processing.</p>\n\n",
                "matched_terms": [
                    "frames",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSLife:</span>\nWe recruit 62 participants to perform daily activities including board games, cooking, exercise, and office work. In MRSDialogue, each participant is compensated $30 per recorded hour. Binaural audio is captured using head-mounted microphones, and third-person video is recorded. In MRSSound scenes such as kung fu or kitchen demonstrations, performers are paid $50 per hour. Both binaural and FOA (First-Order Ambisonics) recordings are collected, along with first-person and third-person video. The binaural dummy head is co-located with the egocentric camera and the Zoom H3-VR (Ambisonic recorder). The egocentric camera is rigidly aligned with the head&#8217;s gaze to capture first-person visuals, while the Ambisonic recorder is mounted 7.5 cm above the head. An exocentric camera is placed at a surveyed position in the scene to provide a third-person view of the environment and object relationships.\nParticipants receive brief scene descriptions and are asked to act naturally while maintaining a single active sound source when possible. The total duration for MRSLife recordings reaches approximately 150 hours.</p>\n\n",
                "matched_terms": [
                    "foa",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSpeech:</span>\nWe employ 44 expressive speakers to read from scripted texts, each paid $30 per hour of recorded audio. The total recorded duration reaches 200 hours, with compensation totaling $40,000. During recording sessions, speakers alternate between standing and walking around the recording area to introduce spatial diversity while maintaining speech clarity. Binaural audio is captured using head-mounted microphones, and clean monaural speech is recorded using lavalier microphones. All sessions are filmed from a third-person perspective.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSSing:</span>\nEighteen professional singers participate in the recordings. Each singer is fluent in at least one of the following languages: Chinese, English, German, or French, and collectively they cover all vocal ranges including soprano, alto, tenor, and bass. Performers are paid $50 per hour, contributing a total of 80 hours of audio. Singing is recorded at a fixed position to ensure spatial consistency. Binaural recordings are captured with head-mounted microphones, and monaural audio is recorded with a studio-grade microphone. Third-person panoramic video is also captured.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MRSMusic:</span>\nWe engage 45 instrumentalists performing 23 Traditional Chinese, Western and Electronic instruments such as violin, erhu, pipa, electric guitar, and keyboard. Each performer receives $60 per recorded hour. A total of 69 hours of solo music performances are recorded. Audio is captured using both head-mounted binaural microphones and reference monaural microphones. Third-person video provides a full-scene view, while first-person cameras are used to capture detailed playing gestures.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Synchronization:</span>\nIn addition to binaural audio, each clip is synchronized with the following modalities: audio, text, video, and position data. Using the segment timestamps, we align all streams temporally. For static sources, we attach manually recorded 3D coordinates; for dynamic sources, we interpolate Ultra-Wideband (UWB) tracking data over the segment duration.\nThis process yields fully synchronized multimodal clips ready for downstream spatial audio tasks.\nFurthermore, for segments where participants&#8217; faces are visible, we apply anonymization by using a face detection model to overlay digital masks during post-processing.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding generalization capacity, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F8\" title=\"Figure 8 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(d) shows that audio segment durations range from approximately 4 to 11 seconds, matching typical training input lengths. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A1.F8\" title=\"Figure 8 &#8227; A.5.3 Statistics of MRSSing &#8227; A.5 Statistics of MRSAudio &#8227; Appendix A Details of Dataset &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>(e) demonstrates that the dataset covers a full range of musical pitch values, supporting tasks that require robust pitch generalization.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We measure waveform similarity using Wave L2, the mean squared error (MSE) between the generated and reference binaural waveforms. Amplitude L2 and Phase L2 are computed after applying Short-Time Fourier Transform (STFT), reflecting errors in the magnitude and phase components respectively. MRSTFT loss<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/csteinmetz1/auraloss\" title=\"\">https://github.com/csteinmetz1/auraloss</a></span></span></span> is also used, combining spectral convergence, log-magnitude, and linear-magnitude terms for better spectral alignment.\nIn addition, we use the perceptual evaluation metric PESQ<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/aliutkus/speechmetrics\" title=\"\">https://github.com/aliutkus/speechmetrics</a></span></span></span> to assess audio quality for speech-related tasks. Since PESQ is designed for speech, it is omitted for MRSLife and MRSMusic. For all metrics except PESQ, lower values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate spatial consistency, we use Spatial-AST<cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#bib.bib44\" title=\"\">2024</a>)</cite> to extract angular and distance embeddings from the binaural audio. Since Spatial-AST predicts positions only for static sources, we compute the cosine similarity between the predicted and ground truth embeddings within 1-second segments and average the results to assess overall spatial fidelity.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build upon BinauralGrad&#8217;s two-stage diffusion-based framework to convert monaural audio into spatial audio. However, in our subsequent generation experiments, the synthesized monaural audio typically corresponds to a centrally positioned source between the ears, effectively serving as the first stage of BinauralGrad. Therefore, for the spatialization experiments presented here, we use only the second stage of BinauralGrad to validate spatial audio generation.\nSpecifically, we first convert binaural recordings to monaural input by averaging the two channels. Next, we apply a DSP-based method to produce a coarse spatial approximation of the binaural signal. This monaural input and its simulated binaural counterpart are then used as input to the BinauralGrad model, which is conditioned on the object&#8217;s motion trajectory to generate spatialized binaural audio.\nWe list the architecture and hyperparameters of BinauralGrad in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T8\" title=\"Table 8 &#8227; B.3 Audio Spatialization &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "channels",
                    "input",
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune two pre-trained models, CosyVoice and F5-TTS, using monaural audio obtained by averaging the binaural recordings from the MRSSpeech subset. This allows the models to learn the generation characteristics specific to our dataset. After monaural generation, we apply a pre-trained spatialization model trained on MRSSpeech to convert the output into spatialized binaural audio. For the ISDrama baseline, we adopt the model variant proposed in the original paper that conditions generation on predefined spatial paths to produce binaural spatial speech directly.</p>\n\n",
                "matched_terms": [
                    "binaural",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the ISDrama baseline, we adopt the spatial-path-conditioned generation framework and extend it by incorporating Rmssinger&#8217;s note encoder, allowing explicit pitch control through musical score input.\nWe use the single stage variation of Rmssinger, a standard singing voice synthesis model, and train it with monaural targets derived from the averaged binaural recordings in the MRSSing subset. This enables the model to generate pitch-accurate audio that reflects the acoustic characteristics. The synthesized monaural audio is then spatialized using the spatialization model trained on MRSSing.\nWe list the architecture and hyperparameters of Rmssinger in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T9\" title=\"Table 9 &#8227; B.4 Spatial Text to Speech &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "binaural",
                    "input",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For spatial music generation, in the ISDrama setting, we follow the path-conditioned generation pipeline but remove the phoneme encoder, using only the note encoder to process symbolic scores as the sole control condition for generating spatialized music.\nWe also adopt Make-An-Audio 2 as our base model and train it using monaural audio derived from averaged MRSMusic binaural recordings. We use the pretrained spectrogram autoencoder. Instrument category and symbolic score information are concatenated into the text prompt to guide the music generation process. The generated audio is subsequently spatialized using the MRSMusic-trained spatialization model. We list the hyper-parameters of Make-An-Audio 2 in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10396v3#A2.T10\" title=\"Table 10 &#8227; B.6 Spatial Music Generation &#8227; Appendix B Details of Experiments &#8227; MRSAudio: A Large-Scale Multimodal Recorded Spatial Audio Dataset with Refined Annotations\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "hyperparameters",
                    "binaural",
                    "audio"
                ]
            }
        ]
    }
}