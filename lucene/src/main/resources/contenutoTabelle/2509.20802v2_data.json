{
    "S3.T1.st1": {
        "source_file": "SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS",
        "caption": "(a) Absolute zero-shot evaluation results on Seed-TTS Eval set and LibriTTS test-clean.\nHigher values indicate better performance for NMOS, SS, and UTMOS, while lower values indicate better performance for RTF and WER.\nMissing entries correspond to trivial cases, e.g., human records or codec/vocoder resynthesis of groundtruth (GT).",
        "body": "Model\nLayers\nParams\nRTF\nNMOS\nSeed-TTS test-en\nLibriTTS test-clean\n\n\nWER\nSS\nUTMOS\nWER\nSS\nUTMOS\n\n\nHuman Record\n-\n-\n-\n3.96±0.143.96\\pm 0.14\n1.471.47\n1.001.00\n3.523.52\n1.851.85\n1.001.00\n4.144.14\n\n\n\n\nVocoder Resyn. of GT\n-\n-\n-\n3.87±0.153.87\\pm 0.15\n1.531.53\n1.001.00\n3.533.53\n1.651.65\n1.001.00\n4.014.01\n\n\n\nCosyVoice 2 44footnotemark: 4 [28]\n\n2424\n0.630.63\n0.610.61\n3.71±0.133.71\\pm 0.13\n2.03{2.03}\n0.66{0.66}\n4.154.15\n1.43{1.43}\n0.810.81\n4.41{4.41}\n\n\nCosyVoice 2 + Ours\n12{12}\n0.38{0.38}\n0.35{0.35}\n3.58±0.14{3.58\\pm 0.14}\n2.712.71\n0.66{0.66}\n4.16{4.16}\n1.591.59\n0.82{0.82}\n4.41{4.41}\n\n\nCosyVoice 2 + Ours\n9{9}\n0.32{0.32}\n0.33{0.33}\n3.55±0.14{3.55\\pm 0.14}\n3.093.09\n0.66{0.66}\n4.15{4.15}\n1.941.94\n0.81{0.81}\n4.40{4.40}\n\n\nCodec Resyn. of GT\n-\n-\n-\n3.59±0.153.59\\pm 0.15\n2.492.49\n1.001.00\n3.693.69\n2.522.52\n1.001.00\n4.014.01\n\n\n\nLLaSA 44footnotemark: 4 [6]\n\n1616\n1.71.7\n0.820.82\n3.37±0.15{3.37\\pm 0.15}\n3.54{3.54}\n0.46{0.46}\n4.13{4.13}\n1.54{1.54}\n0.47{0.47}\n4.414.41\n\n\nLLaSA + Ours\n8{8}\n1.3{1.3}\n0.58{0.58}\n3.11±0.143.11\\pm 0.14\n4.204.20\n0.410.41\n4.064.06\n1.881.88\n0.430.43\n4.40{4.40}",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Layers</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Params</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RTF</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">NMOS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Seed-TTS <span class=\"ltx_text ltx_font_italic\">test-en</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LibriTTS <span class=\"ltx_text ltx_font_italic\">test-clean</span></span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UTMOS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UTMOS</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Human Record</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"3.96\\pm 0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m1\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.96</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.14</mn></mrow><annotation encoding=\"application/x-tex\">3.96\\pm 0.14</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"1.47\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m2\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.47</mn><annotation encoding=\"application/x-tex\">1.47</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"1.00\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m3\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.00</mn><annotation encoding=\"application/x-tex\">1.00</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"3.52\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m4\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3.52</mn><annotation encoding=\"application/x-tex\">3.52</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"1.85\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m5\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.85</mn><annotation encoding=\"application/x-tex\">1.85</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"1.00\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m6\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.00</mn><annotation encoding=\"application/x-tex\">1.00</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"4.14\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m7\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.14</mn><annotation encoding=\"application/x-tex\">4.14</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Vocoder Resyn. of GT</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.87\\pm 0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m8\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.87</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.15</mn></mrow><annotation encoding=\"application/x-tex\">3.87\\pm 0.15</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1.53\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m9\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.53</mn><annotation encoding=\"application/x-tex\">1.53</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1.00\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m10\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.00</mn><annotation encoding=\"application/x-tex\">1.00</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.53\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m11\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3.53</mn><annotation encoding=\"application/x-tex\">3.53</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1.65\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m12\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.65</mn><annotation encoding=\"application/x-tex\">1.65</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1.00\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m13\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.00</mn><annotation encoding=\"application/x-tex\">1.00</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"4.01\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m14\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.01</mn><annotation encoding=\"application/x-tex\">4.01</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice 2&#160;</span><span class=\"ltx_note ltx_role_footnotemark\" id=\"footnotex1\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_type\">footnotemark: </span><span class=\"ltx_tag ltx_tag_note\">4</span></span></span></span><span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib28\" title=\"\">28</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"24\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m15\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">24</mn><annotation encoding=\"application/x-tex\">24</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.63\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m16\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.63</mn><annotation encoding=\"application/x-tex\">0.63</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.61\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m17\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.61</mn><annotation encoding=\"application/x-tex\">0.61</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.71\\pm 0.13\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m18\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.71</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.13</mn></mrow><annotation encoding=\"application/x-tex\">3.71\\pm 0.13</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{2.03}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m19\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2.03</mn><annotation encoding=\"application/x-tex\">{2.03}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.66}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m20\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.66</mn><annotation encoding=\"application/x-tex\">{0.66}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"4.15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m21\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.15</mn><annotation encoding=\"application/x-tex\">4.15</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{1.43}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m22\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.43</mn><annotation encoding=\"application/x-tex\">{1.43}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.81\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m23\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.81</mn><annotation encoding=\"application/x-tex\">0.81</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{4.41}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m24\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.41</mn><annotation encoding=\"application/x-tex\">{4.41}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice 2 + Ours</span></th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{12}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m25\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">12</mn><annotation encoding=\"application/x-tex\">{12}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.38}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m26\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.38</mn><annotation encoding=\"application/x-tex\">{0.38}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.35}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m27\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.35</mn><annotation encoding=\"application/x-tex\">{0.35}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{3.58\\pm 0.14}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m28\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.58</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.14</mn></mrow><annotation encoding=\"application/x-tex\">{3.58\\pm 0.14}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.71\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m29\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2.71</mn><annotation encoding=\"application/x-tex\">2.71</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.66}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m30\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.66</mn><annotation encoding=\"application/x-tex\">{0.66}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{4.16}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m31\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.16</mn><annotation encoding=\"application/x-tex\">{4.16}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m32\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.59</mn><annotation encoding=\"application/x-tex\">1.59</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.82}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m33\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.82</mn><annotation encoding=\"application/x-tex\">{0.82}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{4.41}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m34\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.41</mn><annotation encoding=\"application/x-tex\">{4.41}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice 2 + Ours</span></th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{9}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m35\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">9</mn><annotation encoding=\"application/x-tex\">{9}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.32}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m36\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.32</mn><annotation encoding=\"application/x-tex\">{0.32}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.33}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m37\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.33</mn><annotation encoding=\"application/x-tex\">{0.33}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{3.55\\pm 0.14}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m38\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.55</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.14</mn></mrow><annotation encoding=\"application/x-tex\">{3.55\\pm 0.14}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.09\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m39\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3.09</mn><annotation encoding=\"application/x-tex\">3.09</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.66}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m40\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.66</mn><annotation encoding=\"application/x-tex\">{0.66}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{4.15}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m41\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.15</mn><annotation encoding=\"application/x-tex\">{4.15}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.94\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m42\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.94</mn><annotation encoding=\"application/x-tex\">1.94</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.81}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m43\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.81</mn><annotation encoding=\"application/x-tex\">{0.81}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{4.40}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m44\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.40</mn><annotation encoding=\"application/x-tex\">{4.40}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Codec Resyn. of GT</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.59\\pm 0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m45\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.59</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.15</mn></mrow><annotation encoding=\"application/x-tex\">3.59\\pm 0.15</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.49\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m46\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2.49</mn><annotation encoding=\"application/x-tex\">2.49</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1.00\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m47\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.00</mn><annotation encoding=\"application/x-tex\">1.00</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.69\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m48\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3.69</mn><annotation encoding=\"application/x-tex\">3.69</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.52\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m49\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2.52</mn><annotation encoding=\"application/x-tex\">2.52</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1.00\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m50\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.00</mn><annotation encoding=\"application/x-tex\">1.00</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"4.01\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m51\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.01</mn><annotation encoding=\"application/x-tex\">4.01</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">LLaSA&#160;</span><span class=\"ltx_note ltx_role_footnotemark\" id=\"footnotex2\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_type\">footnotemark: </span><span class=\"ltx_tag ltx_tag_note\">4</span></span></span></span><span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"16\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m52\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">16</mn><annotation encoding=\"application/x-tex\">16</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m53\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.7</mn><annotation encoding=\"application/x-tex\">1.7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.82\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m54\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.82</mn><annotation encoding=\"application/x-tex\">0.82</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{3.37\\pm 0.15}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m55\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.37</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.15</mn></mrow><annotation encoding=\"application/x-tex\">{3.37\\pm 0.15}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{3.54}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m56\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3.54</mn><annotation encoding=\"application/x-tex\">{3.54}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.46}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m57\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.46</mn><annotation encoding=\"application/x-tex\">{0.46}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{4.13}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m58\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.13</mn><annotation encoding=\"application/x-tex\">{4.13}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{1.54}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m59\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.54</mn><annotation encoding=\"application/x-tex\">{1.54}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.47}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m60\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.47</mn><annotation encoding=\"application/x-tex\">{0.47}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"4.41\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m61\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.41</mn><annotation encoding=\"application/x-tex\">4.41</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">LLaSA + Ours</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"{8}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m62\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">8</mn><annotation encoding=\"application/x-tex\">{8}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"{1.3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m63\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.3</mn><annotation encoding=\"application/x-tex\">{1.3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"{0.58}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m64\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.58</mn><annotation encoding=\"application/x-tex\">{0.58}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"3.11\\pm 0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m65\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.11</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.14</mn></mrow><annotation encoding=\"application/x-tex\">3.11\\pm 0.14</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"4.20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m66\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.20</mn><annotation encoding=\"application/x-tex\">4.20</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"0.41\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m67\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.41</mn><annotation encoding=\"application/x-tex\">0.41</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"4.06\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m68\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.06</mn><annotation encoding=\"application/x-tex\">4.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"1.88\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m69\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1.88</mn><annotation encoding=\"application/x-tex\">1.88</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m70\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"{4.40}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st1.m71\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.40</mn><annotation encoding=\"application/x-tex\">{4.40}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "cases",
            "resynthesis",
            "absolute",
            "cosyvoice",
            "387±015387pm",
            "codecvocoder",
            "44footnotemark",
            "seedtts",
            "337±015337pm",
            "ours",
            "358±014358pm",
            "utmos",
            "testclean",
            "lower",
            "records",
            "better",
            "355±014355pm",
            "nmos",
            "zeroshot",
            "359±015359pm",
            "human",
            "vocoder",
            "indicate",
            "wer",
            "371±013371pm",
            "311±014311pm",
            "results",
            "trivial",
            "rtf",
            "llasa",
            "layers",
            "model",
            "params",
            "record",
            "evaluation",
            "values",
            "codec",
            "performance",
            "groundtruth",
            "testen",
            "set",
            "entries",
            "eval",
            "missing",
            "libritts",
            "correspond",
            "while",
            "higher",
            "396±014396pm",
            "resyn"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The goal of this paper is to introduce <span class=\"ltx_text ltx_font_bold\">SPADE</span>, a framework for <span class=\"ltx_text ltx_font_bold\">S</span>tructured <span class=\"ltx_text ltx_font_bold\">P</span>runing and <span class=\"ltx_text ltx_font_bold\">A</span>daptive <span class=\"ltx_text ltx_font_bold\">D</span>istillation for <span class=\"ltx_text ltx_font_bold\">E</span>fficient Large Language Model-based text-to-speech (LLM-TTS). Recent LLM-TTS systems achieve strong controllability and zero-shot generalization, but their large parameter counts and high latency limit real-world deployment. SPADE addresses this by combining (i) a <span class=\"ltx_text ltx_font_italic\">pruning</span> step guided by a word-error-rate-based layer importance index to remove non-essential Transformer layers, with (ii) multi-level <span class=\"ltx_text ltx_font_italic\">knowledge distillation</span> to restore autoregressive coherence. On zero-shot benchmarks, SPADE preserves near-parity perceptual quality while halving Transformer depth, reducing VRAM usage by up to <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math>, and achieving up to <math alttext=\"1.7\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mrow><mn>1.7</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">1.7\\times</annotation></semantics></math> faster real-time factor with less than <math alttext=\"5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">5\\%</annotation></semantics></math> of the original training data. These results show that compact LLM-TTS models can maintain naturalness and speaker similarity while enabling practical real-time speech generation. Audio samples are available at <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mm.kaist.ac.kr/projects/SPADE/\" title=\"\">https://mm.kaist.ac.kr/projects/SPADE/</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "results",
                    "while",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large Language Model (LLM)-based text-to-speech (LLM-TTS) systems such as CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib2\" title=\"\">2</a>]</cite>, VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib3\" title=\"\">3</a>]</cite>, CLaM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib4\" title=\"\">4</a>]</cite>, RALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib5\" title=\"\">5</a>]</cite>, and LLaSA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib6\" title=\"\">6</a>]</cite> have shown advanced controllability, prosody modeling, and zero-shot generalization across speakers and languages. Early approaches trained LLM backbones directly on speech tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib7\" title=\"\">7</a>]</cite>, while recent methods initialize from pretrained text LLMs (e.g., LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib8\" title=\"\">8</a>]</cite>, Qwen2.5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib9\" title=\"\">9</a>]</cite>) and adapt them with speech objectives&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib10\" title=\"\">10</a>]</cite>. Leveraging rich contextual representations, these systems synthesize natural speech conditioned on long prompts, speaker embeddings, and control tokens, pushing TTS closer to human-level performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cosyvoice",
                    "zeroshot",
                    "while",
                    "performance",
                    "llasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present <span class=\"ltx_text ltx_font_bold\">SPADE</span>, a framework for <span class=\"ltx_text ltx_font_bold\">S</span>tructured <span class=\"ltx_text ltx_font_bold\">P</span>runing and <span class=\"ltx_text ltx_font_bold\">A</span>daptive <span class=\"ltx_text ltx_font_bold\">D</span>istillation for <span class=\"ltx_text ltx_font_bold\">E</span>fficient LLM-TTS. By removing non-essential layers through a Word Error Rate (WER) based layer importance index and recovering performance via multi-level distillation, SPADE achieves substantial efficiency gains while preserving perceptual quality. Across zero-shot benchmarks, SPADE halves Transformer depth, reduces overall parameters by up to <math alttext=\"40\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>40</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">40\\%</annotation></semantics></math>, lowers VRAM consumption by up to <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math>, and accelerates inference by as much as <math alttext=\"1.7\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mrow><mn>1.7</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">1.7\\times</annotation></semantics></math>, all while retaining near-parity in perceptual metrics. Moreover, the knowledge distillation process shows remarkably high data-efficiency: The recovery of performance requires less than <math alttext=\"5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m4\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">5\\%</annotation></semantics></math> of the pretraining data of the original checkpoints. These results highlight the framework of pruning and distillation as a practical pathway toward compact, high-fidelity, real-time speech generation.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "zeroshot",
                    "while",
                    "wer",
                    "performance",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework first explores the importance of each layer in LLM backbone and prune non-essential layers to compress the model.\nSubsequently, an efficient knowledge distillation is applied to effectively restore the performance of the pruned model.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "layers",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SPADE is motivated by using the nature of residual connections: <math alttext=\"x^{l}=x^{l-1}+f_{l}(x^{l-1})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>x</mi><mi>l</mi></msup><mo>=</mo><mrow><msup><mi>x</mi><mrow><mi>l</mi><mo>&#8722;</mo><mn>1</mn></mrow></msup><mo>+</mo><mrow><msub><mi>f</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mrow><mi>l</mi><mo>&#8722;</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x^{l}=x^{l-1}+f_{l}(x^{l-1})</annotation></semantics></math>, where <math alttext=\"x^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>l</mi></msup><annotation encoding=\"application/x-tex\">x^{l}</annotation></semantics></math> and <math alttext=\"f^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><msup><mi>f</mi><mi>l</mi></msup><annotation encoding=\"application/x-tex\">f^{l}</annotation></semantics></math> denote the hidden state and transformation in the layer <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>, respectively.\nIn LLM-TTS, each transformer layer contributes by refining latent representations through residual connections&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib22\" title=\"\">22</a>]</cite>.\nAlthough the architecture shows strong performance in diverse applications, recent studies in text-based LLM suggest some layers provide only weak refinements and can be removed with little effect&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib23\" title=\"\">23</a>]</cite>.\nAn established criterion to identify them is to compute the cosine distance between inputs and outputs of a layer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib24\" title=\"\">24</a>]</cite>.\nHowever, Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that cosine-based layer importance (CLI) does not align with the performance contribution of layers in TTS, as indicated by the different patterns in WER change.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "layers",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\theta}_{\\setminus i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mrow><mo rspace=\"0em\">&#8726;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\theta}_{\\setminus i}</annotation></semantics></math> denotes the model parameters without the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th layer, <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> is a subset of the evaluation set, and <math alttext=\"({\\bm{x}}_{1},{\\bm{y}}_{1})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119961;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#119962;</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">({\\bm{x}}_{1},{\\bm{y}}_{1})</annotation></semantics></math> and <math alttext=\"({\\bm{x}}_{2},{\\bm{y}}_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119961;</mi><mn>2</mn></msub><mo>,</mo><msub><mi>&#119962;</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">({\\bm{x}}_{2},{\\bm{y}}_{2})</annotation></semantics></math> denote reference and query text-audio pairs, respectively.\nSpecifically, a layer is considered important only if its absence causes significant degradation in WER.\nUnlike cosine-based layer importance, which only estimates the difference between inputs and outputs of within a layer, WLI directly measures the contribution of each layer to the final performance.\nAs shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> evaluated WLI with Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib25\" title=\"\">25</a>]</cite>, and found that many of the layers have negligible impact on performance, indicating a significant redundancy and aligning with the findings in text-domain LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib13\" title=\"\">13</a>]</cite>.\nMoreover, we find that the earliest, central, and final layers consistently emerge as important across different LLM-TTS models.\nBased on the analysis, we prune transformer layers with low WLI values from the LLM backbones.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "model",
                    "evaluation",
                    "values",
                    "wer",
                    "performance",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the proposed framework effectively reduces the parameters without additional modules, such as parameter-efficient fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib27\" title=\"\">27</a>]</cite>, the pruned model naturally confronts disconnected flow of latent information.\nTo address this, we leverage the original un-pruned model as the teacher and perform a knowledge distillation training that simply <span class=\"ltx_text ltx_font_italic\">heals</span> the pruned model to minimize the loss of performance without any additional parameters.\nTo maximize the restoration, we employ a composite loss that benefits from both supervised learning and teacher-guided knowledge distillation:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, the Cross-Entropy loss <math alttext=\"\\mathcal{L}_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{CE}</annotation></semantics></math> is responsible for the <span class=\"ltx_text ltx_font_bold\">supervised</span> component that directly guides the output distribution.\nThe <span class=\"ltx_text ltx_font_bold\">distillation</span> component comprises 4 elements: Embedding reconstruction loss <math alttext=\"\\mathcal{L}_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>e</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{e}</annotation></semantics></math>, alignment losses on logit <math alttext=\"\\mathcal{L}_{logit}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{logit}</annotation></semantics></math>, latent <math alttext=\"\\mathcal{L}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{l}</annotation></semantics></math>, and attention <math alttext=\"\\mathcal{L}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{a}</annotation></semantics></math>, following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib24\" title=\"\">24</a>]</cite>.\nTo provide more stability, we implement the <math alttext=\"\\mathcal{L}_{logit}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{logit}</annotation></semantics></math> with mixed distribution by leveraging Skew KL Divergence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib16\" title=\"\">16</a>]</cite>.\nFor <math alttext=\"\\mathcal{L}_{e},\\mathcal{L}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m7\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>e</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>l</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{e},\\mathcal{L}_{l}</annotation></semantics></math>, and <math alttext=\"\\mathcal{L}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{a}</annotation></semantics></math>, we calculate Mean Squared Error (MSE) of embedding outputs, intermediate latents, and attention matrices between the teacher and student, respectively.\nTo maximize the distillation of teacher&#8217;s knowledge, we propose to dynamically select layers to apply loss for <math alttext=\"\\mathcal{L}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{l}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{a}</annotation></semantics></math>.\nSpecifically, as shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the target values (latent, attention map) for the student layer (<math alttext=\"l_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m11\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">l_{n}</annotation></semantics></math>) are derived from the last layer before the next retained layer (<math alttext=\"l_{m+2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m12\" intent=\":literal\"><semantics><msub><mi>l</mi><mrow><mi>m</mi><mo>+</mo><mn>2</mn></mrow></msub><annotation encoding=\"application/x-tex\">l_{m+2}</annotation></semantics></math>) from the teacher model.\nWhile simple, this approach allows the pruned student model to retain not only its original capability but also those of the removed layers, culminating in a smaller yet more compact model.\nFinally, we combine the supervised and distillation components by adjustable weight <math alttext=\"alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m13\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">alpha</annotation></semantics></math> to balance the influence of each, where the value is empirically set to <math alttext=\"0.25\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m14\" intent=\":literal\"><semantics><mn>0.25</mn><annotation encoding=\"application/x-tex\">0.25</annotation></semantics></math> to deliberately provide stronger supervised guidance.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "model",
                    "values",
                    "while",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate whether pruning and distillation can truly enable compact yet high-fidelity LLM-TTS systems, we benchmark SPADE against two representative baselines: CosyVoice 2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/FunAudioLLM/CosyVoice\" title=\"\">https://github.com/FunAudioLLM/CosyVoice</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib28\" title=\"\">28</a>]</cite> and LLaSA-1B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/zhenye234/LLaSA_training\" title=\"\">https://github.com/zhenye234/LLaSA_training</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib6\" title=\"\">6</a>]</cite>. Both models are chosen for their strong zero-shot capabilities and public availability of checkpoints.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since a key motivation of SPADE is to minimize retraining cost, we fine-tune each pruned variant on only a fraction of the data: <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math> of LibriHeavy&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib29\" title=\"\">29</a>]</cite> (EN) for LLaSA and LibriTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib30\" title=\"\">30</a>]</cite> (EN) for CosyVoice 2. This corresponds to less than <math alttext=\"5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">5\\%</annotation></semantics></math> of the pretraining corpus size, testing whether our framework can recover quality under data-constrained conditions.</p>\n\n",
                "matched_terms": [
                    "libritts",
                    "llasa",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assess the performance on <span class=\"ltx_text ltx_font_italic\">LibriTTS test-clean</span> and the <span class=\"ltx_text ltx_font_italic\">Seed-TTS eval set</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib31\" title=\"\">31</a>]</cite>, both of which are widely used for zero-shot evaluation. All experiments are conducted on 4<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>NVIDIA A6000 GPUs, with official training scripts left unchanged aside from data size and pruning. CosyVoice 2 is fine-tuned with dynamic batches up to 20,000 tokens, whereas LLaSA uses a batch size of 4. Fine-tuning with SPADE runs for 7 epochs to CosyVoice 2 and for 1 epoch when applying it to LLaSA.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "eval",
                    "seedtts",
                    "evaluation",
                    "zeroshot",
                    "libritts",
                    "testclean",
                    "performance",
                    "set",
                    "llasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate both computational efficiency and perceptual quality, we consider a range of complementary metrics. Efficiency is assessed in terms of model depth, parameter count, and real-time factor (RTF), while intelligibility is measured using word error rate (WER). Perceptual aspects are captured objectively through speaker similarity (SS) and UTMOS using VERSA toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib32\" title=\"\">32</a>]</cite>, and subjectively through the naturalness mean opinion score (NMOS) with 20 listening and 50 random samples from both evaluation sets per model. Together, these metrics reveal whether compact models preserve the qualities essential for real-world TTS deployment.</p>\n\n",
                "matched_terms": [
                    "nmos",
                    "model",
                    "evaluation",
                    "utmos",
                    "while",
                    "wer",
                    "rtf"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S3.T1\" title=\"Table 1 &#8227; 3 Experimental Setup &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main results of applying SPADE to CosyVoice 2 and LLaSA with different configurations.\nFor CosyVoice, pruning to 12 layers halves the depth, reduces parameters by <math alttext=\"39.7\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>39.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">39.7\\%</annotation></semantics></math> and accelerates inference by <math alttext=\"42.6\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>42.6</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">42.6\\%</annotation></semantics></math>.\nMoreover, effective VRAM usage is reduced by <math alttext=\"14\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>14</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">14\\%</annotation></semantics></math>, as depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S3.F3\" title=\"Figure 3 &#8227; 3 Experimental Setup &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nNotably, these gains come with no significant degradation in both quantitative and qualitative metrics, showing slight increase of 0.68 in WER for challenging Seed-TTS dataset and 0.11 reduction in NMOS.\nA more aggressive variant with only 9 layers further reduces parameters by <math alttext=\"49.2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mn>49.2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">49.2\\%</annotation></semantics></math>, improves RTF by <math alttext=\"45.9\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mn>45.9</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">45.9\\%</annotation></semantics></math>, and lowers VRAM usage by <math alttext=\"17\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mn>17</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">17\\%</annotation></semantics></math>.\nWhile this extreme setting renders additional increase in WER, perceptual metrics such as NMOS, SS, and UTMOS remain stable, suggesting that SPADE enables flexible trade-offs between efficiency and intelligibility.\nTo confirm that our framework generalizes beyond a single backbone, we conduct experiment on LLaSA, a larger variant based on speech codec.\nHere, pruning removes half the layers, decreases parameters by <math alttext=\"23.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>23.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">23.5\\%</annotation></semantics></math>, improves RTF by <math alttext=\"29.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>29.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">29.3\\%</annotation></semantics></math> (<math alttext=\"1.41\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1.41</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">1.41\\times</annotation></semantics></math> speed-up), and reduces VRAM by <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math>.\nThe result suggests that, while most of the metrics lie in acceptable range, the performance degradation is relatively larger compared to CosyVoice 2.\nBased on the analysis in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a><span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Metrics are reported based on official checkpoints</span></span></span>, we expect it attributes to overall high WLI values across all layers, indicating each layer contributes similarly to the performance.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "nmos",
                    "cosyvoice",
                    "seedtts",
                    "values",
                    "utmos",
                    "while",
                    "wer",
                    "codec",
                    "performance",
                    "results",
                    "rtf",
                    "llasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the effectiveness of the proposed methodology with systematic ablation study.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S4.T2\" title=\"Table 2 &#8227; 4.2 Ablation study &#8227; 4 Results and Analysis &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents experiments on CosyVoice 2 with LibriTTS test-clean.\nFirst, the proposed WLI-based pruning is replaced with existing cosine-based pruning.\nThe result shows a notable increase in WER and CER, demonstrating the pruning based on WLI, a metric directly related to intelligibility, successfully prevents performance degradation.\nMoreover, dynamic distillation loss is removed and student layer is distilled only with the information from the corresponding teacher layer.\nThe overall decrease in performance suggests that adaptively choosing the target in distillation training is simple yet plays a significant role.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "libritts",
                    "testclean",
                    "wer",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented SPADE, a pruning-based framework for compressing LLM-TTS models, and showed that SPADE achieves substantial efficiency gains while preserving intelligibility and naturalness. We investigate the importance of each layer using the proposed WLI and find that many layers contribute little to audio synthesis.\nExperimental results verify that, by applying SPADE, such layers can be removed without harming perceived quality.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "results",
                    "while"
                ]
            }
        ]
    },
    "S3.T1.st2": {
        "source_file": "SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS",
        "caption": "(b) Relative performance of SPADE models compared to their uncompressed versions.\nFor data usage, the exact amount of English data in the internal LLaSA pretraining set is unknown; we therefore report an upper bound (≤\\leq).",
        "body": "Model Pair\nLayers\nParams\nRTF\nData\nWER\nSS\nUTMOS\n\n\n\n\n\nCosyVoice2 →\\rightarrow CosyVoice2 + Ours (12)\n\n↓50.0%\\downarrow 50.0\\%\n↓39.7%\\downarrow 39.7\\%\n↓42.6%\\downarrow 42.6\\%\n\n2​(0.3)%2(0.3)\\% ↓\\downarrow\n\n\n+0.43+0.43 ↑\\uparrow\n\n\n+0.005+0.005 ↑\\uparrow\n\n\n0.000.00 ↔\\leftrightarrow\n\n\n\n\nCosyVoice2 →\\rightarrow CosyVoice2 + Ours (9)\n\n↓62.5%\\downarrow 62.5\\%\n↓49.2%\\downarrow 49.2\\%\n↓45.9%\\downarrow 45.9\\%\n\n2​(0.3)%2(0.3)\\% ↓\\downarrow\n\n\n+0.79+0.79 ↑\\uparrow\n\n\n+0.000+0.000 ↔\\leftrightarrow\n\n\n0.000.00 ↔\\leftrightarrow\n\n\n\n\nLLaSA →\\rightarrow LLaSA + Ours\n\n↓50.0%\\downarrow 50.0\\%\n↓23.5%\\downarrow 23.5\\%\n↓29.3%\\downarrow 29.3\\%\n\n≤12.5​(5)%\\leq 12.5(5)\\% ↓\\downarrow\n\n\n+0.50+0.50 ↑\\uparrow\n\n\n−0.045-0.045 ↓\\downarrow\n\n\n−0.04-0.04 ↓\\downarrow",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model Pair</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Layers</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Params</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RTF</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Data</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UTMOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2 </span><math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> CosyVoice2 + Ours (12)</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\downarrow 50.0\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><mrow><mn mathsize=\"0.900em\">50.0</mn><mo mathsize=\"0.900em\">%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\downarrow 50.0\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\downarrow 39.7\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m3\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><mrow><mn mathsize=\"0.900em\">39.7</mn><mo mathsize=\"0.900em\">%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\downarrow 39.7\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\downarrow 42.6\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m4\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><mrow><mn mathsize=\"0.900em\">42.6</mn><mo mathsize=\"0.900em\">%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\downarrow 42.6\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"2(0.3)\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m5\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mn mathsize=\"0.900em\">0.3</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><mo mathsize=\"0.900em\">%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">2(0.3)\\%</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"+0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m7\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.900em\">+</mo><mn mathsize=\"0.900em\">0.43</mn></mrow><annotation encoding=\"application/x-tex\">+0.43</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"+0.005\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m9\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.900em\">+</mo><mn mathsize=\"0.900em\">0.005</mn></mrow><annotation encoding=\"application/x-tex\">+0.005</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"0.00\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m11\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.00</mn><annotation encoding=\"application/x-tex\">0.00</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\leftrightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8596;</mo><annotation encoding=\"application/x-tex\">\\leftrightarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice2 </span><math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> CosyVoice2 + Ours (9)</span>\n</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\downarrow 62.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m14\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><mrow><mn mathsize=\"0.900em\">62.5</mn><mo mathsize=\"0.900em\">%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\downarrow 62.5\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\downarrow 49.2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m15\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><mrow><mn mathsize=\"0.900em\">49.2</mn><mo mathsize=\"0.900em\">%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\downarrow 49.2\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\downarrow 45.9\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m16\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><mrow><mn mathsize=\"0.900em\">45.9</mn><mo mathsize=\"0.900em\">%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\downarrow 45.9\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"2(0.3)\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m17\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mn mathsize=\"0.900em\">0.3</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><mo mathsize=\"0.900em\">%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">2(0.3)\\%</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"+0.79\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m19\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.900em\">+</mo><mn mathsize=\"0.900em\">0.79</mn></mrow><annotation encoding=\"application/x-tex\">+0.79</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"+0.000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m21\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.900em\">+</mo><mn mathsize=\"0.900em\">0.000</mn></mrow><annotation encoding=\"application/x-tex\">+0.000</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\leftrightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m22\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8596;</mo><annotation encoding=\"application/x-tex\">\\leftrightarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"0.00\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m23\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">0.00</mn><annotation encoding=\"application/x-tex\">0.00</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\leftrightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m24\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8596;</mo><annotation encoding=\"application/x-tex\">\\leftrightarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">LLaSA </span><math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m25\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> LLaSA + Ours</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\downarrow 50.0\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m26\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><mrow><mn mathsize=\"0.900em\">50.0</mn><mo mathsize=\"0.900em\">%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\downarrow 50.0\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\downarrow 23.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m27\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><mrow><mn mathsize=\"0.900em\">23.5</mn><mo mathsize=\"0.900em\">%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\downarrow 23.5\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\downarrow 29.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m28\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><mrow><mn mathsize=\"0.900em\">29.3</mn><mo mathsize=\"0.900em\">%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\downarrow 29.3\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<math alttext=\"\\leq 12.5(5)\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m29\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&#8804;</mo><mrow><mn mathsize=\"0.900em\">12.5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mn mathsize=\"0.900em\">5</mn><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><mo mathsize=\"0.900em\">%</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq 12.5(5)\\%</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m30\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<math alttext=\"+0.50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m31\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.900em\">+</mo><mn mathsize=\"0.900em\">0.50</mn></mrow><annotation encoding=\"application/x-tex\">+0.50</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m32\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<math alttext=\"-0.045\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m33\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">0.045</mn></mrow><annotation encoding=\"application/x-tex\">-0.045</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m34\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<math alttext=\"-0.04\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m35\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">0.04</mn></mrow><annotation encoding=\"application/x-tex\">-0.04</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.st2.m36\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "bound",
            "2​03203",
            "relative",
            "usage",
            "↓492downarrow",
            "↓downarrow",
            "ours",
            "−004004",
            "versions",
            "utmos",
            "→rightarrow",
            "↓397downarrow",
            "uncompressed",
            "≤leq",
            "↓500downarrow",
            "english",
            "↔leftrightarrow",
            "↓459downarrow",
            "wer",
            "rtf",
            "llasa",
            "spade",
            "exact",
            "layers",
            "report",
            "model",
            "params",
            "pair",
            "↓293downarrow",
            "≤125​5leq",
            "therefore",
            "performance",
            "cosyvoice2",
            "↑uparrow",
            "set",
            "−00450045",
            "↓235downarrow",
            "models",
            "compared",
            "amount",
            "↓625downarrow",
            "unknown",
            "↓426downarrow",
            "pretraining",
            "data",
            "upper",
            "their",
            "internal"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The goal of this paper is to introduce <span class=\"ltx_text ltx_font_bold\">SPADE</span>, a framework for <span class=\"ltx_text ltx_font_bold\">S</span>tructured <span class=\"ltx_text ltx_font_bold\">P</span>runing and <span class=\"ltx_text ltx_font_bold\">A</span>daptive <span class=\"ltx_text ltx_font_bold\">D</span>istillation for <span class=\"ltx_text ltx_font_bold\">E</span>fficient Large Language Model-based text-to-speech (LLM-TTS). Recent LLM-TTS systems achieve strong controllability and zero-shot generalization, but their large parameter counts and high latency limit real-world deployment. SPADE addresses this by combining (i) a <span class=\"ltx_text ltx_font_italic\">pruning</span> step guided by a word-error-rate-based layer importance index to remove non-essential Transformer layers, with (ii) multi-level <span class=\"ltx_text ltx_font_italic\">knowledge distillation</span> to restore autoregressive coherence. On zero-shot benchmarks, SPADE preserves near-parity perceptual quality while halving Transformer depth, reducing VRAM usage by up to <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math>, and achieving up to <math alttext=\"1.7\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mrow><mn>1.7</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">1.7\\times</annotation></semantics></math> faster real-time factor with less than <math alttext=\"5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">5\\%</annotation></semantics></math> of the original training data. These results show that compact LLM-TTS models can maintain naturalness and speaker similarity while enabling practical real-time speech generation. Audio samples are available at <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mm.kaist.ac.kr/projects/SPADE/\" title=\"\">https://mm.kaist.ac.kr/projects/SPADE/</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "spade",
                    "layers",
                    "usage",
                    "models",
                    "data",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large Language Model (LLM)-based text-to-speech (LLM-TTS) systems such as CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib2\" title=\"\">2</a>]</cite>, VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib3\" title=\"\">3</a>]</cite>, CLaM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib4\" title=\"\">4</a>]</cite>, RALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib5\" title=\"\">5</a>]</cite>, and LLaSA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib6\" title=\"\">6</a>]</cite> have shown advanced controllability, prosody modeling, and zero-shot generalization across speakers and languages. Early approaches trained LLM backbones directly on speech tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib7\" title=\"\">7</a>]</cite>, while recent methods initialize from pretrained text LLMs (e.g., LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib8\" title=\"\">8</a>]</cite>, Qwen2.5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib9\" title=\"\">9</a>]</cite>) and adapt them with speech objectives&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib10\" title=\"\">10</a>]</cite>. Leveraging rich contextual representations, these systems synthesize natural speech conditioned on long prompts, speaker embeddings, and control tokens, pushing TTS closer to human-level performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "llasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, LLM-TTS models inherit the costly nature of text-only LLMs, including large parameter counts, high memory usage, and slow autoregressive decoding, and these factors are particularly pronounced in real-time deployment and on-device applications.\nOn the other hand, a line of research in text-only LLM domain has extensively studied model compression, including pruning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib13\" title=\"\">13</a>]</cite>, distillation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib16\" title=\"\">16</a>]</cite>, quantization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib19\" title=\"\">19</a>]</cite>, and adaptive inference methods such as early exiting and token reduction&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib21\" title=\"\">21</a>]</cite>. However, systematic compression methodology for LLM-TTS, where the preservation of prosody, naturalness, and long-context coherence serve as additional key aspects, remains underexplored.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "usage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present <span class=\"ltx_text ltx_font_bold\">SPADE</span>, a framework for <span class=\"ltx_text ltx_font_bold\">S</span>tructured <span class=\"ltx_text ltx_font_bold\">P</span>runing and <span class=\"ltx_text ltx_font_bold\">A</span>daptive <span class=\"ltx_text ltx_font_bold\">D</span>istillation for <span class=\"ltx_text ltx_font_bold\">E</span>fficient LLM-TTS. By removing non-essential layers through a Word Error Rate (WER) based layer importance index and recovering performance via multi-level distillation, SPADE achieves substantial efficiency gains while preserving perceptual quality. Across zero-shot benchmarks, SPADE halves Transformer depth, reduces overall parameters by up to <math alttext=\"40\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>40</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">40\\%</annotation></semantics></math>, lowers VRAM consumption by up to <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math>, and accelerates inference by as much as <math alttext=\"1.7\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mrow><mn>1.7</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">1.7\\times</annotation></semantics></math>, all while retaining near-parity in perceptual metrics. Moreover, the knowledge distillation process shows remarkably high data-efficiency: The recovery of performance requires less than <math alttext=\"5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m4\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">5\\%</annotation></semantics></math> of the pretraining data of the original checkpoints. These results highlight the framework of pruning and distillation as a practical pathway toward compact, high-fidelity, real-time speech generation.</p>\n\n",
                "matched_terms": [
                    "spade",
                    "layers",
                    "wer",
                    "pretraining",
                    "data",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework first explores the importance of each layer in LLM backbone and prune non-essential layers to compress the model.\nSubsequently, an efficient knowledge distillation is applied to effectively restore the performance of the pruned model.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "layers",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SPADE is motivated by using the nature of residual connections: <math alttext=\"x^{l}=x^{l-1}+f_{l}(x^{l-1})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>x</mi><mi>l</mi></msup><mo>=</mo><mrow><msup><mi>x</mi><mrow><mi>l</mi><mo>&#8722;</mo><mn>1</mn></mrow></msup><mo>+</mo><mrow><msub><mi>f</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mrow><mi>l</mi><mo>&#8722;</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x^{l}=x^{l-1}+f_{l}(x^{l-1})</annotation></semantics></math>, where <math alttext=\"x^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>l</mi></msup><annotation encoding=\"application/x-tex\">x^{l}</annotation></semantics></math> and <math alttext=\"f^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><msup><mi>f</mi><mi>l</mi></msup><annotation encoding=\"application/x-tex\">f^{l}</annotation></semantics></math> denote the hidden state and transformation in the layer <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>, respectively.\nIn LLM-TTS, each transformer layer contributes by refining latent representations through residual connections&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib22\" title=\"\">22</a>]</cite>.\nAlthough the architecture shows strong performance in diverse applications, recent studies in text-based LLM suggest some layers provide only weak refinements and can be removed with little effect&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib23\" title=\"\">23</a>]</cite>.\nAn established criterion to identify them is to compute the cosine distance between inputs and outputs of a layer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib24\" title=\"\">24</a>]</cite>.\nHowever, Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that cosine-based layer importance (CLI) does not align with the performance contribution of layers in TTS, as indicated by the different patterns in WER change.</p>\n\n",
                "matched_terms": [
                    "spade",
                    "layers",
                    "performance",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\theta}_{\\setminus i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mrow><mo rspace=\"0em\">&#8726;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\theta}_{\\setminus i}</annotation></semantics></math> denotes the model parameters without the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th layer, <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> is a subset of the evaluation set, and <math alttext=\"({\\bm{x}}_{1},{\\bm{y}}_{1})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119961;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#119962;</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">({\\bm{x}}_{1},{\\bm{y}}_{1})</annotation></semantics></math> and <math alttext=\"({\\bm{x}}_{2},{\\bm{y}}_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119961;</mi><mn>2</mn></msub><mo>,</mo><msub><mi>&#119962;</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">({\\bm{x}}_{2},{\\bm{y}}_{2})</annotation></semantics></math> denote reference and query text-audio pairs, respectively.\nSpecifically, a layer is considered important only if its absence causes significant degradation in WER.\nUnlike cosine-based layer importance, which only estimates the difference between inputs and outputs of within a layer, WLI directly measures the contribution of each layer to the final performance.\nAs shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> evaluated WLI with Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib25\" title=\"\">25</a>]</cite>, and found that many of the layers have negligible impact on performance, indicating a significant redundancy and aligning with the findings in text-domain LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib13\" title=\"\">13</a>]</cite>.\nMoreover, we find that the earliest, central, and final layers consistently emerge as important across different LLM-TTS models.\nBased on the analysis, we prune transformer layers with low WLI values from the LLM backbones.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "model",
                    "models",
                    "wer",
                    "performance",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the proposed framework effectively reduces the parameters without additional modules, such as parameter-efficient fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib27\" title=\"\">27</a>]</cite>, the pruned model naturally confronts disconnected flow of latent information.\nTo address this, we leverage the original un-pruned model as the teacher and perform a knowledge distillation training that simply <span class=\"ltx_text ltx_font_italic\">heals</span> the pruned model to minimize the loss of performance without any additional parameters.\nTo maximize the restoration, we employ a composite loss that benefits from both supervised learning and teacher-guided knowledge distillation:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, the Cross-Entropy loss <math alttext=\"\\mathcal{L}_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{CE}</annotation></semantics></math> is responsible for the <span class=\"ltx_text ltx_font_bold\">supervised</span> component that directly guides the output distribution.\nThe <span class=\"ltx_text ltx_font_bold\">distillation</span> component comprises 4 elements: Embedding reconstruction loss <math alttext=\"\\mathcal{L}_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>e</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{e}</annotation></semantics></math>, alignment losses on logit <math alttext=\"\\mathcal{L}_{logit}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{logit}</annotation></semantics></math>, latent <math alttext=\"\\mathcal{L}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{l}</annotation></semantics></math>, and attention <math alttext=\"\\mathcal{L}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{a}</annotation></semantics></math>, following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib24\" title=\"\">24</a>]</cite>.\nTo provide more stability, we implement the <math alttext=\"\\mathcal{L}_{logit}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{logit}</annotation></semantics></math> with mixed distribution by leveraging Skew KL Divergence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib16\" title=\"\">16</a>]</cite>.\nFor <math alttext=\"\\mathcal{L}_{e},\\mathcal{L}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m7\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>e</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>l</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{e},\\mathcal{L}_{l}</annotation></semantics></math>, and <math alttext=\"\\mathcal{L}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{a}</annotation></semantics></math>, we calculate Mean Squared Error (MSE) of embedding outputs, intermediate latents, and attention matrices between the teacher and student, respectively.\nTo maximize the distillation of teacher&#8217;s knowledge, we propose to dynamically select layers to apply loss for <math alttext=\"\\mathcal{L}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{l}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{a}</annotation></semantics></math>.\nSpecifically, as shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the target values (latent, attention map) for the student layer (<math alttext=\"l_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m11\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">l_{n}</annotation></semantics></math>) are derived from the last layer before the next retained layer (<math alttext=\"l_{m+2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m12\" intent=\":literal\"><semantics><msub><mi>l</mi><mrow><mi>m</mi><mo>+</mo><mn>2</mn></mrow></msub><annotation encoding=\"application/x-tex\">l_{m+2}</annotation></semantics></math>) from the teacher model.\nWhile simple, this approach allows the pruned student model to retain not only its original capability but also those of the removed layers, culminating in a smaller yet more compact model.\nFinally, we combine the supervised and distillation components by adjustable weight <math alttext=\"alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m13\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">alpha</annotation></semantics></math> to balance the influence of each, where the value is empirically set to <math alttext=\"0.25\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m14\" intent=\":literal\"><semantics><mn>0.25</mn><annotation encoding=\"application/x-tex\">0.25</annotation></semantics></math> to deliberately provide stronger supervised guidance.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "model",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate whether pruning and distillation can truly enable compact yet high-fidelity LLM-TTS systems, we benchmark SPADE against two representative baselines: CosyVoice 2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/FunAudioLLM/CosyVoice\" title=\"\">https://github.com/FunAudioLLM/CosyVoice</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib28\" title=\"\">28</a>]</cite> and LLaSA-1B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/zhenye234/LLaSA_training\" title=\"\">https://github.com/zhenye234/LLaSA_training</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib6\" title=\"\">6</a>]</cite>. Both models are chosen for their strong zero-shot capabilities and public availability of checkpoints.</p>\n\n",
                "matched_terms": [
                    "models",
                    "spade",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since a key motivation of SPADE is to minimize retraining cost, we fine-tune each pruned variant on only a fraction of the data: <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math> of LibriHeavy&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib29\" title=\"\">29</a>]</cite> (EN) for LLaSA and LibriTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib30\" title=\"\">30</a>]</cite> (EN) for CosyVoice 2. This corresponds to less than <math alttext=\"5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">5\\%</annotation></semantics></math> of the pretraining corpus size, testing whether our framework can recover quality under data-constrained conditions.</p>\n\n",
                "matched_terms": [
                    "pretraining",
                    "spade",
                    "data",
                    "llasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assess the performance on <span class=\"ltx_text ltx_font_italic\">LibriTTS test-clean</span> and the <span class=\"ltx_text ltx_font_italic\">Seed-TTS eval set</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib31\" title=\"\">31</a>]</cite>, both of which are widely used for zero-shot evaluation. All experiments are conducted on 4<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>NVIDIA A6000 GPUs, with official training scripts left unchanged aside from data size and pruning. CosyVoice 2 is fine-tuned with dynamic batches up to 20,000 tokens, whereas LLaSA uses a batch size of 4. Fine-tuning with SPADE runs for 7 epochs to CosyVoice 2 and for 1 epoch when applying it to LLaSA.</p>\n\n",
                "matched_terms": [
                    "spade",
                    "data",
                    "performance",
                    "set",
                    "llasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate both computational efficiency and perceptual quality, we consider a range of complementary metrics. Efficiency is assessed in terms of model depth, parameter count, and real-time factor (RTF), while intelligibility is measured using word error rate (WER). Perceptual aspects are captured objectively through speaker similarity (SS) and UTMOS using VERSA toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib32\" title=\"\">32</a>]</cite>, and subjectively through the naturalness mean opinion score (NMOS) with 20 listening and 50 random samples from both evaluation sets per model. Together, these metrics reveal whether compact models preserve the qualities essential for real-world TTS deployment.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "utmos",
                    "wer",
                    "rtf"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S3.T1\" title=\"Table 1 &#8227; 3 Experimental Setup &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main results of applying SPADE to CosyVoice 2 and LLaSA with different configurations.\nFor CosyVoice, pruning to 12 layers halves the depth, reduces parameters by <math alttext=\"39.7\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>39.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">39.7\\%</annotation></semantics></math> and accelerates inference by <math alttext=\"42.6\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>42.6</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">42.6\\%</annotation></semantics></math>.\nMoreover, effective VRAM usage is reduced by <math alttext=\"14\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>14</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">14\\%</annotation></semantics></math>, as depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S3.F3\" title=\"Figure 3 &#8227; 3 Experimental Setup &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nNotably, these gains come with no significant degradation in both quantitative and qualitative metrics, showing slight increase of 0.68 in WER for challenging Seed-TTS dataset and 0.11 reduction in NMOS.\nA more aggressive variant with only 9 layers further reduces parameters by <math alttext=\"49.2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mn>49.2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">49.2\\%</annotation></semantics></math>, improves RTF by <math alttext=\"45.9\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mn>45.9</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">45.9\\%</annotation></semantics></math>, and lowers VRAM usage by <math alttext=\"17\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mn>17</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">17\\%</annotation></semantics></math>.\nWhile this extreme setting renders additional increase in WER, perceptual metrics such as NMOS, SS, and UTMOS remain stable, suggesting that SPADE enables flexible trade-offs between efficiency and intelligibility.\nTo confirm that our framework generalizes beyond a single backbone, we conduct experiment on LLaSA, a larger variant based on speech codec.\nHere, pruning removes half the layers, decreases parameters by <math alttext=\"23.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>23.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">23.5\\%</annotation></semantics></math>, improves RTF by <math alttext=\"29.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>29.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">29.3\\%</annotation></semantics></math> (<math alttext=\"1.41\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1.41</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">1.41\\times</annotation></semantics></math> speed-up), and reduces VRAM by <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math>.\nThe result suggests that, while most of the metrics lie in acceptable range, the performance degradation is relatively larger compared to CosyVoice 2.\nBased on the analysis in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a><span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Metrics are reported based on official checkpoints</span></span></span>, we expect it attributes to overall high WLI values across all layers, indicating each layer contributes similarly to the performance.</p>\n\n",
                "matched_terms": [
                    "spade",
                    "layers",
                    "usage",
                    "compared",
                    "utmos",
                    "wer",
                    "performance",
                    "rtf",
                    "llasa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the effectiveness of the proposed methodology with systematic ablation study.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S4.T2\" title=\"Table 2 &#8227; 4.2 Ablation study &#8227; 4 Results and Analysis &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents experiments on CosyVoice 2 with LibriTTS test-clean.\nFirst, the proposed WLI-based pruning is replaced with existing cosine-based pruning.\nThe result shows a notable increase in WER and CER, demonstrating the pruning based on WLI, a metric directly related to intelligibility, successfully prevents performance degradation.\nMoreover, dynamic distillation loss is removed and student layer is distilled only with the information from the corresponding teacher layer.\nThe overall decrease in performance suggests that adaptively choosing the target in distillation training is simple yet plays a significant role.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented SPADE, a pruning-based framework for compressing LLM-TTS models, and showed that SPADE achieves substantial efficiency gains while preserving intelligibility and naturalness. We investigate the importance of each layer using the proposed WLI and find that many layers contribute little to audio synthesis.\nExperimental results verify that, by applying SPADE, such layers can be removed without harming perceived quality.</p>\n\n",
                "matched_terms": [
                    "models",
                    "spade",
                    "layers"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS",
        "caption": "Table 2: \nAblation experiment on LibriTTS test-clean.\nBoth cosine-based pruning and the alternative latent knowledge distillation scheme degrade the overall performance, where cosine-based pruning shows more significant increase in WER and CER.",
        "body": "Expr.\nWER ↓\\downarrow\nCER ↓\\downarrow\nSS ↑\\uparrow\nUTMOS ↑\\uparrow\n\n\nCosyVoice 2\n1.431.43\n0.460.46\n0.810.81\n4.414.41\n\n\n\n\nCosyVoice 2 + Ours\n1.591.59\n0.540.54\n0.820.82\n4.414.41\n\n\nCosine-based pruning\n1.741.74\n0.610.61\n0.810.81\n4.404.40\n\n\nDistill from original layer\n1.651.65\n0.580.58\n0.810.81\n4.404.40",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Expr.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">WER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">CER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">UTMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">CosyVoice 2</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"1.43\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mn>1.43</mn><annotation encoding=\"application/x-tex\">1.43</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"0.46\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mn>0.46</mn><annotation encoding=\"application/x-tex\">0.46</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"0.81\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mn>0.81</mn><annotation encoding=\"application/x-tex\">0.81</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"4.41\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mn>4.41</mn><annotation encoding=\"application/x-tex\">4.41</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">CosyVoice 2 + Ours</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1.59\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mn>1.59</mn><annotation encoding=\"application/x-tex\">1.59</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.54\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mn>0.54</mn><annotation encoding=\"application/x-tex\">0.54</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.82\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mn>0.82</mn><annotation encoding=\"application/x-tex\">0.82</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"4.41\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mn>4.41</mn><annotation encoding=\"application/x-tex\">4.41</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Cosine-based pruning</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.74\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mn>1.74</mn><annotation encoding=\"application/x-tex\">1.74</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.61\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mn>0.61</mn><annotation encoding=\"application/x-tex\">0.61</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.81\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m15\" intent=\":literal\"><semantics><mn>0.81</mn><annotation encoding=\"application/x-tex\">0.81</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"4.40\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m16\" intent=\":literal\"><semantics><mn>4.40</mn><annotation encoding=\"application/x-tex\">4.40</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Distill from original layer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"1.65\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m17\" intent=\":literal\"><semantics><mn>1.65</mn><annotation encoding=\"application/x-tex\">1.65</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"0.58\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m18\" intent=\":literal\"><semantics><mn>0.58</mn><annotation encoding=\"application/x-tex\">0.58</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"0.81\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m19\" intent=\":literal\"><semantics><mn>0.81</mn><annotation encoding=\"application/x-tex\">0.81</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"4.40\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m20\" intent=\":literal\"><semantics><mn>4.40</mn><annotation encoding=\"application/x-tex\">4.40</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "overall",
            "ablation",
            "cosyvoice",
            "↓downarrow",
            "ours",
            "latent",
            "testclean",
            "utmos",
            "more",
            "layer",
            "knowledge",
            "shows",
            "where",
            "from",
            "both",
            "wer",
            "alternative",
            "original",
            "degrade",
            "significant",
            "cer",
            "performance",
            "↑uparrow",
            "distillation",
            "expr",
            "pruning",
            "libritts",
            "cosinebased",
            "experiment",
            "distill",
            "increase",
            "scheme"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate the effectiveness of the proposed methodology with systematic ablation study.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S4.T2\" title=\"Table 2 &#8227; 4.2 Ablation study &#8227; 4 Results and Analysis &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents experiments on CosyVoice 2 with LibriTTS test-clean.\nFirst, the proposed WLI-based pruning is replaced with existing cosine-based pruning.\nThe result shows a notable increase in WER and CER, demonstrating the pruning based on WLI, a metric directly related to intelligibility, successfully prevents performance degradation.\nMoreover, dynamic distillation loss is removed and student layer is distilled only with the information from the corresponding teacher layer.\nThe overall decrease in performance suggests that adaptively choosing the target in distillation training is simple yet plays a significant role.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The goal of this paper is to introduce <span class=\"ltx_text ltx_font_bold\">SPADE</span>, a framework for <span class=\"ltx_text ltx_font_bold\">S</span>tructured <span class=\"ltx_text ltx_font_bold\">P</span>runing and <span class=\"ltx_text ltx_font_bold\">A</span>daptive <span class=\"ltx_text ltx_font_bold\">D</span>istillation for <span class=\"ltx_text ltx_font_bold\">E</span>fficient Large Language Model-based text-to-speech (LLM-TTS). Recent LLM-TTS systems achieve strong controllability and zero-shot generalization, but their large parameter counts and high latency limit real-world deployment. SPADE addresses this by combining (i) a <span class=\"ltx_text ltx_font_italic\">pruning</span> step guided by a word-error-rate-based layer importance index to remove non-essential Transformer layers, with (ii) multi-level <span class=\"ltx_text ltx_font_italic\">knowledge distillation</span> to restore autoregressive coherence. On zero-shot benchmarks, SPADE preserves near-parity perceptual quality while halving Transformer depth, reducing VRAM usage by up to <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math>, and achieving up to <math alttext=\"1.7\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mrow><mn>1.7</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">1.7\\times</annotation></semantics></math> faster real-time factor with less than <math alttext=\"5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">5\\%</annotation></semantics></math> of the original training data. These results show that compact LLM-TTS models can maintain naturalness and speaker similarity while enabling practical real-time speech generation. Audio samples are available at <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://mm.kaist.ac.kr/projects/SPADE/\" title=\"\">https://mm.kaist.ac.kr/projects/SPADE/</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "distillation",
                    "pruning",
                    "layer",
                    "knowledge",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\ntext-to-speech, LLM-TTS, knowledge distillation, pruning, speech synthesis</p>\n\n",
                "matched_terms": [
                    "knowledge",
                    "distillation",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large Language Model (LLM)-based text-to-speech (LLM-TTS) systems such as CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib2\" title=\"\">2</a>]</cite>, VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib3\" title=\"\">3</a>]</cite>, CLaM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib4\" title=\"\">4</a>]</cite>, RALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib5\" title=\"\">5</a>]</cite>, and LLaSA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib6\" title=\"\">6</a>]</cite> have shown advanced controllability, prosody modeling, and zero-shot generalization across speakers and languages. Early approaches trained LLM backbones directly on speech tokens&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib7\" title=\"\">7</a>]</cite>, while recent methods initialize from pretrained text LLMs (e.g., LLaMA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib8\" title=\"\">8</a>]</cite>, Qwen2.5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib9\" title=\"\">9</a>]</cite>) and adapt them with speech objectives&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib10\" title=\"\">10</a>]</cite>. Leveraging rich contextual representations, these systems synthesize natural speech conditioned on long prompts, speaker embeddings, and control tokens, pushing TTS closer to human-level performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "from",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, LLM-TTS models inherit the costly nature of text-only LLMs, including large parameter counts, high memory usage, and slow autoregressive decoding, and these factors are particularly pronounced in real-time deployment and on-device applications.\nOn the other hand, a line of research in text-only LLM domain has extensively studied model compression, including pruning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib13\" title=\"\">13</a>]</cite>, distillation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib16\" title=\"\">16</a>]</cite>, quantization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib19\" title=\"\">19</a>]</cite>, and adaptive inference methods such as early exiting and token reduction&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib20\" title=\"\">20</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib21\" title=\"\">21</a>]</cite>. However, systematic compression methodology for LLM-TTS, where the preservation of prosody, naturalness, and long-context coherence serve as additional key aspects, remains underexplored.</p>\n\n",
                "matched_terms": [
                    "distillation",
                    "pruning",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present <span class=\"ltx_text ltx_font_bold\">SPADE</span>, a framework for <span class=\"ltx_text ltx_font_bold\">S</span>tructured <span class=\"ltx_text ltx_font_bold\">P</span>runing and <span class=\"ltx_text ltx_font_bold\">A</span>daptive <span class=\"ltx_text ltx_font_bold\">D</span>istillation for <span class=\"ltx_text ltx_font_bold\">E</span>fficient LLM-TTS. By removing non-essential layers through a Word Error Rate (WER) based layer importance index and recovering performance via multi-level distillation, SPADE achieves substantial efficiency gains while preserving perceptual quality. Across zero-shot benchmarks, SPADE halves Transformer depth, reduces overall parameters by up to <math alttext=\"40\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>40</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">40\\%</annotation></semantics></math>, lowers VRAM consumption by up to <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m2\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math>, and accelerates inference by as much as <math alttext=\"1.7\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S1.p3.m3\" intent=\":literal\"><semantics><mrow><mn>1.7</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">1.7\\times</annotation></semantics></math>, all while retaining near-parity in perceptual metrics. Moreover, the knowledge distillation process shows remarkably high data-efficiency: The recovery of performance requires less than <math alttext=\"5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m4\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">5\\%</annotation></semantics></math> of the pretraining data of the original checkpoints. These results highlight the framework of pruning and distillation as a practical pathway toward compact, high-fidelity, real-time speech generation.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "distillation",
                    "performance",
                    "pruning",
                    "wer",
                    "layer",
                    "knowledge",
                    "original",
                    "shows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework first explores the importance of each layer in LLM backbone and prune non-essential layers to compress the model.\nSubsequently, an efficient knowledge distillation is applied to effectively restore the performance of the pruned model.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "knowledge",
                    "performance",
                    "distillation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SPADE is motivated by using the nature of residual connections: <math alttext=\"x^{l}=x^{l-1}+f_{l}(x^{l-1})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi>x</mi><mi>l</mi></msup><mo>=</mo><mrow><msup><mi>x</mi><mrow><mi>l</mi><mo>&#8722;</mo><mn>1</mn></mrow></msup><mo>+</mo><mrow><msub><mi>f</mi><mi>l</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>x</mi><mrow><mi>l</mi><mo>&#8722;</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">x^{l}=x^{l-1}+f_{l}(x^{l-1})</annotation></semantics></math>, where <math alttext=\"x^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msup><mi>x</mi><mi>l</mi></msup><annotation encoding=\"application/x-tex\">x^{l}</annotation></semantics></math> and <math alttext=\"f^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><msup><mi>f</mi><mi>l</mi></msup><annotation encoding=\"application/x-tex\">f^{l}</annotation></semantics></math> denote the hidden state and transformation in the layer <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math>, respectively.\nIn LLM-TTS, each transformer layer contributes by refining latent representations through residual connections&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib22\" title=\"\">22</a>]</cite>.\nAlthough the architecture shows strong performance in diverse applications, recent studies in text-based LLM suggest some layers provide only weak refinements and can be removed with little effect&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib23\" title=\"\">23</a>]</cite>.\nAn established criterion to identify them is to compute the cosine distance between inputs and outputs of a layer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib24\" title=\"\">24</a>]</cite>.\nHowever, Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that cosine-based layer importance (CLI) does not align with the performance contribution of layers in TTS, as indicated by the different patterns in WER change.</p>\n\n",
                "matched_terms": [
                    "where",
                    "cosinebased",
                    "latent",
                    "wer",
                    "layer",
                    "performance",
                    "shows"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this, we leverage WER, as the primary interest is the semantic consistency of generation, and propose WER-based layer importance (WLI):</p>\n\n",
                "matched_terms": [
                    "layer",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"{\\theta}_{\\setminus i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mrow><mo rspace=\"0em\">&#8726;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">{\\theta}_{\\setminus i}</annotation></semantics></math> denotes the model parameters without the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th layer, <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> is a subset of the evaluation set, and <math alttext=\"({\\bm{x}}_{1},{\\bm{y}}_{1})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119961;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#119962;</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">({\\bm{x}}_{1},{\\bm{y}}_{1})</annotation></semantics></math> and <math alttext=\"({\\bm{x}}_{2},{\\bm{y}}_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119961;</mi><mn>2</mn></msub><mo>,</mo><msub><mi>&#119962;</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">({\\bm{x}}_{2},{\\bm{y}}_{2})</annotation></semantics></math> denote reference and query text-audio pairs, respectively.\nSpecifically, a layer is considered important only if its absence causes significant degradation in WER.\nUnlike cosine-based layer importance, which only estimates the difference between inputs and outputs of within a layer, WLI directly measures the contribution of each layer to the final performance.\nAs shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> evaluated WLI with Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib25\" title=\"\">25</a>]</cite>, and found that many of the layers have negligible impact on performance, indicating a significant redundancy and aligning with the findings in text-domain LLMs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib13\" title=\"\">13</a>]</cite>.\nMoreover, we find that the earliest, central, and final layers consistently emerge as important across different LLM-TTS models.\nBased on the analysis, we prune transformer layers with low WLI values from the LLM backbones.</p>\n\n",
                "matched_terms": [
                    "where",
                    "cosinebased",
                    "from",
                    "significant",
                    "wer",
                    "layer",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the proposed framework effectively reduces the parameters without additional modules, such as parameter-efficient fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib27\" title=\"\">27</a>]</cite>, the pruned model naturally confronts disconnected flow of latent information.\nTo address this, we leverage the original un-pruned model as the teacher and perform a knowledge distillation training that simply <span class=\"ltx_text ltx_font_italic\">heals</span> the pruned model to minimize the loss of performance without any additional parameters.\nTo maximize the restoration, we employ a composite loss that benefits from both supervised learning and teacher-guided knowledge distillation:</p>\n\n",
                "matched_terms": [
                    "distillation",
                    "performance",
                    "latent",
                    "from",
                    "both",
                    "knowledge",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, the Cross-Entropy loss <math alttext=\"\\mathcal{L}_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{CE}</annotation></semantics></math> is responsible for the <span class=\"ltx_text ltx_font_bold\">supervised</span> component that directly guides the output distribution.\nThe <span class=\"ltx_text ltx_font_bold\">distillation</span> component comprises 4 elements: Embedding reconstruction loss <math alttext=\"\\mathcal{L}_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>e</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{e}</annotation></semantics></math>, alignment losses on logit <math alttext=\"\\mathcal{L}_{logit}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{logit}</annotation></semantics></math>, latent <math alttext=\"\\mathcal{L}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{l}</annotation></semantics></math>, and attention <math alttext=\"\\mathcal{L}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{a}</annotation></semantics></math>, following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib24\" title=\"\">24</a>]</cite>.\nTo provide more stability, we implement the <math alttext=\"\\mathcal{L}_{logit}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{logit}</annotation></semantics></math> with mixed distribution by leveraging Skew KL Divergence&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib16\" title=\"\">16</a>]</cite>.\nFor <math alttext=\"\\mathcal{L}_{e},\\mathcal{L}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m7\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>e</mi></msub><mo>,</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>l</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{e},\\mathcal{L}_{l}</annotation></semantics></math>, and <math alttext=\"\\mathcal{L}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{a}</annotation></semantics></math>, we calculate Mean Squared Error (MSE) of embedding outputs, intermediate latents, and attention matrices between the teacher and student, respectively.\nTo maximize the distillation of teacher&#8217;s knowledge, we propose to dynamically select layers to apply loss for <math alttext=\"\\mathcal{L}_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m9\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{l}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{a}</annotation></semantics></math>.\nSpecifically, as shown in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, the target values (latent, attention map) for the student layer (<math alttext=\"l_{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m11\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>n</mi></msub><annotation encoding=\"application/x-tex\">l_{n}</annotation></semantics></math>) are derived from the last layer before the next retained layer (<math alttext=\"l_{m+2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m12\" intent=\":literal\"><semantics><msub><mi>l</mi><mrow><mi>m</mi><mo>+</mo><mn>2</mn></mrow></msub><annotation encoding=\"application/x-tex\">l_{m+2}</annotation></semantics></math>) from the teacher model.\nWhile simple, this approach allows the pruned student model to retain not only its original capability but also those of the removed layers, culminating in a smaller yet more compact model.\nFinally, we combine the supervised and distillation components by adjustable weight <math alttext=\"alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m13\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">alpha</annotation></semantics></math> to balance the influence of each, where the value is empirically set to <math alttext=\"0.25\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m14\" intent=\":literal\"><semantics><mn>0.25</mn><annotation encoding=\"application/x-tex\">0.25</annotation></semantics></math> to deliberately provide stronger supervised guidance.</p>\n\n",
                "matched_terms": [
                    "distillation",
                    "where",
                    "latent",
                    "from",
                    "more",
                    "layer",
                    "knowledge",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate whether pruning and distillation can truly enable compact yet high-fidelity LLM-TTS systems, we benchmark SPADE against two representative baselines: CosyVoice 2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/FunAudioLLM/CosyVoice\" title=\"\">https://github.com/FunAudioLLM/CosyVoice</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib28\" title=\"\">28</a>]</cite> and LLaSA-1B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/zhenye234/LLaSA_training\" title=\"\">https://github.com/zhenye234/LLaSA_training</a></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib6\" title=\"\">6</a>]</cite>. Both models are chosen for their strong zero-shot capabilities and public availability of checkpoints.</p>\n\n",
                "matched_terms": [
                    "distillation",
                    "both",
                    "pruning",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since a key motivation of SPADE is to minimize retraining cost, we fine-tune each pruned variant on only a fraction of the data: <math alttext=\"25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mrow><mn>25</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">25\\%</annotation></semantics></math> of LibriHeavy&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib29\" title=\"\">29</a>]</cite> (EN) for LLaSA and LibriTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib30\" title=\"\">30</a>]</cite> (EN) for CosyVoice 2. This corresponds to less than <math alttext=\"5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">5\\%</annotation></semantics></math> of the pretraining corpus size, testing whether our framework can recover quality under data-constrained conditions.</p>\n\n",
                "matched_terms": [
                    "libritts",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assess the performance on <span class=\"ltx_text ltx_font_italic\">LibriTTS test-clean</span> and the <span class=\"ltx_text ltx_font_italic\">Seed-TTS eval set</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib31\" title=\"\">31</a>]</cite>, both of which are widely used for zero-shot evaluation. All experiments are conducted on 4<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>NVIDIA A6000 GPUs, with official training scripts left unchanged aside from data size and pruning. CosyVoice 2 is fine-tuned with dynamic batches up to 20,000 tokens, whereas LLaSA uses a batch size of 4. Fine-tuning with SPADE runs for 7 epochs to CosyVoice 2 and for 1 epoch when applying it to LLaSA.</p>\n\n",
                "matched_terms": [
                    "cosyvoice",
                    "pruning",
                    "libritts",
                    "from",
                    "testclean",
                    "both",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate both computational efficiency and perceptual quality, we consider a range of complementary metrics. Efficiency is assessed in terms of model depth, parameter count, and real-time factor (RTF), while intelligibility is measured using word error rate (WER). Perceptual aspects are captured objectively through speaker similarity (SS) and UTMOS using VERSA toolkit&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#bib.bib32\" title=\"\">32</a>]</cite>, and subjectively through the naturalness mean opinion score (NMOS) with 20 listening and 50 random samples from both evaluation sets per model. Together, these metrics reveal whether compact models preserve the qualities essential for real-world TTS deployment.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "from",
                    "wer",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S3.T1\" title=\"Table 1 &#8227; 3 Experimental Setup &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main results of applying SPADE to CosyVoice 2 and LLaSA with different configurations.\nFor CosyVoice, pruning to 12 layers halves the depth, reduces parameters by <math alttext=\"39.7\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>39.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">39.7\\%</annotation></semantics></math> and accelerates inference by <math alttext=\"42.6\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>42.6</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">42.6\\%</annotation></semantics></math>.\nMoreover, effective VRAM usage is reduced by <math alttext=\"14\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>14</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">14\\%</annotation></semantics></math>, as depicted in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S3.F3\" title=\"Figure 3 &#8227; 3 Experimental Setup &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.\nNotably, these gains come with no significant degradation in both quantitative and qualitative metrics, showing slight increase of 0.68 in WER for challenging Seed-TTS dataset and 0.11 reduction in NMOS.\nA more aggressive variant with only 9 layers further reduces parameters by <math alttext=\"49.2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mn>49.2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">49.2\\%</annotation></semantics></math>, improves RTF by <math alttext=\"45.9\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mn>45.9</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">45.9\\%</annotation></semantics></math>, and lowers VRAM usage by <math alttext=\"17\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mn>17</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">17\\%</annotation></semantics></math>.\nWhile this extreme setting renders additional increase in WER, perceptual metrics such as NMOS, SS, and UTMOS remain stable, suggesting that SPADE enables flexible trade-offs between efficiency and intelligibility.\nTo confirm that our framework generalizes beyond a single backbone, we conduct experiment on LLaSA, a larger variant based on speech codec.\nHere, pruning removes half the layers, decreases parameters by <math alttext=\"23.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>23.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">23.5\\%</annotation></semantics></math>, improves RTF by <math alttext=\"29.3\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>29.3</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">29.3\\%</annotation></semantics></math> (<math alttext=\"1.41\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1.41</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">1.41\\times</annotation></semantics></math> speed-up), and reduces VRAM by <math alttext=\"20\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mrow><mn>20</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">20\\%</annotation></semantics></math>.\nThe result suggests that, while most of the metrics lie in acceptable range, the performance degradation is relatively larger compared to CosyVoice 2.\nBased on the analysis in Fig&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20802v2#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a><span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Metrics are reported based on official checkpoints</span></span></span>, we expect it attributes to overall high WLI values across all layers, indicating each layer contributes similarly to the performance.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "cosyvoice",
                    "pruning",
                    "utmos",
                    "both",
                    "significant",
                    "more",
                    "wer",
                    "layer",
                    "experiment",
                    "performance",
                    "increase"
                ]
            }
        ]
    }
}