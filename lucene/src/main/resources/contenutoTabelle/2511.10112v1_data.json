{
    "S3.T1": {
        "source_file": "FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features",
        "caption": "Table 1. Features used for model training and inference.",
        "body": "Feature\nValue/example\nType\n\n\n\n\nWords\nChinese characters\nCategorical\n\n\nPhonemes\nsil n i h ao sil\nCategorical\n\n\nTones\n0 2 3 0\nCategorical\n\n\nWordsBERT\nTensor vector\nNumerical\n\n\nPframe\n8 9 5 10 33 10\nNumerical\n\n\nWframe\n8 14 43 10\nNumerical\n\n\nW2P\n1 2 2 1\nNumerical\n\n\nSpeaker\nspeaker1\nCategorical\n\n\nContentvec\nTensor vector\nNumerical",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Feature</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Value/example</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Type</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Words</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Chinese characters</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Categorical</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Phonemes</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">sil n i h ao sil</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Categorical</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Tones</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0 2 3 0</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Categorical</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">WordsBERT</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Tensor vector</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Numerical</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Pframe</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">8 9 5 10 33 10</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Numerical</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Wframe</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">8 14 43 10</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Numerical</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">W2P</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">1 2 2 1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Numerical</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speaker</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">speaker1</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Categorical</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Contentvec</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Tensor vector</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Numerical</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "type",
            "training",
            "features",
            "w2p",
            "words",
            "used",
            "tensor",
            "speaker1",
            "contentvec",
            "tones",
            "wframe",
            "speaker",
            "wordsbert",
            "model",
            "categorical",
            "valueexample",
            "pframe",
            "chinese",
            "characters",
            "feature",
            "phonemes",
            "numerical",
            "inference",
            "sil",
            "vector"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10112v1#S3.T1\" title=\"Table 1 &#8227; 3.1. Features &#8227; 3. Proposed Methods &#8227; FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we outline the features utilized for model training and inference, using Mandarin Chinese as a demonstrative example. Words, Phonemes, and Tones are obtained through ASR models and related annotation tools, representing the textual content, phonetic information, and tonal characteristics of speech, respectively. Tonal characteristics refers to the pitch variation patterns of a syllable during pronunciation. This supplementation is based on the fact that Mandarin is a tonal language. For non-tonal languages like English, other semantic features can be used during processing. Some speech voice synthesis systems employ BERT <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">devlin2018bert</span>)</cite> features of text to enhance semantic information<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xiao2020improving</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2021extracting</span>)</cite>. Similarly, we derive WordsBERT features from the Words feature using BERT to further enrich the information.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In voice conversion (VC), it is crucial to preserve complete semantic information while accurately modeling the target speaker&#8217;s timbre and prosody. This paper proposes FabasedVC to achieve VC with enhanced similarity in timbre, prosody, and duration to the target speaker, as well as improved content integrity. It is an end-to-end VITS-based VC system that integrates relevant textual modality information, phoneme-level self-supervised learning (SSL) features, and a duration predictor. Specifically, we employ a text feature encoder to encode attributes such as text, phonemes, tones and BERT features. We then process the frame-level SSL features into phoneme-level features using two methods: average pooling and attention mechanism based on each phoneme&#8217;s duration. Moreover, a duration predictor is incorporated to better align the speech rate and prosody of the target speaker. Experimental results demonstrate that our method outperforms competing systems in terms of naturalness, similarity, and content integrity. We strongly recommend that readers listen to our samples.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://fabased-vc.github.io/fabasedvc/</span></span></span></p>\n\n",
                "matched_terms": [
                    "feature",
                    "features",
                    "phonemes",
                    "tones",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the aforementioned objectives, the key to achieving successful voice conversion is to disentangle speaker independent information and speaker dependent timbre information from source and target speech. On this basis, the two types of information can be integrated, thereby achieving the effect of voice conversion. However, several challenges remain in the field. Firstly, the speaker-independent features extracted are entirely derived from the source speech, which means that their duration and prosody closely follow those of the source. However, each speaker possesses a unique speaking style. Following the prosody, pauses, and duration of the source speech strictly when generating converted speech does not adequately reflect the relevant characteristics of the target speaker. This can reduce the perceived similarity to the target speaker among listeners. Secondly, existing voice conversion systems encounter challenges in preserving the integrity of the content in the generated speech. This in turn affects the semantic accuracy and naturalness of the generated speech. Lastly, current disentanglement methods are inadequate in effectively removing timbre information, resulting in issues of timbre leakage during voice conversion tasks.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-to-Speech technology can complement and mitigate some of the shortcomings present in VC technologies. By integrating commonly used semantic-related information from TTS through front-end processing methods, such as ASR, can be leveraged to enhance the content integrity of converted speech in VC systems. The duration predictors utilized in TTS can better simulate speech durations that align more closely with the characteristics of the target speaker.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a novel end-to-end VC system named FabasedVC, which integrates TTS related technologies to address the three aforementioned shortcomings of current VC technologies. Specifically, this system enhances existing VC frameworks by introducing supplementary textual modality information, processing features at the phoneme level, and integrating a duration predictor. Firstly, the system employs ASR and related annotation tools to add various character and phoneme-level textual modality annotations to the existing VC system, thereby improving content completeness. Secondly, We utilize a Forced Aligner tool to obtain phoneme-level timestamps, which we then utilize to convert the disentangled SSL features from the frame level to the phoneme level. This conversion is achieved through phoneme-level average pooling and attention mechanisms. Compared to frame-level features, phoneme-level features provide the advantage of modifiable duration and significantly enhance the disentanglement of speaker timbre information. Lastly, We incorporate a duration predictor into the model which allows the duration of each phoneme to be re-predicted during inference. The re-prediction uses the encoded phoneme-level features combined with the target speaker&#8217;s information. When the re-prediction disabled, VC matches the source speech duration; when enabled, it adjusts the duration to improve similarity. Experimental results demonstrate that FabasedVC effectively preserves the content information from the source speech while achieving a high degree of similarity in timbre, duration, and prosody with the target speaker. The contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "inference",
                    "speaker",
                    "model",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By utilizing pooling and attention mechanisms, we have converted frame-level SSL features to the phoneme-level. This has further separated speaker information and provided the capability to adjust duration in VC.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Voice conversion (VC) has witnessed remarkable progress, evolving from traditional statistical models to advanced deep learning approaches. Early VC methods mainly relied on signal processing and statistical techniques, such as Gaussian mixture models (GMMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stylianou1998continuous</span>)</cite>, frequency warping&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">erro2010voice</span>)</cite>, and exemplar-based strategies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">takashima2012exemplar</span>)</cite>.These methods focused on transforming the spectral features of the source speaker to match those of the target speaker, but often struggled with flexibility and generalization to unseen data.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid development of deep learning, VC systems have significantly advanced in terms of modeling capacity and performance. Generative Adversarial Networks (GANs) have been widely adopted for voice conversion, where adversarial training is employed to perform end-to-end voice conversion or enhance the performance of existing frameworks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2021starganv2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kaneko2021maskcyclegan</span>)</cite>.\nHowever, due to the instability of GANs, the generated speech lacks clarity. Another effective line of work leverages features derived from Automatic Speech Recognition (ASR) systems, such as phoneme posteriorgrams (PPGs) and bottleneck features (BNFs). These representations help disentangle linguistic content from speaker identity, facilitating more robust and speaker-independent VC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2021enriching</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2022disentangling</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ning2023expressive</span>)</cite>.\nThere are also some problems with the intermediate features of ASR: The accuracy and granularity of data annotation affecting the model&#8217;s performance, and the loss of part of the expressiveness in the features. In recent years, self-supervised learning (SSL) has gained increasing attention due to its strong performance on a wide range of speech processing tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hsu2021hubert</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span>)</cite>. Researchers have explored various strategies to disentangle the speaker identity from SSL-based representations for VC purposes. For example, adversarial speaker disentanglement&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2023adversarial</span>)</cite> introduces an adversarial training framework along with an unannotated external corpus to reduce residual speaker information in SSL features. FreeVC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2023freevc</span>)</cite> proposes a data augmentation approach based on spectrogram resizing, which distorts speaker-related information during SSL feature extraction to improve model robustness and disentanglement capability. SoftVC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">van2022comparison</span>)</cite> further enhances semantic representation learning by predicting distributions over discrete units extracted from SSL models.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "model",
                    "training",
                    "features",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional text-to-speech (TTS) systems often struggled with prosody control due to the reliance on handcrafted features and pipeline architectures. The introduction of end-to-end models like Tacotron&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2017tacotron</span>)</cite> greatly simplified the synthesis process but exposed new challenges, such as limited controllability over prosody and issues like word skipping or repetition. To mitigate this, text-speech alignment becomes crucial. FastSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ren2019fastspeech</span>)</cite> addresses this by introducing a phoneme duration predictor and a length regulator, allowing explicit control over speaking rate and prosody.However, its dependence on external autoregressive models for alignment supervision constrained its flexibility. To overcome these limitations, models such as Glow-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2020glow</span>)</cite> and VITS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2021conditional</span>)</cite> adopt Monotonic Alignment Search (MAS), using dynamic programming to learn the alignment between text and speech representations without external models.Furthermore, stochastic duration predictors were introduced during inference to increase prosodic diversity and naturalness. With the rise of large language models (LLMs), many recent approaches adopt discrete codes as intermediate representations for speech synthesis. Models like AudioLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">borsos2023audiolm</span>)</cite> and VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> implicitly learn prosody from prompt audio without explicit prosody predictors. In contrast, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>)</cite> proposes Supervised Semantic Tokens, encoding prosody, timbre, and style into dedicated token embeddings, which are combined with textual inputs for fine-grained prosody control.</p>\n\n",
                "matched_terms": [
                    "inference",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed end-to-end VC system is shown in Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10112v1#S3.F1\" title=\"Figure 1 &#8227; 3. Proposed Methods &#8227; FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The backbone of FabasedVC is inspired by VITS<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2021conditional</span>)</cite>, we describe it as comprising a feature extractor and a Conditional Variational Autoencoder (CVAE)<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kingma2014</span>)</cite>. The CVAE primarily consisting of a posterior encoder, a prior encoder, and a decoder. The key modifications and contributions of this paper focus on front-end feature processing and the prior encoder. The front-end feature processing supplements a large amount of textual modality information for the speech through a series of automated workflows. In the prior encoder of the VC model, we introduce a text feature encoder, an SSL feature encoder that processes SSL features at the phoneme level, and a duration predictor. First, we will introduce the various features utilized by the model, followed by a detailed description of the specific architecture of the model&#8217;s encoder and decoder.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "model",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A forced alignment annotation model can generate timestamps at the phoneme level of speech. By leveraging existing forced alignment annotation models and applying them to speech data as well as the Words and Phones features, we can extract duration information for each element of Words and Phonemes in the speech. By aligning this duration information with the frames of the spectrogram, we obtain Pframe and Wframe features, which represent the frame lengths corresponding to each character and Phoneme, respectively. The W2P feature indicates the number of phonemes associated with each character in a sentence. The Speaker feature conveys speaker-related information linked to the speech, while the Contentvec<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2022contentvec</span>)</cite> feature refers to the disentangled SSL features extracted from the speech. Similar to other voice conversion systems, we also utilize audio and spectrograms.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "model",
                    "features",
                    "phonemes",
                    "pframe",
                    "w2p",
                    "wframe",
                    "words",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The front-end feature processing program we designed can automatically extract all of the aforementioned speech features in a single step during both training and inference.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "inference",
                    "training",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text encoder comprises four encoding modules, each module encodes Words, Phonemes, Tones, and WordsBERT features separately. These modules produce encodings of the same dimension: <math alttext=\"c_{words}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>w</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{words}</annotation></semantics></math>, <math alttext=\"c_{phones}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{phones}</annotation></semantics></math>, <math alttext=\"c_{tones}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{tones}</annotation></semantics></math>, and <math alttext=\"c_{bert}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{bert}</annotation></semantics></math>. The <math alttext=\"c_{text}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{text}</annotation></semantics></math> feature is the sum of these encoding features:</p>\n\n",
                "matched_terms": [
                    "feature",
                    "features",
                    "phonemes",
                    "tones",
                    "words",
                    "wordsbert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SSL feature encoder utilizes both phoneme-level average pooling and an attention-based encoding module to reduce the dimensions of the Contentvec feature <math alttext=\"vec\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">vec</annotation></semantics></math> from the frame level to the phoneme level, resulting in the final representation <math alttext=\"c_{ssl}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{ssl}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "contentvec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Phoneme-level average pooling is guided by the Pframe, duration of each phoneme, denoted as <math alttext=\"dur_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>r</mi><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">dur_{p}</annotation></semantics></math>. Given the Contentvec feature matrix <math alttext=\"vec\\in\\mathbb{R}^{n\\times f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>f</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">vec\\in\\mathbb{R}^{n\\times f}</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> denotes the feature dimension and <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> denotes the frame length, and the phoneme duration vector <math alttext=\"dur_{p}=[d_{1},d_{2},...,d_{d}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>r</mi><mi>p</mi></msub></mrow><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><msub><mi>d</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>d</mi><mi>d</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">dur_{p}=[d_{1},d_{2},...,d_{d}]</annotation></semantics></math>, where <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> represents the number of phonemes and <math alttext=\"\\sum_{i=1}^{d}d_{i}=f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msub><mi>d</mi><mi>i</mi></msub></mrow><mo>=</mo><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">\\sum_{i=1}^{d}d_{i}=f</annotation></semantics></math>. Based on the number of frames <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for each phoneme in <math alttext=\"dur_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>r</mi><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">dur_{p}</annotation></semantics></math>, average pooling operation is performed on <math alttext=\"vec\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m10\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">vec</annotation></semantics></math> to obtain a feature matrix <math alttext=\"vec_{dur}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m11\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>c</mi><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">vec_{dur}</annotation></semantics></math> with dimensions <math alttext=\"[n,d]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m12\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[n,d]</annotation></semantics></math>. The calculation method is as follows:</p>\n\n",
                "matched_terms": [
                    "feature",
                    "contentvec",
                    "phonemes",
                    "pframe",
                    "vector"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To dynamically encode the <math alttext=\"vec\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m1\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">vec</annotation></semantics></math> features at the phoneme level using text features, we propose an attention-based encoding module. For each phoneme, the output of the text feature encoder, <math alttext=\"c_{text}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{text}</annotation></semantics></math>, is used as the query <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m3\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math>. The <math alttext=\"vec\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m4\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">vec</annotation></semantics></math> features serve as both the key <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> and value <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m6\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> to integrate the <math alttext=\"vec\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m7\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">vec</annotation></semantics></math> features corresponding to the phoneme frames. Repeats this process for each phoneme to obtain the encoded feature <math alttext=\"c_{att}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m8\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{att}</annotation></semantics></math>. Following the attention mechanism described in <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2017attention</span>)</cite>, we employ scaled dot-product operations as the similarity measure.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "used",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The frame-level prior network expands the phoneme-level features back to the frame-level features <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> for utilize in subsequent steps. Firstly, we normalizes the sum of <math alttext=\"c_{text}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{text}</annotation></semantics></math> and <math alttext=\"c_{ssl}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{ssl}</annotation></semantics></math>, resulting in <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m4\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>. By utilizing the speaker information <math alttext=\"spk\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m5\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">spk</annotation></semantics></math> as a condition, it processes <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m6\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> through an attention-based normalization flow module to produce the encoded phoneme-level feature <math alttext=\"x_{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p1.m7\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">x_{d}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "feature",
                    "speaker",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To extend the phoneme-level feature <math alttext=\"x_{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">x_{d}</annotation></semantics></math> to the frame level, the phoneme-level feature is replicated according to the frame counts specified by <math alttext=\"dur_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m2\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>r</mi><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">dur_{p}</annotation></semantics></math> during training, resulting in the frame-level feature <math alttext=\"x_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m3\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>f</mi></msub><annotation encoding=\"application/x-tex\">x_{f}</annotation></semantics></math>. During inference, to better match the target speaker&#8217;s duration, the duration predicted by the duration predictor can be utilized to obtain <math alttext=\"x_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>f</mi></msub><annotation encoding=\"application/x-tex\">x_{f}</annotation></semantics></math>. Details regarding the duration predictor will be discussed in the next subsection.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "inference",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ultimate loss of our proposed model, through CVAE and adversarial training, can be articulated as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the experiments, open-source Mandarin data Aishell3 <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2021aishell</span>)</cite> are used to train the model. AISHELL3 contains approximately 85 hours of speech data from around 218 native Mandarin speakers. For evaluation, we randomly selected 10 sentences from each speaker for validation and an additional 10 sentences for testing, using the remaining data for training. Furthermore, we incorporated a dataset featuring a single female speaker from DataBaker<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://www.data-baker.com/#/data/index/source</span></span></span> for testing, with the aim of demonstrating the model&#8217;s robustness in converting speech across different styles.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "used",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation information utilized during training and inference is derived from an ASR model <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2022paraformer</span>)</cite> and an internal forced alignment tool. This forced alignment tool employs the text annotations and phonetic information obtained from the ASR model to generate the corresponding timestamps for the text. We utilized pre-trained 192-dimensional Mandarin BERT<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cui2021pre</span>)</cite> and 256-dimensional ContentVec<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2022contentvec</span>)</cite> models to extract the corresponding features. Through this preprocessing step, we are able to obtain all the features required for training and inference from the speech.</p>\n\n",
                "matched_terms": [
                    "inference",
                    "model",
                    "training",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio samples were resampled to 44,100 Hz. We employed the Short-Time Fourier Transform (STFT) to compute linear spectrograms and 80-band Mel spectrograms. The dimensions for textual information and SSL-encoded features were set to 192. Our model was trained for 500,000 steps on an A100 GPU with a batch size of 16.</p>\n\n",
                "matched_terms": [
                    "model",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of the proposed FabasedVC across various aspects of voice conversion, we compared it with several VC systems. The comparison systems include VQMIVC <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2021vqmivc</span>)</cite>, PPGVC <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Liu2021</span>)</cite>, FreeVC <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2023freevc</span>)</cite>, SOVITS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/svc-develop-team/so-vits-svc</span></span></span>, Cosyvoice<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>)</cite> in voice conversion task, and SeedVC<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024zero</span>)</cite>. VQMIVC achieves effective VC by disentangling speech representations; PPGVC is a VC system based on the BNF framework; FreeVC employs the VITS framework to provide high-quality waveform reconstruction for VC; and SOVITS is a well-regarded open-source SVC and VC project. Cosyvoice is a highly effective zero-shot TTS model that significantly enhances content consistency and speaker similarity. It also supports voice conversion capabilities. SeedVC achieves high-fidelity, low-leakage zero-shot voice conversion via external timbre perturbation and a diffusion Transformer. For CosyVoice and SeedVC, we used the official open-source models for voice conversion, and we trained other VC comparison models using the same dataset as FabasedVC. Additionally, we conducted ablation studies on the SSL encoder and duration predictor to verify the effectiveness of each component.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "used",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In both subjective and objective evaluations, we assessed naturalness, intelligibility, and speaker similarity. For the subjective evaluation, 20 participants rated the naturalness and similarity of the audio using a 5-point Mean Opinion Score (MOS). We randomly selected four target speakers (two males and two females) from the AISHELL3 dataset and evaluated the system under two scenarios: converting the speech of AISHELL speakers to that of target speakers, and converting the speech of a DataBaker speaker to that of target speakers. The DataBaker dataset features standard pronunciation and stable prosody, whereas speakers in the AISHELL3 dataset exhibit various accents and speaking habits. Evaluating under these two scenarios aims to test the robustness of the system and the prosodic diversity generated for different target speakers. This approach better demonstrates the model&#8217;s capabilities in modeling duration and prosody. For the objective evaluation, we adopted three metrics: Character Error Rate (CER), Phoneme Error Rate (PER), and Cosine Similarity (Cos.sim). CER was obtained via an ASR model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://github.com/wenet-e2e/wenet</span></span></span> to measure the character error rate between the source speech and the converted speech. Similarly, PER measures the phoneme error rate following ASR. Cos.sim is derived from the speaker embeddings of the converted speech, objectively quantifying the similarity between the converted speech and the target speaker. In this paper, we utilized a commonly employed speaker embedding extraction model from PPGVC<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://github.com/liusongxiang/ppg-vc/blob/main/speaker_encoder/ckpt</span></span></span> to obtain the speaker embeddings used for testing. Additionally, we evaluated the actual effectiveness of the predicted durations.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "used",
                    "model",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10112v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Experimental setup &#8227; 4. Experiments &#8227; FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> indicate that our model achieves the highest scores on the majority of metrics for both speech naturalness and speaker similarity, surpassing all baseline models, with the exception of a single metric where it ranks second. Even when converting Databaker data to AISHELL speakers, our model maintains a high level of similarity. Despite the significant differences in style and Mandarin proficiency between the source and target speakers, the model achieves it. This clearly demonstrates the role of phoneme-level SSL features and the duration predictor in enhancing speaker similarity. The higher Cos.sim scores further confirm that our model maintains high speaker similarity. Additionally, more comprehensive feature information can facilitate the model in generating speech with higher naturalness. Our proposed model achieves lower CER and PER compared to all other models. This also demonstrates the effectiveness of FabasedVC in addressing the loss of content completeness and better preserving the linguistic content of the source speech. In the ablation experiments, we separately removed the SSL feature encoder modules <math alttext=\"c_{att}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{att}</annotation></semantics></math> and <math alttext=\"c_{avg}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{avg}</annotation></semantics></math> as well as the duration predictor. It was observed that removing <math alttext=\"c_{att}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{att}</annotation></semantics></math> or <math alttext=\"c_{avg}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{avg}</annotation></semantics></math> leads to a noticeable decline in both speech naturalness and similarity. This removal also results in some loss of content completeness. Additionally, disabling the duration predictor results in generating speech with the same timing as the source, which can lead to a decrease in perceived speaker similarity.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "speaker",
                    "model",
                    "features"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown, the converted speech exhibits a low average RDD relative to the source speaker, indicating that the model preserves the original temporal structure and speaking rate of the source utterance. At the same time, the average RDD relative to the target speaker is 11.34%, which is significantly lower than the average RDD<sub class=\"ltx_sub\">source&#8594;target</sub> of 20.85%. This demonstrates that the model successfully shifts the duration characteristics toward those of the target speaker, achieving a partial but meaningful adaptation without fully sacrificing the source timing. This confirms that the proposed approach successfully leverages the source speaking rate while producing duration characteristics similar to those of the target.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose FabasedVC, a VC system with high timbre similarity and content integrity. We adopted a VITS-based framework for waveform reconstruction. To improve content completeness in voice conversion, we enhanced the prior encoder by introducing textual information through front-end feature processing. Moreover, we processed SSL features from the frame-level to the phoneme-level to disentangle timbre information. We also incorporated a duration predictor to adjust duration. Experimental results demonstrate the high naturalness and similarity achieved by our method. In the future, we will continue to focus on enhancing prosody and content integrity in voice conversion.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "features"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features",
        "caption": "Table 2. The subjective evaluation results are presented in terms of MOS with 95% confidence intervals, corresponding to the scenarios of aishell-to-aishell and aishell-to-baker. The objective evaluation results included CER, PER and Cos.Sim.",
        "body": "aishell-to-aishell\ndatabaker-to-aishell\nobjective\n\n\nApproach\nNaturalness \\uparrow\nSimilarity\n\\uparrow\nNaturalness \\uparrow\nSimilarity\n\\uparrow\nCER \\downarrow\n\nPER \\downarrow\n\nCos.Sim\n\\uparrow\n\n\n\nVQMIVC\n\n2.210.062.21\\pm 0.06\n\n2.550.062.55\\pm 0.06\n\n2.260.092.26\\pm 0.09\n\n2.610.072.61\\pm 0.07\n26.64%26.64\\%\n13.61%13.61\\%\n0.64980.6498\n\n\nPPGVC\n\n3.020.073.02\\pm 0.07\n\n2.970.072.97\\pm 0.07\n\n2.820.092.82\\pm 0.09\n\n2.880.112.88\\pm 0.11\n12.08%12.08\\%\n5.38%5.38\\%\n0.70200.7020\n\n\nFreeVC\n\n3.740.083.74\\pm 0.08\n\n3.410.073.41\\pm 0.07\n\n3.720.093.72\\pm 0.09\n\n3.300.113.30\\pm 0.11\n11.97%11.97\\%\n4.51%4.51\\%\n0.73490.7349\n\n\nSOVITS-VC\n\n4.090.064.09\\pm 0.06\n\n3.640.073.64\\pm 0.07\n\n3.650.113.65\\pm 0.11\n\n3.240.083.24\\pm 0.08\n7.51%7.51\\%\n2.26%2.26\\%\n0.74320.7432\n\n\nCosyvoice-VC\n\n3.820.083.82\\pm 0.08\n\n4.050.084.05\\pm 0.08\n\n3.780.073.78\\pm 0.07\n\n3.810.093.81\\pm 0.09\n10.80%10.80\\%\n4.05%4.05\\%\n0.78360.7836\n\n\nSeedVC\n\n4.080.064.08\\pm 0.06\n\n4.110.084.11\\pm 0.08\n\n3.900.08\\mathbf{3.90}\\pm\\mathbf{0.08}\n\n3.880.073.88\\pm 0.07\n8.15%8.15\\%\n3.17%3.17\\%\n0.78270.7827\n\n\nFabasedVC\n\n4.430.06\\mathbf{4.43}\\pm\\mathbf{0.06}\n\n4.490.05\\mathbf{4.49}\\pm\\mathbf{0.05}\n\n3.850.083.85\\pm 0.08\n\n4.270.07\\mathbf{4.27}\\pm\\mathbf{0.07}\n6.27%\\mathbf{6.27\\%}\n2.22%\\mathbf{2.22\\%}\n0.8088\\mathbf{0.8088}\n\n\n- SSL Pooling\n\n3.810.063.81\\pm 0.06\n\n4.290.064.29\\pm 0.06\n\n3.530.053.53\\pm 0.05\n\n4.090.084.09\\pm 0.08\n19.19%19.19\\%\n8.48%8.48\\%\n0.79250.7925\n\n\n- SSL Attention\n\n3.960.063.96\\pm 0.06\n\n4.340.064.34\\pm 0.06\n\n3.620.063.62\\pm 0.06\n\n4.160.074.16\\pm 0.07\n9.45%9.45\\%\n3.22%3.22\\%\n0.79940.7994\n\n\n- Duration predictor\n\n4.230.074.23\\pm 0.07\n\n4.250.054.25\\pm 0.05\n\n3.650.073.65\\pm 0.07\n\n4.040.084.04\\pm 0.08\n11.35%11.35\\%\n4.05%4.05\\%\n0.80780.8078",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">aishell-to-aishell</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">databaker-to-aishell</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">objective</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Approach</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">Naturalness <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t\">Similarity\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">Naturalness <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t\">Similarity\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">CER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">PER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">Cos.Sim\n<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">VQMIVC</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">\n<math alttext=\"2.21\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mrow><mn>2.21</mn><mo>&#177;</mo><mn>0.06</mn></mrow><annotation encoding=\"application/x-tex\">2.21\\pm 0.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t\">\n<math alttext=\"2.55\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mrow><mn>2.55</mn><mo>&#177;</mo><mn>0.06</mn></mrow><annotation encoding=\"application/x-tex\">2.55\\pm 0.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">\n<math alttext=\"2.26\\pm 0.09\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mrow><mn>2.26</mn><mo>&#177;</mo><mn>0.09</mn></mrow><annotation encoding=\"application/x-tex\">2.26\\pm 0.09</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t\">\n<math alttext=\"2.61\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mrow><mn>2.61</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">2.61\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><math alttext=\"26.64\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mrow><mn>26.64</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">26.64\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><math alttext=\"13.61\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mrow><mn>13.61</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">13.61\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><math alttext=\"0.6498\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mn>0.6498</mn><annotation encoding=\"application/x-tex\">0.6498</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">PPGVC</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"3.02\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m15\" intent=\":literal\"><semantics><mrow><mn>3.02</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">3.02\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"2.97\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m16\" intent=\":literal\"><semantics><mrow><mn>2.97</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">2.97\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"2.82\\pm 0.09\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m17\" intent=\":literal\"><semantics><mrow><mn>2.82</mn><mo>&#177;</mo><mn>0.09</mn></mrow><annotation encoding=\"application/x-tex\">2.82\\pm 0.09</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"2.88\\pm 0.11\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m18\" intent=\":literal\"><semantics><mrow><mn>2.88</mn><mo>&#177;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">2.88\\pm 0.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"12.08\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m19\" intent=\":literal\"><semantics><mrow><mn>12.08</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">12.08\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"5.38\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m20\" intent=\":literal\"><semantics><mrow><mn>5.38</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">5.38\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"0.7020\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m21\" intent=\":literal\"><semantics><mn>0.7020</mn><annotation encoding=\"application/x-tex\">0.7020</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">FreeVC</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"3.74\\pm 0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m22\" intent=\":literal\"><semantics><mrow><mn>3.74</mn><mo>&#177;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">3.74\\pm 0.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"3.41\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m23\" intent=\":literal\"><semantics><mrow><mn>3.41</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">3.41\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"3.72\\pm 0.09\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m24\" intent=\":literal\"><semantics><mrow><mn>3.72</mn><mo>&#177;</mo><mn>0.09</mn></mrow><annotation encoding=\"application/x-tex\">3.72\\pm 0.09</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"3.30\\pm 0.11\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m25\" intent=\":literal\"><semantics><mrow><mn>3.30</mn><mo>&#177;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">3.30\\pm 0.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"11.97\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m26\" intent=\":literal\"><semantics><mrow><mn>11.97</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">11.97\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"4.51\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m27\" intent=\":literal\"><semantics><mrow><mn>4.51</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">4.51\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"0.7349\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m28\" intent=\":literal\"><semantics><mn>0.7349</mn><annotation encoding=\"application/x-tex\">0.7349</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">SOVITS-VC</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"4.09\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m29\" intent=\":literal\"><semantics><mrow><mn>4.09</mn><mo>&#177;</mo><mn>0.06</mn></mrow><annotation encoding=\"application/x-tex\">4.09\\pm 0.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"3.64\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m30\" intent=\":literal\"><semantics><mrow><mn>3.64</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">3.64\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"3.65\\pm 0.11\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m31\" intent=\":literal\"><semantics><mrow><mn>3.65</mn><mo>&#177;</mo><mn>0.11</mn></mrow><annotation encoding=\"application/x-tex\">3.65\\pm 0.11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"3.24\\pm 0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m32\" intent=\":literal\"><semantics><mrow><mn>3.24</mn><mo>&#177;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">3.24\\pm 0.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"7.51\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m33\" intent=\":literal\"><semantics><mrow><mn>7.51</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">7.51\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"2.26\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m34\" intent=\":literal\"><semantics><mrow><mn>2.26</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2.26\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"0.7432\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m35\" intent=\":literal\"><semantics><mn>0.7432</mn><annotation encoding=\"application/x-tex\">0.7432</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Cosyvoice-VC</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"3.82\\pm 0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m36\" intent=\":literal\"><semantics><mrow><mn>3.82</mn><mo>&#177;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">3.82\\pm 0.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"4.05\\pm 0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m37\" intent=\":literal\"><semantics><mrow><mn>4.05</mn><mo>&#177;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">4.05\\pm 0.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"3.78\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m38\" intent=\":literal\"><semantics><mrow><mn>3.78</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">3.78\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"3.81\\pm 0.09\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m39\" intent=\":literal\"><semantics><mrow><mn>3.81</mn><mo>&#177;</mo><mn>0.09</mn></mrow><annotation encoding=\"application/x-tex\">3.81\\pm 0.09</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"10.80\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m40\" intent=\":literal\"><semantics><mrow><mn>10.80</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">10.80\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"4.05\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m41\" intent=\":literal\"><semantics><mrow><mn>4.05</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">4.05\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"0.7836\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m42\" intent=\":literal\"><semantics><mn>0.7836</mn><annotation encoding=\"application/x-tex\">0.7836</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">SeedVC</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"4.08\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m43\" intent=\":literal\"><semantics><mrow><mn>4.08</mn><mo>&#177;</mo><mn>0.06</mn></mrow><annotation encoding=\"application/x-tex\">4.08\\pm 0.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"4.11\\pm 0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m44\" intent=\":literal\"><semantics><mrow><mn>4.11</mn><mo>&#177;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">4.11\\pm 0.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"\\mathbf{3.90}\\pm\\mathbf{0.08}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m45\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">3.90</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.08</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{3.90}\\pm\\mathbf{0.08}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"3.88\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m46\" intent=\":literal\"><semantics><mrow><mn>3.88</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">3.88\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"8.15\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m47\" intent=\":literal\"><semantics><mrow><mn>8.15</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">8.15\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"3.17\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m48\" intent=\":literal\"><semantics><mrow><mn>3.17</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.17\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"0.7827\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m49\" intent=\":literal\"><semantics><mn>0.7827</mn><annotation encoding=\"application/x-tex\">0.7827</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">FabasedVC</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"\\mathbf{4.43}\\pm\\mathbf{0.06}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m50\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">4.43</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.06</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{4.43}\\pm\\mathbf{0.06}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"\\mathbf{4.49}\\pm\\mathbf{0.05}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m51\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">4.49</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{4.49}\\pm\\mathbf{0.05}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"3.85\\pm 0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m52\" intent=\":literal\"><semantics><mrow><mn>3.85</mn><mo>&#177;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">3.85\\pm 0.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"\\mathbf{4.27}\\pm\\mathbf{0.07}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m53\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">4.27</mn><mo>&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.07</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{4.27}\\pm\\mathbf{0.07}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"\\mathbf{6.27\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m54\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">6.27</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{6.27\\%}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"\\mathbf{2.22\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m55\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">2.22</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{2.22\\%}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"\\mathbf{0.8088}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m56\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.8088</mn><annotation encoding=\"application/x-tex\">\\mathbf{0.8088}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">- SSL Pooling</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">\n<math alttext=\"3.81\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m57\" intent=\":literal\"><semantics><mrow><mn>3.81</mn><mo>&#177;</mo><mn>0.06</mn></mrow><annotation encoding=\"application/x-tex\">3.81\\pm 0.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t\">\n<math alttext=\"4.29\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m58\" intent=\":literal\"><semantics><mrow><mn>4.29</mn><mo>&#177;</mo><mn>0.06</mn></mrow><annotation encoding=\"application/x-tex\">4.29\\pm 0.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">\n<math alttext=\"3.53\\pm 0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m59\" intent=\":literal\"><semantics><mrow><mn>3.53</mn><mo>&#177;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">3.53\\pm 0.05</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r ltx_border_t\">\n<math alttext=\"4.09\\pm 0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m60\" intent=\":literal\"><semantics><mrow><mn>4.09</mn><mo>&#177;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">4.09\\pm 0.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><math alttext=\"19.19\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m61\" intent=\":literal\"><semantics><mrow><mn>19.19</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">19.19\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><math alttext=\"8.48\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m62\" intent=\":literal\"><semantics><mrow><mn>8.48</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">8.48\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><math alttext=\"0.7925\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m63\" intent=\":literal\"><semantics><mn>0.7925</mn><annotation encoding=\"application/x-tex\">0.7925</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">- SSL Attention</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"3.96\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m64\" intent=\":literal\"><semantics><mrow><mn>3.96</mn><mo>&#177;</mo><mn>0.06</mn></mrow><annotation encoding=\"application/x-tex\">3.96\\pm 0.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"4.34\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m65\" intent=\":literal\"><semantics><mrow><mn>4.34</mn><mo>&#177;</mo><mn>0.06</mn></mrow><annotation encoding=\"application/x-tex\">4.34\\pm 0.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">\n<math alttext=\"3.62\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m66\" intent=\":literal\"><semantics><mrow><mn>3.62</mn><mo>&#177;</mo><mn>0.06</mn></mrow><annotation encoding=\"application/x-tex\">3.62\\pm 0.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_r\">\n<math alttext=\"4.16\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m67\" intent=\":literal\"><semantics><mrow><mn>4.16</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">4.16\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"9.45\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m68\" intent=\":literal\"><semantics><mrow><mn>9.45</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">9.45\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"3.22\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m69\" intent=\":literal\"><semantics><mrow><mn>3.22</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">3.22\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><math alttext=\"0.7994\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m70\" intent=\":literal\"><semantics><mn>0.7994</mn><annotation encoding=\"application/x-tex\">0.7994</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\">- Duration predictor</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\">\n<math alttext=\"4.23\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m71\" intent=\":literal\"><semantics><mrow><mn>4.23</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">4.23\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_r\">\n<math alttext=\"4.25\\pm 0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m72\" intent=\":literal\"><semantics><mrow><mn>4.25</mn><mo>&#177;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">4.25\\pm 0.05</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\">\n<math alttext=\"3.65\\pm 0.07\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m73\" intent=\":literal\"><semantics><mrow><mn>3.65</mn><mo>&#177;</mo><mn>0.07</mn></mrow><annotation encoding=\"application/x-tex\">3.65\\pm 0.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_r\">\n<math alttext=\"4.04\\pm 0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m74\" intent=\":literal\"><semantics><mrow><mn>4.04</mn><mo>&#177;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">4.04\\pm 0.08</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\"><math alttext=\"11.35\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m75\" intent=\":literal\"><semantics><mrow><mn>11.35</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">11.35\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\"><math alttext=\"4.05\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m76\" intent=\":literal\"><semantics><mrow><mn>4.05</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">4.05\\%</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\"><math alttext=\"0.8078\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m77\" intent=\":literal\"><semantics><mn>0.8078</mn><annotation encoding=\"application/x-tex\">0.8078</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "naturalness",
            "downarrow",
            "364007364pm",
            "341007341pm",
            "objective",
            "411008411pm",
            "297007297pm",
            "08088mathbf08088",
            "fabasedvc",
            "404008404pm",
            "aishelltoaishell",
            "results",
            "ssl",
            "duration",
            "mos",
            "evaluation",
            "ppgvc",
            "seedvc",
            "scenarios",
            "221006221pm",
            "365011365pm",
            "388007388pm",
            "409006409pm",
            "approach",
            "intervals",
            "382008382pm",
            "423007423pm",
            "freevc",
            "390008mathbf390pmmathbf008",
            "261007261pm",
            "429006429pm",
            "405008405pm",
            "378007378pm",
            "396006396pm",
            "416007416pm",
            "385008385pm",
            "288011288pm",
            "425005425pm",
            "cossim",
            "372009372pm",
            "similarity",
            "uparrow",
            "449005mathbf449pmmathbf005",
            "aishelltoaishell",
            "included",
            "confidence",
            "427007mathbf427pmmathbf007",
            "365007365pm",
            "409008409pm",
            "vqmivc",
            "443006mathbf443pmmathbf006",
            "predictor",
            "330011330pm",
            "353005353pm",
            "cer",
            "presented",
            "corresponding",
            "aishelltobaker",
            "408006408pm",
            "627mathbf627",
            "255006255pm",
            "subjective",
            "282009282pm",
            "381006381pm",
            "381009381pm",
            "databakertoaishell",
            "cosyvoicevc",
            "sovitsvc",
            "362006362pm",
            "222mathbf222",
            "374008374pm",
            "pooling",
            "terms",
            "226009226pm",
            "attention",
            "324008324pm",
            "434006434pm",
            "302007302pm"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10112v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Experimental setup &#8227; 4. Experiments &#8227; FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> indicate that our model achieves the highest scores on the majority of metrics for both speech naturalness and speaker similarity, surpassing all baseline models, with the exception of a single metric where it ranks second. Even when converting Databaker data to AISHELL speakers, our model maintains a high level of similarity. Despite the significant differences in style and Mandarin proficiency between the source and target speakers, the model achieves it. This clearly demonstrates the role of phoneme-level SSL features and the duration predictor in enhancing speaker similarity. The higher Cos.sim scores further confirm that our model maintains high speaker similarity. Additionally, more comprehensive feature information can facilitate the model in generating speech with higher naturalness. Our proposed model achieves lower CER and PER compared to all other models. This also demonstrates the effectiveness of FabasedVC in addressing the loss of content completeness and better preserving the linguistic content of the source speech. In the ablation experiments, we separately removed the SSL feature encoder modules <math alttext=\"c_{att}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{att}</annotation></semantics></math> and <math alttext=\"c_{avg}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{avg}</annotation></semantics></math> as well as the duration predictor. It was observed that removing <math alttext=\"c_{att}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{att}</annotation></semantics></math> or <math alttext=\"c_{avg}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{avg}</annotation></semantics></math> leads to a noticeable decline in both speech naturalness and similarity. This removal also results in some loss of content completeness. Additionally, disabling the duration predictor results in generating speech with the same timing as the source, which can lead to a decrease in perceived speaker similarity.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In voice conversion (VC), it is crucial to preserve complete semantic information while accurately modeling the target speaker&#8217;s timbre and prosody. This paper proposes FabasedVC to achieve VC with enhanced similarity in timbre, prosody, and duration to the target speaker, as well as improved content integrity. It is an end-to-end VITS-based VC system that integrates relevant textual modality information, phoneme-level self-supervised learning (SSL) features, and a duration predictor. Specifically, we employ a text feature encoder to encode attributes such as text, phonemes, tones and BERT features. We then process the frame-level SSL features into phoneme-level features using two methods: average pooling and attention mechanism based on each phoneme&#8217;s duration. Moreover, a duration predictor is incorporated to better align the speech rate and prosody of the target speaker. Experimental results demonstrate that our method outperforms competing systems in terms of naturalness, similarity, and content integrity. We strongly recommend that readers listen to our samples.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://fabased-vc.github.io/fabasedvc/</span></span></span></p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "terms",
                    "ssl",
                    "duration",
                    "predictor",
                    "fabasedvc",
                    "attention",
                    "similarity",
                    "results",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the aforementioned objectives, the key to achieving successful voice conversion is to disentangle speaker independent information and speaker dependent timbre information from source and target speech. On this basis, the two types of information can be integrated, thereby achieving the effect of voice conversion. However, several challenges remain in the field. Firstly, the speaker-independent features extracted are entirely derived from the source speech, which means that their duration and prosody closely follow those of the source. However, each speaker possesses a unique speaking style. Following the prosody, pauses, and duration of the source speech strictly when generating converted speech does not adequately reflect the relevant characteristics of the target speaker. This can reduce the perceived similarity to the target speaker among listeners. Secondly, existing voice conversion systems encounter challenges in preserving the integrity of the content in the generated speech. This in turn affects the semantic accuracy and naturalness of the generated speech. Lastly, current disentanglement methods are inadequate in effectively removing timbre information, resulting in issues of timbre leakage during voice conversion tasks.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "similarity",
                    "duration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a novel end-to-end VC system named FabasedVC, which integrates TTS related technologies to address the three aforementioned shortcomings of current VC technologies. Specifically, this system enhances existing VC frameworks by introducing supplementary textual modality information, processing features at the phoneme level, and integrating a duration predictor. Firstly, the system employs ASR and related annotation tools to add various character and phoneme-level textual modality annotations to the existing VC system, thereby improving content completeness. Secondly, We utilize a Forced Aligner tool to obtain phoneme-level timestamps, which we then utilize to convert the disentangled SSL features from the frame level to the phoneme level. This conversion is achieved through phoneme-level average pooling and attention mechanisms. Compared to frame-level features, phoneme-level features provide the advantage of modifiable duration and significantly enhance the disentanglement of speaker timbre information. Lastly, We incorporate a duration predictor into the model which allows the duration of each phoneme to be re-predicted during inference. The re-prediction uses the encoded phoneme-level features combined with the target speaker&#8217;s information. When the re-prediction disabled, VC matches the source speech duration; when enabled, it adjusts the duration to improve similarity. Experimental results demonstrate that FabasedVC effectively preserves the content information from the source speech while achieving a high degree of similarity in timbre, duration, and prosody with the target speaker. The contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "duration",
                    "predictor",
                    "fabasedvc",
                    "attention",
                    "similarity",
                    "results",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By utilizing pooling and attention mechanisms, we have converted frame-level SSL features to the phoneme-level. This has further separated speaker information and provided the capability to adjust duration in VC.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "attention",
                    "duration",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We have added a duration predictor to the VC system that, when enabled, re-estimates the duration of each phoneme for the target speaker.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "predictor"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid development of deep learning, VC systems have significantly advanced in terms of modeling capacity and performance. Generative Adversarial Networks (GANs) have been widely adopted for voice conversion, where adversarial training is employed to perform end-to-end voice conversion or enhance the performance of existing frameworks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2021starganv2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kaneko2021maskcyclegan</span>)</cite>.\nHowever, due to the instability of GANs, the generated speech lacks clarity. Another effective line of work leverages features derived from Automatic Speech Recognition (ASR) systems, such as phoneme posteriorgrams (PPGs) and bottleneck features (BNFs). These representations help disentangle linguistic content from speaker identity, facilitating more robust and speaker-independent VC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2021enriching</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2022disentangling</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ning2023expressive</span>)</cite>.\nThere are also some problems with the intermediate features of ASR: The accuracy and granularity of data annotation affecting the model&#8217;s performance, and the loss of part of the expressiveness in the features. In recent years, self-supervised learning (SSL) has gained increasing attention due to its strong performance on a wide range of speech processing tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hsu2021hubert</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span>)</cite>. Researchers have explored various strategies to disentangle the speaker identity from SSL-based representations for VC purposes. For example, adversarial speaker disentanglement&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2023adversarial</span>)</cite> introduces an adversarial training framework along with an unannotated external corpus to reduce residual speaker information in SSL features. FreeVC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2023freevc</span>)</cite> proposes a data augmentation approach based on spectrogram resizing, which distorts speaker-related information during SSL feature extraction to improve model robustness and disentanglement capability. SoftVC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">van2022comparison</span>)</cite> further enhances semantic representation learning by predicting distributions over discrete units extracted from SSL models.</p>\n\n",
                "matched_terms": [
                    "terms",
                    "ssl",
                    "freevc",
                    "attention",
                    "approach"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional text-to-speech (TTS) systems often struggled with prosody control due to the reliance on handcrafted features and pipeline architectures. The introduction of end-to-end models like Tacotron&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2017tacotron</span>)</cite> greatly simplified the synthesis process but exposed new challenges, such as limited controllability over prosody and issues like word skipping or repetition. To mitigate this, text-speech alignment becomes crucial. FastSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ren2019fastspeech</span>)</cite> addresses this by introducing a phoneme duration predictor and a length regulator, allowing explicit control over speaking rate and prosody.However, its dependence on external autoregressive models for alignment supervision constrained its flexibility. To overcome these limitations, models such as Glow-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2020glow</span>)</cite> and VITS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2021conditional</span>)</cite> adopt Monotonic Alignment Search (MAS), using dynamic programming to learn the alignment between text and speech representations without external models.Furthermore, stochastic duration predictors were introduced during inference to increase prosodic diversity and naturalness. With the rise of large language models (LLMs), many recent approaches adopt discrete codes as intermediate representations for speech synthesis. Models like AudioLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">borsos2023audiolm</span>)</cite> and VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> implicitly learn prosody from prompt audio without explicit prosody predictors. In contrast, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>)</cite> proposes Supervised Semantic Tokens, encoding prosody, timbre, and style into dedicated token embeddings, which are combined with textual inputs for fine-grained prosody control.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "duration",
                    "predictor"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed end-to-end VC system is shown in Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10112v1#S3.F1\" title=\"Figure 1 &#8227; 3. Proposed Methods &#8227; FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The backbone of FabasedVC is inspired by VITS<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2021conditional</span>)</cite>, we describe it as comprising a feature extractor and a Conditional Variational Autoencoder (CVAE)<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kingma2014</span>)</cite>. The CVAE primarily consisting of a posterior encoder, a prior encoder, and a decoder. The key modifications and contributions of this paper focus on front-end feature processing and the prior encoder. The front-end feature processing supplements a large amount of textual modality information for the speech through a series of automated workflows. In the prior encoder of the VC model, we introduce a text feature encoder, an SSL feature encoder that processes SSL features at the phoneme level, and a duration predictor. First, we will introduce the various features utilized by the model, followed by a detailed description of the specific architecture of the model&#8217;s encoder and decoder.</p>\n\n",
                "matched_terms": [
                    "fabasedvc",
                    "predictor",
                    "ssl",
                    "duration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A forced alignment annotation model can generate timestamps at the phoneme level of speech. By leveraging existing forced alignment annotation models and applying them to speech data as well as the Words and Phones features, we can extract duration information for each element of Words and Phonemes in the speech. By aligning this duration information with the frames of the spectrogram, we obtain Pframe and Wframe features, which represent the frame lengths corresponding to each character and Phoneme, respectively. The W2P feature indicates the number of phonemes associated with each character in a sentence. The Speaker feature conveys speaker-related information linked to the speech, while the Contentvec<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2022contentvec</span>)</cite> feature refers to the disentangled SSL features extracted from the speech. Similar to other voice conversion systems, we also utilize audio and spectrograms.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "corresponding",
                    "ssl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The prior encoder consists of a text feature encoder, an SSL feature encoder, a frame-level prior network, and a duration predictor.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "predictor",
                    "duration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SSL feature encoder utilizes both phoneme-level average pooling and an attention-based encoding module to reduce the dimensions of the Contentvec feature <math alttext=\"vec\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">vec</annotation></semantics></math> from the frame level to the phoneme level, resulting in the final representation <math alttext=\"c_{ssl}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{ssl}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Phoneme-level average pooling is guided by the Pframe, duration of each phoneme, denoted as <math alttext=\"dur_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>r</mi><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">dur_{p}</annotation></semantics></math>. Given the Contentvec feature matrix <math alttext=\"vec\\in\\mathbb{R}^{n\\times f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>f</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">vec\\in\\mathbb{R}^{n\\times f}</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> denotes the feature dimension and <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> denotes the frame length, and the phoneme duration vector <math alttext=\"dur_{p}=[d_{1},d_{2},...,d_{d}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>r</mi><mi>p</mi></msub></mrow><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><msub><mi>d</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>d</mi><mi>d</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">dur_{p}=[d_{1},d_{2},...,d_{d}]</annotation></semantics></math>, where <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> represents the number of phonemes and <math alttext=\"\\sum_{i=1}^{d}d_{i}=f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msub><mi>d</mi><mi>i</mi></msub></mrow><mo>=</mo><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">\\sum_{i=1}^{d}d_{i}=f</annotation></semantics></math>. Based on the number of frames <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for each phoneme in <math alttext=\"dur_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>r</mi><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">dur_{p}</annotation></semantics></math>, average pooling operation is performed on <math alttext=\"vec\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m10\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">vec</annotation></semantics></math> to obtain a feature matrix <math alttext=\"vec_{dur}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m11\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>c</mi><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">vec_{dur}</annotation></semantics></math> with dimensions <math alttext=\"[n,d]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m12\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[n,d]</annotation></semantics></math>. The calculation method is as follows:</p>\n\n",
                "matched_terms": [
                    "duration",
                    "pooling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To dynamically encode the <math alttext=\"vec\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m1\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">vec</annotation></semantics></math> features at the phoneme level using text features, we propose an attention-based encoding module. For each phoneme, the output of the text feature encoder, <math alttext=\"c_{text}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{text}</annotation></semantics></math>, is used as the query <math alttext=\"Q\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m3\" intent=\":literal\"><semantics><mi>Q</mi><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math>. The <math alttext=\"vec\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m4\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">vec</annotation></semantics></math> features serve as both the key <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m5\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> and value <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m6\" intent=\":literal\"><semantics><mi>V</mi><annotation encoding=\"application/x-tex\">V</annotation></semantics></math> to integrate the <math alttext=\"vec\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m7\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">vec</annotation></semantics></math> features corresponding to the phoneme frames. Repeats this process for each phoneme to obtain the encoded feature <math alttext=\"c_{att}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p5.m8\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{att}</annotation></semantics></math>. Following the attention mechanism described in <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">vaswani2017attention</span>)</cite>, we employ scaled dot-product operations as the similarity measure.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "corresponding",
                    "attention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To extend the phoneme-level feature <math alttext=\"x_{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">x_{d}</annotation></semantics></math> to the frame level, the phoneme-level feature is replicated according to the frame counts specified by <math alttext=\"dur_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m2\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>r</mi><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">dur_{p}</annotation></semantics></math> during training, resulting in the frame-level feature <math alttext=\"x_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m3\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>f</mi></msub><annotation encoding=\"application/x-tex\">x_{f}</annotation></semantics></math>. During inference, to better match the target speaker&#8217;s duration, the duration predicted by the duration predictor can be utilized to obtain <math alttext=\"x_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS3.p3.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>f</mi></msub><annotation encoding=\"application/x-tex\">x_{f}</annotation></semantics></math>. Details regarding the duration predictor will be discussed in the next subsection.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "predictor"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The duration predictor <math alttext=\"dp\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">dp</annotation></semantics></math> re-predicts the logarithm duration <math alttext=\"logw_{-}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>w</mi><mo>&#8722;</mo></msub></mrow><annotation encoding=\"application/x-tex\">logw_{-}</annotation></semantics></math> using <math alttext=\"x_{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m3\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">x_{d}</annotation></semantics></math> and <math alttext=\"spk\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">spk</annotation></semantics></math>. This helps to better match the durations of different target speakers:</p>\n\n",
                "matched_terms": [
                    "duration",
                    "predictor"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of the proposed FabasedVC across various aspects of voice conversion, we compared it with several VC systems. The comparison systems include VQMIVC <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2021vqmivc</span>)</cite>, PPGVC <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Liu2021</span>)</cite>, FreeVC <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2023freevc</span>)</cite>, SOVITS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/svc-develop-team/so-vits-svc</span></span></span>, Cosyvoice<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>)</cite> in voice conversion task, and SeedVC<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024zero</span>)</cite>. VQMIVC achieves effective VC by disentangling speech representations; PPGVC is a VC system based on the BNF framework; FreeVC employs the VITS framework to provide high-quality waveform reconstruction for VC; and SOVITS is a well-regarded open-source SVC and VC project. Cosyvoice is a highly effective zero-shot TTS model that significantly enhances content consistency and speaker similarity. It also supports voice conversion capabilities. SeedVC achieves high-fidelity, low-leakage zero-shot voice conversion via external timbre perturbation and a diffusion Transformer. For CosyVoice and SeedVC, we used the official open-source models for voice conversion, and we trained other VC comparison models using the same dataset as FabasedVC. Additionally, we conducted ablation studies on the SSL encoder and duration predictor to verify the effectiveness of each component.</p>\n\n",
                "matched_terms": [
                    "ssl",
                    "duration",
                    "fabasedvc",
                    "ppgvc",
                    "freevc",
                    "seedvc",
                    "similarity",
                    "vqmivc",
                    "predictor"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In both subjective and objective evaluations, we assessed naturalness, intelligibility, and speaker similarity. For the subjective evaluation, 20 participants rated the naturalness and similarity of the audio using a 5-point Mean Opinion Score (MOS). We randomly selected four target speakers (two males and two females) from the AISHELL3 dataset and evaluated the system under two scenarios: converting the speech of AISHELL speakers to that of target speakers, and converting the speech of a DataBaker speaker to that of target speakers. The DataBaker dataset features standard pronunciation and stable prosody, whereas speakers in the AISHELL3 dataset exhibit various accents and speaking habits. Evaluating under these two scenarios aims to test the robustness of the system and the prosodic diversity generated for different target speakers. This approach better demonstrates the model&#8217;s capabilities in modeling duration and prosody. For the objective evaluation, we adopted three metrics: Character Error Rate (CER), Phoneme Error Rate (PER), and Cosine Similarity (Cos.sim). CER was obtained via an ASR model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://github.com/wenet-e2e/wenet</span></span></span> to measure the character error rate between the source speech and the converted speech. Similarly, PER measures the phoneme error rate following ASR. Cos.sim is derived from the speaker embeddings of the converted speech, objectively quantifying the similarity between the converted speech and the target speaker. In this paper, we utilized a commonly employed speaker embedding extraction model from PPGVC<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://github.com/liusongxiang/ppg-vc/blob/main/speaker_encoder/ckpt</span></span></span> to obtain the speaker embeddings used for testing. Additionally, we evaluated the actual effectiveness of the predicted durations.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "duration",
                    "subjective",
                    "mos",
                    "evaluation",
                    "cossim",
                    "similarity",
                    "scenarios",
                    "cer",
                    "approach",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown, the converted speech exhibits a low average RDD relative to the source speaker, indicating that the model preserves the original temporal structure and speaking rate of the source utterance. At the same time, the average RDD relative to the target speaker is 11.34%, which is significantly lower than the average RDD<sub class=\"ltx_sub\">source&#8594;target</sub> of 20.85%. This demonstrates that the model successfully shifts the duration characteristics toward those of the target speaker, achieving a partial but meaningful adaptation without fully sacrificing the source timing. This confirms that the proposed approach successfully leverages the source speaking rate while producing duration characteristics similar to those of the target.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "approach"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose FabasedVC, a VC system with high timbre similarity and content integrity. We adopted a VITS-based framework for waveform reconstruction. To improve content completeness in voice conversion, we enhanced the prior encoder by introducing textual information through front-end feature processing. Moreover, we processed SSL features from the frame-level to the phoneme-level to disentangle timbre information. We also incorporated a duration predictor to adjust duration. Experimental results demonstrate the high naturalness and similarity achieved by our method. In the future, we will continue to focus on enhancing prosody and content integrity in voice conversion.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "ssl",
                    "duration",
                    "fabasedvc",
                    "similarity",
                    "predictor",
                    "results"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features",
        "caption": "Table 3. Analysis of Relative Duration Deviation (RDD) for voice conversion",
        "body": "Speakers\n\nRDDsource\n\nRDDtarget\nRDDsourcetarget\n\n\n\n\nSSB0338\n0.28%\n7.32%\n13.92%\n\n\nSSB0817\n4.67%\n2.64%\n4.27%\n\n\nSSB1585\n6.38%\n21.87%\n41.91%\n\n\nSSB1935\n1.66%\n14.52%\n23.29%\n\n\nAverage\n3.25%\n11.34%\n20.85%",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Speakers</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">RDD<sub class=\"ltx_sub\">source</sub></span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">RDD<sub class=\"ltx_sub\">target</sub></span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">RDD<sub class=\"ltx_sub\">source&#8594;target</sub></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">SSB0338</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">0.28%</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">7.32%</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">13.92%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SSB0817</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">4.67%</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">2.64%</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">4.27%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SSB1585</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">6.38%</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">21.87%</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">41.91%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SSB1935</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">1.66%</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">14.52%</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">23.29%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\">Average</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t\">3.25%</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t\">11.34%</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t\">20.85%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "duration",
            "relative",
            "voice",
            "ssb0817",
            "conversion",
            "rddsourcetarget",
            "deviation",
            "rddsource",
            "ssb1585",
            "analysis",
            "ssb1935",
            "rdd",
            "average",
            "ssb0338",
            "rddtarget",
            "speakers"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10112v1#S4.T3\" title=\"Table 3 &#8227; 4.3. Results and Analysis &#8227; 4. Experiments &#8227; FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a comprehensive analysis of the Relative Duration Deviation (RDD) in the voice conversion task. The RDD metric is computed as:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In voice conversion (VC), it is crucial to preserve complete semantic information while accurately modeling the target speaker&#8217;s timbre and prosody. This paper proposes FabasedVC to achieve VC with enhanced similarity in timbre, prosody, and duration to the target speaker, as well as improved content integrity. It is an end-to-end VITS-based VC system that integrates relevant textual modality information, phoneme-level self-supervised learning (SSL) features, and a duration predictor. Specifically, we employ a text feature encoder to encode attributes such as text, phonemes, tones and BERT features. We then process the frame-level SSL features into phoneme-level features using two methods: average pooling and attention mechanism based on each phoneme&#8217;s duration. Moreover, a duration predictor is incorporated to better align the speech rate and prosody of the target speaker. Experimental results demonstrate that our method outperforms competing systems in terms of naturalness, similarity, and content integrity. We strongly recommend that readers listen to our samples.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://fabased-vc.github.io/fabasedvc/</span></span></span></p>\n\n",
                "matched_terms": [
                    "conversion",
                    "duration",
                    "voice",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Voice conversion (VC) is a technology that modifies the style and timbre of a source speaker&#8217;s speech to mimic that of a target speaker, while maintaining the original linguistic content<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mohammadi2017overview</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2020transferring</span>)</cite>. Both Voice Conversion and Text-to-Speech (TTS) fall under the umbrella of speech processing technologies; however, they differ in their primary focuses. TTS systems<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2021conditional</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kong2023vits2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>)</cite> primarily focus on generating natural and fluent speech expressions from text. VC systems aims to retain the content information of the source speech while preserving non-linguistic elements such as emotion and expressiveness. It seeks to generate converted speech that combines these elements with the timbre information of the target speaker.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the aforementioned objectives, the key to achieving successful voice conversion is to disentangle speaker independent information and speaker dependent timbre information from source and target speech. On this basis, the two types of information can be integrated, thereby achieving the effect of voice conversion. However, several challenges remain in the field. Firstly, the speaker-independent features extracted are entirely derived from the source speech, which means that their duration and prosody closely follow those of the source. However, each speaker possesses a unique speaking style. Following the prosody, pauses, and duration of the source speech strictly when generating converted speech does not adequately reflect the relevant characteristics of the target speaker. This can reduce the perceived similarity to the target speaker among listeners. Secondly, existing voice conversion systems encounter challenges in preserving the integrity of the content in the generated speech. This in turn affects the semantic accuracy and naturalness of the generated speech. Lastly, current disentanglement methods are inadequate in effectively removing timbre information, resulting in issues of timbre leakage during voice conversion tasks.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "duration",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a novel end-to-end VC system named FabasedVC, which integrates TTS related technologies to address the three aforementioned shortcomings of current VC technologies. Specifically, this system enhances existing VC frameworks by introducing supplementary textual modality information, processing features at the phoneme level, and integrating a duration predictor. Firstly, the system employs ASR and related annotation tools to add various character and phoneme-level textual modality annotations to the existing VC system, thereby improving content completeness. Secondly, We utilize a Forced Aligner tool to obtain phoneme-level timestamps, which we then utilize to convert the disentangled SSL features from the frame level to the phoneme level. This conversion is achieved through phoneme-level average pooling and attention mechanisms. Compared to frame-level features, phoneme-level features provide the advantage of modifiable duration and significantly enhance the disentanglement of speaker timbre information. Lastly, We incorporate a duration predictor into the model which allows the duration of each phoneme to be re-predicted during inference. The re-prediction uses the encoded phoneme-level features combined with the target speaker&#8217;s information. When the re-prediction disabled, VC matches the source speech duration; when enabled, it adjusts the duration to improve similarity. Experimental results demonstrate that FabasedVC effectively preserves the content information from the source speech while achieving a high degree of similarity in timbre, duration, and prosody with the target speaker. The contributions of this paper are as follows:</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "duration",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Voice conversion (VC) has witnessed remarkable progress, evolving from traditional statistical models to advanced deep learning approaches. Early VC methods mainly relied on signal processing and statistical techniques, such as Gaussian mixture models (GMMs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">stylianou1998continuous</span>)</cite>, frequency warping&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">erro2010voice</span>)</cite>, and exemplar-based strategies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">takashima2012exemplar</span>)</cite>.These methods focused on transforming the spectral features of the source speaker to match those of the target speaker, but often struggled with flexibility and generalization to unseen data.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid development of deep learning, VC systems have significantly advanced in terms of modeling capacity and performance. Generative Adversarial Networks (GANs) have been widely adopted for voice conversion, where adversarial training is employed to perform end-to-end voice conversion or enhance the performance of existing frameworks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2021starganv2</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kaneko2021maskcyclegan</span>)</cite>.\nHowever, due to the instability of GANs, the generated speech lacks clarity. Another effective line of work leverages features derived from Automatic Speech Recognition (ASR) systems, such as phoneme posteriorgrams (PPGs) and bottleneck features (BNFs). These representations help disentangle linguistic content from speaker identity, facilitating more robust and speaker-independent VC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2021enriching</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2022disentangling</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ning2023expressive</span>)</cite>.\nThere are also some problems with the intermediate features of ASR: The accuracy and granularity of data annotation affecting the model&#8217;s performance, and the loss of part of the expressiveness in the features. In recent years, self-supervised learning (SSL) has gained increasing attention due to its strong performance on a wide range of speech processing tasks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hsu2021hubert</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span>)</cite>. Researchers have explored various strategies to disentangle the speaker identity from SSL-based representations for VC purposes. For example, adversarial speaker disentanglement&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhao2023adversarial</span>)</cite> introduces an adversarial training framework along with an unannotated external corpus to reduce residual speaker information in SSL features. FreeVC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2023freevc</span>)</cite> proposes a data augmentation approach based on spectrogram resizing, which distorts speaker-related information during SSL feature extraction to improve model robustness and disentanglement capability. SoftVC&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">van2022comparison</span>)</cite> further enhances semantic representation learning by predicting distributions over discrete units extracted from SSL models.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A forced alignment annotation model can generate timestamps at the phoneme level of speech. By leveraging existing forced alignment annotation models and applying them to speech data as well as the Words and Phones features, we can extract duration information for each element of Words and Phonemes in the speech. By aligning this duration information with the frames of the spectrogram, we obtain Pframe and Wframe features, which represent the frame lengths corresponding to each character and Phoneme, respectively. The W2P feature indicates the number of phonemes associated with each character in a sentence. The Speaker feature conveys speaker-related information linked to the speech, while the Contentvec<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qian2022contentvec</span>)</cite> feature refers to the disentangled SSL features extracted from the speech. Similar to other voice conversion systems, we also utilize audio and spectrograms.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "duration",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Phoneme-level average pooling is guided by the Pframe, duration of each phoneme, denoted as <math alttext=\"dur_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>r</mi><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">dur_{p}</annotation></semantics></math>. Given the Contentvec feature matrix <math alttext=\"vec\\in\\mathbb{R}^{n\\times f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>f</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">vec\\in\\mathbb{R}^{n\\times f}</annotation></semantics></math>, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> denotes the feature dimension and <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> denotes the frame length, and the phoneme duration vector <math alttext=\"dur_{p}=[d_{1},d_{2},...,d_{d}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>r</mi><mi>p</mi></msub></mrow><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><msub><mi>d</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>d</mi><mi>d</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">dur_{p}=[d_{1},d_{2},...,d_{d}]</annotation></semantics></math>, where <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> represents the number of phonemes and <math alttext=\"\\sum_{i=1}^{d}d_{i}=f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mo>&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></msubsup><msub><mi>d</mi><mi>i</mi></msub></mrow><mo>=</mo><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">\\sum_{i=1}^{d}d_{i}=f</annotation></semantics></math>. Based on the number of frames <math alttext=\"d_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">d_{i}</annotation></semantics></math> for each phoneme in <math alttext=\"dur_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>r</mi><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">dur_{p}</annotation></semantics></math>, average pooling operation is performed on <math alttext=\"vec\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m10\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">vec</annotation></semantics></math> to obtain a feature matrix <math alttext=\"vec_{dur}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m11\" intent=\":literal\"><semantics><mrow><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>c</mi><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">vec_{dur}</annotation></semantics></math> with dimensions <math alttext=\"[n,d]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m12\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>n</mi><mo>,</mo><mi>d</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[n,d]</annotation></semantics></math>. The calculation method is as follows:</p>\n\n",
                "matched_terms": [
                    "average",
                    "duration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The duration predictor <math alttext=\"dp\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">dp</annotation></semantics></math> re-predicts the logarithm duration <math alttext=\"logw_{-}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>w</mi><mo>&#8722;</mo></msub></mrow><annotation encoding=\"application/x-tex\">logw_{-}</annotation></semantics></math> using <math alttext=\"x_{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m3\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>d</mi></msub><annotation encoding=\"application/x-tex\">x_{d}</annotation></semantics></math> and <math alttext=\"spk\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">spk</annotation></semantics></math>. This helps to better match the durations of different target speakers:</p>\n\n",
                "matched_terms": [
                    "duration",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of the proposed FabasedVC across various aspects of voice conversion, we compared it with several VC systems. The comparison systems include VQMIVC <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2021vqmivc</span>)</cite>, PPGVC <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Liu2021</span>)</cite>, FreeVC <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2023freevc</span>)</cite>, SOVITS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>https://github.com/svc-develop-team/so-vits-svc</span></span></span>, Cosyvoice<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice</span>)</cite> in voice conversion task, and SeedVC<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024zero</span>)</cite>. VQMIVC achieves effective VC by disentangling speech representations; PPGVC is a VC system based on the BNF framework; FreeVC employs the VITS framework to provide high-quality waveform reconstruction for VC; and SOVITS is a well-regarded open-source SVC and VC project. Cosyvoice is a highly effective zero-shot TTS model that significantly enhances content consistency and speaker similarity. It also supports voice conversion capabilities. SeedVC achieves high-fidelity, low-leakage zero-shot voice conversion via external timbre perturbation and a diffusion Transformer. For CosyVoice and SeedVC, we used the official open-source models for voice conversion, and we trained other VC comparison models using the same dataset as FabasedVC. Additionally, we conducted ablation studies on the SSL encoder and duration predictor to verify the effectiveness of each component.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "duration",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In both subjective and objective evaluations, we assessed naturalness, intelligibility, and speaker similarity. For the subjective evaluation, 20 participants rated the naturalness and similarity of the audio using a 5-point Mean Opinion Score (MOS). We randomly selected four target speakers (two males and two females) from the AISHELL3 dataset and evaluated the system under two scenarios: converting the speech of AISHELL speakers to that of target speakers, and converting the speech of a DataBaker speaker to that of target speakers. The DataBaker dataset features standard pronunciation and stable prosody, whereas speakers in the AISHELL3 dataset exhibit various accents and speaking habits. Evaluating under these two scenarios aims to test the robustness of the system and the prosodic diversity generated for different target speakers. This approach better demonstrates the model&#8217;s capabilities in modeling duration and prosody. For the objective evaluation, we adopted three metrics: Character Error Rate (CER), Phoneme Error Rate (PER), and Cosine Similarity (Cos.sim). CER was obtained via an ASR model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>https://github.com/wenet-e2e/wenet</span></span></span> to measure the character error rate between the source speech and the converted speech. Similarly, PER measures the phoneme error rate following ASR. Cos.sim is derived from the speaker embeddings of the converted speech, objectively quantifying the similarity between the converted speech and the target speaker. In this paper, we utilized a commonly employed speaker embedding extraction model from PPGVC<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://github.com/liusongxiang/ppg-vc/blob/main/speaker_encoder/ckpt</span></span></span> to obtain the speaker embeddings used for testing. Additionally, we evaluated the actual effectiveness of the predicted durations.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10112v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Experimental setup &#8227; 4. Experiments &#8227; FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> indicate that our model achieves the highest scores on the majority of metrics for both speech naturalness and speaker similarity, surpassing all baseline models, with the exception of a single metric where it ranks second. Even when converting Databaker data to AISHELL speakers, our model maintains a high level of similarity. Despite the significant differences in style and Mandarin proficiency between the source and target speakers, the model achieves it. This clearly demonstrates the role of phoneme-level SSL features and the duration predictor in enhancing speaker similarity. The higher Cos.sim scores further confirm that our model maintains high speaker similarity. Additionally, more comprehensive feature information can facilitate the model in generating speech with higher naturalness. Our proposed model achieves lower CER and PER compared to all other models. This also demonstrates the effectiveness of FabasedVC in addressing the loss of content completeness and better preserving the linguistic content of the source speech. In the ablation experiments, we separately removed the SSL feature encoder modules <math alttext=\"c_{att}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{att}</annotation></semantics></math> and <math alttext=\"c_{avg}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{avg}</annotation></semantics></math> as well as the duration predictor. It was observed that removing <math alttext=\"c_{att}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{att}</annotation></semantics></math> or <math alttext=\"c_{avg}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{avg}</annotation></semantics></math> leads to a noticeable decline in both speech naturalness and similarity. This removal also results in some loss of content completeness. Additionally, disabling the duration predictor results in generating speech with the same timing as the source, which can lead to a decrease in perceived speaker similarity.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"D_{\\text{conv}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>D</mi><mtext>conv</mtext></msub><annotation encoding=\"application/x-tex\">D_{\\text{conv}}</annotation></semantics></math> and <math alttext=\"D_{\\text{ref}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>D</mi><mtext>ref</mtext></msub><annotation encoding=\"application/x-tex\">D_{\\text{ref}}</annotation></semantics></math> represent the average phoneme duration of the converted speech and the reference speech, respectively. This formula quantifies the percentage deviation in phoneme duration between the converted utterance and the reference, providing a normalized measure that enables meaningful comparison across speakers with different speaking rates.</p>\n\n",
                "matched_terms": [
                    "average",
                    "duration",
                    "speakers",
                    "deviation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown, the converted speech exhibits a low average RDD relative to the source speaker, indicating that the model preserves the original temporal structure and speaking rate of the source utterance. At the same time, the average RDD relative to the target speaker is 11.34%, which is significantly lower than the average RDD<sub class=\"ltx_sub\">source&#8594;target</sub> of 20.85%. This demonstrates that the model successfully shifts the duration characteristics toward those of the target speaker, achieving a partial but meaningful adaptation without fully sacrificing the source timing. This confirms that the proposed approach successfully leverages the source speaking rate while producing duration characteristics similar to those of the target.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "relative",
                    "rddsourcetarget",
                    "rdd",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose FabasedVC, a VC system with high timbre similarity and content integrity. We adopted a VITS-based framework for waveform reconstruction. To improve content completeness in voice conversion, we enhanced the prior encoder by introducing textual information through front-end feature processing. Moreover, we processed SSL features from the frame-level to the phoneme-level to disentangle timbre information. We also incorporated a duration predictor to adjust duration. Experimental results demonstrate the high naturalness and similarity achieved by our method. In the future, we will continue to focus on enhancing prosody and content integrity in voice conversion.</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "duration",
                    "voice"
                ]
            }
        ]
    }
}