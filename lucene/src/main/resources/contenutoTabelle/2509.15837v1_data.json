{
    "S2.T1": {
        "caption": "Table 1: Semantic groups with average concreteness ratings and average pairwise Levenshtein distances (Â± 1 std. dev.)",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Category (#words)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Avg. Concreteness</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Avg. Phon. Dist.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Musical instruments (10)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.91 &#177; 0.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.64 &#177; 0.11</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Clothing articles (19)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.87 &#177; 0.12</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.63 &#177; 0.10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Vegetables (19)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.86 &#177; 0.16</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.64 &#177; 0.13</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Vehicles (14)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.85 &#177; 0.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.68 &#177; 0.12</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Building materials (16)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.78 &#177; 0.14</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.67 &#177; 0.12</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Organs (8)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.65 &#177; 0.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.65 &#177; 0.11</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Financial terms (13)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.11 &#177; 0.40</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.62 &#177; 0.10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Emotions (10)</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.10 &#177; 0.41</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.70 &#177; 0.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Ethical/ Legal terms (8)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.84 &#177; 0.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.45pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.64 &#177; 0.11</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "building",
            "articles",
            "emotions",
            "ethical",
            "semantic",
            "legal",
            "materials",
            "words",
            "levenshtein",
            "vegetables",
            "pairwise",
            "average",
            "terms",
            "category",
            "organs",
            "distances",
            "financial",
            "avg",
            "vehicles",
            "musical",
            "dev",
            "concreteness",
            "groups",
            "clothing",
            "ratings",
            "std",
            "dist",
            "phon",
            "instruments"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For our </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">semantic clustering</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset, we manually assembled nine groups of semantically related words (shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.2.2 Phonetic &amp; semantic clustering &#8227; 2.2 Representational analyses &#8227; 2 Experimental setup &#8227; The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), each containing at least 8 words (avg. 13 words per group) &#8211; e.g., &#8216;musical instruments&#8217; (words like piano, guitar, violin) and &#8216;ethical/legal terms&#8217; (words like justice, fairness, honesty). Of these semantic groups, six are made up of concrete words (avg. concreteness rating in the top 10%), and three are made up of abstract words (avg. concreteness rating in the bottom 25%). Semantic groups are validated using GloVe embeddings, ensuring that all pairwise cosine similarities of words within semantic groups fall within the top 15% of all pairwise similarities in the MALD dataset. Additionally, words within semantic groups were selected to be phonetically distinct from each other (avg. norm. Levenshtein distance </span>\n  <math alttext=\"d&gt;0.6\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">d</mi>\n        <mo mathsize=\"0.900em\">&gt;</mo>\n        <mn mathsize=\"0.900em\">0.6</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">d&gt;0.6</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">How does visual information included in training affect language processing in audio- and text-based deep learning models?\nWe explore how such <em class=\"ltx_emph ltx_font_italic\">visual grounding</em> affects model-internal representations of words, and find substantially different effects in speech- vs. text-based language encoders.\nFirstly, global representational comparisons reveal that visual grounding increases alignment between representations of spoken and written language, but this effect seems mainly driven by enhanced encoding of word identity rather than meaning. We then apply targeted clustering analyses to probe for phonetic vs. semantic discriminability in model representations. Speech-based representations remain phonetically dominated with visual grounding, but in contrast to text-based representations, visual grounding does not improve semantic discriminability. Our findings could usefully inform the development of more efficient methods to enrich speech-based models with visually-informed semantics.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "semantic",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">One reported benefit of visually grounded representation spaces is their enhanced semantic structuring. Compared to their unimodal counterparts, Text-based Language Encoders (TLEs) enriched with visual information in training show higher alignment with human word similarity ratings </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and better clustering by broad semantic categories </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The semantic enhancement offered by visual grounding would seem to be particularly promising for Speech-based Language Encoder models (SLEs), given that unimodal SLE representations are generally dominated by sound- rather than meaning-based similarities </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and capture less abstract semantic concepts than text-based models </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, there is currently no direct comparison of the effects of visual grounding on speech- vs. text-based encoder models that would demonstrate such an advantage.</span>\n</p>\n\n",
                "matched_terms": [
                    "ratings",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we therefore analyze word representations in a visually grounded SLE, and compare them to its ungrounded counterpart as well as to text-based models. In our first set of analyses, we find that visually grounded speech representations globally show greater alignment with text-based models, but no particular enhancement of meaning-based similarities. In follow-up analyses, we shift our focus to </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">subspaces</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of the embedding space tuned for specific kinds of information. We design two datasets of phonetic and semantic word groups, to measure the degree of clustering for each in both visually grounded and ungrounded models. While phonetic clustering remains similar, semantic clustering in speech-based language encoders shows surprising degradation after visual grounding. These results suggest that current approaches to visual grounding can only enhance the clustering of semantic categories in representation spaces which already saliently encode meaning &#8212; an important consideration for future developments in cross-modal representation learning. All code and data are provided for reproducibility</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/adrian-sauter/visual_grounding_speech_analysis\" title=\"\">https://github.com/adrian-sauter/visual_grounding_speech_analysis</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "groups",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We first ask how visual grounding globally affects the representation space of speech- and text-based models, as well as the alignment between them. We measure the similarity between model-internal representations of speech vs. text inputs using Centered Kernel Alignment (CKA; </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). CKA\nallows for comparisons between models with different architectures and representation spaces, by comparing pairwise similarity matrices\nextracted from each individual model. For this purpose, we create a large dataset of parallel SLE and TLE embeddings by sampling words from LibriSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using existing forced alignments, yielding over 9,500 unique words and 70,000 word tokens from 80 speakers. Since LibriSpeech contains spoken sequences rather than individual words, we obtain word-level representations by audio slicing, i.e. using only the isolated target word as model input and excluding surrounding context (following </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "pairwise",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We next zoom in on similarities between particular classes of word pairs to more specifically analyze what types of information drive changes in SLE word representations after visual grounding. Replicating earlier analyses on unimodal SLEs </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we measure cosine similarities between same-speaker, same-word, near-homonym, and synonym pairs. These analyses aim to quantify the salience of different types of information (i.e. speaker identity, word identity, word pronunciation, word meaning) in model-internal representations.\nWe follow </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in obtaining sets of audio-sliced word pairs from LibriSpeech and additional sources of word annotations. Synonyms are obtained through the synsets (cognitive synonyms) in WordNet </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To find near-homophones, we compute the normalized Levenshtein distances between phonemic transcriptions of word pairs from the CMU Pronunciation Dictionary </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Near-homophones are defined as word pairs with normalized Levenshtein distance </span>\n  <math alttext=\"d\\leq 0.4\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">d</mi>\n        <mo mathsize=\"0.900em\">&#8804;</mo>\n        <mn mathsize=\"0.900em\">0.4</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">d\\leq 0.4</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, corresponding to the top 0.1% of random word pairs in LibriSpeech. Same-word pairs (identical words) and same-speaker pairs (unique words from the same speaker) are obtained from annotations included in LibriSpeech. Finally, random word pairs form a lower bound on cosine similarities, and similarities between all other word pair classes are normalized by subtracting the mean similarity between random pairs. To assess the reliability for our word-pair analyses, we sample 10k word utterance five times, and report the average value across the five experiments with 95% confidence intervals.</span>\n</p>\n\n",
                "matched_terms": [
                    "levenshtein",
                    "average",
                    "distances",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While previous work has shown that unimodal SLE representations are more dominated by phonetic than semantic information </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, analyses involving learned projections have shown that word representations from those same SLEs do show significant alignment with text-based semantic embeddings like GloVe </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and also align with sentence-level semantic similarity judgments better than acoustic and text-only baselines </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To investigate the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">decodability</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of different kinds of word-level information as compared to their salience, we measure the degree of clustering for two types of word categories, in full model-internal representations as compared to two dimensionality-reduced subspaces. We quantify the degree of clustering by computing silhouette coefficients, which measure how well datapoints fit with their assigned groups by comparing mean cosine distances for within-group datapoints to mean distances for across-group datapoints </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We compare dimensionality-reduced projections obtained by Principal Component Analysis (PCA) to dimensionality-reduced projections obtained by Linear Discriminant Analysis (LDA). While PCA projections optimize for explained variance in the original embedding space, LDA projections are linearly optimized for separability between target groups with supervision from group labels. LDA has previously been used to analyze both audio- and text model representations </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib22\" title=\"\">22</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "groups",
                    "semantic",
                    "distances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In our experiments on phonetic and semantic clustering, we make use of the Massive Auditory Lexical Decision dataset (MALD; </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), which consists of more than 26,000 English word recordings from a single speaker. This allows us to create our two targeted analysis datasets for studying within-speaker structuring of phonetic and semantic information, controlling for semantic and phonetic distances respectively, as well as average concreteness scores across word groups. Concreteness scores are human ratings collected by </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, who define the concreteness of a word as the degree to which its referent is a perceptible entity.</span>\n</p>\n\n",
                "matched_terms": [
                    "ratings",
                    "average",
                    "semantic",
                    "concreteness",
                    "groups",
                    "distances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For our </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">phonetic clustering</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset, we identified groups of phonetically similar but semantically distinct words. We employed the following process until 14 groups across 81 unique words were found, with each group containing at least 5 words (avg. 5.79 words per group) and none containing identical words. First, a random word from the MALD dataset was selected. Next, phonetically similar words were identified based on normalised Levenshtein distances between phonemic transcriptions (</span>\n  <math alttext=\"d\\leq 0.529\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">d</mi>\n        <mo mathsize=\"0.900em\">&#8804;</mo>\n        <mn mathsize=\"0.900em\">0.529</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">d\\leq 0.529</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, top 10% in MALD), while ensuring they were dissimilar (</span>\n  <math alttext=\"d&gt;0.529\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">d</mi>\n        <mo mathsize=\"0.900em\">&gt;</mo>\n        <mn mathsize=\"0.900em\">0.529</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">d&gt;0.529</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) from words in previously identified groups. Semantically similar words were excluded by retaining only those with a cosine similarity below </span>\n  <math alttext=\"0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">0.1</mn>\n      <annotation encoding=\"application/x-tex\">0.1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in GloVe embedding space </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Finally, we only included words classified as very concrete (top 25%) or very abstract (bottom 25%) according to the concreteness ratings provided in MALD, ultimately resulting in 7 groups of concrete words and 7 groups of abstract words. Example groups include (1) handshake, handbrake, handbook, handmaid, handmade (avg. concreteness: 4.42), and (2) assume, astute, allure, acute, akin (avg. concreteness: 1.83).</span>\n</p>\n\n",
                "matched_terms": [
                    "levenshtein",
                    "avg",
                    "ratings",
                    "words",
                    "concreteness",
                    "groups",
                    "distances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given the relatively small dataset sizes for our phonetic and semantic clustering analyses, we use a leave-one-out approach to report silhouette score results, iteratively excluding one category to mitigate spurious patterns and assess robustness. We report the mean over all iterations with 95% confidence intervals.</span>\n</p>\n\n",
                "matched_terms": [
                    "category",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The representational analyses in the previous section revealed what information is most saliently represented in SLEs, and how this is influenced by visual grounding. As described in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#S2.SS2.SSS2\" style=\"font-size:90%;\" title=\"2.2.2 Phonetic &amp; semantic clustering &#8227; 2.2 Representational analyses &#8227; 2 Experimental setup &#8227; The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we will now shift our focus from representational salience to the decodability of particular categories of interest (phonetically and semantically grouped words), while avoiding speaker- and word-identity effects by analyzing groups of unique words recorded by a single speaker.</span>\n</p>\n\n",
                "matched_terms": [
                    "groups",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#S3.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 3.2 Effects of visual grounding on phonetic clustering &#8227; 3 Findings &#8227; The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the degree of clustering by phonetic groups in both SLEs, as computed across the models&#8217; full (768-dimensional) embeddings, the top-8 PCA components, and the top-8 LDA dimensions optimized for separability between phonetic groups. The full and PCA-based scores show that phonetically similar (but not identical) words are not particularly tightly clustered in either models&#8217; native embedding space, though somewhat more strongly in </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">FaST-VGS+</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> than in </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">wav2vec2</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (aligning with the slightly increased similarities observed for near-homophones in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#S3.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 3.1 Visual grounding bridges modalities by enhancing word identity representation &#8227; 3 Findings &#8227; The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). However, the LDA-based scores show that phonetic groups are successfully decodable from both SLE models after an optimized linear projection. This highlights LDA&#8217;s effectiveness at decoding class-specific information from model representations, and we therefore report LDA-based clustering scores for our further analyses.</span>\n</p>\n\n",
                "matched_terms": [
                    "groups",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#S3.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 3.3 Effects of visual grounding on semantic clustering &#8227; 3 Findings &#8227; The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the degree of clustering by semantic groups across layers of both SLE and TLE models. In line with previous results on visual grounding in TLEs </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we find that the decodability of semantic word clusters is improved in </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">VG-BERT</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as compared to </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">BERT</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (bottom-right plot), and this effect increases from middle model layers onwards, with the final output representations of </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">VG-BERT</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> nearing the topline clustering scores for </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">GloVe</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (high scores are expected for </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">GloVe</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, since we defined semantic groups based on similarity in </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">GloVe</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> embedding space). In contrast, the decodability of semantic clusters in SLE representations peaks in layer 7 of the ungrounded </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">wav2vec2</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model. This peak is absent after visual grounding, with the </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">FaST-VGS+</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model also showing generally lower semantic clustering performance. The degradation of more abstract linguistic information in final model layers is a common finding in SLE representational analyses, and has been attributed to these models&#8217; low-level acoustic prediction objectives in self-supervised training </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib26\" title=\"\">26</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, the general lack of improvement in semantic clustering for </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">FaST-VGS+</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as compared to </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">wav2vec2</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reveals a striking difference in the effects of visual grounding on text- vs. speech-based word representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "groups",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">It is a well-established finding that both unimodal and multimodal models learn concrete concepts more easily than abstract ones </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a trend also found in humans </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Because we designed our word clustering datasets to include both highly concrete and highly abstract word groups (see Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#S2.SS2.SSS2\" style=\"font-size:90%;\" title=\"2.2.2 Phonetic &amp; semantic clustering &#8227; 2.2 Representational analyses &#8227; 2 Experimental setup &#8227; The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders\">\n    <span class=\"ltx_text ltx_ref_tag\">2.2.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), we can examine whether this effect holds for the observed phonetic and semantic clustering patterns in SLE and TLE word representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "groups",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For semantic groups, the concreteness advantage does hold in both ungrounded and grounded TLEs: average silhouette scores in </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">BERT</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> layer 11 are 0.34 for abstract and 0.52 for concrete categories, compared to 0.42 and 0.62 respectively in </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">VG-BERT</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> layer 11. The ungrounded </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">wav2vec2</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> representations form an exception to this pattern, with higher (layer 7) scores for abstract categories (0.54) compared to concrete ones (0.50), while its visually grounded counterpart does follow the concreteness advantage despite its generally lower scores (</span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">FaST-VGS+</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> layer 10 representations score 0.14 on average for abstract, vs. 0.21 for concrete categories). We note that the concreteness effect is generally stronger in visually grounded models; this is likely due to the dominance of concrete concepts in the image captioning datasets used for visually grounded training.</span>\n</p>\n\n",
                "matched_terms": [
                    "concreteness",
                    "average",
                    "groups",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We analyzed the effect of visual grounding in word representations extracted from speech-based SLEs compared to textb-based TLEs. Previous work on SLEs has measured the encoding of linguistic features using trained classification or regression probes </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib26\" title=\"\">26</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, or by directly analyzing distances in the representation space </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15837v1#bib.bib26\" title=\"\">26</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These two types of measures\nproduce similar results when we are dealing with\nfeatures that are very saliently encoded across a model&#8217;s representation space (e.g., phonetics). However, they can show intriguing differences\nwhen we are probing for information that is represented only in a small subspace of the whole embedding space (for instance, semantic information in an embedding space that overwhelmingly represents acoustic information).</span>\n</p>\n\n",
                "matched_terms": [
                    "semantic",
                    "distances"
                ]
            }
        ]
    }
}