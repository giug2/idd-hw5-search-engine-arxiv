{
    "S5.T1": {
        "source_file": "CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance",
        "caption": "TABLE I: The prosody similarity between synthesized and prompt speech in terms of differences in pitch, energy, and other prosodic indicators. Lower values indicate higher similarity.",
        "body": "LibriTTS\nPitch\nEnergy\nOthers\n\n\nMean ↓\\downarrow\n\nStd ↓\\downarrow\n\nSkew ↓\\downarrow\n\nKurt ↓\\downarrow\n\nMean ↓\\downarrow\n\nStd ↓\\downarrow\n\nSkew ↓\\downarrow\n\nKurt ↓\\downarrow\n\nJitter ↓\\downarrow\n\nShimmer ↓\\downarrow\n\nHNR ↓\\downarrow\n\n\n\nPaired\n19.03\n22.79\n2.43\n19.68\n1.80\n1.54\n0.33\n0.95\n0.63\n0.56\n3.28\n\n\nUnpaired\n53.41\n32.47\n2.81\n21.82\n4.60\n2.28\n0.46\n1.23\n0.89\n0.78\n3.79\n\n\nAISHELL-3\nPitch\nEnergy\nOthers\n\n\nMean ↓\\downarrow\n\nStd ↓\\downarrow\n\nSkew ↓\\downarrow\n\nKurt ↓\\downarrow\n\nMean ↓\\downarrow\n\nStd ↓\\downarrow\n\nSkew ↓\\downarrow\n\nKurt ↓\\downarrow\n\nJitter ↓\\downarrow\n\nShimmer ↓\\downarrow\n\nHNR ↓\\downarrow\n\n\n\nPaired\n42.70\n26.29\n1.13\n3.85\n2.94\n2.38\n0.44\n1.71\n0.78\n0.43\n4.42\n\n\nUnpaired\n65.73\n29.87\n1.63\n7.05\n5.05\n2.58\n0.57\n1.83\n0.94\n0.58\n4.95",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">LibriTTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Pitch</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Energy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Others</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Mean <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Std <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Skew <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Kurt <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Mean <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Std <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Skew <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Kurt <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Jitter <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Shimmer <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">HNR <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m11\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">Paired</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">19.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">22.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">2.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">19.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">1.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">1.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">0.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">0.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">0.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">3.28</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Unpaired</td>\n<td class=\"ltx_td ltx_align_center\">53.41</td>\n<td class=\"ltx_td ltx_align_center\">32.47</td>\n<td class=\"ltx_td ltx_align_center\">2.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">21.82</td>\n<td class=\"ltx_td ltx_align_center\">4.60</td>\n<td class=\"ltx_td ltx_align_center\">2.28</td>\n<td class=\"ltx_td ltx_align_center\">0.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.23</td>\n<td class=\"ltx_td ltx_align_center\">0.89</td>\n<td class=\"ltx_td ltx_align_center\">0.78</td>\n<td class=\"ltx_td ltx_align_center\">3.79</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">AISHELL-3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Pitch</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Energy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Others</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Mean <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m12\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Std <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m13\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Skew <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m14\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Kurt <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Mean <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m16\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Std <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m17\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Skew <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m18\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Kurt <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m19\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Jitter <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m20\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Shimmer <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m21\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">HNR <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m22\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">Paired</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">42.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">26.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">1.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">3.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">2.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">2.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">0.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">1.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">0.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">0.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">4.42</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Unpaired</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">65.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">29.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">7.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">5.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">1.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.95</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "libritts",
            "prompt",
            "speech",
            "energy",
            "lower",
            "skew",
            "unpaired",
            "indicators",
            "pitch",
            "aishell3",
            "prosody",
            "hnr",
            "std",
            "synthesized",
            "kurt",
            "↓downarrow",
            "similarity",
            "prosodic",
            "others",
            "higher",
            "differences",
            "indicate",
            "terms",
            "jitter",
            "values",
            "paired",
            "other",
            "between",
            "mean",
            "shimmer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T1\" title=\"TABLE I &#8227; V-A Evaluating Prompt-Induced Prosody Similarity in MaskGCT &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> presents a quantitative analysis of prompt-induced prosody similarity by comparing the acoustic differences between synthesized and prompt speech under paired and unpaired conditions. Across both LibriTTS and AISHELL-3, paired prompts consistently yield lower differences in pitch, energy, and other prosodic indicators, confirming stronger alignment in prosodic patterns. In contrast, the unpaired condition results in noticeably higher deviations, particularly in pitch mean, energy mean, and jitter, suggesting that the synthesized outputs are heavily influenced by the prosodic characteristics of the prompt.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Singing Voice Synthesis (SVS) aims to generate expressive vocal performances from structured musical inputs such as lyrics and pitch sequences. While recent progress in discrete codec-based speech synthesis has enabled zero-shot generation via in-context learning, directly extending these techniques to SVS remains non-trivial due to the requirement for precise melody control. In particular, prompt-based generation often introduces prosody leakage, where pitch information is inadvertently entangled within the timbre prompt, compromising controllability. We present CoMelSinger, a zero-shot SVS framework that enables structured and disentangled melody control within a discrete codec modeling paradigm. Built on the non-autoregressive MaskGCT architecture, CoMelSinger replaces conventional text inputs with lyric and pitch tokens, preserving in-context generalization while enhancing melody conditioning. To suppress prosody leakage, we propose a coarse-to-fine contrastive learning strategy that explicitly regularizes pitch redundancy between the acoustic prompt and melody input. Furthermore, we incorporate a lightweight encoder-only Singing Voice Transcription (SVT) module to align acoustic tokens with pitch and duration, offering fine-grained frame-level supervision. Experimental results demonstrate that CoMelSinger achieves notable improvements in pitch accuracy, timbre consistency, and zero-shot transferability over competitive baselines.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "prompt",
                    "speech",
                    "prosody",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Singing voice synthesis (SVS) aims to transform structured musical inputs&#8212;most often lyrics and pitch sequences&#8212;into expressive, high-quality vocal performances. Over the past decade, it has moved from a niche research topic to an essential tool in creative audio technologies, propelled by the rise of AI-driven music generation, virtual performers, and personalized media experiences. Its applications now extend well beyond traditional karaoke systems, finding a place in virtual idol production, game soundtracks, and content creation for social platforms. Parallel to these expanding use cases, advances in deep generative models have brought marked gains in timbre fidelity, pitch accuracy, and the overall naturalness of synthesized voices <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib1\" title=\"\">1</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib2\" title=\"\">2</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib3\" title=\"\">3</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib4\" title=\"\">4</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib5\" title=\"\">5</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib6\" title=\"\">6</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib7\" title=\"\">7</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib8\" title=\"\">8</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "synthesized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete token-based architectures show great in-context learning capabilities and provide a promising pathway for such zero-shot generation. In a related task, text-to-speech (TTS) has witnessed rapid progress with discrete acoustic tokens derived from vector quantization and neural audio codecs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib16\" title=\"\">16</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib17\" title=\"\">17</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>]</cite>. By mapping complex waveforms into a quantized latent space, these tokens capture timbre, prosody, and phonetic content, thereby reformulating speech synthesis as symbolic sequence modeling akin to language modeling. The success of token-based TTS systems is largely enabled by large-scale multi-speaker corpora, which provide sufficient diversity to learn robust and generalizable representations. In contrast, the scarcity and limited diversity of singing data make token-based modeling for SVS significantly more challenging.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Built upon this data-rich foundation, recent TTS systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib20\" title=\"\">20</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib21\" title=\"\">21</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib16\" title=\"\">16</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib17\" title=\"\">17</a>]</cite>, have adopted large language model (LLM)-style architectures to model the conditional distribution of acoustic tokens given phoneme sequences and optional prompts. Within this framework, in-context learning (ICL) becomes feasible: a short segment of reference speech, represented as discrete tokens, serves as an acoustic prompt to guide synthesis in terms of speaker identity and style. This approach, exemplified by models such as VALL-E <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite>, enables zero-shot speech synthesis by treating speech generation as a form of conditional codec language modeling, without requiring speaker labels or model adaptation. Inspired by these advances, researchers have begun exploring discrete token-based methods for SVS.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "speech",
                    "terms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, directly extending TTS by replacing textual input with structured musical inputs&#8212;such as lyrics and pitch tokens&#8212;while reusing the same modeling pipeline reveals a unique challenge in the singing domain: prosody leakage from the acoustic prompt, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. In prompt-based synthesis, the acoustic prompt is intended to provide timbral cues, yet pitch-related attributes&#8212;including contour and timing&#8212;are often inadvertently encoded into its latent representation. This unintended encoding leads to timbre&#8211;melody entanglement, where the prompt simultaneously influences vocal timbre and melodic realization. As a result, the system&#8217;s control over the explicitly specified pitch sequence is weakened, undermining the precise separation between timbre conditioning and melody generation that SVS requires.</p>\n\n",
                "matched_terms": [
                    "between",
                    "prompt",
                    "pitch",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The disparity in dataset size between singing <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib22\" title=\"\">22</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib23\" title=\"\">23</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib24\" title=\"\">24</a>]</cite>, and speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib25\" title=\"\">25</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib26\" title=\"\">26</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib27\" title=\"\">27</a>]</cite>, further exacerbates the difficulty of addressing prosody leakage. Importantly, the effectiveness of in-context learning in TTS relies on large-scale, diverse speech corpora, which enable robust learning of disentangled token representations. In contrast, singing datasets are generally smaller and less varied, making it harder to avoid prosodic interference and to generalize prompt-based conditioning. Consequently, directly transferring token-based prompting strategies from TTS to SVS often leads to weaker melody control in zero-shot scenarios. In this case, achieving reliable melody control is particularly crucial for maintaining alignment with the musical score.</p>\n\n",
                "matched_terms": [
                    "between",
                    "prosodic",
                    "speech",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, SVS requires much finer control over pitch and melody than TTS or voice conversion, as the generated singing must accurately follow the musical score while preserving timbre. Although attribute control has been explored through adversarial training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>]</cite>, contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib28\" title=\"\">28</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib29\" title=\"\">29</a>]</cite>, and information-bottleneck methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib30\" title=\"\">30</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib31\" title=\"\">31</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib32\" title=\"\">32</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib33\" title=\"\">33</a>]</cite>, these approaches primarily focus on coarse prosodic patterns or emotional cues and provide limited support for fine-grained melody control. Make-A-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite>, as a representative discrete-token SVS system, adopts prompt-guided conditioning but lacks explicit mechanisms to prevent the acoustic prompt from influencing melody realization. To the best of our knowledge, no prior work has systematically addressed prosody leakage in discrete token-based SVS, leaving a critical gap in achieving precise and faithful melody control in zero-shot scenarios.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prosodic",
                    "pitch",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenges of prosody leakage and limited melody controllability in zero-shot SVS, we propose CoMelSinger, a discrete codec-based framework with structured melody control.\nCoMelSinger builds on the non-autoregressive MaskGCT architecture&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, adapting it to accept musical inputs consisting of lyrics and pitch tokens.\nTo achieve better melody&#8211;timbre control, we introduce a coarse-to-fine contrastive learning strategy that limits excessive pitch-related information in the acoustic prompt, allowing the explicit pitch condition to guide melodic realization more effectively.\nWe further incorporate a lightweight, encoder-only Singing Voice Transcription (SVT) module to provide fine-grained, frame-level supervision by aligning acoustic tokens with pitch and duration sequences.\nTogether, these designs enable accurate melody modeling, maintain timbre consistency, and preserve the in-context learning capability of discrete token-based systems.\nExtensive experiments on both seen and unseen singers demonstrate that CoMelSinger delivers substantial improvements in pitch accuracy, timbre consistency, and overall synthesis quality compared with state-of-the-art SVS baselines in zero-shot scenarios. The main contributions of this work are:</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "pitch",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of discrete token modeling in TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib16\" title=\"\">16</a>]</cite>, recent SVS studies have explored token-based representations to improve generalization.\nTokSing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib41\" title=\"\">41</a>]</cite> employs a non-autoregressive Transformer conditioned on lyrics and pitch embeddings to predict discrete acoustic tokens.\nHiddenSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib3\" title=\"\">3</a>]</cite> integrates a diffusion-based decoder guided by discrete pitch and semantic tokens, achieving high-quality synthesis.\nMake-A-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite> unifies speech and singing synthesis through a shared discrete representation, but uses a relatively small proportion of singing data and does not incorporate prompt-based in-context learning.\nConsequently, it lacks explicit melody conditioning and provides limited flexibility in zero-shot singing scenarios.\nIn contrast, our approach introduces melody inputs and improved melody control in prompt-based conditioning, enabling more accurate and controllable zero-shot singing voice synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of large language models (LLMs), recent approaches formulate speech synthesis as autoregressive generation over discrete codec tokens.\nVALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite> pioneered this direction by conditioning on both text and a short acoustic prompt to synthesize high-fidelity speech.\nIts extensions&#8212;VALL-E X&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib44\" title=\"\">44</a>]</cite>, VALL-E 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib45\" title=\"\">45</a>]</cite>, and VALL-E R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib46\" title=\"\">46</a>]</cite>&#8212;extend the paradigm to cross-lingual synthesis, streaming generation, and improved alignment.\nSoCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib20\" title=\"\">20</a>]</cite> further improves efficiency through semantic-ordered multi-stream tokenization and segment-level modeling.\nThrough prompt-based conditioning, these models demonstrate strong zero-shot capability and robust speaker generalization.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this foundation, we adapt the MaskGCT framework to singing voice synthesis.\nOur approach incorporates structured melody conditioning and improved melody&#8211;timbre control in prompt-based synthesis to address the challenges of pitch fidelity and prosody leakage in zero-shot settings.\nThis design improves controllability over melodic realization while preserving the inference efficiency and generalization strengths of discrete token-based modeling.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fine-grained prosody control&#8212;particularly over fundamental frequency (F0) and phoneme duration&#8212;is essential for expressive speech and singing synthesis.\nSeveral TTS studies have incorporated explicit prosodic supervision to guide model learning.\nProsody-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib48\" title=\"\">48</a>]</cite> augments an end-to-end architecture with auxiliary predictors for phoneme-level F0 and duration, enabling precise rhythm and pitch control without degrading naturalness.\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib49\" title=\"\">49</a>]</cite> adopt utterance-level prosodic features in a hierarchical non-autoregressive model, providing interpretable style modulation across prosodic dimensions while maintaining synthesis quality.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "speech",
                    "pitch",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances extend prosody control to diffusion-based synthesis.\nDiffStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib50\" title=\"\">50</a>]</cite> combines a diffusion decoder with classifier-free guidance to model prosodic style at both coarse and phoneme-level scales, supporting flexible pitch&#8211;duration manipulation.\nDrawSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib51\" title=\"\">51</a>]</cite> enables editing by conditioning on user-drawn pitch&#8211;energy sketches, which are refined into high-resolution prosody contours.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "prosody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In SVS, accurate melody control often requires note-level F0 alignment.\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib52\" title=\"\">52</a>]</cite> combine dual-path pitch encoders with local style tokens to capture expressiveness beyond score constraints.\nDiscrete-token SVS frameworks such as TokSing<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib41\" title=\"\">41</a>]</cite> incorporate explicit melody tokens to enrich synthetic singing, while Prompt-Singer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib53\" title=\"\">53</a>]</cite> decouples vocal range from melody contour in prompt conditioning to preserve pitch accuracy across timbres.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, few systems address conflicts between external melody guidance and prompt-derived timbre cues in discrete-token SVS.\nWe address this gap by introducing coarse-to-fine contrastive learning with frame-level pitch supervision to reduce melody leakage from prompts and strengthen external melody control, enabling high-fidelity pitch realization without sacrificing timbre consistency or generalization.</p>\n\n",
                "matched_terms": [
                    "between",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable zero-shot singing voice synthesis with accurate melody control and disentangled prompt conditioning, we propose <em class=\"ltx_emph ltx_font_italic\">CoMelSinger</em>, a two-stage framework illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S3.F3\" title=\"Figure 3 &#8227; III Method &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Inspired by MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite> and Make-A-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite>, CoMelSinger comprises a Text-to-Semantic (T2S) stage and a Semantic-to-Acoustic (S2A) stage. The T2S module <math alttext=\"f_{\\mathrm{T2S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>T2S</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{T2S}}</annotation></semantics></math> transforms a lyric token sequence <math alttext=\"\\tilde{\\mathbf{m}}^{l}=[\\tilde{m}^{l}_{1},\\dots,\\tilde{m}^{l}_{S}]\\in\\mathcal{V}_{\\mathrm{lyr}}^{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>l</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mover accent=\"true\"><mi>m</mi><mo>~</mo></mover><mn>1</mn><mi>l</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>m</mi><mo>~</mo></mover><mi>S</mi><mi>l</mi></msubsup><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>lyr</mi><mi>S</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{l}=[\\tilde{m}^{l}_{1},\\dots,\\tilde{m}^{l}_{S}]\\in\\mathcal{V}_{\\mathrm{lyr}}^{S}</annotation></semantics></math>, obtained from a Grapheme-to-Phoneme (G2P) converter, and a semantic prompt <math alttext=\"\\mathbf{s}^{r}=E_{S}(\\mathbf{w}^{r})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119852;</mi><mi>r</mi></msup><mo>=</mo><mrow><msub><mi>E</mi><mi>S</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119856;</mi><mi>r</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}^{r}=E_{S}(\\mathbf{w}^{r})</annotation></semantics></math> extracted from the reference waveform <math alttext=\"\\mathbf{w}^{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119856;</mi><mi>r</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{w}^{r}</annotation></semantics></math>, into a semantic token sequence <math alttext=\"\\mathbf{s}\\in\\mathcal{V}_{\\mathrm{sem}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#119852;</mi><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>sem</mi><mi>L</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}\\in\\mathcal{V}_{\\mathrm{sem}}^{L}</annotation></semantics></math>, where <math alttext=\"\\mathcal{V}_{\\mathrm{sem}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>sem</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{V}_{\\mathrm{sem}}</annotation></semantics></math> denotes the semantic vocabulary and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> the sequence length. The S2A module <math alttext=\"f_{\\mathrm{S2A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>S2A</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{S2A}}</annotation></semantics></math> then predicts acoustic tokens <math alttext=\"\\mathbf{a}\\in\\mathcal{V}_{\\mathrm{aco}}^{L\\times N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mi>&#119834;</mi><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>aco</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{a}\\in\\mathcal{V}_{\\mathrm{aco}}^{L\\times N}</annotation></semantics></math>, conditioned on the semantic tokens <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math>, an acoustic prompt <math alttext=\"\\mathbf{a}^{r}=E_{A}(\\mathbf{w}^{r})\\in\\mathcal{V}_{\\mathrm{aco}}^{L_{r}\\times N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m11\" intent=\":literal\"><semantics><mrow><msup><mi>&#119834;</mi><mi>r</mi></msup><mo>=</mo><mrow><msub><mi>E</mi><mi>A</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119856;</mi><mi>r</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>aco</mi><mrow><msub><mi>L</mi><mi>r</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r}=E_{A}(\\mathbf{w}^{r})\\in\\mathcal{V}_{\\mathrm{aco}}^{L_{r}\\times N}</annotation></semantics></math>, and a regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\\in\\mathcal{V}_{\\mathrm{pit}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m12\" intent=\":literal\"><semantics><mrow><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>pit</mi><mi>L</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}\\in\\mathcal{V}_{\\mathrm{pit}}^{L}</annotation></semantics></math>. Here, <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m13\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> denotes the number of residual vector quantization (RVQ) codebooks in the acoustic codec, and the regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m14\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math> is derived from the pitch <math alttext=\"\\mathbf{m}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m15\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{p}</annotation></semantics></math> and duration <math alttext=\"\\mathbf{m}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m16\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>d</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{d}</annotation></semantics></math> sequence through the length expansion module <math alttext=\"LE(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m17\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">LE(\\cdot)</annotation></semantics></math>. Both the semantic and acoustic tokens are produced using discrete codec tokenizers following the MaskGCT setup, and the final waveform <math alttext=\"\\mathbf{w}^{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m18\" intent=\":literal\"><semantics><msup><mi>&#119856;</mi><mi>o</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{w}^{o}</annotation></semantics></math> is reconstructed from acoustic tokens via the decoder <math alttext=\"D_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m19\" intent=\":literal\"><semantics><msub><mi>D</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">D_{A}</annotation></semantics></math>. The complete pipeline is summarized as:</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance melody controllability and suppress interference from prompt-induced timbre cues, we propose a coarse-to-fine contrastive learning framework. At the sequence level, a contrastive loss encourages the predicted acoustic token sequence <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math> to preserve the overall pitch contour defined by <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>. At the frame level, a token-wise contrastive objective aligns fine-grained acoustic features with localized pitch variations, thereby reinforcing frame-level pitch fidelity. In addition, we introduce an auxiliary singing voice transcription (SVT) model, trained to estimate pitch sequences directly from acoustic tokens. The SVT model provides pseudo pitch labels that serve as external supervision during S2A training. This auxiliary signal further improves alignment between the synthesized melody and the target pitch contour.</p>\n\n",
                "matched_terms": [
                    "between",
                    "pitch",
                    "synthesized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Contrastive learning has been increasingly adopted in speech and audio modeling to enforce factor-specific consistency while suppressing undesired variations such as speaker identity or prompt interference. For instance, CLAPSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib55\" title=\"\">55</a>]</cite> applies multi-scale contrastive learning between textual prosody embeddings and corresponding acoustic realizations, improving prosodic expressivity across varying textual contexts. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib56\" title=\"\">56</a>]</cite> propose a contrastive loss to enhance the modeling of prosodic focus&#8212;including F0, duration, and intensity&#8212;by encouraging TTS systems to distinguish emphasized from neutral phonetic segments. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib57\" title=\"\">57</a>]</cite> use contrastive self-supervision to extract prosody-specific embeddings disentangled from speaker identity, which are useful for style transfer and anonymized generation. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib58\" title=\"\">58</a>]</cite> further explore contrastive pretraining to align textual context with expressive speech realizations, facilitating zero-shot expressive TTS with better generalization.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "speech",
                    "prosody",
                    "between",
                    "prosodic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To promote melody-consistent synthesis under varying acoustic prompts, we introduce a sequence-level symmetric contrastive loss.\nGiven a batch of <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> training samples that share the same semantic token sequence <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math> but differ in pitch sequences <math alttext=\"\\tilde{\\mathbf{m}}^{p}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}_{k}</annotation></semantics></math>, we construct two sets of inputs:\n<math alttext=\"\\mathcal{S}^{g}_{A}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,A}_{k},\\mathbf{a}^{A}_{k,t}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>A</mi><mi>g</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mi>&#119852;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>A</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}^{g}_{A}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,A}_{k},\\mathbf{a}^{A}_{k,t}\\}_{k=1}^{K}</annotation></semantics></math> and\n<math alttext=\"\\mathcal{S}^{g}_{B}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,B}_{k},\\mathbf{a}^{B}_{k,t}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>B</mi><mi>g</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mi>&#119852;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>B</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}^{g}_{B}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,B}_{k},\\mathbf{a}^{B}_{k,t}\\}_{k=1}^{K}</annotation></semantics></math>,\nwhere <math alttext=\"\\mathbf{a}^{A}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{A}_{k,t}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{B}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>B</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{B}_{k,t}</annotation></semantics></math> denote masked acoustic tokens, <math alttext=\"\\mathbf{a}^{r,A}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,A}_{k}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{r,B}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,B}_{k}</annotation></semantics></math> denote acoustic prompts sampled from distinct utterances of the same singer, ensuring consistent timbre across the pair.\nThe prompt length is randomly selected from the range <math alttext=\"\\left[\\min\\left(\\left\\lfloor L/4\\right\\rfloor,5\\right),\\left\\lfloor L/2\\right\\rfloor\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><mrow><mo>[</mo><mrow><mi>min</mi><mo>&#8289;</mo><mrow><mo>(</mo><mrow><mo>&#8970;</mo><mrow><mi>L</mi><mo>/</mo><mn>4</mn></mrow><mo>&#8971;</mo></mrow><mo>,</mo><mn>5</mn><mo>)</mo></mrow></mrow><mo>,</mo><mrow><mo>&#8970;</mo><mrow><mi>L</mi><mo>/</mo><mn>2</mn></mrow><mo>&#8971;</mo></mrow><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left[\\min\\left(\\left\\lfloor L/4\\right\\rfloor,5\\right),\\left\\lfloor L/2\\right\\rfloor\\right)</annotation></semantics></math>, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the length of the semantic sequence. Each input is processed by the S2A model to produce acoustic token embeddings <math alttext=\"\\mathbf{g}^{a},\\mathbf{g}^{b}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>&#119840;</mi><mi>a</mi></msup><mo>,</mo><msup><mi>&#119840;</mi><mi>b</mi></msup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{g}^{a},\\mathbf{g}^{b}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math>, which are mean-pooled across the time dimension to yield global acoustic representations <math alttext=\"\\tilde{\\mathbf{g}}^{a},\\tilde{\\mathbf{g}}^{b}\\in\\mathbb{R}^{K\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mrow><mrow><msup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>a</mi></msup><mo>,</mo><msup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>b</mi></msup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{g}}^{a},\\tilde{\\mathbf{g}}^{b}\\in\\mathbb{R}^{K\\times D}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To align acoustic outputs with the shared pitch condition while remaining invariant to prompt variation, we apply a symmetric contrastive loss (SCE)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib59\" title=\"\">59</a>]</cite> defined as:</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is a temperature hyperparameter. Each positive pair <math alttext=\"(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>i</mi><mi>a</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>i</mi><mi>b</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{i})</annotation></semantics></math> corresponds to index-aligned embeddings generated under identical semantic tokens <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m3\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math> and regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m4\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, but conditioned on different acoustic prompts <math alttext=\"\\mathbf{a}^{r,A}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>i</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,A}_{i}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{r,B}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>i</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,B}_{i}</annotation></semantics></math>. These prompts are randomly selected from non-overlapping segments of the same singer&#8217;s recordings, ensuring consistent timbre while introducing natural variation. In contrast, off-diagonal pairs <math alttext=\"(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>i</mi><mi>a</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>j</mi><mi>b</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{j})</annotation></semantics></math> for <math alttext=\"i\\neq j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m8\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8800;</mo><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">i\\neq j</annotation></semantics></math> serve as negatives due to mismatched pitch sequences, even though semantic tokens and speaker identity remain the same. These negatives are nontrivial, as they reflect realistic melodic differences under otherwise comparable contextual and timbral conditions. This design encourages the model to focus on capturing global pitch structure while remaining invariant to prompt-induced variability.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "differences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the sequence contrastive loss promotes utterance-level melody consistency, it does not directly enforce fine-grained pitch alignment at the frame level&#8212;an essential factor in singing synthesis due to rapid and expressive melodic changes. To address this limitation, we introduce a frame-level contrastive objective that supervises token-wise alignment between generated acoustic representations and the input pitch contour. Our design is motivated by recent work such as CTAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib60\" title=\"\">60</a>]</cite>, which leverage contrastive learning to align discrete phoneme sequences with speech features for TTS, voice conversion, and ASR tasks under limited supervision. Although our formulation differs in both granularity and modality&#8212;operating on pitch tokens rather than phonemes, and targeting melody alignment in singing synthesis&#8212;these studies underscore the effectiveness of contrastive supervision for bridging symbolic and acoustic representations. By extending this idea to the SVS domain, our frame-level contrastive loss enhances local pitch fidelity while remaining robust to prompt-induced variation, thereby enabling more precise and expressive melody control in zero-shot scenarios.</p>\n\n",
                "matched_terms": [
                    "between",
                    "speech",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each input is passed through the S2A model to produce frame-level acoustic embeddings <math alttext=\"\\mathbf{f}^{a},\\mathbf{f}^{b}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>&#119839;</mi><mi>a</mi></msup><mo>,</mo><msup><mi>&#119839;</mi><mi>b</mi></msup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{f}^{a},\\mathbf{f}^{b}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math>. For each training sample <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>, we compute a cosine similarity matrix <math alttext=\"\\mathbf{S}^{k}\\in\\mathbb{R}^{L\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mi>k</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{k}\\in\\mathbb{R}^{L\\times L}</annotation></semantics></math> between <math alttext=\"\\mathbf{f}^{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m4\" intent=\":literal\"><semantics><msup><mi>&#119839;</mi><mi>a</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{f}^{a}</annotation></semantics></math> and <math alttext=\"\\mathbf{f}^{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m5\" intent=\":literal\"><semantics><msup><mi>&#119839;</mi><mi>b</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{f}^{b}</annotation></semantics></math>. To supervise the similarity learning, we define a soft label matrix <math alttext=\"\\mathbf{Y}^{k}\\in[-1,1]^{L\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m6\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>k</mi></msup><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{k}\\in[-1,1]^{L\\times L}</annotation></semantics></math> capturing pitch and semantic alignment:</p>\n\n",
                "matched_terms": [
                    "between",
                    "pitch",
                    "similarity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This formulation encourages the model to produce highly similar acoustic embeddings when both pitch and semantic content align, moderately similar embeddings when only pitch aligns, and dissimilar embeddings otherwise. The similarity is computed within each utterance because the vocal range is typically locally bounded, making repeated pitch tokens more likely. In contrast, pitch overlap across utterances is rare and thus excluded. Additionally, even within the same utterance, repeated pitches may be associated with different semantic tokens, resulting in subtle acoustic variation. Our soft labeling mechanism accounts for this by assigning intermediate similarity, thereby avoiding over-penalization while promoting melody-consistent synthesis. Hence, the final contrastive learning objective is as follows:</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "similarity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, we integrate the SVT module directly into the training pipeline to provide explicit frame-level pitch supervision, thereby enhancing melody modeling and alignment. Specifically, the SVT model predicts frame-wise discrete pitch tokens from acoustic codec representations, which are then compared against the ground-truth pitch sequence. This supervision enforces alignment between the generated acoustic tokens and the intended melody, encouraging consistent pitch realization, particularly under zero-shot conditions. Furthermore, as the SVT module operates entirely on discrete representations, it is naturally compatible with our codec-based SVS framework and does not require raw audio or continuous F0 contours.</p>\n\n",
                "matched_terms": [
                    "between",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SVT model adopts a lightweight encoder-only Transformer architecture. Given a sequence of acoustic tokens <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, it predicts the corresponding pitch token sequence <math alttext=\"\\hat{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>^</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{m}}^{p}</annotation></semantics></math> of length <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>. The encoder consists of four Transformer layers with a hidden size of 512 and eight attention heads. Each input frame comprises 12 discrete acoustic codes, which are individually embedded, concatenated, and projected to a 512-dimensional representation, followed by layer normalization. The resulting embedding sequence is then passed through a linear classification head to predict a pitch token for each frame. The model is trained using a standard cross-entropy loss between the predicted and reference pitch sequences.</p>\n\n",
                "matched_terms": [
                    "between",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide frame-level supervision for pitch modeling, we leverage the pretrained SVT model as a pitch predictor to generate pseudo labels in the form of pitch token sequences <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m1\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, which are temporally aligned with the acoustic frames. As the primary training objective, we apply a cross-entropy loss <math alttext=\"\\mathcal{L}_{\\rm CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>CE</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm CE}</annotation></semantics></math> between the predicted pitch tokens <math alttext=\"\\hat{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m3\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>^</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{m}}^{p}</annotation></semantics></math> and the SVT-derived ground-truth <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m4\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, encouraging accurate token-level classification. However, the cross-entropy objective alone does not account for the temporal continuity inherent in repeated pitch tokens. This often results in jittery predictions, fragmented note segments, and rhythmically unstable outputs.</p>\n\n",
                "matched_terms": [
                    "between",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"b_{t}=\\mathds{1}[\\tilde{m}^{p}_{t}\\neq\\tilde{m}^{p}_{t-1}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>b</mi><mi>t</mi></msub><mo>=</mo><mrow><mn>&#120793;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msubsup><mover accent=\"true\"><mi>m</mi><mo>~</mo></mover><mi>t</mi><mi>p</mi></msubsup><mo>&#8800;</mo><msubsup><mover accent=\"true\"><mi>m</mi><mo>~</mo></mover><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>p</mi></msubsup></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">b_{t}=\\mathds{1}[\\tilde{m}^{p}_{t}\\neq\\tilde{m}^{p}_{t-1}]</annotation></semantics></math> is a binary indicator marking ground-truth pitch boundaries, and <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m6\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> is a fixed margin that enforces dissimilarity across transitions. This formulation penalizes minimal variation within sustained pitch regions while promoting sharper contrast at pitch change boundaries, thereby enhancing segment continuity and expressive phrasing in the synthesized output.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "synthesized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by recent advances in speech synthesis that emphasize the importance of temporal alignment and duration modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib37\" title=\"\">37</a>]</cite>, we introduce a soft duration loss <math alttext=\"\\mathcal{L}_{\\text{dur}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dur}}</annotation></semantics></math> to enhance the rhythmic fidelity of frame-level pitch predictions. Prior works such as FastSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib63\" title=\"\">63</a>]</cite> and XiaoiceSing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib37\" title=\"\">37</a>]</cite> employ explicit duration predictors or auxiliary alignment modules to supervise temporal structures. While effective, these methods often introduce architectural overhead or struggle to generalize in expressive singing scenarios. In contrast, our approach provides a fully differentiable supervision signal by directly supervising the temporal distribution of pitch token probabilities using the softmax outputs of the model, without requiring any external duration modeling.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin by training the Singing Voice Transcription (SVT) model to provide frame-level pitch supervision for subsequent Semantic-to-Acoustic (S2A) adaptation. The SVT model is optimized using a cross-entropy loss <math alttext=\"\\mathcal{L}_{\\text{CE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CE</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CE}}</annotation></semantics></math> between the regulated pitch token sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math> and the acoustic token sequence <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, thereby learning to predict temporally aligned pitch trajectories from acoustic inputs. Once trained, the SVT module is frozen to serve as a fixed auxiliary supervisor during S2A training.</p>\n\n",
                "matched_terms": [
                    "between",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then fine-tune the S2A model built upon the MaskGCT framework&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, which leverages masked acoustic modeling for non-autoregressive generation. The training objective for the S2A model comprises three components: (1) the mask token prediction loss <math alttext=\"\\mathcal{L}_{\\text{mask}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>mask</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{mask}}</annotation></semantics></math>, which reconstructs randomly masked acoustic tokens from noisy inputs using a masked denoising objective; (2) a coarse-to-fine contrastive loss <math alttext=\"\\mathcal{L}_{\\text{CL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CL}}</annotation></semantics></math>, composed of sequence-level (<math alttext=\"\\mathcal{L}_{\\text{SCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>SCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{SCL}}</annotation></semantics></math>) and frame-level (<math alttext=\"\\mathcal{L}_{\\text{FCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>FCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{FCL}}</annotation></semantics></math>) terms, to enforce consistency between the melody condition and generated acoustic tokens while mitigating prosody leakage from the prompt; and (3) an auxiliary SVT loss <math alttext=\"\\mathcal{L}_{\\text{SVT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>SVT</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{SVT}}</annotation></semantics></math>, which encourages the predicted acoustic tokens to be rhythmically and melodically consistent with the SVT-inferred pitch contour. The fine-tuning algorithm for the S2A model is summarized in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#alg1\" title=\"Algorithm 1 &#8227; III-D Training and Inference Procedures &#8227; III Method &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Each training sample <math alttext=\"\\mathbf{x}_{k}\\in\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#119857;</mi><mi>k</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{k}\\in\\mathcal{B}</annotation></semantics></math> comprises a semantic sequence <math alttext=\"\\mathbf{s}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m7\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{k}</annotation></semantics></math>, a regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}_{k}</annotation></semantics></math>, and a time-aligned acoustic token sequence <math alttext=\"\\mathbf{a}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m9\" intent=\":literal\"><semantics><msub><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{a}_{k,t}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "prompt",
                    "terms",
                    "prosody",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the performance of our system in terms of pitch accuracy, timbre consistency, and perceptual quality, we conduct objective evaluations under both seen-singer and zero-shot settings. The following metrics are employed:</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "terms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MCD is used to evaluate spectral fidelity by computing the frame-wise Euclidean distance between mel-cepstral coefficients of the synthesized and reference audio. It serves as a proxy for spectral similarity, where lower values indicate more accurate spectral reconstruction and reduced distortion.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "indicate",
                    "lower",
                    "similarity",
                    "values",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">F0-RMSE measures pitch prediction accuracy by calculating the root mean squared error between the fundamental frequency (F0) trajectories of the generated and reference waveforms. A lower F0-RMSE reflects better alignment with the intended melody and more precise pitch control.</p>\n\n",
                "matched_terms": [
                    "between",
                    "lower",
                    "pitch",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess timbre similarity, we compute cosine similarity between speaker embeddings extracted from the synthesized and reference audio using a WavLM-based speaker verification model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib66\" title=\"\">66</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/microsoft/wavlm-base-sv\" title=\"\">https://huggingface.co/microsoft/wavlm-base-sv</a></span></span></span>. SECS values range from 0 to 1, with higher scores indicating closer alignment in vocal identity.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "similarity",
                    "values",
                    "between",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib67\" title=\"\">67</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/South-Twilight/SingMOS\" title=\"\">https://github.com/South-Twilight/SingMOS</a></span></span></span> is a learned metric trained to predict human perceptual ratings of singing voice quality. It is based on a curated dataset of professional ratings for both natural and synthesized singing in Chinese and Japanese, addressing the annotation scarcity in the singing domain. SingMOS produces scores in the range of 0 to 5, with higher values indicating greater naturalness and perceptual quality. As a reference-free metric, it enables scalable automatic evaluation in zero-shot and low-resource conditions.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "values",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess perceptual quality, we conducted a Mean Opinion Score (MOS) evaluation with 20 participants who have formal training in singing and experience in vocal performance <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>This study has been approved by the Department Ethics Review Committee\n(DERC) at the National University of Singapore under\nDERC Ref Code: 000479.</span></span></span>. Each participant rated the synthesized samples based on overall naturalness (MOS-N), audio quality (MOS-Q), and timbre similarity (SMOS). A 5-point Likert scale was used, where a score of 5 indicates excellent perceptual quality and 1 denotes poor quality.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "synthesized",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several recent TTS systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib68\" title=\"\">68</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib69\" title=\"\">69</a>]</cite> have reported high prosodic similarity between the speech prompt and the synthesized output. While this may appear beneficial in TTS, it reveals a form of prosody leakage, where expressive cues from the prompt inadvertently influence the generated speech. This issue becomes particularly problematic in singing voice synthesis (SVS), where pitch and rhythm should be governed solely by the input music score. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S1\" title=\"I Introduction &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, we refer to this phenomenon as prosody leakage.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "synthesized",
                    "prompt",
                    "speech",
                    "prosody",
                    "similarity",
                    "between",
                    "prosodic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate whether MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite> exhibits such behavior, we conduct a prosody similarity analysis following prior evaluation protocols. NaturalSpeech 2 and 3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>]</cite> quantify prosodic similarity by comparing pitch and duration features between the prompt and output, while StyleTTS-ZS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib69\" title=\"\">69</a>]</cite> computes Pearson correlation coefficients of acoustic features to evaluate prosodic alignment.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "prompt",
                    "prosody",
                    "similarity",
                    "between",
                    "prosodic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by these approaches, we evaluate the prosodic similarity of MaskGCT in both English and Mandarin using the LibriTTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib25\" title=\"\">25</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.openslr.org/60/\" title=\"\">https://www.openslr.org/60/</a></span></span></span> and AISHELL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib70\" title=\"\">70</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openslr.org/93/\" title=\"\">https://openslr.org/93/</a></span></span></span> datasets, respectively. For each speaker, we randomly sample 50 utterances to construct the test sets. During inference, we synthesize 50 utterances per dataset by conditioning on the same target text but using different speech prompts. We then compute the acoustic-level similarity between each synthesized utterance and: (1) its paired prompt (i.e., the one used during generation), and (2) an unpaired prompt from the same speaker. This comparison allows us to quantify the extent of prompt-induced prosody similarity, which serves as an indicator of potential prosody leakage in the model.</p>\n\n",
                "matched_terms": [
                    "libritts",
                    "synthesized",
                    "prompt",
                    "speech",
                    "prosody",
                    "similarity",
                    "paired",
                    "between",
                    "prosodic",
                    "unpaired"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, baseline systems show notable declines in key timbre- and melody-related metrics such as SMOS, SECS, and F0-RMSE, underscoring their limited ability to generalize to unseen vocal identities. CoMelSinger&#8217;s robustness in zero-shot scenarios is attributed to the synergy between in-context prompting&#8212;which leverages short acoustic references to anchor timbre&#8212;and large-scale speech pretraining, which imparts transferable prosodic priors.</p>\n\n",
                "matched_terms": [
                    "between",
                    "prosodic",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the inherent challenge of handling unseen timbres, CoMelSinger continues to achieve high speaker similarity while preserving accurate pitch trajectories. This balance between identity retention and melodic fidelity demonstrates the model&#8217;s strong generalization capacity. While many existing approaches face trade-offs between controllability and naturalness, CoMelSinger effectively reconciles both through its structured architecture and explicit conditioning scheme. These findings position CoMelSinger as a strong baseline for zero-shot singing voice synthesis with discrete representations.</p>\n\n",
                "matched_terms": [
                    "between",
                    "pitch",
                    "similarity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate the impact of the singing voice transcription (SVT) module, which provides auxiliary pitch supervision. Excluding SVT results in higher F0-RMSE and lower SingMOS, confirming the benefit of explicit alignment signals for structured melody control. The most severe degradation occurs when both CL and SVT are removed, indicating their complementary roles in pitch&#8211;timbre disentanglement and temporal stability.</p>\n\n",
                "matched_terms": [
                    "lower",
                    "pitch",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T5\" title=\"TABLE V &#8227; Comparison of Fine-Tuning Strategies &#8227; V-D Ablation Study &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> presents a comparison of six fine-tuning strategies in terms of both objective and subjective performance, along with their respective trainable parameter ratios. FT-LoRA delivers the best overall performance, achieving the lowest F0-RMSE, highest SingMOS, and highest SECS, while updating only 4.81% of the parameters&#8212;highlighting the effectiveness of low-rank adaptation for efficient model tuning. FT-PGS achieves the lowest MCD, suggesting enhanced spectral fidelity through gradual unfreezing, though its pitch accuracy is affected by delayed optimization of lower layers. FT-Prefix and FT-Pitch yield consistent results with minimal overhead, demonstrating the utility of lightweight adaptation modules. In contrast, FT-LLRD and FT-Full fine-tune all parameters yet underperform across most metrics, indicating that full-capacity adaptation may lead to overfitting or instability in data-scarce settings. These results underscore that parameter-efficient strategies, particularly LoRA, can match or surpass full-model fine-tuning while substantially reducing computational cost.</p>\n\n",
                "matched_terms": [
                    "lower",
                    "pitch",
                    "terms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present CoMelSinger, a zero-shot singing voice synthesis framework that extends discrete token-based TTS models to support structured and controllable melody generation. Built upon the non-autoregressive MaskGCT architecture, CoMelSinger incorporates lyrics and pitch tokens as inputs, enabling fine-grained alignment between the musical score and the generated voice. To address the challenge of prosody leakage from prompt-based conditioning&#8212;an issue unique to singing synthesis&#8212;we propose a coarse-to-fine contrastive learning strategy that explicitly disentangles pitch information from the timbre prompt. Furthermore, we introduce a lightweight singing voice transcription (SVT) module to provide frame-level pitch and duration supervision, enhancing the model&#8217;s ability to follow the intended melody with precision. Extensive experiments on both seen and unseen singers demonstrate that CoMelSinger achieves strong zero-shot generalization, consistently outperforming competitive SVS baselines in pitch accuracy, timbre consistency, and subjective quality. Our results confirm that structured melody control and contrastive disentanglement are essential for scalable and expressive singing synthesis. We believe CoMelSinger opens new possibilities for discrete token-based SVS, enabling scalable and zero-shot singing generation.</p>\n\n",
                "matched_terms": [
                    "between",
                    "prompt",
                    "pitch",
                    "prosody"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance",
        "caption": "TABLE II: Evaluation results on the seen test set for singing voice synthesis. Subjective metrics are reported with 95% confidence intervals. GT stands for Ground Truth.",
        "body": "Model\nSubjective Evaluations\nObjective Evaluations\n\n\nMOS-Q ↑\\uparrow\n\nMOS-N ↑\\uparrow\n\nSMOS ↑\\uparrow\n\nMCD ↓\\downarrow\n\nF0-RMSE ↓\\downarrow\n\nSingMOS ↑\\uparrow\n\nSECS ↑\\uparrow\n\n\n\nGT\n4.17 ±\\pm 0.16\n4.38 ±\\pm 0.18\n4.41 ±\\pm 0.14\n-\n-\n4.37\n0.925\n\n\nGT (Acoustic Codec)\n4.01 ±\\pm 0.22\n4.19 ±\\pm 0.19\n4.48 ±\\pm 0.12\n0.93\n0.012\n4.31\n0.906\n\n\nDiffSinger [4]\n\n3.68 ±\\pm 0.20\n3.79 ±\\pm 0.15\n3.86 ±\\pm 0.16\n4.59\n0.084\n4.13\n0.769\n\n\nVISinger2 [9]\n\n3.59 ±\\pm 0.22\n3.86 ±\\pm 0.16\n3.91 ±\\pm 0.16\n5.36\n0.061\n4.15\n0.792\n\n\nStyleSinger [7]\n\n3.67 ±\\pm 0.15\n3.92 ±\\pm 0.21\n4.11 ±\\pm 0.16\n4.95\n0.112\n4.19\n0.833\n\n\nSPSinger [13]\n\n3.81 ±\\pm 0.18\n4.10 ±\\pm 0.12\n4.06 ±\\pm 0.17\n4.28\n0.054\n4.28\n0.860\n\n\nCoMelSinger (ours)\n3.90 ±\\pm 0.16\n4.02 ±\\pm 0.12\n4.22 ±\\pm 0.15\n4.17\n0.042\n4.32\n0.912",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Subjective Evaluations</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Objective Evaluations</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MOS-Q <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MOS-N <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">SMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MCD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">F0-RMSE <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SingMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SECS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">GT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.17 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.38 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4.41 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.925</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GT (Acoustic Codec)</td>\n<td class=\"ltx_td ltx_align_center\">4.01 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.22</td>\n<td class=\"ltx_td ltx_align_center\">4.19 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.48 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.12</td>\n<td class=\"ltx_td ltx_align_center\">0.93</td>\n<td class=\"ltx_td ltx_align_center\">0.012</td>\n<td class=\"ltx_td ltx_align_center\">4.31</td>\n<td class=\"ltx_td ltx_align_center\">0.906</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">DiffSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib4\" title=\"\">4</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.68 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.79 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.86 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.084</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.769</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VISinger2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib9\" title=\"\">9</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">3.59 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m17\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.22</td>\n<td class=\"ltx_td ltx_align_center\">3.86 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m18\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.91 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m19\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.16</td>\n<td class=\"ltx_td ltx_align_center\">5.36</td>\n<td class=\"ltx_td ltx_align_center\">0.061</td>\n<td class=\"ltx_td ltx_align_center\">4.15</td>\n<td class=\"ltx_td ltx_align_center\">0.792</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">StyleSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib7\" title=\"\">7</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">3.67 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m20\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.15</td>\n<td class=\"ltx_td ltx_align_center\">3.92 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m21\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.11 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m22\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.16</td>\n<td class=\"ltx_td ltx_align_center\">4.95</td>\n<td class=\"ltx_td ltx_align_center\">0.112</td>\n<td class=\"ltx_td ltx_align_center\">4.19</td>\n<td class=\"ltx_td ltx_align_center\">0.833</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SPSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib13\" title=\"\">13</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">3.81 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m23\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.18</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.10 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m24\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.06 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m25\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.17</td>\n<td class=\"ltx_td ltx_align_center\">4.28</td>\n<td class=\"ltx_td ltx_align_center\">0.054</td>\n<td class=\"ltx_td ltx_align_center\">4.28</td>\n<td class=\"ltx_td ltx_align_center\">0.860</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">CoMelSinger (ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">3.90 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m26\" intent=\":literal\"><semantics><mo mathbackground=\"#F2F2F2\" style=\"--ltx-bg-color:#F2F2F2;\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">4.02 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m27\" intent=\":literal\"><semantics><mo mathbackground=\"#F2F2F2\" style=\"--ltx-bg-color:#F2F2F2;\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">4.22 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m28\" intent=\":literal\"><semantics><mo mathbackground=\"#F2F2F2\" style=\"--ltx-bg-color:#F2F2F2;\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">4.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">0.042</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">4.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">0.912</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "codec",
            "f0rmse",
            "subjective",
            "reported",
            "±pm",
            "seen",
            "spsinger",
            "↑uparrow",
            "intervals",
            "stands",
            "confidence",
            "evaluation",
            "visinger2",
            "singing",
            "mcd",
            "objective",
            "results",
            "acoustic",
            "truth",
            "metrics",
            "smos",
            "diffsinger",
            "voice",
            "comelsinger",
            "synthesis",
            "↓downarrow",
            "stylesinger",
            "secs",
            "evaluations",
            "test",
            "ours",
            "ground",
            "mosq",
            "set",
            "mosn",
            "model",
            "singmos"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For the seen singer evaluation, we compare CoMelSinger with four strong baseline systems: DiffSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib4\" title=\"\">4</a>]</cite>, VISinger2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib9\" title=\"\">9</a>]</cite>, SPSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib13\" title=\"\">13</a>]</cite>, and StyleSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib7\" title=\"\">7</a>]</cite>. To ensure a fair comparison, all models adopt HiFi-GAN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib71\" title=\"\">71</a>]</cite> as the vocoder during both training and inference. As presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T2\" title=\"TABLE II &#8227; V-A Evaluating Prompt-Induced Prosody Similarity in MaskGCT &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>, CoMelSinger achieves the highest scores across both subjective and objective metrics, demonstrating its capability to synthesize natural and expressive singing voices from seen singers.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Singing Voice Synthesis (SVS) aims to generate expressive vocal performances from structured musical inputs such as lyrics and pitch sequences. While recent progress in discrete codec-based speech synthesis has enabled zero-shot generation via in-context learning, directly extending these techniques to SVS remains non-trivial due to the requirement for precise melody control. In particular, prompt-based generation often introduces prosody leakage, where pitch information is inadvertently entangled within the timbre prompt, compromising controllability. We present CoMelSinger, a zero-shot SVS framework that enables structured and disentangled melody control within a discrete codec modeling paradigm. Built on the non-autoregressive MaskGCT architecture, CoMelSinger replaces conventional text inputs with lyric and pitch tokens, preserving in-context generalization while enhancing melody conditioning. To suppress prosody leakage, we propose a coarse-to-fine contrastive learning strategy that explicitly regularizes pitch redundancy between the acoustic prompt and melody input. Furthermore, we incorporate a lightweight encoder-only Singing Voice Transcription (SVT) module to align acoustic tokens with pitch and duration, offering fine-grained frame-level supervision. Experimental results demonstrate that CoMelSinger achieves notable improvements in pitch accuracy, timbre consistency, and zero-shot transferability over competitive baselines.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "codec",
                    "comelsinger",
                    "synthesis",
                    "singing",
                    "results",
                    "acoustic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Singing voice synthesis (SVS) aims to transform structured musical inputs&#8212;most often lyrics and pitch sequences&#8212;into expressive, high-quality vocal performances. Over the past decade, it has moved from a niche research topic to an essential tool in creative audio technologies, propelled by the rise of AI-driven music generation, virtual performers, and personalized media experiences. Its applications now extend well beyond traditional karaoke systems, finding a place in virtual idol production, game soundtracks, and content creation for social platforms. Parallel to these expanding use cases, advances in deep generative models have brought marked gains in timbre fidelity, pitch accuracy, and the overall naturalness of synthesized voices <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib1\" title=\"\">1</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib2\" title=\"\">2</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib3\" title=\"\">3</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib4\" title=\"\">4</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib5\" title=\"\">5</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib6\" title=\"\">6</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib7\" title=\"\">7</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib8\" title=\"\">8</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "singing",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent SVS frameworks, including end-to-end&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib9\" title=\"\">9</a>]</cite> and diffusion-based architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib3\" title=\"\">3</a>]</cite>, have demonstrated strong performance in supervised scenarios with seen singers. Nevertheless, zero-shot SVS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib13\" title=\"\">13</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib10\" title=\"\">10</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib14\" title=\"\">14</a>]</cite>&#8212;synthesizing singing voices for unseen speakers without additional fine-tuning&#8212;still has substantial room for improvement.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "seen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete token-based architectures show great in-context learning capabilities and provide a promising pathway for such zero-shot generation. In a related task, text-to-speech (TTS) has witnessed rapid progress with discrete acoustic tokens derived from vector quantization and neural audio codecs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib16\" title=\"\">16</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib17\" title=\"\">17</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>]</cite>. By mapping complex waveforms into a quantized latent space, these tokens capture timbre, prosody, and phonetic content, thereby reformulating speech synthesis as symbolic sequence modeling akin to language modeling. The success of token-based TTS systems is largely enabled by large-scale multi-speaker corpora, which provide sufficient diversity to learn robust and generalizable representations. In contrast, the scarcity and limited diversity of singing data make token-based modeling for SVS significantly more challenging.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "acoustic",
                    "singing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Built upon this data-rich foundation, recent TTS systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib20\" title=\"\">20</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib21\" title=\"\">21</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib16\" title=\"\">16</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib17\" title=\"\">17</a>]</cite>, have adopted large language model (LLM)-style architectures to model the conditional distribution of acoustic tokens given phoneme sequences and optional prompts. Within this framework, in-context learning (ICL) becomes feasible: a short segment of reference speech, represented as discrete tokens, serves as an acoustic prompt to guide synthesis in terms of speaker identity and style. This approach, exemplified by models such as VALL-E <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite>, enables zero-shot speech synthesis by treating speech generation as a form of conditional codec language modeling, without requiring speaker labels or model adaptation. Inspired by these advances, researchers have begun exploring discrete token-based methods for SVS.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "acoustic",
                    "model",
                    "codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, directly extending TTS by replacing textual input with structured musical inputs&#8212;such as lyrics and pitch tokens&#8212;while reusing the same modeling pipeline reveals a unique challenge in the singing domain: prosody leakage from the acoustic prompt, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. In prompt-based synthesis, the acoustic prompt is intended to provide timbral cues, yet pitch-related attributes&#8212;including contour and timing&#8212;are often inadvertently encoded into its latent representation. This unintended encoding leads to timbre&#8211;melody entanglement, where the prompt simultaneously influences vocal timbre and melodic realization. As a result, the system&#8217;s control over the explicitly specified pitch sequence is weakened, undermining the precise separation between timbre conditioning and melody generation that SVS requires.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "acoustic",
                    "singing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, SVS requires much finer control over pitch and melody than TTS or voice conversion, as the generated singing must accurately follow the musical score while preserving timbre. Although attribute control has been explored through adversarial training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>]</cite>, contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib28\" title=\"\">28</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib29\" title=\"\">29</a>]</cite>, and information-bottleneck methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib30\" title=\"\">30</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib31\" title=\"\">31</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib32\" title=\"\">32</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib33\" title=\"\">33</a>]</cite>, these approaches primarily focus on coarse prosodic patterns or emotional cues and provide limited support for fine-grained melody control. Make-A-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite>, as a representative discrete-token SVS system, adopts prompt-guided conditioning but lacks explicit mechanisms to prevent the acoustic prompt from influencing melody realization. To the best of our knowledge, no prior work has systematically addressed prosody leakage in discrete token-based SVS, leaving a critical gap in achieving precise and faithful melody control in zero-shot scenarios.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "singing",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenges of prosody leakage and limited melody controllability in zero-shot SVS, we propose CoMelSinger, a discrete codec-based framework with structured melody control.\nCoMelSinger builds on the non-autoregressive MaskGCT architecture&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, adapting it to accept musical inputs consisting of lyrics and pitch tokens.\nTo achieve better melody&#8211;timbre control, we introduce a coarse-to-fine contrastive learning strategy that limits excessive pitch-related information in the acoustic prompt, allowing the explicit pitch condition to guide melodic realization more effectively.\nWe further incorporate a lightweight, encoder-only Singing Voice Transcription (SVT) module to provide fine-grained, frame-level supervision by aligning acoustic tokens with pitch and duration sequences.\nTogether, these designs enable accurate melody modeling, maintain timbre consistency, and preserve the in-context learning capability of discrete token-based systems.\nExtensive experiments on both seen and unseen singers demonstrate that CoMelSinger delivers substantial improvements in pitch accuracy, timbre consistency, and overall synthesis quality compared with state-of-the-art SVS baselines in zero-shot scenarios. The main contributions of this work are:</p>\n\n",
                "matched_terms": [
                    "voice",
                    "comelsinger",
                    "synthesis",
                    "singing",
                    "acoustic",
                    "seen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose CoMelSinger, a discrete token-based SVS framework for zero-shot synthesis with structured melody control.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "comelsinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Singing voice synthesis (SVS) aims to produce expressive vocal performances from structured musical inputs such as note pitch, duration, and lyrics.\nCompared with text-to-speech (TTS), SVS presents unique challenges, including a wider pitch range and sustained phonation, which demand fine-grained melody modeling.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S2.F2\" title=\"Figure 2 &#8227; II-A Singing Voice Synthesis &#8227; II Related Works &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the evolution of representative SVS architectures, from continuous-feature pipelines to end-to-end models and, more recently, discrete codec-based frameworks.\nEarly SVS systems relied on unit-selection synthesis (e.g., VOCALOID&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib35\" title=\"\">35</a>]</cite>) or statistical approaches based on hidden Markov models (HMMs)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib36\" title=\"\">36</a>]</cite>.\nWith the advent of deep learning, SVS models increasingly adopted a two-stage architecture&#8212;comprising an acoustic model followed by a vocoder&#8212;including XiaoiceSing<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib37\" title=\"\">37</a>]</cite>, DeepSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib38\" title=\"\">38</a>]</cite>, and Sinsy&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib2\" title=\"\">2</a>]</cite>.\nSubsequent advances incorporated generative adversarial networks (GANs)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib39\" title=\"\">39</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib40\" title=\"\">40</a>]</cite> to enhance timbre realism, while more recent work using denoising diffusion probabilistic models (DDPMs)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib8\" title=\"\">8</a>]</cite> has achieved further gains in fidelity and temporal coherence.\nIn parallel, end-to-end systems such as VISinger 1/2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib9\" title=\"\">9</a>]</cite> have been developed to generate waveforms directly from musical scores without relying on explicit intermediate features.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "synthesis",
                    "singing",
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of discrete token modeling in TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib16\" title=\"\">16</a>]</cite>, recent SVS studies have explored token-based representations to improve generalization.\nTokSing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib41\" title=\"\">41</a>]</cite> employs a non-autoregressive Transformer conditioned on lyrics and pitch embeddings to predict discrete acoustic tokens.\nHiddenSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib3\" title=\"\">3</a>]</cite> integrates a diffusion-based decoder guided by discrete pitch and semantic tokens, achieving high-quality synthesis.\nMake-A-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite> unifies speech and singing synthesis through a shared discrete representation, but uses a relatively small proportion of singing data and does not incorporate prompt-based in-context learning.\nConsequently, it lacks explicit melody conditioning and provides limited flexibility in zero-shot singing scenarios.\nIn contrast, our approach introduces melody inputs and improved melody control in prompt-based conditioning, enabling more accurate and controllable zero-shot singing voice synthesis.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "acoustic",
                    "singing",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of large language models (LLMs), recent approaches formulate speech synthesis as autoregressive generation over discrete codec tokens.\nVALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite> pioneered this direction by conditioning on both text and a short acoustic prompt to synthesize high-fidelity speech.\nIts extensions&#8212;VALL-E X&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib44\" title=\"\">44</a>]</cite>, VALL-E 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib45\" title=\"\">45</a>]</cite>, and VALL-E R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib46\" title=\"\">46</a>]</cite>&#8212;extend the paradigm to cross-lingual synthesis, streaming generation, and improved alignment.\nSoCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib20\" title=\"\">20</a>]</cite> further improves efficiency through semantic-ordered multi-stream tokenization and segment-level modeling.\nThrough prompt-based conditioning, these models demonstrate strong zero-shot capability and robust speaker generalization.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "acoustic",
                    "codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To reduce inference latency, non-autoregressive (NAR) decoding frameworks have been developed.\nSoundStorm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib21\" title=\"\">21</a>]</cite> employs a bidirectional Transformer with confidence-based masked token modeling, generating audio tokens in parallel while maintaining autoregressive-level quality.\nMulti-token prediction and speculative decoding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib47\" title=\"\">47</a>]</cite> further accelerate synthesis by predicting multiple codec tokens per decoding step.\nMaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite> adopts a masked generative training strategy inspired by masked language modeling, enabling fast and parallel decoding while supporting in-context learning through prompt-aware input masking.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this foundation, we adapt the MaskGCT framework to singing voice synthesis.\nOur approach incorporates structured melody conditioning and improved melody&#8211;timbre control in prompt-based synthesis to address the challenges of pitch fidelity and prosody leakage in zero-shot settings.\nThis design improves controllability over melodic realization while preserving the inference efficiency and generalization strengths of discrete token-based modeling.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "singing",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fine-grained prosody control&#8212;particularly over fundamental frequency (F0) and phoneme duration&#8212;is essential for expressive speech and singing synthesis.\nSeveral TTS studies have incorporated explicit prosodic supervision to guide model learning.\nProsody-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib48\" title=\"\">48</a>]</cite> augments an end-to-end architecture with auxiliary predictors for phoneme-level F0 and duration, enabling precise rhythm and pitch control without degrading naturalness.\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib49\" title=\"\">49</a>]</cite> adopt utterance-level prosodic features in a hierarchical non-autoregressive model, providing interpretable style modulation across prosodic dimensions while maintaining synthesis quality.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "singing",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances extend prosody control to diffusion-based synthesis.\nDiffStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib50\" title=\"\">50</a>]</cite> combines a diffusion decoder with classifier-free guidance to model prosodic style at both coarse and phoneme-level scales, supporting flexible pitch&#8211;duration manipulation.\nDrawSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib51\" title=\"\">51</a>]</cite> enables editing by conditioning on user-drawn pitch&#8211;energy sketches, which are refined into high-resolution prosody contours.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable zero-shot singing voice synthesis with accurate melody control and disentangled prompt conditioning, we propose <em class=\"ltx_emph ltx_font_italic\">CoMelSinger</em>, a two-stage framework illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S3.F3\" title=\"Figure 3 &#8227; III Method &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Inspired by MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite> and Make-A-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite>, CoMelSinger comprises a Text-to-Semantic (T2S) stage and a Semantic-to-Acoustic (S2A) stage. The T2S module <math alttext=\"f_{\\mathrm{T2S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>T2S</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{T2S}}</annotation></semantics></math> transforms a lyric token sequence <math alttext=\"\\tilde{\\mathbf{m}}^{l}=[\\tilde{m}^{l}_{1},\\dots,\\tilde{m}^{l}_{S}]\\in\\mathcal{V}_{\\mathrm{lyr}}^{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>l</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mover accent=\"true\"><mi>m</mi><mo>~</mo></mover><mn>1</mn><mi>l</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>m</mi><mo>~</mo></mover><mi>S</mi><mi>l</mi></msubsup><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>lyr</mi><mi>S</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{l}=[\\tilde{m}^{l}_{1},\\dots,\\tilde{m}^{l}_{S}]\\in\\mathcal{V}_{\\mathrm{lyr}}^{S}</annotation></semantics></math>, obtained from a Grapheme-to-Phoneme (G2P) converter, and a semantic prompt <math alttext=\"\\mathbf{s}^{r}=E_{S}(\\mathbf{w}^{r})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119852;</mi><mi>r</mi></msup><mo>=</mo><mrow><msub><mi>E</mi><mi>S</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119856;</mi><mi>r</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}^{r}=E_{S}(\\mathbf{w}^{r})</annotation></semantics></math> extracted from the reference waveform <math alttext=\"\\mathbf{w}^{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119856;</mi><mi>r</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{w}^{r}</annotation></semantics></math>, into a semantic token sequence <math alttext=\"\\mathbf{s}\\in\\mathcal{V}_{\\mathrm{sem}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#119852;</mi><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>sem</mi><mi>L</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}\\in\\mathcal{V}_{\\mathrm{sem}}^{L}</annotation></semantics></math>, where <math alttext=\"\\mathcal{V}_{\\mathrm{sem}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>sem</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{V}_{\\mathrm{sem}}</annotation></semantics></math> denotes the semantic vocabulary and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> the sequence length. The S2A module <math alttext=\"f_{\\mathrm{S2A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>S2A</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{S2A}}</annotation></semantics></math> then predicts acoustic tokens <math alttext=\"\\mathbf{a}\\in\\mathcal{V}_{\\mathrm{aco}}^{L\\times N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mi>&#119834;</mi><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>aco</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{a}\\in\\mathcal{V}_{\\mathrm{aco}}^{L\\times N}</annotation></semantics></math>, conditioned on the semantic tokens <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math>, an acoustic prompt <math alttext=\"\\mathbf{a}^{r}=E_{A}(\\mathbf{w}^{r})\\in\\mathcal{V}_{\\mathrm{aco}}^{L_{r}\\times N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m11\" intent=\":literal\"><semantics><mrow><msup><mi>&#119834;</mi><mi>r</mi></msup><mo>=</mo><mrow><msub><mi>E</mi><mi>A</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119856;</mi><mi>r</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>aco</mi><mrow><msub><mi>L</mi><mi>r</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r}=E_{A}(\\mathbf{w}^{r})\\in\\mathcal{V}_{\\mathrm{aco}}^{L_{r}\\times N}</annotation></semantics></math>, and a regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\\in\\mathcal{V}_{\\mathrm{pit}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m12\" intent=\":literal\"><semantics><mrow><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>pit</mi><mi>L</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}\\in\\mathcal{V}_{\\mathrm{pit}}^{L}</annotation></semantics></math>. Here, <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m13\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> denotes the number of residual vector quantization (RVQ) codebooks in the acoustic codec, and the regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m14\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math> is derived from the pitch <math alttext=\"\\mathbf{m}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m15\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{p}</annotation></semantics></math> and duration <math alttext=\"\\mathbf{m}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m16\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>d</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{d}</annotation></semantics></math> sequence through the length expansion module <math alttext=\"LE(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m17\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">LE(\\cdot)</annotation></semantics></math>. Both the semantic and acoustic tokens are produced using discrete codec tokenizers following the MaskGCT setup, and the final waveform <math alttext=\"\\mathbf{w}^{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m18\" intent=\":literal\"><semantics><msup><mi>&#119856;</mi><mi>o</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{w}^{o}</annotation></semantics></math> is reconstructed from acoustic tokens via the decoder <math alttext=\"D_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m19\" intent=\":literal\"><semantics><msub><mi>D</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">D_{A}</annotation></semantics></math>. The complete pipeline is summarized as:</p>\n\n",
                "matched_terms": [
                    "voice",
                    "codec",
                    "comelsinger",
                    "synthesis",
                    "singing",
                    "acoustic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, CoMelSinger adopts a non-autoregressive masked generative modeling paradigm for both stages. In this framework, the model learns to reconstruct masked tokens within a sequence conditioned on surrounding context and external inputs, rather than generating tokens sequentially. This allows for parallel decoding and better handling of global context compared to traditional autoregressive models. Specifically, we model the conditional probabilities:\n<math alttext=\"p(\\mathbf{s}\\mid\\mathbf{s}_{t};\\mathbf{m}^{l},\\mathbf{s}^{r};f_{\\theta,\\mathrm{T2S}}),\\quad p(\\mathbf{a}\\mid\\mathbf{a}_{t};\\mathbf{s},\\tilde{\\mathbf{m}}^{p},\\mathbf{a}^{r};f_{\\theta,\\mathrm{S2A}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119852;</mi><mo>&#8739;</mo><mrow><msub><mi>&#119852;</mi><mi>t</mi></msub><mo>;</mo><msup><mi>&#119846;</mi><mi>l</mi></msup><mo>,</mo><msup><mi>&#119852;</mi><mi>r</mi></msup><mo>;</mo><msub><mi>f</mi><mrow><mi>&#952;</mi><mo>,</mo><mi>T2S</mi></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"1.167em\">,</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119834;</mi><mo>&#8739;</mo><mrow><msub><mi>&#119834;</mi><mi>t</mi></msub><mo>;</mo><mi>&#119852;</mi><mo>,</mo><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><mo>,</mo><msup><mi>&#119834;</mi><mi>r</mi></msup><mo>;</mo><msub><mi>f</mi><mrow><mi>&#952;</mi><mo>,</mo><mi>S2A</mi></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">p(\\mathbf{s}\\mid\\mathbf{s}_{t};\\mathbf{m}^{l},\\mathbf{s}^{r};f_{\\theta,\\mathrm{T2S}}),\\quad p(\\mathbf{a}\\mid\\mathbf{a}_{t};\\mathbf{s},\\tilde{\\mathbf{m}}^{p},\\mathbf{a}^{r};f_{\\theta,\\mathrm{S2A}})</annotation></semantics></math>.\nHere, <math alttext=\"\\mathbf{s}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{t}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119834;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{a}_{t}</annotation></semantics></math> are the partially masked semantic and acoustic token sequences, and the generation is conditioned on lyric inputs <math alttext=\"\\mathbf{m}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>l</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{l}</annotation></semantics></math>, pitch-aligned sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, and prompt tokens <math alttext=\"(\\mathbf{s}^{r},\\mathbf{a}^{r})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119852;</mi><mi>r</mi></msup><mo>,</mo><msup><mi>&#119834;</mi><mi>r</mi></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{s}^{r},\\mathbf{a}^{r})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model",
                    "comelsinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the original MaskGCT architecture, which employs a LLaMA-style Transformer backbone <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib54\" title=\"\">54</a>]</cite>, we extend the S2A model by introducing an additional embedding layer for the regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>. This pitch embedding <math alttext=\"\\mathbf{e}_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#119838;</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{e}_{p}</annotation></semantics></math> is element-wise added to the semantic token embedding <math alttext=\"\\mathbf{e}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>&#119838;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{e}_{s}</annotation></semantics></math> to form the composite conditioning input <math alttext=\"\\mathbf{e}_{c}=\\mathbf{e}_{p}+\\mathbf{e}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#119838;</mi><mi>c</mi></msub><mo>=</mo><mrow><msub><mi>&#119838;</mi><mi>p</mi></msub><mo>+</mo><msub><mi>&#119838;</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{e}_{c}=\\mathbf{e}_{p}+\\mathbf{e}_{s}</annotation></semantics></math>, thereby enabling the model to incorporate both linguistic and melodic information during acoustic token generation.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance melody controllability and suppress interference from prompt-induced timbre cues, we propose a coarse-to-fine contrastive learning framework. At the sequence level, a contrastive loss encourages the predicted acoustic token sequence <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math> to preserve the overall pitch contour defined by <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>. At the frame level, a token-wise contrastive objective aligns fine-grained acoustic features with localized pitch variations, thereby reinforcing frame-level pitch fidelity. In addition, we introduce an auxiliary singing voice transcription (SVT) model, trained to estimate pitch sequences directly from acoustic tokens. The SVT model provides pseudo pitch labels that serve as external supervision during S2A training. This auxiliary signal further improves alignment between the synthesized melody and the target pitch contour.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "singing",
                    "objective",
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, our method extends contrastive learning to the discrete-token SVS setting by incorporating hierarchical supervision. At the sequence level, we enforce prompt-invariant acoustic consistency conditioned on identical semantic and pitch tokens, encouraging the model to preserve global melody shape. At the local level, a frame-wise contrastive loss is applied to align acoustic token features with fine-grained pitch variations. This coarse-to-fine scheme allows our model to disentangle pitch conditioning from acoustic prompts and enhances melody fidelity under zero-shot generation.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To promote melody-consistent synthesis under varying acoustic prompts, we introduce a sequence-level symmetric contrastive loss.\nGiven a batch of <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> training samples that share the same semantic token sequence <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math> but differ in pitch sequences <math alttext=\"\\tilde{\\mathbf{m}}^{p}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}_{k}</annotation></semantics></math>, we construct two sets of inputs:\n<math alttext=\"\\mathcal{S}^{g}_{A}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,A}_{k},\\mathbf{a}^{A}_{k,t}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>A</mi><mi>g</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mi>&#119852;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>A</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}^{g}_{A}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,A}_{k},\\mathbf{a}^{A}_{k,t}\\}_{k=1}^{K}</annotation></semantics></math> and\n<math alttext=\"\\mathcal{S}^{g}_{B}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,B}_{k},\\mathbf{a}^{B}_{k,t}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>B</mi><mi>g</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mi>&#119852;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>B</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}^{g}_{B}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,B}_{k},\\mathbf{a}^{B}_{k,t}\\}_{k=1}^{K}</annotation></semantics></math>,\nwhere <math alttext=\"\\mathbf{a}^{A}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{A}_{k,t}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{B}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>B</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{B}_{k,t}</annotation></semantics></math> denote masked acoustic tokens, <math alttext=\"\\mathbf{a}^{r,A}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,A}_{k}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{r,B}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,B}_{k}</annotation></semantics></math> denote acoustic prompts sampled from distinct utterances of the same singer, ensuring consistent timbre across the pair.\nThe prompt length is randomly selected from the range <math alttext=\"\\left[\\min\\left(\\left\\lfloor L/4\\right\\rfloor,5\\right),\\left\\lfloor L/2\\right\\rfloor\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><mrow><mo>[</mo><mrow><mi>min</mi><mo>&#8289;</mo><mrow><mo>(</mo><mrow><mo>&#8970;</mo><mrow><mi>L</mi><mo>/</mo><mn>4</mn></mrow><mo>&#8971;</mo></mrow><mo>,</mo><mn>5</mn><mo>)</mo></mrow></mrow><mo>,</mo><mrow><mo>&#8970;</mo><mrow><mi>L</mi><mo>/</mo><mn>2</mn></mrow><mo>&#8971;</mo></mrow><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left[\\min\\left(\\left\\lfloor L/4\\right\\rfloor,5\\right),\\left\\lfloor L/2\\right\\rfloor\\right)</annotation></semantics></math>, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the length of the semantic sequence. Each input is processed by the S2A model to produce acoustic token embeddings <math alttext=\"\\mathbf{g}^{a},\\mathbf{g}^{b}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>&#119840;</mi><mi>a</mi></msup><mo>,</mo><msup><mi>&#119840;</mi><mi>b</mi></msup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{g}^{a},\\mathbf{g}^{b}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math>, which are mean-pooled across the time dimension to yield global acoustic representations <math alttext=\"\\tilde{\\mathbf{g}}^{a},\\tilde{\\mathbf{g}}^{b}\\in\\mathbb{R}^{K\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mrow><mrow><msup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>a</mi></msup><mo>,</mo><msup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>b</mi></msup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{g}}^{a},\\tilde{\\mathbf{g}}^{b}\\in\\mathbb{R}^{K\\times D}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is a temperature hyperparameter. Each positive pair <math alttext=\"(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>i</mi><mi>a</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>i</mi><mi>b</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{i})</annotation></semantics></math> corresponds to index-aligned embeddings generated under identical semantic tokens <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m3\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math> and regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m4\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, but conditioned on different acoustic prompts <math alttext=\"\\mathbf{a}^{r,A}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>i</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,A}_{i}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{r,B}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>i</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,B}_{i}</annotation></semantics></math>. These prompts are randomly selected from non-overlapping segments of the same singer&#8217;s recordings, ensuring consistent timbre while introducing natural variation. In contrast, off-diagonal pairs <math alttext=\"(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>i</mi><mi>a</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>j</mi><mi>b</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{j})</annotation></semantics></math> for <math alttext=\"i\\neq j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m8\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8800;</mo><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">i\\neq j</annotation></semantics></math> serve as negatives due to mismatched pitch sequences, even though semantic tokens and speaker identity remain the same. These negatives are nontrivial, as they reflect realistic melodic differences under otherwise comparable contextual and timbral conditions. This design encourages the model to focus on capturing global pitch structure while remaining invariant to prompt-induced variability.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the sequence contrastive loss promotes utterance-level melody consistency, it does not directly enforce fine-grained pitch alignment at the frame level&#8212;an essential factor in singing synthesis due to rapid and expressive melodic changes. To address this limitation, we introduce a frame-level contrastive objective that supervises token-wise alignment between generated acoustic representations and the input pitch contour. Our design is motivated by recent work such as CTAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib60\" title=\"\">60</a>]</cite>, which leverage contrastive learning to align discrete phoneme sequences with speech features for TTS, voice conversion, and ASR tasks under limited supervision. Although our formulation differs in both granularity and modality&#8212;operating on pitch tokens rather than phonemes, and targeting melody alignment in singing synthesis&#8212;these studies underscore the effectiveness of contrastive supervision for bridging symbolic and acoustic representations. By extending this idea to the SVS domain, our frame-level contrastive loss enhances local pitch fidelity while remaining robust to prompt-induced variation, thereby enabling more precise and expressive melody control in zero-shot scenarios.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "synthesis",
                    "singing",
                    "objective",
                    "acoustic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each input is passed through the S2A model to produce frame-level acoustic embeddings <math alttext=\"\\mathbf{f}^{a},\\mathbf{f}^{b}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>&#119839;</mi><mi>a</mi></msup><mo>,</mo><msup><mi>&#119839;</mi><mi>b</mi></msup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{f}^{a},\\mathbf{f}^{b}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math>. For each training sample <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>, we compute a cosine similarity matrix <math alttext=\"\\mathbf{S}^{k}\\in\\mathbb{R}^{L\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mi>k</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{k}\\in\\mathbb{R}^{L\\times L}</annotation></semantics></math> between <math alttext=\"\\mathbf{f}^{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m4\" intent=\":literal\"><semantics><msup><mi>&#119839;</mi><mi>a</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{f}^{a}</annotation></semantics></math> and <math alttext=\"\\mathbf{f}^{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m5\" intent=\":literal\"><semantics><msup><mi>&#119839;</mi><mi>b</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{f}^{b}</annotation></semantics></math>. To supervise the similarity learning, we define a soft label matrix <math alttext=\"\\mathbf{Y}^{k}\\in[-1,1]^{L\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m6\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>k</mi></msup><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{k}\\in[-1,1]^{L\\times L}</annotation></semantics></math> capturing pitch and semantic alignment:</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This formulation encourages the model to produce highly similar acoustic embeddings when both pitch and semantic content align, moderately similar embeddings when only pitch aligns, and dissimilar embeddings otherwise. The similarity is computed within each utterance because the vocal range is typically locally bounded, making repeated pitch tokens more likely. In contrast, pitch overlap across utterances is rare and thus excluded. Additionally, even within the same utterance, repeated pitches may be associated with different semantic tokens, resulting in subtle acoustic variation. Our soft labeling mechanism accounts for this by assigning intermediate similarity, thereby avoiding over-penalization while promoting melody-consistent synthesis. Hence, the final contrastive learning objective is as follows:</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "acoustic",
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent studies have explored the use of Singing Voice Transcription (SVT) to support singing voice synthesis (SVS). For example, ROSVOT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib61\" title=\"\">61</a>]</cite> proposes a robust SVT model to produce high-quality pitch annotations for large-scale singing datasets, thereby improving SVS performance by enhancing training data quality. However, such approaches treat SVT as an independent preprocessing tool, disconnected from the synthesis process.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "singing",
                    "model",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, we integrate the SVT module directly into the training pipeline to provide explicit frame-level pitch supervision, thereby enhancing melody modeling and alignment. Specifically, the SVT model predicts frame-wise discrete pitch tokens from acoustic codec representations, which are then compared against the ground-truth pitch sequence. This supervision enforces alignment between the generated acoustic tokens and the intended melody, encouraging consistent pitch realization, particularly under zero-shot conditions. Furthermore, as the SVT module operates entirely on discrete representations, it is naturally compatible with our codec-based SVS framework and does not require raw audio or continuous F0 contours.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model",
                    "codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SVT model adopts a lightweight encoder-only Transformer architecture. Given a sequence of acoustic tokens <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, it predicts the corresponding pitch token sequence <math alttext=\"\\hat{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>^</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{m}}^{p}</annotation></semantics></math> of length <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>. The encoder consists of four Transformer layers with a hidden size of 512 and eight attention heads. Each input frame comprises 12 discrete acoustic codes, which are individually embedded, concatenated, and projected to a 512-dimensional representation, followed by layer normalization. The resulting embedding sequence is then passed through a linear classification head to predict a pitch token for each frame. The model is trained using a standard cross-entropy loss between the predicted and reference pitch sequences.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide frame-level supervision for pitch modeling, we leverage the pretrained SVT model as a pitch predictor to generate pseudo labels in the form of pitch token sequences <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m1\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, which are temporally aligned with the acoustic frames. As the primary training objective, we apply a cross-entropy loss <math alttext=\"\\mathcal{L}_{\\rm CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>CE</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm CE}</annotation></semantics></math> between the predicted pitch tokens <math alttext=\"\\hat{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m3\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>^</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{m}}^{p}</annotation></semantics></math> and the SVT-derived ground-truth <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m4\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, encouraging accurate token-level classification. However, the cross-entropy objective alone does not account for the temporal continuity inherent in repeated pitch tokens. This often results in jittery predictions, fragmented note segments, and rhythmically unstable outputs.</p>\n\n",
                "matched_terms": [
                    "results",
                    "acoustic",
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior studies have highlighted the importance of modeling temporal structure for natural singing synthesis. XiaoiceSing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib37\" title=\"\">37</a>]</cite> introduces syllable-level duration modeling to preserve rhythmic consistency, while Singing-Tacotron&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib62\" title=\"\">62</a>]</cite> enhances segmental alignment through transition tokens and duration-informed attention mechanisms.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "singing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by recent advances in speech synthesis that emphasize the importance of temporal alignment and duration modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib37\" title=\"\">37</a>]</cite>, we introduce a soft duration loss <math alttext=\"\\mathcal{L}_{\\text{dur}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dur}}</annotation></semantics></math> to enhance the rhythmic fidelity of frame-level pitch predictions. Prior works such as FastSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib63\" title=\"\">63</a>]</cite> and XiaoiceSing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib37\" title=\"\">37</a>]</cite> employ explicit duration predictors or auxiliary alignment modules to supervise temporal structures. While effective, these methods often introduce architectural overhead or struggle to generalize in expressive singing scenarios. In contrast, our approach provides a fully differentiable supervision signal by directly supervising the temporal distribution of pitch token probabilities using the softmax outputs of the model, without requiring any external duration modeling.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "singing",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This formulation encourages the model to allocate appropriate probability mass to each pitch token across time, thereby promoting temporally coherent and rhythmically faithful melody generation. The final training objective for the SVT module combines the cross-entropy loss, segment transition loss, and soft duration loss:</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin by training the Singing Voice Transcription (SVT) model to provide frame-level pitch supervision for subsequent Semantic-to-Acoustic (S2A) adaptation. The SVT model is optimized using a cross-entropy loss <math alttext=\"\\mathcal{L}_{\\text{CE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CE</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CE}}</annotation></semantics></math> between the regulated pitch token sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math> and the acoustic token sequence <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, thereby learning to predict temporally aligned pitch trajectories from acoustic inputs. Once trained, the SVT module is frozen to serve as a fixed auxiliary supervisor during S2A training.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "singing",
                    "model",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then fine-tune the S2A model built upon the MaskGCT framework&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, which leverages masked acoustic modeling for non-autoregressive generation. The training objective for the S2A model comprises three components: (1) the mask token prediction loss <math alttext=\"\\mathcal{L}_{\\text{mask}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>mask</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{mask}}</annotation></semantics></math>, which reconstructs randomly masked acoustic tokens from noisy inputs using a masked denoising objective; (2) a coarse-to-fine contrastive loss <math alttext=\"\\mathcal{L}_{\\text{CL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CL}}</annotation></semantics></math>, composed of sequence-level (<math alttext=\"\\mathcal{L}_{\\text{SCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>SCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{SCL}}</annotation></semantics></math>) and frame-level (<math alttext=\"\\mathcal{L}_{\\text{FCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>FCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{FCL}}</annotation></semantics></math>) terms, to enforce consistency between the melody condition and generated acoustic tokens while mitigating prosody leakage from the prompt; and (3) an auxiliary SVT loss <math alttext=\"\\mathcal{L}_{\\text{SVT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>SVT</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{SVT}}</annotation></semantics></math>, which encourages the predicted acoustic tokens to be rhythmically and melodically consistent with the SVT-inferred pitch contour. The fine-tuning algorithm for the S2A model is summarized in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#alg1\" title=\"Algorithm 1 &#8227; III-D Training and Inference Procedures &#8227; III Method &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Each training sample <math alttext=\"\\mathbf{x}_{k}\\in\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#119857;</mi><mi>k</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{k}\\in\\mathcal{B}</annotation></semantics></math> comprises a semantic sequence <math alttext=\"\\mathbf{s}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m7\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{k}</annotation></semantics></math>, a regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}_{k}</annotation></semantics></math>, and a time-aligned acoustic token sequence <math alttext=\"\\mathbf{a}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m9\" intent=\":literal\"><semantics><msub><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{a}_{k,t}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve adaptation efficiency and reduce overfitting, we apply Low-Rank Adaptation (LoRA)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib64\" title=\"\">64</a>]</cite> to fine-tune the S2A model&#8217;s diffusion estimator module, which is implemented using a DiffLlama-style architecture. LoRA introduces trainable low-rank matrices into the linear layers of pretrained models, enabling efficient fine-tuning by updating only a small subset of parameters while keeping the original weights frozen. This allows our model to retain prior knowledge from the TTS domain while adapting to the stylistic nuances of singing voice synthesis with limited data.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "singing",
                    "model",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments on two publicly available mandarin singing corpora: M4Singer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib23\" title=\"\">23</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/M4Singer/M4Singer\" title=\"\">https://github.com/M4Singer/M4Singer</a></span></span></span> and Opencpop <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib22\" title=\"\">22</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xinshengwang.github.io/opencpop/\" title=\"\">https://xinshengwang.github.io/opencpop/</a></span></span></span>. The M4Singer dataset comprises studio-quality recordings from 20 professional singers spanning SATB vocal ranges, along with comprehensive annotations including lyrics, pitch, note duration, and slur information. The Opencpop dataset contains 100 Chinese pop songs sung by a professional female vocalist, with precise phoneme, note, and syllable-level annotations aligned to the music score. For evaluation, we construct both seen- and unseen-singer test sets. For seen-singer evaluation, we randomly select 50 utterances each from the M4Singer and Opencpop datasets. For zero-shot (unseen-singer) evaluation, we use 10 male and 10 female singers from the OpenSinger <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib65\" title=\"\">65</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Multi-Singer/Multi-Singer.github.io?tab=readme-ov-file\" title=\"\">https://github.com/Multi-Singer/Multi-Singer.github.io?tab=readme-ov-file</a></span></span></span> dataset. Since OpenSinger lacks complete music score annotations, we pair it with M4Singer&#8217;s score sequences to enable evaluation. All audio is uniformly down-sampled to 24 kHz with 16-bit quantization.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "singing",
                    "seen",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fine-tune the S2A model, we preprocess the dataset to obtain temporally aligned semantic tokens <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math>, acoustic tokens <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, and regulated pitch tokens <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>. All audio segments are first converted to mono and resampled to 24&#160;kHz. We then utilize the pretrained semantic and acoustic codec models from the MaskGCT framework<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/open-mmlab/Amphion/tree/main/models/tts/maskgct\" title=\"\">https://github.com/open-mmlab/Amphion/tree/main/models/tts/maskgct</a></span></span></span> to extract <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>. For samples with mismatched token lengths, we apply zero-padding to align their temporal dimensions for frame-level supervision. The regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math> is derived by expanding the original pitch sequence <math alttext=\"\\mathbf{m}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{p}</annotation></semantics></math> according to the duration sequence <math alttext=\"\\mathbf{m}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>d</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{d}</annotation></semantics></math>, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S3.SS1\" title=\"III-A Overview &#8227; III Method &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">III-A</span></a>. The SVT model is trained using the preprocessed acoustic tokens <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math> and the corresponding regulated pitch tokens <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m10\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model",
                    "codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SVT model is trained on a single NVIDIA RTX A5000 GPU using the AdamW optimizer with a learning rate of 1e&#8722;5, weight decay of 0.01, and a cosine learning rate schedule with 5K warm-up steps over 50K updates. Training is performed for 100 epochs with a batch size of 32 using mixed-precision (FP16) computation. Subsequently, the S2A model is fine-tuned on four NVIDIA RTX A5000 GPUs with data parallelism. We adopt the AdamW optimizer with a learning rate of 1e&#8722;5, 32K warm-up steps, and the inverse square root learning rate schedule. Fine-tuning is conducted for 300K steps with a total batch size of 32, where the first 8 samples are used for sequence contrastive learning and the remaining 24 for frame-level contrastive learning. We apply dropout (0.1), label smoothing (0.1), and gradient clipping to stabilize training. During this stage, all model components are frozen except the S2A decoder. The loss weights are set as follows: <math alttext=\"\\lambda_{\\text{SCL}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>SCL</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{SCL}}=0.5</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{FCL}}=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>FCL</mtext></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{FCL}}=1.0</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{CL}}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>CL</mtext></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{CL}}=0.1</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{seg}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>seg</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{seg}}=0.5</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{dur}}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>dur</mtext></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{dur}}=0.3</annotation></semantics></math>, and <math alttext=\"\\lambda_{\\text{SVT}}=0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>SVT</mtext></msub><mo>=</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{SVT}}=0.2</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the performance of our system in terms of pitch accuracy, timbre consistency, and perceptual quality, we conduct objective evaluations under both seen-singer and zero-shot settings. The following metrics are employed:</p>\n\n",
                "matched_terms": [
                    "evaluations",
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess timbre similarity, we compute cosine similarity between speaker embeddings extracted from the synthesized and reference audio using a WavLM-based speaker verification model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib66\" title=\"\">66</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/microsoft/wavlm-base-sv\" title=\"\">https://huggingface.co/microsoft/wavlm-base-sv</a></span></span></span>. SECS values range from 0 to 1, with higher scores indicating closer alignment in vocal identity.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib67\" title=\"\">67</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/South-Twilight/SingMOS\" title=\"\">https://github.com/South-Twilight/SingMOS</a></span></span></span> is a learned metric trained to predict human perceptual ratings of singing voice quality. It is based on a curated dataset of professional ratings for both natural and synthesized singing in Chinese and Japanese, addressing the annotation scarcity in the singing domain. SingMOS produces scores in the range of 0 to 5, with higher values indicating greater naturalness and perceptual quality. As a reference-free metric, it enables scalable automatic evaluation in zero-shot and low-resource conditions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "singing",
                    "voice",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess perceptual quality, we conducted a Mean Opinion Score (MOS) evaluation with 20 participants who have formal training in singing and experience in vocal performance <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>This study has been approved by the Department Ethics Review Committee\n(DERC) at the National University of Singapore under\nDERC Ref Code: 000479.</span></span></span>. Each participant rated the synthesized samples based on overall naturalness (MOS-N), audio quality (MOS-Q), and timbre similarity (SMOS). A 5-point Likert scale was used, where a score of 5 indicates excellent perceptual quality and 1 denotes poor quality.</p>\n\n",
                "matched_terms": [
                    "smos",
                    "evaluation",
                    "mosq",
                    "singing",
                    "mosn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several recent TTS systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib68\" title=\"\">68</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib69\" title=\"\">69</a>]</cite> have reported high prosodic similarity between the speech prompt and the synthesized output. While this may appear beneficial in TTS, it reveals a form of prosody leakage, where expressive cues from the prompt inadvertently influence the generated speech. This issue becomes particularly problematic in singing voice synthesis (SVS), where pitch and rhythm should be governed solely by the input music score. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S1\" title=\"I Introduction &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, we refer to this phenomenon as prosody leakage.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "singing",
                    "voice",
                    "reported"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate whether MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite> exhibits such behavior, we conduct a prosody similarity analysis following prior evaluation protocols. NaturalSpeech 2 and 3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>]</cite> quantify prosodic similarity by comparing pitch and duration features between the prompt and output, while StyleTTS-ZS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib69\" title=\"\">69</a>]</cite> computes Pearson correlation coefficients of acoustic features to evaluate prosodic alignment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "acoustic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by these approaches, we evaluate the prosodic similarity of MaskGCT in both English and Mandarin using the LibriTTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib25\" title=\"\">25</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.openslr.org/60/\" title=\"\">https://www.openslr.org/60/</a></span></span></span> and AISHELL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib70\" title=\"\">70</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openslr.org/93/\" title=\"\">https://openslr.org/93/</a></span></span></span> datasets, respectively. For each speaker, we randomly sample 50 utterances to construct the test sets. During inference, we synthesize 50 utterances per dataset by conditioning on the same target text but using different speech prompts. We then compute the acoustic-level similarity between each synthesized utterance and: (1) its paired prompt (i.e., the one used during generation), and (2) an unpaired prompt from the same speaker. This comparison allows us to quantify the extent of prompt-induced prosody similarity, which serves as an indicator of potential prosody leakage in the model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T1\" title=\"TABLE I &#8227; V-A Evaluating Prompt-Induced Prosody Similarity in MaskGCT &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> presents a quantitative analysis of prompt-induced prosody similarity by comparing the acoustic differences between synthesized and prompt speech under paired and unpaired conditions. Across both LibriTTS and AISHELL-3, paired prompts consistently yield lower differences in pitch, energy, and other prosodic indicators, confirming stronger alignment in prosodic patterns. In contrast, the unpaired condition results in noticeably higher deviations, particularly in pitch mean, energy mean, and jitter, suggesting that the synthesized outputs are heavily influenced by the prosodic characteristics of the prompt.</p>\n\n",
                "matched_terms": [
                    "results",
                    "acoustic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first note that the performance gap between the Ground Truth (GT) and GT with Acoustic Codec is minimal across all metrics, confirming that the discrete acoustic token representation introduces negligible degradation and establishing a strong upper bound for token-based SVS systems. CoMelSinger approaches this bound closely, suggesting that its improvements arise from architectural designs&#8212;particularly the disentangled modeling of melody and timbre&#8212;rather than signal-level enhancements. Among all models, CoMelSinger achieves the highest SMOS and SECS scores, indicating strong timbre consistency and accurate preservation of speaker identity, which validates the effectiveness of the in-context prompting mechanism. It also attains the lowest F0-RMSE and one of the highest SingMOS scores, reflecting precise melody reproduction and high perceptual naturalness. Furthermore, its competitive MCD score demonstrates the model&#8217;s ability to reconstruct spectral features with smooth and consistent vocal quality, confirming the effectiveness of the proposed structured melody control strategy.</p>\n\n",
                "matched_terms": [
                    "smos",
                    "codec",
                    "comelsinger",
                    "ground",
                    "f0rmse",
                    "mcd",
                    "acoustic",
                    "secs",
                    "truth",
                    "metrics",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further evaluate CoMelSinger in a zero-shot setting, where the model synthesizes singing voices from speakers not seen during training. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T3\" title=\"TABLE III &#8227; V-C Zero-Shot Singing Voice Synthesis &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, CoMelSinger maintains strong performance across all subjective and objective metrics, exhibiting only minimal degradation compared to the seen condition.</p>\n\n",
                "matched_terms": [
                    "comelsinger",
                    "singing",
                    "subjective",
                    "objective",
                    "seen",
                    "model",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, baseline systems show notable declines in key timbre- and melody-related metrics such as SMOS, SECS, and F0-RMSE, underscoring their limited ability to generalize to unseen vocal identities. CoMelSinger&#8217;s robustness in zero-shot scenarios is attributed to the synergy between in-context prompting&#8212;which leverages short acoustic references to anchor timbre&#8212;and large-scale speech pretraining, which imparts transferable prosodic priors.</p>\n\n",
                "matched_terms": [
                    "smos",
                    "f0rmse",
                    "acoustic",
                    "secs",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the inherent challenge of handling unseen timbres, CoMelSinger continues to achieve high speaker similarity while preserving accurate pitch trajectories. This balance between identity retention and melodic fidelity demonstrates the model&#8217;s strong generalization capacity. While many existing approaches face trade-offs between controllability and naturalness, CoMelSinger effectively reconciles both through its structured architecture and explicit conditioning scheme. These findings position CoMelSinger as a strong baseline for zero-shot singing voice synthesis with discrete representations.</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "singing",
                    "voice",
                    "comelsinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">(a) Ground Truth (GT)</p>\n\n",
                "matched_terms": [
                    "ground",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the contribution of each component in CoMelSinger, we perform ablation studies by systematically disabling key modules. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T4\" title=\"TABLE IV &#8227; V-D Ablation Study &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> reports results on four objective metrics: MCD, F0-RMSE, SingMOS, and SECS. Removing the entire coarse-to-fine contrastive learning (CL) framework leads to substantial degradation across all metrics, indicating reduced pitch accuracy and speaker consistency. This highlights the importance of contrastive objectives in disentangling pitch from timbre and improving input&#8211;output alignment.</p>\n\n",
                "matched_terms": [
                    "comelsinger",
                    "f0rmse",
                    "mcd",
                    "objective",
                    "results",
                    "secs",
                    "metrics",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To isolate the effects of each contrastive branch, we further ablate sequence contrastive learning (SCL) and frame-level contrastive learning (FCL) individually. Excluding SCL moderately affects MCD and SECS, suggesting its role in maintaining global speaker identity. In contrast, removing FCL causes a larger drop in F0-RMSE and SingMOS, confirming its effectiveness in modeling fine-grained pitch details and promoting melodic continuity. These results validate the hierarchical design of our contrastive learning framework.</p>\n\n",
                "matched_terms": [
                    "f0rmse",
                    "mcd",
                    "results",
                    "secs",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate the impact of the singing voice transcription (SVT) module, which provides auxiliary pitch supervision. Excluding SVT results in higher F0-RMSE and lower SingMOS, confirming the benefit of explicit alignment signals for structured melody control. The most severe degradation occurs when both CL and SVT are removed, indicating their complementary roles in pitch&#8211;timbre disentanglement and temporal stability.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "singing",
                    "f0rmse",
                    "results",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.F5\" title=\"Figure 5 &#8227; Component Analysis &#8227; V-D Ablation Study &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> visualizes mel-spectrograms and pitch contours for the ground truth, our model, and two ablated variants. Predicted pitch trajectories (red) are overlaid with ground-truth pitch (blue), with word-level boundaries marked by dashed lines and pinyin annotations. Compared to the ablated models, CoMelSinger achieves better pitch alignment and smoother contours, illustrating the effectiveness of both CL and SVT in preserving melodic structure.</p>\n\n",
                "matched_terms": [
                    "ground",
                    "model",
                    "comelsinger",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T5\" title=\"TABLE V &#8227; Comparison of Fine-Tuning Strategies &#8227; V-D Ablation Study &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> presents a comparison of six fine-tuning strategies in terms of both objective and subjective performance, along with their respective trainable parameter ratios. FT-LoRA delivers the best overall performance, achieving the lowest F0-RMSE, highest SingMOS, and highest SECS, while updating only 4.81% of the parameters&#8212;highlighting the effectiveness of low-rank adaptation for efficient model tuning. FT-PGS achieves the lowest MCD, suggesting enhanced spectral fidelity through gradual unfreezing, though its pitch accuracy is affected by delayed optimization of lower layers. FT-Prefix and FT-Pitch yield consistent results with minimal overhead, demonstrating the utility of lightweight adaptation modules. In contrast, FT-LLRD and FT-Full fine-tune all parameters yet underperform across most metrics, indicating that full-capacity adaptation may lead to overfitting or instability in data-scarce settings. These results underscore that parameter-efficient strategies, particularly LoRA, can match or surpass full-model fine-tuning while substantially reducing computational cost.</p>\n\n",
                "matched_terms": [
                    "f0rmse",
                    "mcd",
                    "subjective",
                    "objective",
                    "results",
                    "secs",
                    "model",
                    "metrics",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present CoMelSinger, a zero-shot singing voice synthesis framework that extends discrete token-based TTS models to support structured and controllable melody generation. Built upon the non-autoregressive MaskGCT architecture, CoMelSinger incorporates lyrics and pitch tokens as inputs, enabling fine-grained alignment between the musical score and the generated voice. To address the challenge of prosody leakage from prompt-based conditioning&#8212;an issue unique to singing synthesis&#8212;we propose a coarse-to-fine contrastive learning strategy that explicitly disentangles pitch information from the timbre prompt. Furthermore, we introduce a lightweight singing voice transcription (SVT) module to provide frame-level pitch and duration supervision, enhancing the model&#8217;s ability to follow the intended melody with precision. Extensive experiments on both seen and unseen singers demonstrate that CoMelSinger achieves strong zero-shot generalization, consistently outperforming competitive SVS baselines in pitch accuracy, timbre consistency, and subjective quality. Our results confirm that structured melody control and contrastive disentanglement are essential for scalable and expressive singing synthesis. We believe CoMelSinger opens new possibilities for discrete token-based SVS, enabling scalable and zero-shot singing generation.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "comelsinger",
                    "synthesis",
                    "singing",
                    "subjective",
                    "results",
                    "seen"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance",
        "caption": "TABLE III: Subjective (with 95% confidence intervals) and objective evaluation results on the unseen test set for zero-shot SVS. Note that MCD is excluded since ground-truth alignments are unavailable in this setting.",
        "body": "Model\nSubjective Evaluations\nObjective Evaluations\n\n\nMOS-Q ↑\\uparrow\n\nMOS-N ↑\\uparrow\n\nSMOS ↑\\uparrow\n\nF0-RMSE ↓\\downarrow\n\nSingMOS ↑\\uparrow\n\nSECS ↑\\uparrow\n\n\n\nGT\n4.20 ±\\pm 0.12\n4.35 ±\\pm 0.14\n4.55 ±\\pm 0.15\n-\n4.41\n0.932\n\n\nGT (Acoustic Codec)\n4.07 ±\\pm 0.18\n4.22 ±\\pm 0.15\n4.32 ±\\pm 0.11\n0.015\n4.66\n0.921\n\n\nDiffSinger\n3.75 ±\\pm 0.16\n3.72 ±\\pm 0.18\n3.25 ±\\pm 0.12\n0.098\n4.11\n0.658\n\n\nVISinger2\n3.72 ±\\pm 0.19\n3.74 ±\\pm 0.20\n3.31 ±\\pm 0.16\n0.074\n4.08\n0.704\n\n\nStyleSinger\n3.48 ±\\pm 0.11\n3.82 ±\\pm 0.18\n3.85 ±\\pm 0.15\n0.125\n4.22\n0.853\n\n\nSPSinger\n3.92 ±\\pm 0.15\n4.03 ±\\pm 0.15\n3.76 ±\\pm 0.10\n0.065\n4.29\n0.844\n\n\nCoMelSinger (ours)\n3.87 ±\\pm 0.18\n4.11 ±\\pm 0.15\n4.14 ±\\pm 0.14\n0.048\n4.25\n0.897",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Subjective Evaluations</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Objective Evaluations</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MOS-Q <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MOS-N <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">SMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">F0-RMSE <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SingMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SECS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">GT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.20 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.35 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4.55 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.932</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GT (Acoustic Codec)</td>\n<td class=\"ltx_td ltx_align_center\">4.07 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.18</td>\n<td class=\"ltx_td ltx_align_center\">4.22 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">4.32 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.11</td>\n<td class=\"ltx_td ltx_align_center\">0.015</td>\n<td class=\"ltx_td ltx_align_center\">4.66</td>\n<td class=\"ltx_td ltx_align_center\">0.921</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">DiffSinger</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.75 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.72 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.25 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.098</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.658</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">VISinger2</td>\n<td class=\"ltx_td ltx_align_center\">3.72 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.19</td>\n<td class=\"ltx_td ltx_align_center\">3.74 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m17\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.31 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m18\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.16</td>\n<td class=\"ltx_td ltx_align_center\">0.074</td>\n<td class=\"ltx_td ltx_align_center\">4.08</td>\n<td class=\"ltx_td ltx_align_center\">0.704</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">StyleSinger</td>\n<td class=\"ltx_td ltx_align_center\">3.48 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m19\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.11</td>\n<td class=\"ltx_td ltx_align_center\">3.82 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m20\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.85 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m21\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.15</td>\n<td class=\"ltx_td ltx_align_center\">0.125</td>\n<td class=\"ltx_td ltx_align_center\">4.22</td>\n<td class=\"ltx_td ltx_align_center\">0.853</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SPSinger</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.92 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m22\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.15</span></td>\n<td class=\"ltx_td ltx_align_center\">4.03 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m23\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.76 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m24\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.10</td>\n<td class=\"ltx_td ltx_align_center\">0.065</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.29</span></td>\n<td class=\"ltx_td ltx_align_center\">0.844</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">CoMelSinger (ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">3.87 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m25\" intent=\":literal\"><semantics><mo mathbackground=\"#F2F2F2\" style=\"--ltx-bg-color:#F2F2F2;\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">4.11 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m26\" intent=\":literal\"><semantics><mo mathbackground=\"#F2F2F2\" style=\"--ltx-bg-color:#F2F2F2;\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">4.14 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m27\" intent=\":literal\"><semantics><mo mathbackground=\"#F2F2F2\" style=\"--ltx-bg-color:#F2F2F2;\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">0.048</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F2F2F2;\">4.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">0.897</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "codec",
            "f0rmse",
            "subjective",
            "±pm",
            "groundtruth",
            "spsinger",
            "↑uparrow",
            "unavailable",
            "intervals",
            "confidence",
            "evaluation",
            "alignments",
            "visinger2",
            "mcd",
            "singmos",
            "objective",
            "results",
            "acoustic",
            "note",
            "unseen",
            "svs",
            "smos",
            "diffsinger",
            "comelsinger",
            "↓downarrow",
            "stylesinger",
            "secs",
            "evaluations",
            "iii",
            "zeroshot",
            "test",
            "ours",
            "mosq",
            "set",
            "excluded",
            "mosn",
            "model",
            "since",
            "setting"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We further evaluate CoMelSinger in a zero-shot setting, where the model synthesizes singing voices from speakers not seen during training. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T3\" title=\"TABLE III &#8227; V-C Zero-Shot Singing Voice Synthesis &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, CoMelSinger maintains strong performance across all subjective and objective metrics, exhibiting only minimal degradation compared to the seen condition.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Singing Voice Synthesis (SVS) aims to generate expressive vocal performances from structured musical inputs such as lyrics and pitch sequences. While recent progress in discrete codec-based speech synthesis has enabled zero-shot generation via in-context learning, directly extending these techniques to SVS remains non-trivial due to the requirement for precise melody control. In particular, prompt-based generation often introduces prosody leakage, where pitch information is inadvertently entangled within the timbre prompt, compromising controllability. We present CoMelSinger, a zero-shot SVS framework that enables structured and disentangled melody control within a discrete codec modeling paradigm. Built on the non-autoregressive MaskGCT architecture, CoMelSinger replaces conventional text inputs with lyric and pitch tokens, preserving in-context generalization while enhancing melody conditioning. To suppress prosody leakage, we propose a coarse-to-fine contrastive learning strategy that explicitly regularizes pitch redundancy between the acoustic prompt and melody input. Furthermore, we incorporate a lightweight encoder-only Singing Voice Transcription (SVT) module to align acoustic tokens with pitch and duration, offering fine-grained frame-level supervision. Experimental results demonstrate that CoMelSinger achieves notable improvements in pitch accuracy, timbre consistency, and zero-shot transferability over competitive baselines.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "codec",
                    "comelsinger",
                    "results",
                    "acoustic",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent SVS frameworks, including end-to-end&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib9\" title=\"\">9</a>]</cite> and diffusion-based architectures&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib3\" title=\"\">3</a>]</cite>, have demonstrated strong performance in supervised scenarios with seen singers. Nevertheless, zero-shot SVS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib13\" title=\"\">13</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib10\" title=\"\">10</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib14\" title=\"\">14</a>]</cite>&#8212;synthesizing singing voices for unseen speakers without additional fine-tuning&#8212;still has substantial room for improvement.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "zeroshot",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete token-based architectures show great in-context learning capabilities and provide a promising pathway for such zero-shot generation. In a related task, text-to-speech (TTS) has witnessed rapid progress with discrete acoustic tokens derived from vector quantization and neural audio codecs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib16\" title=\"\">16</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib17\" title=\"\">17</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>]</cite>. By mapping complex waveforms into a quantized latent space, these tokens capture timbre, prosody, and phonetic content, thereby reformulating speech synthesis as symbolic sequence modeling akin to language modeling. The success of token-based TTS systems is largely enabled by large-scale multi-speaker corpora, which provide sufficient diversity to learn robust and generalizable representations. In contrast, the scarcity and limited diversity of singing data make token-based modeling for SVS significantly more challenging.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "zeroshot",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Built upon this data-rich foundation, recent TTS systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib20\" title=\"\">20</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib21\" title=\"\">21</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib16\" title=\"\">16</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib17\" title=\"\">17</a>]</cite>, have adopted large language model (LLM)-style architectures to model the conditional distribution of acoustic tokens given phoneme sequences and optional prompts. Within this framework, in-context learning (ICL) becomes feasible: a short segment of reference speech, represented as discrete tokens, serves as an acoustic prompt to guide synthesis in terms of speaker identity and style. This approach, exemplified by models such as VALL-E <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite>, enables zero-shot speech synthesis by treating speech generation as a form of conditional codec language modeling, without requiring speaker labels or model adaptation. Inspired by these advances, researchers have begun exploring discrete token-based methods for SVS.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "codec",
                    "model",
                    "acoustic",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, directly extending TTS by replacing textual input with structured musical inputs&#8212;such as lyrics and pitch tokens&#8212;while reusing the same modeling pipeline reveals a unique challenge in the singing domain: prosody leakage from the acoustic prompt, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. In prompt-based synthesis, the acoustic prompt is intended to provide timbral cues, yet pitch-related attributes&#8212;including contour and timing&#8212;are often inadvertently encoded into its latent representation. This unintended encoding leads to timbre&#8211;melody entanglement, where the prompt simultaneously influences vocal timbre and melodic realization. As a result, the system&#8217;s control over the explicitly specified pitch sequence is weakened, undermining the precise separation between timbre conditioning and melody generation that SVS requires.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The disparity in dataset size between singing <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib22\" title=\"\">22</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib23\" title=\"\">23</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib24\" title=\"\">24</a>]</cite>, and speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib25\" title=\"\">25</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib26\" title=\"\">26</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib27\" title=\"\">27</a>]</cite>, further exacerbates the difficulty of addressing prosody leakage. Importantly, the effectiveness of in-context learning in TTS relies on large-scale, diverse speech corpora, which enable robust learning of disentangled token representations. In contrast, singing datasets are generally smaller and less varied, making it harder to avoid prosodic interference and to generalize prompt-based conditioning. Consequently, directly transferring token-based prompting strategies from TTS to SVS often leads to weaker melody control in zero-shot scenarios. In this case, achieving reliable melody control is particularly crucial for maintaining alignment with the musical score.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, SVS requires much finer control over pitch and melody than TTS or voice conversion, as the generated singing must accurately follow the musical score while preserving timbre. Although attribute control has been explored through adversarial training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>]</cite>, contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib28\" title=\"\">28</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib29\" title=\"\">29</a>]</cite>, and information-bottleneck methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib30\" title=\"\">30</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib31\" title=\"\">31</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib32\" title=\"\">32</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib33\" title=\"\">33</a>]</cite>, these approaches primarily focus on coarse prosodic patterns or emotional cues and provide limited support for fine-grained melody control. Make-A-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite>, as a representative discrete-token SVS system, adopts prompt-guided conditioning but lacks explicit mechanisms to prevent the acoustic prompt from influencing melody realization. To the best of our knowledge, no prior work has systematically addressed prosody leakage in discrete token-based SVS, leaving a critical gap in achieving precise and faithful melody control in zero-shot scenarios.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "zeroshot",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenges of prosody leakage and limited melody controllability in zero-shot SVS, we propose CoMelSinger, a discrete codec-based framework with structured melody control.\nCoMelSinger builds on the non-autoregressive MaskGCT architecture&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, adapting it to accept musical inputs consisting of lyrics and pitch tokens.\nTo achieve better melody&#8211;timbre control, we introduce a coarse-to-fine contrastive learning strategy that limits excessive pitch-related information in the acoustic prompt, allowing the explicit pitch condition to guide melodic realization more effectively.\nWe further incorporate a lightweight, encoder-only Singing Voice Transcription (SVT) module to provide fine-grained, frame-level supervision by aligning acoustic tokens with pitch and duration sequences.\nTogether, these designs enable accurate melody modeling, maintain timbre consistency, and preserve the in-context learning capability of discrete token-based systems.\nExtensive experiments on both seen and unseen singers demonstrate that CoMelSinger delivers substantial improvements in pitch accuracy, timbre consistency, and overall synthesis quality compared with state-of-the-art SVS baselines in zero-shot scenarios. The main contributions of this work are:</p>\n\n",
                "matched_terms": [
                    "svs",
                    "comelsinger",
                    "acoustic",
                    "zeroshot",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose CoMelSinger, a discrete token-based SVS framework for zero-shot synthesis with structured melody control.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "zeroshot",
                    "comelsinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive experiments on public SVS datasets demonstrate that CoMelSinger achieves superior pitch accuracy, timbre consistency, and generalization to unseen singers.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "comelsinger",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Singing voice synthesis (SVS) aims to produce expressive vocal performances from structured musical inputs such as note pitch, duration, and lyrics.\nCompared with text-to-speech (TTS), SVS presents unique challenges, including a wider pitch range and sustained phonation, which demand fine-grained melody modeling.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S2.F2\" title=\"Figure 2 &#8227; II-A Singing Voice Synthesis &#8227; II Related Works &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the evolution of representative SVS architectures, from continuous-feature pipelines to end-to-end models and, more recently, discrete codec-based frameworks.\nEarly SVS systems relied on unit-selection synthesis (e.g., VOCALOID&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib35\" title=\"\">35</a>]</cite>) or statistical approaches based on hidden Markov models (HMMs)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib36\" title=\"\">36</a>]</cite>.\nWith the advent of deep learning, SVS models increasingly adopted a two-stage architecture&#8212;comprising an acoustic model followed by a vocoder&#8212;including XiaoiceSing<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib37\" title=\"\">37</a>]</cite>, DeepSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib38\" title=\"\">38</a>]</cite>, and Sinsy&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib2\" title=\"\">2</a>]</cite>.\nSubsequent advances incorporated generative adversarial networks (GANs)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib39\" title=\"\">39</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib40\" title=\"\">40</a>]</cite> to enhance timbre realism, while more recent work using denoising diffusion probabilistic models (DDPMs)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib8\" title=\"\">8</a>]</cite> has achieved further gains in fidelity and temporal coherence.\nIn parallel, end-to-end systems such as VISinger 1/2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib9\" title=\"\">9</a>]</cite> have been developed to generate waveforms directly from musical scores without relying on explicit intermediate features.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model",
                    "svs",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of discrete token modeling in TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib16\" title=\"\">16</a>]</cite>, recent SVS studies have explored token-based representations to improve generalization.\nTokSing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib41\" title=\"\">41</a>]</cite> employs a non-autoregressive Transformer conditioned on lyrics and pitch embeddings to predict discrete acoustic tokens.\nHiddenSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib3\" title=\"\">3</a>]</cite> integrates a diffusion-based decoder guided by discrete pitch and semantic tokens, achieving high-quality synthesis.\nMake-A-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite> unifies speech and singing synthesis through a shared discrete representation, but uses a relatively small proportion of singing data and does not incorporate prompt-based in-context learning.\nConsequently, it lacks explicit melody conditioning and provides limited flexibility in zero-shot singing scenarios.\nIn contrast, our approach introduces melody inputs and improved melody control in prompt-based conditioning, enabling more accurate and controllable zero-shot singing voice synthesis.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "zeroshot",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of large language models (LLMs), recent approaches formulate speech synthesis as autoregressive generation over discrete codec tokens.\nVALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite> pioneered this direction by conditioning on both text and a short acoustic prompt to synthesize high-fidelity speech.\nIts extensions&#8212;VALL-E X&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib44\" title=\"\">44</a>]</cite>, VALL-E 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib45\" title=\"\">45</a>]</cite>, and VALL-E R&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib46\" title=\"\">46</a>]</cite>&#8212;extend the paradigm to cross-lingual synthesis, streaming generation, and improved alignment.\nSoCodec&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib20\" title=\"\">20</a>]</cite> further improves efficiency through semantic-ordered multi-stream tokenization and segment-level modeling.\nThrough prompt-based conditioning, these models demonstrate strong zero-shot capability and robust speaker generalization.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "zeroshot",
                    "codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable zero-shot singing voice synthesis with accurate melody control and disentangled prompt conditioning, we propose <em class=\"ltx_emph ltx_font_italic\">CoMelSinger</em>, a two-stage framework illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S3.F3\" title=\"Figure 3 &#8227; III Method &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Inspired by MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite> and Make-A-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite>, CoMelSinger comprises a Text-to-Semantic (T2S) stage and a Semantic-to-Acoustic (S2A) stage. The T2S module <math alttext=\"f_{\\mathrm{T2S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>T2S</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{T2S}}</annotation></semantics></math> transforms a lyric token sequence <math alttext=\"\\tilde{\\mathbf{m}}^{l}=[\\tilde{m}^{l}_{1},\\dots,\\tilde{m}^{l}_{S}]\\in\\mathcal{V}_{\\mathrm{lyr}}^{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>l</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mover accent=\"true\"><mi>m</mi><mo>~</mo></mover><mn>1</mn><mi>l</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>m</mi><mo>~</mo></mover><mi>S</mi><mi>l</mi></msubsup><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>lyr</mi><mi>S</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{l}=[\\tilde{m}^{l}_{1},\\dots,\\tilde{m}^{l}_{S}]\\in\\mathcal{V}_{\\mathrm{lyr}}^{S}</annotation></semantics></math>, obtained from a Grapheme-to-Phoneme (G2P) converter, and a semantic prompt <math alttext=\"\\mathbf{s}^{r}=E_{S}(\\mathbf{w}^{r})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119852;</mi><mi>r</mi></msup><mo>=</mo><mrow><msub><mi>E</mi><mi>S</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119856;</mi><mi>r</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}^{r}=E_{S}(\\mathbf{w}^{r})</annotation></semantics></math> extracted from the reference waveform <math alttext=\"\\mathbf{w}^{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119856;</mi><mi>r</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{w}^{r}</annotation></semantics></math>, into a semantic token sequence <math alttext=\"\\mathbf{s}\\in\\mathcal{V}_{\\mathrm{sem}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#119852;</mi><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>sem</mi><mi>L</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}\\in\\mathcal{V}_{\\mathrm{sem}}^{L}</annotation></semantics></math>, where <math alttext=\"\\mathcal{V}_{\\mathrm{sem}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>sem</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{V}_{\\mathrm{sem}}</annotation></semantics></math> denotes the semantic vocabulary and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> the sequence length. The S2A module <math alttext=\"f_{\\mathrm{S2A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>S2A</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{S2A}}</annotation></semantics></math> then predicts acoustic tokens <math alttext=\"\\mathbf{a}\\in\\mathcal{V}_{\\mathrm{aco}}^{L\\times N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mi>&#119834;</mi><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>aco</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{a}\\in\\mathcal{V}_{\\mathrm{aco}}^{L\\times N}</annotation></semantics></math>, conditioned on the semantic tokens <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math>, an acoustic prompt <math alttext=\"\\mathbf{a}^{r}=E_{A}(\\mathbf{w}^{r})\\in\\mathcal{V}_{\\mathrm{aco}}^{L_{r}\\times N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m11\" intent=\":literal\"><semantics><mrow><msup><mi>&#119834;</mi><mi>r</mi></msup><mo>=</mo><mrow><msub><mi>E</mi><mi>A</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119856;</mi><mi>r</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>aco</mi><mrow><msub><mi>L</mi><mi>r</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r}=E_{A}(\\mathbf{w}^{r})\\in\\mathcal{V}_{\\mathrm{aco}}^{L_{r}\\times N}</annotation></semantics></math>, and a regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\\in\\mathcal{V}_{\\mathrm{pit}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m12\" intent=\":literal\"><semantics><mrow><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>pit</mi><mi>L</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}\\in\\mathcal{V}_{\\mathrm{pit}}^{L}</annotation></semantics></math>. Here, <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m13\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> denotes the number of residual vector quantization (RVQ) codebooks in the acoustic codec, and the regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m14\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math> is derived from the pitch <math alttext=\"\\mathbf{m}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m15\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{p}</annotation></semantics></math> and duration <math alttext=\"\\mathbf{m}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m16\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>d</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{d}</annotation></semantics></math> sequence through the length expansion module <math alttext=\"LE(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m17\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">LE(\\cdot)</annotation></semantics></math>. Both the semantic and acoustic tokens are produced using discrete codec tokenizers following the MaskGCT setup, and the final waveform <math alttext=\"\\mathbf{w}^{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m18\" intent=\":literal\"><semantics><msup><mi>&#119856;</mi><mi>o</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{w}^{o}</annotation></semantics></math> is reconstructed from acoustic tokens via the decoder <math alttext=\"D_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m19\" intent=\":literal\"><semantics><msub><mi>D</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">D_{A}</annotation></semantics></math>. The complete pipeline is summarized as:</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "zeroshot",
                    "codec",
                    "comelsinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, CoMelSinger adopts a non-autoregressive masked generative modeling paradigm for both stages. In this framework, the model learns to reconstruct masked tokens within a sequence conditioned on surrounding context and external inputs, rather than generating tokens sequentially. This allows for parallel decoding and better handling of global context compared to traditional autoregressive models. Specifically, we model the conditional probabilities:\n<math alttext=\"p(\\mathbf{s}\\mid\\mathbf{s}_{t};\\mathbf{m}^{l},\\mathbf{s}^{r};f_{\\theta,\\mathrm{T2S}}),\\quad p(\\mathbf{a}\\mid\\mathbf{a}_{t};\\mathbf{s},\\tilde{\\mathbf{m}}^{p},\\mathbf{a}^{r};f_{\\theta,\\mathrm{S2A}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119852;</mi><mo>&#8739;</mo><mrow><msub><mi>&#119852;</mi><mi>t</mi></msub><mo>;</mo><msup><mi>&#119846;</mi><mi>l</mi></msup><mo>,</mo><msup><mi>&#119852;</mi><mi>r</mi></msup><mo>;</mo><msub><mi>f</mi><mrow><mi>&#952;</mi><mo>,</mo><mi>T2S</mi></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"1.167em\">,</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119834;</mi><mo>&#8739;</mo><mrow><msub><mi>&#119834;</mi><mi>t</mi></msub><mo>;</mo><mi>&#119852;</mi><mo>,</mo><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><mo>,</mo><msup><mi>&#119834;</mi><mi>r</mi></msup><mo>;</mo><msub><mi>f</mi><mrow><mi>&#952;</mi><mo>,</mo><mi>S2A</mi></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">p(\\mathbf{s}\\mid\\mathbf{s}_{t};\\mathbf{m}^{l},\\mathbf{s}^{r};f_{\\theta,\\mathrm{T2S}}),\\quad p(\\mathbf{a}\\mid\\mathbf{a}_{t};\\mathbf{s},\\tilde{\\mathbf{m}}^{p},\\mathbf{a}^{r};f_{\\theta,\\mathrm{S2A}})</annotation></semantics></math>.\nHere, <math alttext=\"\\mathbf{s}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{t}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119834;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{a}_{t}</annotation></semantics></math> are the partially masked semantic and acoustic token sequences, and the generation is conditioned on lyric inputs <math alttext=\"\\mathbf{m}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>l</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{l}</annotation></semantics></math>, pitch-aligned sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, and prompt tokens <math alttext=\"(\\mathbf{s}^{r},\\mathbf{a}^{r})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119852;</mi><mi>r</mi></msup><mo>,</mo><msup><mi>&#119834;</mi><mi>r</mi></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{s}^{r},\\mathbf{a}^{r})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model",
                    "comelsinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the original MaskGCT architecture, which employs a LLaMA-style Transformer backbone <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib54\" title=\"\">54</a>]</cite>, we extend the S2A model by introducing an additional embedding layer for the regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>. This pitch embedding <math alttext=\"\\mathbf{e}_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#119838;</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{e}_{p}</annotation></semantics></math> is element-wise added to the semantic token embedding <math alttext=\"\\mathbf{e}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>&#119838;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{e}_{s}</annotation></semantics></math> to form the composite conditioning input <math alttext=\"\\mathbf{e}_{c}=\\mathbf{e}_{p}+\\mathbf{e}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#119838;</mi><mi>c</mi></msub><mo>=</mo><mrow><msub><mi>&#119838;</mi><mi>p</mi></msub><mo>+</mo><msub><mi>&#119838;</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{e}_{c}=\\mathbf{e}_{p}+\\mathbf{e}_{s}</annotation></semantics></math>, thereby enabling the model to incorporate both linguistic and melodic information during acoustic token generation.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance melody controllability and suppress interference from prompt-induced timbre cues, we propose a coarse-to-fine contrastive learning framework. At the sequence level, a contrastive loss encourages the predicted acoustic token sequence <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math> to preserve the overall pitch contour defined by <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>. At the frame level, a token-wise contrastive objective aligns fine-grained acoustic features with localized pitch variations, thereby reinforcing frame-level pitch fidelity. In addition, we introduce an auxiliary singing voice transcription (SVT) model, trained to estimate pitch sequences directly from acoustic tokens. The SVT model provides pseudo pitch labels that serve as external supervision during S2A training. This auxiliary signal further improves alignment between the synthesized melody and the target pitch contour.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Contrastive learning has been increasingly adopted in speech and audio modeling to enforce factor-specific consistency while suppressing undesired variations such as speaker identity or prompt interference. For instance, CLAPSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib55\" title=\"\">55</a>]</cite> applies multi-scale contrastive learning between textual prosody embeddings and corresponding acoustic realizations, improving prosodic expressivity across varying textual contexts. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib56\" title=\"\">56</a>]</cite> propose a contrastive loss to enhance the modeling of prosodic focus&#8212;including F0, duration, and intensity&#8212;by encouraging TTS systems to distinguish emphasized from neutral phonetic segments. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib57\" title=\"\">57</a>]</cite> use contrastive self-supervision to extract prosody-specific embeddings disentangled from speaker identity, which are useful for style transfer and anonymized generation. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib58\" title=\"\">58</a>]</cite> further explore contrastive pretraining to align textual context with expressive speech realizations, facilitating zero-shot expressive TTS with better generalization.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, our method extends contrastive learning to the discrete-token SVS setting by incorporating hierarchical supervision. At the sequence level, we enforce prompt-invariant acoustic consistency conditioned on identical semantic and pitch tokens, encouraging the model to preserve global melody shape. At the local level, a frame-wise contrastive loss is applied to align acoustic token features with fine-grained pitch variations. This coarse-to-fine scheme allows our model to disentangle pitch conditioning from acoustic prompts and enhances melody fidelity under zero-shot generation.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "model",
                    "acoustic",
                    "zeroshot",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To promote melody-consistent synthesis under varying acoustic prompts, we introduce a sequence-level symmetric contrastive loss.\nGiven a batch of <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> training samples that share the same semantic token sequence <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math> but differ in pitch sequences <math alttext=\"\\tilde{\\mathbf{m}}^{p}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}_{k}</annotation></semantics></math>, we construct two sets of inputs:\n<math alttext=\"\\mathcal{S}^{g}_{A}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,A}_{k},\\mathbf{a}^{A}_{k,t}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>A</mi><mi>g</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mi>&#119852;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>A</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}^{g}_{A}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,A}_{k},\\mathbf{a}^{A}_{k,t}\\}_{k=1}^{K}</annotation></semantics></math> and\n<math alttext=\"\\mathcal{S}^{g}_{B}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,B}_{k},\\mathbf{a}^{B}_{k,t}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>B</mi><mi>g</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mi>&#119852;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>B</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}^{g}_{B}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,B}_{k},\\mathbf{a}^{B}_{k,t}\\}_{k=1}^{K}</annotation></semantics></math>,\nwhere <math alttext=\"\\mathbf{a}^{A}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{A}_{k,t}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{B}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>B</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{B}_{k,t}</annotation></semantics></math> denote masked acoustic tokens, <math alttext=\"\\mathbf{a}^{r,A}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,A}_{k}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{r,B}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,B}_{k}</annotation></semantics></math> denote acoustic prompts sampled from distinct utterances of the same singer, ensuring consistent timbre across the pair.\nThe prompt length is randomly selected from the range <math alttext=\"\\left[\\min\\left(\\left\\lfloor L/4\\right\\rfloor,5\\right),\\left\\lfloor L/2\\right\\rfloor\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><mrow><mo>[</mo><mrow><mi>min</mi><mo>&#8289;</mo><mrow><mo>(</mo><mrow><mo>&#8970;</mo><mrow><mi>L</mi><mo>/</mo><mn>4</mn></mrow><mo>&#8971;</mo></mrow><mo>,</mo><mn>5</mn><mo>)</mo></mrow></mrow><mo>,</mo><mrow><mo>&#8970;</mo><mrow><mi>L</mi><mo>/</mo><mn>2</mn></mrow><mo>&#8971;</mo></mrow><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left[\\min\\left(\\left\\lfloor L/4\\right\\rfloor,5\\right),\\left\\lfloor L/2\\right\\rfloor\\right)</annotation></semantics></math>, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the length of the semantic sequence. Each input is processed by the S2A model to produce acoustic token embeddings <math alttext=\"\\mathbf{g}^{a},\\mathbf{g}^{b}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>&#119840;</mi><mi>a</mi></msup><mo>,</mo><msup><mi>&#119840;</mi><mi>b</mi></msup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{g}^{a},\\mathbf{g}^{b}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math>, which are mean-pooled across the time dimension to yield global acoustic representations <math alttext=\"\\tilde{\\mathbf{g}}^{a},\\tilde{\\mathbf{g}}^{b}\\in\\mathbb{R}^{K\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mrow><mrow><msup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>a</mi></msup><mo>,</mo><msup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>b</mi></msup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{g}}^{a},\\tilde{\\mathbf{g}}^{b}\\in\\mathbb{R}^{K\\times D}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is a temperature hyperparameter. Each positive pair <math alttext=\"(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>i</mi><mi>a</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>i</mi><mi>b</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{i})</annotation></semantics></math> corresponds to index-aligned embeddings generated under identical semantic tokens <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m3\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math> and regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m4\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, but conditioned on different acoustic prompts <math alttext=\"\\mathbf{a}^{r,A}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>i</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,A}_{i}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{r,B}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>i</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,B}_{i}</annotation></semantics></math>. These prompts are randomly selected from non-overlapping segments of the same singer&#8217;s recordings, ensuring consistent timbre while introducing natural variation. In contrast, off-diagonal pairs <math alttext=\"(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>i</mi><mi>a</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>j</mi><mi>b</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{j})</annotation></semantics></math> for <math alttext=\"i\\neq j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m8\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8800;</mo><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">i\\neq j</annotation></semantics></math> serve as negatives due to mismatched pitch sequences, even though semantic tokens and speaker identity remain the same. These negatives are nontrivial, as they reflect realistic melodic differences under otherwise comparable contextual and timbral conditions. This design encourages the model to focus on capturing global pitch structure while remaining invariant to prompt-induced variability.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the sequence contrastive loss promotes utterance-level melody consistency, it does not directly enforce fine-grained pitch alignment at the frame level&#8212;an essential factor in singing synthesis due to rapid and expressive melodic changes. To address this limitation, we introduce a frame-level contrastive objective that supervises token-wise alignment between generated acoustic representations and the input pitch contour. Our design is motivated by recent work such as CTAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib60\" title=\"\">60</a>]</cite>, which leverage contrastive learning to align discrete phoneme sequences with speech features for TTS, voice conversion, and ASR tasks under limited supervision. Although our formulation differs in both granularity and modality&#8212;operating on pitch tokens rather than phonemes, and targeting melody alignment in singing synthesis&#8212;these studies underscore the effectiveness of contrastive supervision for bridging symbolic and acoustic representations. By extending this idea to the SVS domain, our frame-level contrastive loss enhances local pitch fidelity while remaining robust to prompt-induced variation, thereby enabling more precise and expressive melody control in zero-shot scenarios.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "zeroshot",
                    "svs",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each input is passed through the S2A model to produce frame-level acoustic embeddings <math alttext=\"\\mathbf{f}^{a},\\mathbf{f}^{b}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>&#119839;</mi><mi>a</mi></msup><mo>,</mo><msup><mi>&#119839;</mi><mi>b</mi></msup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{f}^{a},\\mathbf{f}^{b}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math>. For each training sample <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>, we compute a cosine similarity matrix <math alttext=\"\\mathbf{S}^{k}\\in\\mathbb{R}^{L\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mi>k</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{k}\\in\\mathbb{R}^{L\\times L}</annotation></semantics></math> between <math alttext=\"\\mathbf{f}^{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m4\" intent=\":literal\"><semantics><msup><mi>&#119839;</mi><mi>a</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{f}^{a}</annotation></semantics></math> and <math alttext=\"\\mathbf{f}^{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m5\" intent=\":literal\"><semantics><msup><mi>&#119839;</mi><mi>b</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{f}^{b}</annotation></semantics></math>. To supervise the similarity learning, we define a soft label matrix <math alttext=\"\\mathbf{Y}^{k}\\in[-1,1]^{L\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m6\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>k</mi></msup><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{k}\\in[-1,1]^{L\\times L}</annotation></semantics></math> capturing pitch and semantic alignment:</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This formulation encourages the model to produce highly similar acoustic embeddings when both pitch and semantic content align, moderately similar embeddings when only pitch aligns, and dissimilar embeddings otherwise. The similarity is computed within each utterance because the vocal range is typically locally bounded, making repeated pitch tokens more likely. In contrast, pitch overlap across utterances is rare and thus excluded. Additionally, even within the same utterance, repeated pitches may be associated with different semantic tokens, resulting in subtle acoustic variation. Our soft labeling mechanism accounts for this by assigning intermediate similarity, thereby avoiding over-penalization while promoting melody-consistent synthesis. Hence, the final contrastive learning objective is as follows:</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model",
                    "excluded",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent studies have explored the use of Singing Voice Transcription (SVT) to support singing voice synthesis (SVS). For example, ROSVOT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib61\" title=\"\">61</a>]</cite> proposes a robust SVT model to produce high-quality pitch annotations for large-scale singing datasets, thereby improving SVS performance by enhancing training data quality. However, such approaches treat SVT as an independent preprocessing tool, disconnected from the synthesis process.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, we integrate the SVT module directly into the training pipeline to provide explicit frame-level pitch supervision, thereby enhancing melody modeling and alignment. Specifically, the SVT model predicts frame-wise discrete pitch tokens from acoustic codec representations, which are then compared against the ground-truth pitch sequence. This supervision enforces alignment between the generated acoustic tokens and the intended melody, encouraging consistent pitch realization, particularly under zero-shot conditions. Furthermore, as the SVT module operates entirely on discrete representations, it is naturally compatible with our codec-based SVS framework and does not require raw audio or continuous F0 contours.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "codec",
                    "model",
                    "acoustic",
                    "groundtruth",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SVT model adopts a lightweight encoder-only Transformer architecture. Given a sequence of acoustic tokens <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, it predicts the corresponding pitch token sequence <math alttext=\"\\hat{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>^</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{m}}^{p}</annotation></semantics></math> of length <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>. The encoder consists of four Transformer layers with a hidden size of 512 and eight attention heads. Each input frame comprises 12 discrete acoustic codes, which are individually embedded, concatenated, and projected to a 512-dimensional representation, followed by layer normalization. The resulting embedding sequence is then passed through a linear classification head to predict a pitch token for each frame. The model is trained using a standard cross-entropy loss between the predicted and reference pitch sequences.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide frame-level supervision for pitch modeling, we leverage the pretrained SVT model as a pitch predictor to generate pseudo labels in the form of pitch token sequences <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m1\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, which are temporally aligned with the acoustic frames. As the primary training objective, we apply a cross-entropy loss <math alttext=\"\\mathcal{L}_{\\rm CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>CE</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm CE}</annotation></semantics></math> between the predicted pitch tokens <math alttext=\"\\hat{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m3\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>^</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{m}}^{p}</annotation></semantics></math> and the SVT-derived ground-truth <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m4\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, encouraging accurate token-level classification. However, the cross-entropy objective alone does not account for the temporal continuity inherent in repeated pitch tokens. This often results in jittery predictions, fragmented note segments, and rhythmically unstable outputs.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "results",
                    "acoustic",
                    "note",
                    "groundtruth",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This formulation encourages the model to allocate appropriate probability mass to each pitch token across time, thereby promoting temporally coherent and rhythmically faithful melody generation. The final training objective for the SVT module combines the cross-entropy loss, segment transition loss, and soft duration loss:</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin by training the Singing Voice Transcription (SVT) model to provide frame-level pitch supervision for subsequent Semantic-to-Acoustic (S2A) adaptation. The SVT model is optimized using a cross-entropy loss <math alttext=\"\\mathcal{L}_{\\text{CE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CE</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CE}}</annotation></semantics></math> between the regulated pitch token sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math> and the acoustic token sequence <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, thereby learning to predict temporally aligned pitch trajectories from acoustic inputs. Once trained, the SVT module is frozen to serve as a fixed auxiliary supervisor during S2A training.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then fine-tune the S2A model built upon the MaskGCT framework&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, which leverages masked acoustic modeling for non-autoregressive generation. The training objective for the S2A model comprises three components: (1) the mask token prediction loss <math alttext=\"\\mathcal{L}_{\\text{mask}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>mask</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{mask}}</annotation></semantics></math>, which reconstructs randomly masked acoustic tokens from noisy inputs using a masked denoising objective; (2) a coarse-to-fine contrastive loss <math alttext=\"\\mathcal{L}_{\\text{CL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CL}}</annotation></semantics></math>, composed of sequence-level (<math alttext=\"\\mathcal{L}_{\\text{SCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>SCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{SCL}}</annotation></semantics></math>) and frame-level (<math alttext=\"\\mathcal{L}_{\\text{FCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>FCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{FCL}}</annotation></semantics></math>) terms, to enforce consistency between the melody condition and generated acoustic tokens while mitigating prosody leakage from the prompt; and (3) an auxiliary SVT loss <math alttext=\"\\mathcal{L}_{\\text{SVT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>SVT</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{SVT}}</annotation></semantics></math>, which encourages the predicted acoustic tokens to be rhythmically and melodically consistent with the SVT-inferred pitch contour. The fine-tuning algorithm for the S2A model is summarized in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#alg1\" title=\"Algorithm 1 &#8227; III-D Training and Inference Procedures &#8227; III Method &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Each training sample <math alttext=\"\\mathbf{x}_{k}\\in\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#119857;</mi><mi>k</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{k}\\in\\mathcal{B}</annotation></semantics></math> comprises a semantic sequence <math alttext=\"\\mathbf{s}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m7\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{k}</annotation></semantics></math>, a regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}_{k}</annotation></semantics></math>, and a time-aligned acoustic token sequence <math alttext=\"\\mathbf{a}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m9\" intent=\":literal\"><semantics><msub><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{a}_{k,t}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During inference, we follow the parallel iterative decoding introduced in MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite> to generate acoustic token sequences. Unlike the original setup, which requires either an explicit duration input or a learned duration predictor to determine the output length, we directly use the ground-truth duration sequence <math alttext=\"\\mathbf{m}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>d</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{d}</annotation></semantics></math> extracted from the input music score. This allows precise control over the number of generated tokens&#8212;equal to the total duration <math alttext=\"D=\\sum\\limits_{i}m_{i}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mrow><mi>D</mi><mo rspace=\"0.111em\">=</mo><mrow><munder><mo movablelimits=\"false\">&#8721;</mo><mi>i</mi></munder><msubsup><mi>m</mi><mi>i</mi><mi>d</mi></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">D=\\sum\\limits_{i}m_{i}^{d}</annotation></semantics></math>&#8212;thus preserving the intended temporal structure of the synthesized performance.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "groundtruth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments on two publicly available mandarin singing corpora: M4Singer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib23\" title=\"\">23</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/M4Singer/M4Singer\" title=\"\">https://github.com/M4Singer/M4Singer</a></span></span></span> and Opencpop <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib22\" title=\"\">22</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xinshengwang.github.io/opencpop/\" title=\"\">https://xinshengwang.github.io/opencpop/</a></span></span></span>. The M4Singer dataset comprises studio-quality recordings from 20 professional singers spanning SATB vocal ranges, along with comprehensive annotations including lyrics, pitch, note duration, and slur information. The Opencpop dataset contains 100 Chinese pop songs sung by a professional female vocalist, with precise phoneme, note, and syllable-level annotations aligned to the music score. For evaluation, we construct both seen- and unseen-singer test sets. For seen-singer evaluation, we randomly select 50 utterances each from the M4Singer and Opencpop datasets. For zero-shot (unseen-singer) evaluation, we use 10 male and 10 female singers from the OpenSinger <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib65\" title=\"\">65</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Multi-Singer/Multi-Singer.github.io?tab=readme-ov-file\" title=\"\">https://github.com/Multi-Singer/Multi-Singer.github.io?tab=readme-ov-file</a></span></span></span> dataset. Since OpenSinger lacks complete music score annotations, we pair it with M4Singer&#8217;s score sequences to enable evaluation. All audio is uniformly down-sampled to 24 kHz with 16-bit quantization.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "note",
                    "zeroshot",
                    "since",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fine-tune the S2A model, we preprocess the dataset to obtain temporally aligned semantic tokens <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math>, acoustic tokens <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, and regulated pitch tokens <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>. All audio segments are first converted to mono and resampled to 24&#160;kHz. We then utilize the pretrained semantic and acoustic codec models from the MaskGCT framework<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/open-mmlab/Amphion/tree/main/models/tts/maskgct\" title=\"\">https://github.com/open-mmlab/Amphion/tree/main/models/tts/maskgct</a></span></span></span> to extract <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>. For samples with mismatched token lengths, we apply zero-padding to align their temporal dimensions for frame-level supervision. The regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math> is derived by expanding the original pitch sequence <math alttext=\"\\mathbf{m}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{p}</annotation></semantics></math> according to the duration sequence <math alttext=\"\\mathbf{m}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>d</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{d}</annotation></semantics></math>, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S3.SS1\" title=\"III-A Overview &#8227; III Method &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">III-A</span></a>. The SVT model is trained using the preprocessed acoustic tokens <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math> and the corresponding regulated pitch tokens <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m10\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "model",
                    "codec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SVT model is trained on a single NVIDIA RTX A5000 GPU using the AdamW optimizer with a learning rate of 1e&#8722;5, weight decay of 0.01, and a cosine learning rate schedule with 5K warm-up steps over 50K updates. Training is performed for 100 epochs with a batch size of 32 using mixed-precision (FP16) computation. Subsequently, the S2A model is fine-tuned on four NVIDIA RTX A5000 GPUs with data parallelism. We adopt the AdamW optimizer with a learning rate of 1e&#8722;5, 32K warm-up steps, and the inverse square root learning rate schedule. Fine-tuning is conducted for 300K steps with a total batch size of 32, where the first 8 samples are used for sequence contrastive learning and the remaining 24 for frame-level contrastive learning. We apply dropout (0.1), label smoothing (0.1), and gradient clipping to stabilize training. During this stage, all model components are frozen except the S2A decoder. The loss weights are set as follows: <math alttext=\"\\lambda_{\\text{SCL}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>SCL</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{SCL}}=0.5</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{FCL}}=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>FCL</mtext></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{FCL}}=1.0</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{CL}}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>CL</mtext></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{CL}}=0.1</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{seg}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>seg</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{seg}}=0.5</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{dur}}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>dur</mtext></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{dur}}=0.3</annotation></semantics></math>, and <math alttext=\"\\lambda_{\\text{SVT}}=0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>SVT</mtext></msub><mo>=</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{SVT}}=0.2</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the performance of our system in terms of pitch accuracy, timbre consistency, and perceptual quality, we conduct objective evaluations under both seen-singer and zero-shot settings. The following metrics are employed:</p>\n\n",
                "matched_terms": [
                    "evaluations",
                    "zeroshot",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess timbre similarity, we compute cosine similarity between speaker embeddings extracted from the synthesized and reference audio using a WavLM-based speaker verification model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib66\" title=\"\">66</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/microsoft/wavlm-base-sv\" title=\"\">https://huggingface.co/microsoft/wavlm-base-sv</a></span></span></span>. SECS values range from 0 to 1, with higher scores indicating closer alignment in vocal identity.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib67\" title=\"\">67</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/South-Twilight/SingMOS\" title=\"\">https://github.com/South-Twilight/SingMOS</a></span></span></span> is a learned metric trained to predict human perceptual ratings of singing voice quality. It is based on a curated dataset of professional ratings for both natural and synthesized singing in Chinese and Japanese, addressing the annotation scarcity in the singing domain. SingMOS produces scores in the range of 0 to 5, with higher values indicating greater naturalness and perceptual quality. As a reference-free metric, it enables scalable automatic evaluation in zero-shot and low-resource conditions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "zeroshot",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess perceptual quality, we conducted a Mean Opinion Score (MOS) evaluation with 20 participants who have formal training in singing and experience in vocal performance <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>This study has been approved by the Department Ethics Review Committee\n(DERC) at the National University of Singapore under\nDERC Ref Code: 000479.</span></span></span>. Each participant rated the synthesized samples based on overall naturalness (MOS-N), audio quality (MOS-Q), and timbre similarity (SMOS). A 5-point Likert scale was used, where a score of 5 indicates excellent perceptual quality and 1 denotes poor quality.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "smos",
                    "mosq",
                    "mosn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate whether MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite> exhibits such behavior, we conduct a prosody similarity analysis following prior evaluation protocols. NaturalSpeech 2 and 3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>]</cite> quantify prosodic similarity by comparing pitch and duration features between the prompt and output, while StyleTTS-ZS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib69\" title=\"\">69</a>]</cite> computes Pearson correlation coefficients of acoustic features to evaluate prosodic alignment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "acoustic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by these approaches, we evaluate the prosodic similarity of MaskGCT in both English and Mandarin using the LibriTTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib25\" title=\"\">25</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.openslr.org/60/\" title=\"\">https://www.openslr.org/60/</a></span></span></span> and AISHELL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib70\" title=\"\">70</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openslr.org/93/\" title=\"\">https://openslr.org/93/</a></span></span></span> datasets, respectively. For each speaker, we randomly sample 50 utterances to construct the test sets. During inference, we synthesize 50 utterances per dataset by conditioning on the same target text but using different speech prompts. We then compute the acoustic-level similarity between each synthesized utterance and: (1) its paired prompt (i.e., the one used during generation), and (2) an unpaired prompt from the same speaker. This comparison allows us to quantify the extent of prompt-induced prosody similarity, which serves as an indicator of potential prosody leakage in the model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T1\" title=\"TABLE I &#8227; V-A Evaluating Prompt-Induced Prosody Similarity in MaskGCT &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> presents a quantitative analysis of prompt-induced prosody similarity by comparing the acoustic differences between synthesized and prompt speech under paired and unpaired conditions. Across both LibriTTS and AISHELL-3, paired prompts consistently yield lower differences in pitch, energy, and other prosodic indicators, confirming stronger alignment in prosodic patterns. In contrast, the unpaired condition results in noticeably higher deviations, particularly in pitch mean, energy mean, and jitter, suggesting that the synthesized outputs are heavily influenced by the prosodic characteristics of the prompt.</p>\n\n",
                "matched_terms": [
                    "results",
                    "acoustic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the seen singer evaluation, we compare CoMelSinger with four strong baseline systems: DiffSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib4\" title=\"\">4</a>]</cite>, VISinger2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib9\" title=\"\">9</a>]</cite>, SPSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib13\" title=\"\">13</a>]</cite>, and StyleSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib7\" title=\"\">7</a>]</cite>. To ensure a fair comparison, all models adopt HiFi-GAN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib71\" title=\"\">71</a>]</cite> as the vocoder during both training and inference. As presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T2\" title=\"TABLE II &#8227; V-A Evaluating Prompt-Induced Prosody Similarity in MaskGCT &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>, CoMelSinger achieves the highest scores across both subjective and objective metrics, demonstrating its capability to synthesize natural and expressive singing voices from seen singers.</p>\n\n",
                "matched_terms": [
                    "diffsinger",
                    "comelsinger",
                    "evaluation",
                    "visinger2",
                    "subjective",
                    "objective",
                    "stylesinger",
                    "spsinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first note that the performance gap between the Ground Truth (GT) and GT with Acoustic Codec is minimal across all metrics, confirming that the discrete acoustic token representation introduces negligible degradation and establishing a strong upper bound for token-based SVS systems. CoMelSinger approaches this bound closely, suggesting that its improvements arise from architectural designs&#8212;particularly the disentangled modeling of melody and timbre&#8212;rather than signal-level enhancements. Among all models, CoMelSinger achieves the highest SMOS and SECS scores, indicating strong timbre consistency and accurate preservation of speaker identity, which validates the effectiveness of the in-context prompting mechanism. It also attains the lowest F0-RMSE and one of the highest SingMOS scores, reflecting precise melody reproduction and high perceptual naturalness. Furthermore, its competitive MCD score demonstrates the model&#8217;s ability to reconstruct spectral features with smooth and consistent vocal quality, confirming the effectiveness of the proposed structured melody control strategy.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "smos",
                    "codec",
                    "comelsinger",
                    "f0rmse",
                    "mcd",
                    "acoustic",
                    "secs",
                    "note",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, baseline systems show notable declines in key timbre- and melody-related metrics such as SMOS, SECS, and F0-RMSE, underscoring their limited ability to generalize to unseen vocal identities. CoMelSinger&#8217;s robustness in zero-shot scenarios is attributed to the synergy between in-context prompting&#8212;which leverages short acoustic references to anchor timbre&#8212;and large-scale speech pretraining, which imparts transferable prosodic priors.</p>\n\n",
                "matched_terms": [
                    "smos",
                    "f0rmse",
                    "acoustic",
                    "secs",
                    "zeroshot",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the inherent challenge of handling unseen timbres, CoMelSinger continues to achieve high speaker similarity while preserving accurate pitch trajectories. This balance between identity retention and melodic fidelity demonstrates the model&#8217;s strong generalization capacity. While many existing approaches face trade-offs between controllability and naturalness, CoMelSinger effectively reconciles both through its structured architecture and explicit conditioning scheme. These findings position CoMelSinger as a strong baseline for zero-shot singing voice synthesis with discrete representations.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "comelsinger",
                    "unseen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the contribution of each component in CoMelSinger, we perform ablation studies by systematically disabling key modules. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T4\" title=\"TABLE IV &#8227; V-D Ablation Study &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> reports results on four objective metrics: MCD, F0-RMSE, SingMOS, and SECS. Removing the entire coarse-to-fine contrastive learning (CL) framework leads to substantial degradation across all metrics, indicating reduced pitch accuracy and speaker consistency. This highlights the importance of contrastive objectives in disentangling pitch from timbre and improving input&#8211;output alignment.</p>\n\n",
                "matched_terms": [
                    "comelsinger",
                    "f0rmse",
                    "mcd",
                    "objective",
                    "results",
                    "secs",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To isolate the effects of each contrastive branch, we further ablate sequence contrastive learning (SCL) and frame-level contrastive learning (FCL) individually. Excluding SCL moderately affects MCD and SECS, suggesting its role in maintaining global speaker identity. In contrast, removing FCL causes a larger drop in F0-RMSE and SingMOS, confirming its effectiveness in modeling fine-grained pitch details and promoting melodic continuity. These results validate the hierarchical design of our contrastive learning framework.</p>\n\n",
                "matched_terms": [
                    "f0rmse",
                    "mcd",
                    "results",
                    "secs",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate the impact of the singing voice transcription (SVT) module, which provides auxiliary pitch supervision. Excluding SVT results in higher F0-RMSE and lower SingMOS, confirming the benefit of explicit alignment signals for structured melody control. The most severe degradation occurs when both CL and SVT are removed, indicating their complementary roles in pitch&#8211;timbre disentanglement and temporal stability.</p>\n\n",
                "matched_terms": [
                    "results",
                    "f0rmse",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.F5\" title=\"Figure 5 &#8227; Component Analysis &#8227; V-D Ablation Study &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> visualizes mel-spectrograms and pitch contours for the ground truth, our model, and two ablated variants. Predicted pitch trajectories (red) are overlaid with ground-truth pitch (blue), with word-level boundaries marked by dashed lines and pinyin annotations. Compared to the ablated models, CoMelSinger achieves better pitch alignment and smoother contours, illustrating the effectiveness of both CL and SVT in preserving melodic structure.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comelsinger",
                    "groundtruth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T5\" title=\"TABLE V &#8227; Comparison of Fine-Tuning Strategies &#8227; V-D Ablation Study &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> presents a comparison of six fine-tuning strategies in terms of both objective and subjective performance, along with their respective trainable parameter ratios. FT-LoRA delivers the best overall performance, achieving the lowest F0-RMSE, highest SingMOS, and highest SECS, while updating only 4.81% of the parameters&#8212;highlighting the effectiveness of low-rank adaptation for efficient model tuning. FT-PGS achieves the lowest MCD, suggesting enhanced spectral fidelity through gradual unfreezing, though its pitch accuracy is affected by delayed optimization of lower layers. FT-Prefix and FT-Pitch yield consistent results with minimal overhead, demonstrating the utility of lightweight adaptation modules. In contrast, FT-LLRD and FT-Full fine-tune all parameters yet underperform across most metrics, indicating that full-capacity adaptation may lead to overfitting or instability in data-scarce settings. These results underscore that parameter-efficient strategies, particularly LoRA, can match or surpass full-model fine-tuning while substantially reducing computational cost.</p>\n\n",
                "matched_terms": [
                    "f0rmse",
                    "mcd",
                    "subjective",
                    "objective",
                    "results",
                    "secs",
                    "model",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present CoMelSinger, a zero-shot singing voice synthesis framework that extends discrete token-based TTS models to support structured and controllable melody generation. Built upon the non-autoregressive MaskGCT architecture, CoMelSinger incorporates lyrics and pitch tokens as inputs, enabling fine-grained alignment between the musical score and the generated voice. To address the challenge of prosody leakage from prompt-based conditioning&#8212;an issue unique to singing synthesis&#8212;we propose a coarse-to-fine contrastive learning strategy that explicitly disentangles pitch information from the timbre prompt. Furthermore, we introduce a lightweight singing voice transcription (SVT) module to provide frame-level pitch and duration supervision, enhancing the model&#8217;s ability to follow the intended melody with precision. Extensive experiments on both seen and unseen singers demonstrate that CoMelSinger achieves strong zero-shot generalization, consistently outperforming competitive SVS baselines in pitch accuracy, timbre consistency, and subjective quality. Our results confirm that structured melody control and contrastive disentanglement are essential for scalable and expressive singing synthesis. We believe CoMelSinger opens new possibilities for discrete token-based SVS, enabling scalable and zero-shot singing generation.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "comelsinger",
                    "subjective",
                    "results",
                    "zeroshot",
                    "unseen"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance",
        "caption": "TABLE IV: Objective evaluation results for the ablation study. CL represents the coarse-to-fine contrastive learning strategy, where SCL and FCL respectively represents sequence and frame-level contrastive learning, SVT represents using SVT for pitch guidance.",
        "body": "Model\nMCD ↓\\downarrow\n\nF0-RMSE ↓\\downarrow\n\nSingMOS ↑\\uparrow\n\nSECS ↑\\uparrow\n\n\n\nCoMelSinger\n4.17\n0.042\n4.32\n0.912\n\n\n\n\n-w/o CL\n4.91\n0.080\n4.12\n0.895\n\n\n   -w/o SCL\n4.53\n0.062\n4.25\n0.900\n\n\n   -w/o FCL\n4.82\n0.075\n4.18\n0.892\n\n\n-w/o SVT\n5.53\n0.194\n3.95\n0.883\n\n\n-w/o CL + SVT\n5.89\n0.210\n3.83\n0.874",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MCD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F0-RMSE <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SingMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SECS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">CoMelSinger</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.17</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.042</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.32</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.912</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">-w/o CL</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.080</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.895</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">&#8194;&#8202;&#8195;-w/o SCL</th>\n<td class=\"ltx_td ltx_align_center\">4.53</td>\n<td class=\"ltx_td ltx_align_center\">0.062</td>\n<td class=\"ltx_td ltx_align_center\">4.25</td>\n<td class=\"ltx_td ltx_align_center\">0.900</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">&#8194;&#8202;&#8195;-w/o FCL</th>\n<td class=\"ltx_td ltx_align_center\">4.82</td>\n<td class=\"ltx_td ltx_align_center\">0.075</td>\n<td class=\"ltx_td ltx_align_center\">4.18</td>\n<td class=\"ltx_td ltx_align_center\">0.892</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">-w/o SVT</th>\n<td class=\"ltx_td ltx_align_center\">5.53</td>\n<td class=\"ltx_td ltx_align_center\">0.194</td>\n<td class=\"ltx_td ltx_align_center\">3.95</td>\n<td class=\"ltx_td ltx_align_center\">0.883</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">-w/o CL + SVT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">5.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.210</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.874</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "scl",
            "respectively",
            "sequence",
            "f0rmse",
            "study",
            "coarsetofine",
            "where",
            "↑uparrow",
            "pitch",
            "evaluation",
            "mcd",
            "objective",
            "results",
            "learning",
            "strategy",
            "ablation",
            "svt",
            "↓downarrow",
            "comelsinger",
            "fcl",
            "secs",
            "framelevel",
            "guidance",
            "contrastive",
            "model",
            "represents",
            "singmos"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To assess the contribution of each component in CoMelSinger, we perform ablation studies by systematically disabling key modules. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T4\" title=\"TABLE IV &#8227; V-D Ablation Study &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> reports results on four objective metrics: MCD, F0-RMSE, SingMOS, and SECS. Removing the entire coarse-to-fine contrastive learning (CL) framework leads to substantial degradation across all metrics, indicating reduced pitch accuracy and speaker consistency. This highlights the importance of contrastive objectives in disentangling pitch from timbre and improving input&#8211;output alignment.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Singing Voice Synthesis (SVS) aims to generate expressive vocal performances from structured musical inputs such as lyrics and pitch sequences. While recent progress in discrete codec-based speech synthesis has enabled zero-shot generation via in-context learning, directly extending these techniques to SVS remains non-trivial due to the requirement for precise melody control. In particular, prompt-based generation often introduces prosody leakage, where pitch information is inadvertently entangled within the timbre prompt, compromising controllability. We present CoMelSinger, a zero-shot SVS framework that enables structured and disentangled melody control within a discrete codec modeling paradigm. Built on the non-autoregressive MaskGCT architecture, CoMelSinger replaces conventional text inputs with lyric and pitch tokens, preserving in-context generalization while enhancing melody conditioning. To suppress prosody leakage, we propose a coarse-to-fine contrastive learning strategy that explicitly regularizes pitch redundancy between the acoustic prompt and melody input. Furthermore, we incorporate a lightweight encoder-only Singing Voice Transcription (SVT) module to align acoustic tokens with pitch and duration, offering fine-grained frame-level supervision. Experimental results demonstrate that CoMelSinger achieves notable improvements in pitch accuracy, timbre consistency, and zero-shot transferability over competitive baselines.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch",
                    "strategy",
                    "comelsinger",
                    "svt",
                    "results",
                    "contrastive",
                    "coarsetofine",
                    "where",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Discrete token-based architectures show great in-context learning capabilities and provide a promising pathway for such zero-shot generation. In a related task, text-to-speech (TTS) has witnessed rapid progress with discrete acoustic tokens derived from vector quantization and neural audio codecs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib16\" title=\"\">16</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib17\" title=\"\">17</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>]</cite>. By mapping complex waveforms into a quantized latent space, these tokens capture timbre, prosody, and phonetic content, thereby reformulating speech synthesis as symbolic sequence modeling akin to language modeling. The success of token-based TTS systems is largely enabled by large-scale multi-speaker corpora, which provide sufficient diversity to learn robust and generalizable representations. In contrast, the scarcity and limited diversity of singing data make token-based modeling for SVS significantly more challenging.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Built upon this data-rich foundation, recent TTS systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib20\" title=\"\">20</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib21\" title=\"\">21</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib16\" title=\"\">16</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib17\" title=\"\">17</a>]</cite>, have adopted large language model (LLM)-style architectures to model the conditional distribution of acoustic tokens given phoneme sequences and optional prompts. Within this framework, in-context learning (ICL) becomes feasible: a short segment of reference speech, represented as discrete tokens, serves as an acoustic prompt to guide synthesis in terms of speaker identity and style. This approach, exemplified by models such as VALL-E <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>]</cite>, enables zero-shot speech synthesis by treating speech generation as a form of conditional codec language modeling, without requiring speaker labels or model adaptation. Inspired by these advances, researchers have begun exploring discrete token-based methods for SVS.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, directly extending TTS by replacing textual input with structured musical inputs&#8212;such as lyrics and pitch tokens&#8212;while reusing the same modeling pipeline reveals a unique challenge in the singing domain: prosody leakage from the acoustic prompt, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S1.F1\" title=\"Figure 1 &#8227; I Introduction &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. In prompt-based synthesis, the acoustic prompt is intended to provide timbral cues, yet pitch-related attributes&#8212;including contour and timing&#8212;are often inadvertently encoded into its latent representation. This unintended encoding leads to timbre&#8211;melody entanglement, where the prompt simultaneously influences vocal timbre and melodic realization. As a result, the system&#8217;s control over the explicitly specified pitch sequence is weakened, undermining the precise separation between timbre conditioning and melody generation that SVS requires.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "where",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, SVS requires much finer control over pitch and melody than TTS or voice conversion, as the generated singing must accurately follow the musical score while preserving timbre. Although attribute control has been explored through adversarial training&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>]</cite>, contrastive learning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib28\" title=\"\">28</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib29\" title=\"\">29</a>]</cite>, and information-bottleneck methods&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib30\" title=\"\">30</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib31\" title=\"\">31</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib32\" title=\"\">32</a>]</cite><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib33\" title=\"\">33</a>]</cite>, these approaches primarily focus on coarse prosodic patterns or emotional cues and provide limited support for fine-grained melody control. Make-A-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite>, as a representative discrete-token SVS system, adopts prompt-guided conditioning but lacks explicit mechanisms to prevent the acoustic prompt from influencing melody realization. To the best of our knowledge, no prior work has systematically addressed prosody leakage in discrete token-based SVS, leaving a critical gap in achieving precise and faithful melody control in zero-shot scenarios.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "learning",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenges of prosody leakage and limited melody controllability in zero-shot SVS, we propose CoMelSinger, a discrete codec-based framework with structured melody control.\nCoMelSinger builds on the non-autoregressive MaskGCT architecture&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, adapting it to accept musical inputs consisting of lyrics and pitch tokens.\nTo achieve better melody&#8211;timbre control, we introduce a coarse-to-fine contrastive learning strategy that limits excessive pitch-related information in the acoustic prompt, allowing the explicit pitch condition to guide melodic realization more effectively.\nWe further incorporate a lightweight, encoder-only Singing Voice Transcription (SVT) module to provide fine-grained, frame-level supervision by aligning acoustic tokens with pitch and duration sequences.\nTogether, these designs enable accurate melody modeling, maintain timbre consistency, and preserve the in-context learning capability of discrete token-based systems.\nExtensive experiments on both seen and unseen singers demonstrate that CoMelSinger delivers substantial improvements in pitch accuracy, timbre consistency, and overall synthesis quality compared with state-of-the-art SVS baselines in zero-shot scenarios. The main contributions of this work are:</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch",
                    "strategy",
                    "comelsinger",
                    "svt",
                    "contrastive",
                    "coarsetofine",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a coarse-to-fine contrastive learning mechanism to improve melody&#8211;timbre control by limiting excessive pitch-related information in the acoustic prompt.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "learning",
                    "coarsetofine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop a lightweight SVT module that aligns acoustic tokens with pitch and duration, providing frame-level supervision to improve melody fidelity.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "framelevel",
                    "svt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comprehensive experiments on public SVS datasets demonstrate that CoMelSinger achieves superior pitch accuracy, timbre consistency, and generalization to unseen singers.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "comelsinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Singing voice synthesis (SVS) aims to produce expressive vocal performances from structured musical inputs such as note pitch, duration, and lyrics.\nCompared with text-to-speech (TTS), SVS presents unique challenges, including a wider pitch range and sustained phonation, which demand fine-grained melody modeling.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S2.F2\" title=\"Figure 2 &#8227; II-A Singing Voice Synthesis &#8227; II Related Works &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the evolution of representative SVS architectures, from continuous-feature pipelines to end-to-end models and, more recently, discrete codec-based frameworks.\nEarly SVS systems relied on unit-selection synthesis (e.g., VOCALOID&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib35\" title=\"\">35</a>]</cite>) or statistical approaches based on hidden Markov models (HMMs)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib36\" title=\"\">36</a>]</cite>.\nWith the advent of deep learning, SVS models increasingly adopted a two-stage architecture&#8212;comprising an acoustic model followed by a vocoder&#8212;including XiaoiceSing<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib37\" title=\"\">37</a>]</cite>, DeepSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib38\" title=\"\">38</a>]</cite>, and Sinsy&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib2\" title=\"\">2</a>]</cite>.\nSubsequent advances incorporated generative adversarial networks (GANs)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib39\" title=\"\">39</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib40\" title=\"\">40</a>]</cite> to enhance timbre realism, while more recent work using denoising diffusion probabilistic models (DDPMs)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib8\" title=\"\">8</a>]</cite> has achieved further gains in fidelity and temporal coherence.\nIn parallel, end-to-end systems such as VISinger 1/2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib9\" title=\"\">9</a>]</cite> have been developed to generate waveforms directly from musical scores without relying on explicit intermediate features.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by the success of discrete token modeling in TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib16\" title=\"\">16</a>]</cite>, recent SVS studies have explored token-based representations to improve generalization.\nTokSing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib41\" title=\"\">41</a>]</cite> employs a non-autoregressive Transformer conditioned on lyrics and pitch embeddings to predict discrete acoustic tokens.\nHiddenSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib3\" title=\"\">3</a>]</cite> integrates a diffusion-based decoder guided by discrete pitch and semantic tokens, achieving high-quality synthesis.\nMake-A-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite> unifies speech and singing synthesis through a shared discrete representation, but uses a relatively small proportion of singing data and does not incorporate prompt-based in-context learning.\nConsequently, it lacks explicit melody conditioning and provides limited flexibility in zero-shot singing scenarios.\nIn contrast, our approach introduces melody inputs and improved melody control in prompt-based conditioning, enabling more accurate and controllable zero-shot singing voice synthesis.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To reduce inference latency, non-autoregressive (NAR) decoding frameworks have been developed.\nSoundStorm&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib21\" title=\"\">21</a>]</cite> employs a bidirectional Transformer with confidence-based masked token modeling, generating audio tokens in parallel while maintaining autoregressive-level quality.\nMulti-token prediction and speculative decoding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib47\" title=\"\">47</a>]</cite> further accelerate synthesis by predicting multiple codec tokens per decoding step.\nMaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite> adopts a masked generative training strategy inspired by masked language modeling, enabling fast and parallel decoding while supporting in-context learning through prompt-aware input masking.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "strategy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fine-grained prosody control&#8212;particularly over fundamental frequency (F0) and phoneme duration&#8212;is essential for expressive speech and singing synthesis.\nSeveral TTS studies have incorporated explicit prosodic supervision to guide model learning.\nProsody-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib48\" title=\"\">48</a>]</cite> augments an end-to-end architecture with auxiliary predictors for phoneme-level F0 and duration, enabling precise rhythm and pitch control without degrading naturalness.\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib49\" title=\"\">49</a>]</cite> adopt utterance-level prosodic features in a hierarchical non-autoregressive model, providing interpretable style modulation across prosodic dimensions while maintaining synthesis quality.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances extend prosody control to diffusion-based synthesis.\nDiffStyleTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib50\" title=\"\">50</a>]</cite> combines a diffusion decoder with classifier-free guidance to model prosodic style at both coarse and phoneme-level scales, supporting flexible pitch&#8211;duration manipulation.\nDrawSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib51\" title=\"\">51</a>]</cite> enables editing by conditioning on user-drawn pitch&#8211;energy sketches, which are refined into high-resolution prosody contours.</p>\n\n",
                "matched_terms": [
                    "model",
                    "guidance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, few systems address conflicts between external melody guidance and prompt-derived timbre cues in discrete-token SVS.\nWe address this gap by introducing coarse-to-fine contrastive learning with frame-level pitch supervision to reduce melody leakage from prompts and strengthen external melody control, enabling high-fidelity pitch realization without sacrificing timbre consistency or generalization.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch",
                    "contrastive",
                    "coarsetofine",
                    "framelevel",
                    "guidance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">(a) Sequence-level Contrastive Learning.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">(b) Frame-level Contrastive Learning.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "framelevel",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable zero-shot singing voice synthesis with accurate melody control and disentangled prompt conditioning, we propose <em class=\"ltx_emph ltx_font_italic\">CoMelSinger</em>, a two-stage framework illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S3.F3\" title=\"Figure 3 &#8227; III Method &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Inspired by MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite> and Make-A-Voice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib12\" title=\"\">12</a>]</cite>, CoMelSinger comprises a Text-to-Semantic (T2S) stage and a Semantic-to-Acoustic (S2A) stage. The T2S module <math alttext=\"f_{\\mathrm{T2S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>T2S</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{T2S}}</annotation></semantics></math> transforms a lyric token sequence <math alttext=\"\\tilde{\\mathbf{m}}^{l}=[\\tilde{m}^{l}_{1},\\dots,\\tilde{m}^{l}_{S}]\\in\\mathcal{V}_{\\mathrm{lyr}}^{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>l</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mover accent=\"true\"><mi>m</mi><mo>~</mo></mover><mn>1</mn><mi>l</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>m</mi><mo>~</mo></mover><mi>S</mi><mi>l</mi></msubsup><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>lyr</mi><mi>S</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{l}=[\\tilde{m}^{l}_{1},\\dots,\\tilde{m}^{l}_{S}]\\in\\mathcal{V}_{\\mathrm{lyr}}^{S}</annotation></semantics></math>, obtained from a Grapheme-to-Phoneme (G2P) converter, and a semantic prompt <math alttext=\"\\mathbf{s}^{r}=E_{S}(\\mathbf{w}^{r})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119852;</mi><mi>r</mi></msup><mo>=</mo><mrow><msub><mi>E</mi><mi>S</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119856;</mi><mi>r</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}^{r}=E_{S}(\\mathbf{w}^{r})</annotation></semantics></math> extracted from the reference waveform <math alttext=\"\\mathbf{w}^{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119856;</mi><mi>r</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{w}^{r}</annotation></semantics></math>, into a semantic token sequence <math alttext=\"\\mathbf{s}\\in\\mathcal{V}_{\\mathrm{sem}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#119852;</mi><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>sem</mi><mi>L</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}\\in\\mathcal{V}_{\\mathrm{sem}}^{L}</annotation></semantics></math>, where <math alttext=\"\\mathcal{V}_{\\mathrm{sem}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>sem</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{V}_{\\mathrm{sem}}</annotation></semantics></math> denotes the semantic vocabulary and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> the sequence length. The S2A module <math alttext=\"f_{\\mathrm{S2A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>S2A</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{S2A}}</annotation></semantics></math> then predicts acoustic tokens <math alttext=\"\\mathbf{a}\\in\\mathcal{V}_{\\mathrm{aco}}^{L\\times N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mi>&#119834;</mi><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>aco</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{a}\\in\\mathcal{V}_{\\mathrm{aco}}^{L\\times N}</annotation></semantics></math>, conditioned on the semantic tokens <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math>, an acoustic prompt <math alttext=\"\\mathbf{a}^{r}=E_{A}(\\mathbf{w}^{r})\\in\\mathcal{V}_{\\mathrm{aco}}^{L_{r}\\times N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m11\" intent=\":literal\"><semantics><mrow><msup><mi>&#119834;</mi><mi>r</mi></msup><mo>=</mo><mrow><msub><mi>E</mi><mi>A</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119856;</mi><mi>r</mi></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>aco</mi><mrow><msub><mi>L</mi><mi>r</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>N</mi></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r}=E_{A}(\\mathbf{w}^{r})\\in\\mathcal{V}_{\\mathrm{aco}}^{L_{r}\\times N}</annotation></semantics></math>, and a regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\\in\\mathcal{V}_{\\mathrm{pit}}^{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m12\" intent=\":literal\"><semantics><mrow><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><mo>&#8712;</mo><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mi>pit</mi><mi>L</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}\\in\\mathcal{V}_{\\mathrm{pit}}^{L}</annotation></semantics></math>. Here, <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m13\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> denotes the number of residual vector quantization (RVQ) codebooks in the acoustic codec, and the regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m14\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math> is derived from the pitch <math alttext=\"\\mathbf{m}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m15\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{p}</annotation></semantics></math> and duration <math alttext=\"\\mathbf{m}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m16\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>d</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{d}</annotation></semantics></math> sequence through the length expansion module <math alttext=\"LE(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m17\" intent=\":literal\"><semantics><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">LE(\\cdot)</annotation></semantics></math>. Both the semantic and acoustic tokens are produced using discrete codec tokenizers following the MaskGCT setup, and the final waveform <math alttext=\"\\mathbf{w}^{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m18\" intent=\":literal\"><semantics><msup><mi>&#119856;</mi><mi>o</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{w}^{o}</annotation></semantics></math> is reconstructed from acoustic tokens via the decoder <math alttext=\"D_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m19\" intent=\":literal\"><semantics><msub><mi>D</mi><mi>A</mi></msub><annotation encoding=\"application/x-tex\">D_{A}</annotation></semantics></math>. The complete pipeline is summarized as:</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "where",
                    "pitch",
                    "comelsinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To synchronize pitch information with frame-level features, we map phonetic durations onto the frame index space. Given a pitch sequence <math alttext=\"\\mathbf{m}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{p}</annotation></semantics></math> with corresponding durations <math alttext=\"\\mathbf{m}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>d</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{d}</annotation></semantics></math>, the total duration is <math alttext=\"D=\\sum_{i}m^{d}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>D</mi><mo rspace=\"0.111em\">=</mo><mrow><msub><mo>&#8721;</mo><mi>i</mi></msub><msubsup><mi>m</mi><mi>i</mi><mi>d</mi></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">D=\\sum_{i}m^{d}_{i}</annotation></semantics></math>. For each pitch token <math alttext=\"m^{p}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>m</mi><mi>i</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">m^{p}_{i}</annotation></semantics></math>, its frame span is derived by rounding the cumulative normalized duration:</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "pitch",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> denotes the length of the semantic feature sequence. The frame-aligned pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math> is then obtained by repeating each <math alttext=\"m^{p}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>m</mi><mi>i</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">m^{p}_{i}</annotation></semantics></math> for <math alttext=\"n_{i}=k_{\\text{end},i}-k_{\\text{start},i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>=</mo><mrow><msub><mi>k</mi><mrow><mtext>end</mtext><mo>,</mo><mi>i</mi></mrow></msub><mo>&#8722;</mo><msub><mi>k</mi><mrow><mtext>start</mtext><mo>,</mo><mi>i</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">n_{i}=k_{\\text{end},i}-k_{\\text{start},i}</annotation></semantics></math> frames, ensuring an exact length correspondence with the semantic features.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "where",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, CoMelSinger adopts a non-autoregressive masked generative modeling paradigm for both stages. In this framework, the model learns to reconstruct masked tokens within a sequence conditioned on surrounding context and external inputs, rather than generating tokens sequentially. This allows for parallel decoding and better handling of global context compared to traditional autoregressive models. Specifically, we model the conditional probabilities:\n<math alttext=\"p(\\mathbf{s}\\mid\\mathbf{s}_{t};\\mathbf{m}^{l},\\mathbf{s}^{r};f_{\\theta,\\mathrm{T2S}}),\\quad p(\\mathbf{a}\\mid\\mathbf{a}_{t};\\mathbf{s},\\tilde{\\mathbf{m}}^{p},\\mathbf{a}^{r};f_{\\theta,\\mathrm{S2A}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119852;</mi><mo>&#8739;</mo><mrow><msub><mi>&#119852;</mi><mi>t</mi></msub><mo>;</mo><msup><mi>&#119846;</mi><mi>l</mi></msup><mo>,</mo><msup><mi>&#119852;</mi><mi>r</mi></msup><mo>;</mo><msub><mi>f</mi><mrow><mi>&#952;</mi><mo>,</mo><mi>T2S</mi></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"1.167em\">,</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#119834;</mi><mo>&#8739;</mo><mrow><msub><mi>&#119834;</mi><mi>t</mi></msub><mo>;</mo><mi>&#119852;</mi><mo>,</mo><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><mo>,</mo><msup><mi>&#119834;</mi><mi>r</mi></msup><mo>;</mo><msub><mi>f</mi><mrow><mi>&#952;</mi><mo>,</mo><mi>S2A</mi></mrow></msub></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">p(\\mathbf{s}\\mid\\mathbf{s}_{t};\\mathbf{m}^{l},\\mathbf{s}^{r};f_{\\theta,\\mathrm{T2S}}),\\quad p(\\mathbf{a}\\mid\\mathbf{a}_{t};\\mathbf{s},\\tilde{\\mathbf{m}}^{p},\\mathbf{a}^{r};f_{\\theta,\\mathrm{S2A}})</annotation></semantics></math>.\nHere, <math alttext=\"\\mathbf{s}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{t}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119834;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{a}_{t}</annotation></semantics></math> are the partially masked semantic and acoustic token sequences, and the generation is conditioned on lyric inputs <math alttext=\"\\mathbf{m}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>l</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{l}</annotation></semantics></math>, pitch-aligned sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, and prompt tokens <math alttext=\"(\\mathbf{s}^{r},\\mathbf{a}^{r})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msup><mi>&#119852;</mi><mi>r</mi></msup><mo>,</mo><msup><mi>&#119834;</mi><mi>r</mi></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{s}^{r},\\mathbf{a}^{r})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "model",
                    "comelsinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building upon the original MaskGCT architecture, which employs a LLaMA-style Transformer backbone <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib54\" title=\"\">54</a>]</cite>, we extend the S2A model by introducing an additional embedding layer for the regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>. This pitch embedding <math alttext=\"\\mathbf{e}_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#119838;</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{e}_{p}</annotation></semantics></math> is element-wise added to the semantic token embedding <math alttext=\"\\mathbf{e}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>&#119838;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{e}_{s}</annotation></semantics></math> to form the composite conditioning input <math alttext=\"\\mathbf{e}_{c}=\\mathbf{e}_{p}+\\mathbf{e}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#119838;</mi><mi>c</mi></msub><mo>=</mo><mrow><msub><mi>&#119838;</mi><mi>p</mi></msub><mo>+</mo><msub><mi>&#119838;</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{e}_{c}=\\mathbf{e}_{p}+\\mathbf{e}_{s}</annotation></semantics></math>, thereby enabling the model to incorporate both linguistic and melodic information during acoustic token generation.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "pitch",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further enhance melody controllability and suppress interference from prompt-induced timbre cues, we propose a coarse-to-fine contrastive learning framework. At the sequence level, a contrastive loss encourages the predicted acoustic token sequence <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math> to preserve the overall pitch contour defined by <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>. At the frame level, a token-wise contrastive objective aligns fine-grained acoustic features with localized pitch variations, thereby reinforcing frame-level pitch fidelity. In addition, we introduce an auxiliary singing voice transcription (SVT) model, trained to estimate pitch sequences directly from acoustic tokens. The SVT model provides pseudo pitch labels that serve as external supervision during S2A training. This auxiliary signal further improves alignment between the synthesized melody and the target pitch contour.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch",
                    "svt",
                    "sequence",
                    "model",
                    "objective",
                    "contrastive",
                    "coarsetofine",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Contrastive learning has been increasingly adopted in speech and audio modeling to enforce factor-specific consistency while suppressing undesired variations such as speaker identity or prompt interference. For instance, CLAPSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib55\" title=\"\">55</a>]</cite> applies multi-scale contrastive learning between textual prosody embeddings and corresponding acoustic realizations, improving prosodic expressivity across varying textual contexts. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib56\" title=\"\">56</a>]</cite> propose a contrastive loss to enhance the modeling of prosodic focus&#8212;including F0, duration, and intensity&#8212;by encouraging TTS systems to distinguish emphasized from neutral phonetic segments. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib57\" title=\"\">57</a>]</cite> use contrastive self-supervision to extract prosody-specific embeddings disentangled from speaker identity, which are useful for style transfer and anonymized generation. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib58\" title=\"\">58</a>]</cite> further explore contrastive pretraining to align textual context with expressive speech realizations, facilitating zero-shot expressive TTS with better generalization.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, our method extends contrastive learning to the discrete-token SVS setting by incorporating hierarchical supervision. At the sequence level, we enforce prompt-invariant acoustic consistency conditioned on identical semantic and pitch tokens, encouraging the model to preserve global melody shape. At the local level, a frame-wise contrastive loss is applied to align acoustic token features with fine-grained pitch variations. This coarse-to-fine scheme allows our model to disentangle pitch conditioning from acoustic prompts and enhances melody fidelity under zero-shot generation.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch",
                    "sequence",
                    "contrastive",
                    "coarsetofine",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To promote melody-consistent synthesis under varying acoustic prompts, we introduce a sequence-level symmetric contrastive loss.\nGiven a batch of <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> training samples that share the same semantic token sequence <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math> but differ in pitch sequences <math alttext=\"\\tilde{\\mathbf{m}}^{p}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}_{k}</annotation></semantics></math>, we construct two sets of inputs:\n<math alttext=\"\\mathcal{S}^{g}_{A}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,A}_{k},\\mathbf{a}^{A}_{k,t}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>A</mi><mi>g</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mi>&#119852;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>A</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}^{g}_{A}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,A}_{k},\\mathbf{a}^{A}_{k,t}\\}_{k=1}^{K}</annotation></semantics></math> and\n<math alttext=\"\\mathcal{S}^{g}_{B}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,B}_{k},\\mathbf{a}^{B}_{k,t}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>B</mi><mi>g</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><mi>&#119852;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>B</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}^{g}_{B}=\\{\\mathbf{s},\\tilde{\\mathbf{m}}^{p}_{k},\\mathbf{a}^{r,B}_{k},\\mathbf{a}^{B}_{k,t}\\}_{k=1}^{K}</annotation></semantics></math>,\nwhere <math alttext=\"\\mathbf{a}^{A}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m6\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{A}_{k,t}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{B}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m7\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>B</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{B}_{k,t}</annotation></semantics></math> denote masked acoustic tokens, <math alttext=\"\\mathbf{a}^{r,A}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,A}_{k}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{r,B}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,B}_{k}</annotation></semantics></math> denote acoustic prompts sampled from distinct utterances of the same singer, ensuring consistent timbre across the pair.\nThe prompt length is randomly selected from the range <math alttext=\"\\left[\\min\\left(\\left\\lfloor L/4\\right\\rfloor,5\\right),\\left\\lfloor L/2\\right\\rfloor\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m10\" intent=\":literal\"><semantics><mrow><mo>[</mo><mrow><mi>min</mi><mo>&#8289;</mo><mrow><mo>(</mo><mrow><mo>&#8970;</mo><mrow><mi>L</mi><mo>/</mo><mn>4</mn></mrow><mo>&#8971;</mo></mrow><mo>,</mo><mn>5</mn><mo>)</mo></mrow></mrow><mo>,</mo><mrow><mo>&#8970;</mo><mrow><mi>L</mi><mo>/</mo><mn>2</mn></mrow><mo>&#8971;</mo></mrow><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left[\\min\\left(\\left\\lfloor L/4\\right\\rfloor,5\\right),\\left\\lfloor L/2\\right\\rfloor\\right)</annotation></semantics></math>, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m11\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the length of the semantic sequence. Each input is processed by the S2A model to produce acoustic token embeddings <math alttext=\"\\mathbf{g}^{a},\\mathbf{g}^{b}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m12\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>&#119840;</mi><mi>a</mi></msup><mo>,</mo><msup><mi>&#119840;</mi><mi>b</mi></msup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{g}^{a},\\mathbf{g}^{b}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math>, which are mean-pooled across the time dimension to yield global acoustic representations <math alttext=\"\\tilde{\\mathbf{g}}^{a},\\tilde{\\mathbf{g}}^{b}\\in\\mathbb{R}^{K\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m13\" intent=\":literal\"><semantics><mrow><mrow><msup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>a</mi></msup><mo>,</mo><msup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>b</mi></msup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{g}}^{a},\\tilde{\\mathbf{g}}^{b}\\in\\mathbb{R}^{K\\times D}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "sequence",
                    "contrastive",
                    "where",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To align acoustic outputs with the shared pitch condition while remaining invariant to prompt variation, we apply a symmetric contrastive loss (SCE)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib59\" title=\"\">59</a>]</cite> defined as:</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> is a temperature hyperparameter. Each positive pair <math alttext=\"(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>i</mi><mi>a</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>i</mi><mi>b</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{i})</annotation></semantics></math> corresponds to index-aligned embeddings generated under identical semantic tokens <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m3\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math> and regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m4\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, but conditioned on different acoustic prompts <math alttext=\"\\mathbf{a}^{r,A}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m5\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>i</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,A}_{i}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{r,B}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>i</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,B}_{i}</annotation></semantics></math>. These prompts are randomly selected from non-overlapping segments of the same singer&#8217;s recordings, ensuring consistent timbre while introducing natural variation. In contrast, off-diagonal pairs <math alttext=\"(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>i</mi><mi>a</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119840;</mi><mo>~</mo></mover><mi>j</mi><mi>b</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\tilde{\\mathbf{g}}^{a}_{i},\\tilde{\\mathbf{g}}^{b}_{j})</annotation></semantics></math> for <math alttext=\"i\\neq j\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p2.m8\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8800;</mo><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">i\\neq j</annotation></semantics></math> serve as negatives due to mismatched pitch sequences, even though semantic tokens and speaker identity remain the same. These negatives are nontrivial, as they reflect realistic melodic differences under otherwise comparable contextual and timbral conditions. This design encourages the model to focus on capturing global pitch structure while remaining invariant to prompt-induced variability.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "where",
                    "pitch",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this sequence-level objective enforces high-level melodic consistency, it does not explicitly supervise token-level alignment. We therefore complement it with a frame-level contrastive loss to further enhance fine-grained melody control.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "framelevel",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the sequence contrastive loss promotes utterance-level melody consistency, it does not directly enforce fine-grained pitch alignment at the frame level&#8212;an essential factor in singing synthesis due to rapid and expressive melodic changes. To address this limitation, we introduce a frame-level contrastive objective that supervises token-wise alignment between generated acoustic representations and the input pitch contour. Our design is motivated by recent work such as CTAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib60\" title=\"\">60</a>]</cite>, which leverage contrastive learning to align discrete phoneme sequences with speech features for TTS, voice conversion, and ASR tasks under limited supervision. Although our formulation differs in both granularity and modality&#8212;operating on pitch tokens rather than phonemes, and targeting melody alignment in singing synthesis&#8212;these studies underscore the effectiveness of contrastive supervision for bridging symbolic and acoustic representations. By extending this idea to the SVS domain, our frame-level contrastive loss enhances local pitch fidelity while remaining robust to prompt-induced variation, thereby enabling more precise and expressive melody control in zero-shot scenarios.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch",
                    "sequence",
                    "objective",
                    "contrastive",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a batch of <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> training samples with distinct semantic token sequences <math alttext=\"\\mathbf{s}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{k}</annotation></semantics></math> and corresponding pitch sequences <math alttext=\"\\mathbf{m}^{p}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119846;</mi><mi>k</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{p}_{k}</annotation></semantics></math>, we construct two sets of inputs:\n<math alttext=\"\\mathcal{S}^{f}_{A}=\\{\\mathbf{s}_{k},\\tilde{\\mathbf{m}}^{p,A}_{k},\\mathbf{a}^{r,A}_{k},\\mathbf{a}^{A}_{k,t}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>A</mi><mi>f</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119852;</mi><mi>k</mi></msub><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mrow><mi>p</mi><mo>,</mo><mi>A</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>A</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}^{f}_{A}=\\{\\mathbf{s}_{k},\\tilde{\\mathbf{m}}^{p,A}_{k},\\mathbf{a}^{r,A}_{k},\\mathbf{a}^{A}_{k,t}\\}_{k=1}^{K}</annotation></semantics></math> and <math alttext=\"\\mathcal{S}^{f}_{B}=\\{\\mathbf{s}_{k},\\tilde{\\mathbf{m}}^{p,B}_{k},\\mathbf{a}^{r,B}_{k},\\mathbf{a}^{B}_{k,t}\\}_{k=1}^{K},\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m5\" intent=\":literal\"><semantics><mrow><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mi>B</mi><mi>f</mi></msubsup><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119852;</mi><mi>k</mi></msub><mo>,</mo><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mrow><mi>p</mi><mo>,</mo><mi>B</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>B</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}^{f}_{B}=\\{\\mathbf{s}_{k},\\tilde{\\mathbf{m}}^{p,B}_{k},\\mathbf{a}^{r,B}_{k},\\mathbf{a}^{B}_{k,t}\\}_{k=1}^{K},</annotation></semantics></math>\nwhere <math alttext=\"\\mathbf{a}^{A}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m6\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>A</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{A}_{k,t}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{B}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m7\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow><mi>B</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{B}_{k,t}</annotation></semantics></math> denote the masked acoustic tokens, <math alttext=\"\\tilde{\\mathbf{m}}^{p,A}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mrow><mi>p</mi><mo>,</mo><mi>A</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p,A}_{k}</annotation></semantics></math> and <math alttext=\"\\tilde{\\mathbf{m}}^{p,B}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m9\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mrow><mi>p</mi><mo>,</mo><mi>B</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p,B}_{k}</annotation></semantics></math> denote the regulated pitch sequences derived from the original pitch tokens <math alttext=\"\\mathbf{m}^{p}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119846;</mi><mi>k</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{p}_{k}</annotation></semantics></math> and their perturbed variants <math alttext=\"P(\\mathbf{m}^{p}_{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m11\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119846;</mi><mi>k</mi><mi>p</mi></msubsup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(\\mathbf{m}^{p}_{k})</annotation></semantics></math>, respectively. The acoustic prompts <math alttext=\"\\mathbf{a}^{r,A}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m12\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>A</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,A}_{k}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}^{r,B}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m13\" intent=\":literal\"><semantics><msubsup><mi>&#119834;</mi><mi>k</mi><mrow><mi>r</mi><mo>,</mo><mi>B</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{a}^{r,B}_{k}</annotation></semantics></math> are sampled from different utterances of the same singer to ensure consistent timbre across pairs.\nThe perturbation function <math alttext=\"P(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m14\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(\\cdot)</annotation></semantics></math> offsets 50% of pitch tokens by integers randomly sampled from <math alttext=\"[-6,6]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m15\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mrow><mo>&#8722;</mo><mn>6</mn></mrow><mo>,</mo><mn>6</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[-6,6]</annotation></semantics></math>, while preserving the original duration sequence <math alttext=\"\\mathbf{m}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p2.m16\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>d</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{d}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "where",
                    "pitch",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each input is passed through the S2A model to produce frame-level acoustic embeddings <math alttext=\"\\mathbf{f}^{a},\\mathbf{f}^{b}\\in\\mathbb{R}^{K\\times L\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msup><mi>&#119839;</mi><mi>a</mi></msup><mo>,</mo><msup><mi>&#119839;</mi><mi>b</mi></msup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>K</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{f}^{a},\\mathbf{f}^{b}\\in\\mathbb{R}^{K\\times L\\times D}</annotation></semantics></math>. For each training sample <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>, we compute a cosine similarity matrix <math alttext=\"\\mathbf{S}^{k}\\in\\mathbb{R}^{L\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m3\" intent=\":literal\"><semantics><mrow><msup><mi>&#119826;</mi><mi>k</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{S}^{k}\\in\\mathbb{R}^{L\\times L}</annotation></semantics></math> between <math alttext=\"\\mathbf{f}^{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m4\" intent=\":literal\"><semantics><msup><mi>&#119839;</mi><mi>a</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{f}^{a}</annotation></semantics></math> and <math alttext=\"\\mathbf{f}^{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m5\" intent=\":literal\"><semantics><msup><mi>&#119839;</mi><mi>b</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{f}^{b}</annotation></semantics></math>. To supervise the similarity learning, we define a soft label matrix <math alttext=\"\\mathbf{Y}^{k}\\in[-1,1]^{L\\times L}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m6\" intent=\":literal\"><semantics><mrow><msup><mi>&#119832;</mi><mi>k</mi></msup><mo>&#8712;</mo><msup><mrow><mo stretchy=\"false\">[</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>L</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Y}^{k}\\in[-1,1]^{L\\times L}</annotation></semantics></math> capturing pitch and semantic alignment:</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch",
                    "model",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\alpha\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p3.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in(0,1)</annotation></semantics></math> is a tunable coefficient that softly downweights semantically mismatched but pitch-aligned pairs. The frame-level contrastive loss is formulated as a masked regression objective:</p>\n\n",
                "matched_terms": [
                    "where",
                    "contrastive",
                    "framelevel",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This formulation encourages the model to produce highly similar acoustic embeddings when both pitch and semantic content align, moderately similar embeddings when only pitch aligns, and dissimilar embeddings otherwise. The similarity is computed within each utterance because the vocal range is typically locally bounded, making repeated pitch tokens more likely. In contrast, pitch overlap across utterances is rare and thus excluded. Additionally, even within the same utterance, repeated pitches may be associated with different semantic tokens, resulting in subtle acoustic variation. Our soft labeling mechanism accounts for this by assigning intermediate similarity, thereby avoiding over-penalization while promoting melody-consistent synthesis. Hence, the final contrastive learning objective is as follows:</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch",
                    "objective",
                    "contrastive",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\lambda_{\\text{SCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mtext>SCL</mtext></msub><annotation encoding=\"application/x-tex\">\\lambda_{\\text{SCL}}</annotation></semantics></math> and <math alttext=\"\\lambda_{\\text{FCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#955;</mi><mtext>FCL</mtext></msub><annotation encoding=\"application/x-tex\">\\lambda_{\\text{FCL}}</annotation></semantics></math> are weighting coefficients that balance the contributions of sequence-level and frame-level supervision, respectively.</p>\n\n",
                "matched_terms": [
                    "where",
                    "framelevel",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent studies have explored the use of Singing Voice Transcription (SVT) to support singing voice synthesis (SVS). For example, ROSVOT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib61\" title=\"\">61</a>]</cite> proposes a robust SVT model to produce high-quality pitch annotations for large-scale singing datasets, thereby improving SVS performance by enhancing training data quality. However, such approaches treat SVT as an independent preprocessing tool, disconnected from the synthesis process.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "model",
                    "svt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, we integrate the SVT module directly into the training pipeline to provide explicit frame-level pitch supervision, thereby enhancing melody modeling and alignment. Specifically, the SVT model predicts frame-wise discrete pitch tokens from acoustic codec representations, which are then compared against the ground-truth pitch sequence. This supervision enforces alignment between the generated acoustic tokens and the intended melody, encouraging consistent pitch realization, particularly under zero-shot conditions. Furthermore, as the SVT module operates entirely on discrete representations, it is naturally compatible with our codec-based SVS framework and does not require raw audio or continuous F0 contours.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "svt",
                    "sequence",
                    "framelevel",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SVT model adopts a lightweight encoder-only Transformer architecture. Given a sequence of acoustic tokens <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, it predicts the corresponding pitch token sequence <math alttext=\"\\hat{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>^</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{m}}^{p}</annotation></semantics></math> of length <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>. The encoder consists of four Transformer layers with a hidden size of 512 and eight attention heads. Each input frame comprises 12 discrete acoustic codes, which are individually embedded, concatenated, and projected to a 512-dimensional representation, followed by layer normalization. The resulting embedding sequence is then passed through a linear classification head to predict a pitch token for each frame. The model is trained using a standard cross-entropy loss between the predicted and reference pitch sequences.</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "pitch",
                    "model",
                    "svt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide frame-level supervision for pitch modeling, we leverage the pretrained SVT model as a pitch predictor to generate pseudo labels in the form of pitch token sequences <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m1\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, which are temporally aligned with the acoustic frames. As the primary training objective, we apply a cross-entropy loss <math alttext=\"\\mathcal{L}_{\\rm CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>CE</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\rm CE}</annotation></semantics></math> between the predicted pitch tokens <math alttext=\"\\hat{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m3\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>^</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\hat{\\mathbf{m}}^{p}</annotation></semantics></math> and the SVT-derived ground-truth <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p4.m4\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>, encouraging accurate token-level classification. However, the cross-entropy objective alone does not account for the temporal continuity inherent in repeated pitch tokens. This often results in jittery predictions, fragmented note segments, and rhythmically unstable outputs.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "svt",
                    "model",
                    "objective",
                    "results",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, we propose a segment transition loss <math alttext=\"\\mathcal{L}_{\\text{seg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>seg</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{seg}}</annotation></semantics></math> to impose structural regularity on the predicted pitch token sequence. Let <math alttext=\"\\mathbf{p}\\in\\mathbb{R}^{L\\times C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#119849;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{p}\\in\\mathbb{R}^{L\\times C}</annotation></semantics></math> denote the predicted frame-level pitch token distribution obtained by applying a softmax to the decoder logits, where <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the number of frames and <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> is the size of the pitch token vocabulary. The loss is defined as follows:</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "where",
                    "pitch",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"b_{t}=\\mathds{1}[\\tilde{m}^{p}_{t}\\neq\\tilde{m}^{p}_{t-1}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>b</mi><mi>t</mi></msub><mo>=</mo><mrow><mn>&#120793;</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mrow><msubsup><mover accent=\"true\"><mi>m</mi><mo>~</mo></mover><mi>t</mi><mi>p</mi></msubsup><mo>&#8800;</mo><msubsup><mover accent=\"true\"><mi>m</mi><mo>~</mo></mover><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>p</mi></msubsup></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">b_{t}=\\mathds{1}[\\tilde{m}^{p}_{t}\\neq\\tilde{m}^{p}_{t-1}]</annotation></semantics></math> is a binary indicator marking ground-truth pitch boundaries, and <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m6\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> is a fixed margin that enforces dissimilarity across transitions. This formulation penalizes minimal variation within sustained pitch regions while promoting sharper contrast at pitch change boundaries, thereby enhancing segment continuity and expressive phrasing in the synthesized output.</p>\n\n",
                "matched_terms": [
                    "where",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by recent advances in speech synthesis that emphasize the importance of temporal alignment and duration modeling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib63\" title=\"\">63</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib37\" title=\"\">37</a>]</cite>, we introduce a soft duration loss <math alttext=\"\\mathcal{L}_{\\text{dur}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>dur</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{dur}}</annotation></semantics></math> to enhance the rhythmic fidelity of frame-level pitch predictions. Prior works such as FastSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib63\" title=\"\">63</a>]</cite> and XiaoiceSing&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib37\" title=\"\">37</a>]</cite> employ explicit duration predictors or auxiliary alignment modules to supervise temporal structures. While effective, these methods often introduce architectural overhead or struggle to generalize in expressive singing scenarios. In contrast, our approach provides a fully differentiable supervision signal by directly supervising the temporal distribution of pitch token probabilities using the softmax outputs of the model, without requiring any external duration modeling.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "model",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a symbolic duration sequence <math alttext=\"\\mathbf{m}^{d}=[m_{1}^{d},\\ldots,m_{S}^{d}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><mrow><msup><mi>&#119846;</mi><mi>d</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>m</mi><mn>1</mn><mi>d</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>m</mi><mi>S</mi><mi>d</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{m}^{d}=[m_{1}^{d},\\ldots,m_{S}^{d}]</annotation></semantics></math>, we normalize it into a frame-level allocation <math alttext=\"\\mathbf{a}^{d}=[a_{1}^{d},\\ldots,a_{S}^{d}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#119834;</mi><mi>d</mi></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>a</mi><mn>1</mn><mi>d</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>a</mi><mi>S</mi><mi>d</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{a}^{d}=[a_{1}^{d},\\ldots,a_{S}^{d}]</annotation></semantics></math>, where each element is computed as <math alttext=\"a_{i}^{d}=\\left\\lfloor\\frac{m_{i}^{d}}{D}\\cdot L\\right\\rfloor\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi>a</mi><mi>i</mi><mi>d</mi></msubsup><mo>=</mo><mrow><mo>&#8970;</mo><mrow><mfrac><msubsup><mi>m</mi><mi>i</mi><mi>d</mi></msubsup><mi>D</mi></mfrac><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><mi>L</mi></mrow><mo>&#8971;</mo></mrow></mrow><annotation encoding=\"application/x-tex\">a_{i}^{d}=\\left\\lfloor\\frac{m_{i}^{d}}{D}\\cdot L\\right\\rfloor</annotation></semantics></math>, where <math alttext=\"D=\\sum\\limits_{i=1}^{S}m^{d}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi>D</mi><mo rspace=\"0.111em\">=</mo><mrow><munderover><mo movablelimits=\"false\">&#8721;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><msubsup><mi>m</mi><mi>i</mi><mi>d</mi></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">D=\\sum\\limits_{i=1}^{S}m^{d}_{i}</annotation></semantics></math> and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m5\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> denotes the total number of frames. Let <math alttext=\"\\mathbf{p}\\in\\mathbb{R}^{L\\times C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m6\" intent=\":literal\"><semantics><mrow><mi>&#119849;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>L</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>C</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{p}\\in\\mathbb{R}^{L\\times C}</annotation></semantics></math> denote the predicted frame-level pitch token distribution obtained via softmax, where <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m7\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> is the size of the pitch vocabulary. For each target pitch token <math alttext=\"m_{i}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>m</mi><mi>i</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">m_{i}^{p}</annotation></semantics></math>, we define its soft duration as the cumulative probability mass <math alttext=\"\\mathbf{p}_{t}[m_{i}^{p}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m9\" intent=\":literal\"><semantics><mrow><msub><mi>&#119849;</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>m</mi><mi>i</mi><mi>p</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{p}_{t}[m_{i}^{p}]</annotation></semantics></math> over its allocated segment of length <math alttext=\"a_{i}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m10\" intent=\":literal\"><semantics><msubsup><mi>a</mi><mi>i</mi><mi>d</mi></msubsup><annotation encoding=\"application/x-tex\">a_{i}^{d}</annotation></semantics></math>. The soft duration loss is given by</p>\n\n",
                "matched_terms": [
                    "sequence",
                    "where",
                    "pitch",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"T_{i}=\\sum\\limits_{j=1}^{i-1}a_{j}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m11\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mi>i</mi></msub><mo rspace=\"0.111em\">=</mo><mrow><munderover><mo movablelimits=\"false\">&#8721;</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></munderover><msubsup><mi>a</mi><mi>j</mi><mi>d</mi></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">T_{i}=\\sum\\limits_{j=1}^{i-1}a_{j}^{d}</annotation></semantics></math> denotes the starting frame index for the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m12\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>th pitch token.</p>\n\n",
                "matched_terms": [
                    "where",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This formulation encourages the model to allocate appropriate probability mass to each pitch token across time, thereby promoting temporally coherent and rhythmically faithful melody generation. The final training objective for the SVT module combines the cross-entropy loss, segment transition loss, and soft duration loss:</p>\n\n",
                "matched_terms": [
                    "svt",
                    "pitch",
                    "model",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We begin by training the Singing Voice Transcription (SVT) model to provide frame-level pitch supervision for subsequent Semantic-to-Acoustic (S2A) adaptation. The SVT model is optimized using a cross-entropy loss <math alttext=\"\\mathcal{L}_{\\text{CE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CE</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CE}}</annotation></semantics></math> between the regulated pitch token sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math> and the acoustic token sequence <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, thereby learning to predict temporally aligned pitch trajectories from acoustic inputs. Once trained, the SVT module is frozen to serve as a fixed auxiliary supervisor during S2A training.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch",
                    "svt",
                    "sequence",
                    "model",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then fine-tune the S2A model built upon the MaskGCT framework&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite>, which leverages masked acoustic modeling for non-autoregressive generation. The training objective for the S2A model comprises three components: (1) the mask token prediction loss <math alttext=\"\\mathcal{L}_{\\text{mask}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>mask</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{mask}}</annotation></semantics></math>, which reconstructs randomly masked acoustic tokens from noisy inputs using a masked denoising objective; (2) a coarse-to-fine contrastive loss <math alttext=\"\\mathcal{L}_{\\text{CL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CL}}</annotation></semantics></math>, composed of sequence-level (<math alttext=\"\\mathcal{L}_{\\text{SCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>SCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{SCL}}</annotation></semantics></math>) and frame-level (<math alttext=\"\\mathcal{L}_{\\text{FCL}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>FCL</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{FCL}}</annotation></semantics></math>) terms, to enforce consistency between the melody condition and generated acoustic tokens while mitigating prosody leakage from the prompt; and (3) an auxiliary SVT loss <math alttext=\"\\mathcal{L}_{\\text{SVT}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>SVT</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{SVT}}</annotation></semantics></math>, which encourages the predicted acoustic tokens to be rhythmically and melodically consistent with the SVT-inferred pitch contour. The fine-tuning algorithm for the S2A model is summarized in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#alg1\" title=\"Algorithm 1 &#8227; III-D Training and Inference Procedures &#8227; III Method &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Each training sample <math alttext=\"\\mathbf{x}_{k}\\in\\mathcal{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#119857;</mi><mi>k</mi></msub><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#8492;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{x}_{k}\\in\\mathcal{B}</annotation></semantics></math> comprises a semantic sequence <math alttext=\"\\mathbf{s}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m7\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{k}</annotation></semantics></math>, a regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m8\" intent=\":literal\"><semantics><msubsup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>k</mi><mi>p</mi></msubsup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}_{k}</annotation></semantics></math>, and a time-aligned acoustic token sequence <math alttext=\"\\mathbf{a}_{k,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m9\" intent=\":literal\"><semantics><msub><mi>&#119834;</mi><mrow><mi>k</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{a}_{k,t}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "svt",
                    "sequence",
                    "model",
                    "objective",
                    "contrastive",
                    "coarsetofine",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments on two publicly available mandarin singing corpora: M4Singer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib23\" title=\"\">23</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/M4Singer/M4Singer\" title=\"\">https://github.com/M4Singer/M4Singer</a></span></span></span> and Opencpop <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib22\" title=\"\">22</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://xinshengwang.github.io/opencpop/\" title=\"\">https://xinshengwang.github.io/opencpop/</a></span></span></span>. The M4Singer dataset comprises studio-quality recordings from 20 professional singers spanning SATB vocal ranges, along with comprehensive annotations including lyrics, pitch, note duration, and slur information. The Opencpop dataset contains 100 Chinese pop songs sung by a professional female vocalist, with precise phoneme, note, and syllable-level annotations aligned to the music score. For evaluation, we construct both seen- and unseen-singer test sets. For seen-singer evaluation, we randomly select 50 utterances each from the M4Singer and Opencpop datasets. For zero-shot (unseen-singer) evaluation, we use 10 male and 10 female singers from the OpenSinger <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib65\" title=\"\">65</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Multi-Singer/Multi-Singer.github.io?tab=readme-ov-file\" title=\"\">https://github.com/Multi-Singer/Multi-Singer.github.io?tab=readme-ov-file</a></span></span></span> dataset. Since OpenSinger lacks complete music score annotations, we pair it with M4Singer&#8217;s score sequences to enable evaluation. All audio is uniformly down-sampled to 24 kHz with 16-bit quantization.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To fine-tune the S2A model, we preprocess the dataset to obtain temporally aligned semantic tokens <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math>, acoustic tokens <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>, and regulated pitch tokens <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>. All audio segments are first converted to mono and resampled to 24&#160;kHz. We then utilize the pretrained semantic and acoustic codec models from the MaskGCT framework<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/open-mmlab/Amphion/tree/main/models/tts/maskgct\" title=\"\">https://github.com/open-mmlab/Amphion/tree/main/models/tts/maskgct</a></span></span></span> to extract <math alttext=\"\\mathbf{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mi>&#119852;</mi><annotation encoding=\"application/x-tex\">\\mathbf{s}</annotation></semantics></math> and <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math>. For samples with mismatched token lengths, we apply zero-padding to align their temporal dimensions for frame-level supervision. The regulated pitch sequence <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math> is derived by expanding the original pitch sequence <math alttext=\"\\mathbf{m}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{p}</annotation></semantics></math> according to the duration sequence <math alttext=\"\\mathbf{m}^{d}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m8\" intent=\":literal\"><semantics><msup><mi>&#119846;</mi><mi>d</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{m}^{d}</annotation></semantics></math>, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S3.SS1\" title=\"III-A Overview &#8227; III Method &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">III-A</span></a>. The SVT model is trained using the preprocessed acoustic tokens <math alttext=\"\\mathbf{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m9\" intent=\":literal\"><semantics><mi>&#119834;</mi><annotation encoding=\"application/x-tex\">\\mathbf{a}</annotation></semantics></math> and the corresponding regulated pitch tokens <math alttext=\"\\tilde{\\mathbf{m}}^{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m10\" intent=\":literal\"><semantics><msup><mover accent=\"true\"><mi>&#119846;</mi><mo>~</mo></mover><mi>p</mi></msup><annotation encoding=\"application/x-tex\">\\tilde{\\mathbf{m}}^{p}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "svt",
                    "sequence",
                    "framelevel",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The SVT model is trained on a single NVIDIA RTX A5000 GPU using the AdamW optimizer with a learning rate of 1e&#8722;5, weight decay of 0.01, and a cosine learning rate schedule with 5K warm-up steps over 50K updates. Training is performed for 100 epochs with a batch size of 32 using mixed-precision (FP16) computation. Subsequently, the S2A model is fine-tuned on four NVIDIA RTX A5000 GPUs with data parallelism. We adopt the AdamW optimizer with a learning rate of 1e&#8722;5, 32K warm-up steps, and the inverse square root learning rate schedule. Fine-tuning is conducted for 300K steps with a total batch size of 32, where the first 8 samples are used for sequence contrastive learning and the remaining 24 for frame-level contrastive learning. We apply dropout (0.1), label smoothing (0.1), and gradient clipping to stabilize training. During this stage, all model components are frozen except the S2A decoder. The loss weights are set as follows: <math alttext=\"\\lambda_{\\text{SCL}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>SCL</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{SCL}}=0.5</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{FCL}}=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>FCL</mtext></msub><mo>=</mo><mn>1.0</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{FCL}}=1.0</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{CL}}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>CL</mtext></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{CL}}=0.1</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{seg}}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>seg</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{seg}}=0.5</annotation></semantics></math>, <math alttext=\"\\lambda_{\\text{dur}}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>dur</mtext></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{dur}}=0.3</annotation></semantics></math>, and <math alttext=\"\\lambda_{\\text{SVT}}=0.2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mtext>SVT</mtext></msub><mo>=</mo><mn>0.2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda_{\\text{SVT}}=0.2</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "svt",
                    "sequence",
                    "model",
                    "contrastive",
                    "where",
                    "framelevel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the performance of our system in terms of pitch accuracy, timbre consistency, and perceptual quality, we conduct objective evaluations under both seen-singer and zero-shot settings. The following metrics are employed:</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MCD is used to evaluate spectral fidelity by computing the frame-wise Euclidean distance between mel-cepstral coefficients of the synthesized and reference audio. It serves as a proxy for spectral similarity, where lower values indicate more accurate spectral reconstruction and reduced distortion.</p>\n\n",
                "matched_terms": [
                    "where",
                    "mcd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">F0-RMSE measures pitch prediction accuracy by calculating the root mean squared error between the fundamental frequency (F0) trajectories of the generated and reference waveforms. A lower F0-RMSE reflects better alignment with the intended melody and more precise pitch control.</p>\n\n",
                "matched_terms": [
                    "f0rmse",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess timbre similarity, we compute cosine similarity between speaker embeddings extracted from the synthesized and reference audio using a WavLM-based speaker verification model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib66\" title=\"\">66</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/microsoft/wavlm-base-sv\" title=\"\">https://huggingface.co/microsoft/wavlm-base-sv</a></span></span></span>. SECS values range from 0 to 1, with higher scores indicating closer alignment in vocal identity.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib67\" title=\"\">67</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/South-Twilight/SingMOS\" title=\"\">https://github.com/South-Twilight/SingMOS</a></span></span></span> is a learned metric trained to predict human perceptual ratings of singing voice quality. It is based on a curated dataset of professional ratings for both natural and synthesized singing in Chinese and Japanese, addressing the annotation scarcity in the singing domain. SingMOS produces scores in the range of 0 to 5, with higher values indicating greater naturalness and perceptual quality. As a reference-free metric, it enables scalable automatic evaluation in zero-shot and low-resource conditions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess perceptual quality, we conducted a Mean Opinion Score (MOS) evaluation with 20 participants who have formal training in singing and experience in vocal performance <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>This study has been approved by the Department Ethics Review Committee\n(DERC) at the National University of Singapore under\nDERC Ref Code: 000479.</span></span></span>. Each participant rated the synthesized samples based on overall naturalness (MOS-N), audio quality (MOS-Q), and timbre similarity (SMOS). A 5-point Likert scale was used, where a score of 5 indicates excellent perceptual quality and 1 denotes poor quality.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "where",
                    "study"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Several recent TTS systems&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib68\" title=\"\">68</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib69\" title=\"\">69</a>]</cite> have reported high prosodic similarity between the speech prompt and the synthesized output. While this may appear beneficial in TTS, it reveals a form of prosody leakage, where expressive cues from the prompt inadvertently influence the generated speech. This issue becomes particularly problematic in singing voice synthesis (SVS), where pitch and rhythm should be governed solely by the input music score. As discussed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S1\" title=\"I Introduction &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>, we refer to this phenomenon as prosody leakage.</p>\n\n",
                "matched_terms": [
                    "where",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate whether MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib18\" title=\"\">18</a>]</cite> exhibits such behavior, we conduct a prosody similarity analysis following prior evaluation protocols. NaturalSpeech 2 and 3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib19\" title=\"\">19</a>]</cite> quantify prosodic similarity by comparing pitch and duration features between the prompt and output, while StyleTTS-ZS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib69\" title=\"\">69</a>]</cite> computes Pearson correlation coefficients of acoustic features to evaluate prosodic alignment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Inspired by these approaches, we evaluate the prosodic similarity of MaskGCT in both English and Mandarin using the LibriTTS <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib25\" title=\"\">25</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.openslr.org/60/\" title=\"\">https://www.openslr.org/60/</a></span></span></span> and AISHELL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib70\" title=\"\">70</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openslr.org/93/\" title=\"\">https://openslr.org/93/</a></span></span></span> datasets, respectively. For each speaker, we randomly sample 50 utterances to construct the test sets. During inference, we synthesize 50 utterances per dataset by conditioning on the same target text but using different speech prompts. We then compute the acoustic-level similarity between each synthesized utterance and: (1) its paired prompt (i.e., the one used during generation), and (2) an unpaired prompt from the same speaker. This comparison allows us to quantify the extent of prompt-induced prosody similarity, which serves as an indicator of potential prosody leakage in the model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T1\" title=\"TABLE I &#8227; V-A Evaluating Prompt-Induced Prosody Similarity in MaskGCT &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> presents a quantitative analysis of prompt-induced prosody similarity by comparing the acoustic differences between synthesized and prompt speech under paired and unpaired conditions. Across both LibriTTS and AISHELL-3, paired prompts consistently yield lower differences in pitch, energy, and other prosodic indicators, confirming stronger alignment in prosodic patterns. In contrast, the unpaired condition results in noticeably higher deviations, particularly in pitch mean, energy mean, and jitter, suggesting that the synthesized outputs are heavily influenced by the prosodic characteristics of the prompt.</p>\n\n",
                "matched_terms": [
                    "results",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the seen singer evaluation, we compare CoMelSinger with four strong baseline systems: DiffSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib4\" title=\"\">4</a>]</cite>, VISinger2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib9\" title=\"\">9</a>]</cite>, SPSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib13\" title=\"\">13</a>]</cite>, and StyleSinger&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib7\" title=\"\">7</a>]</cite>. To ensure a fair comparison, all models adopt HiFi-GAN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib71\" title=\"\">71</a>]</cite> as the vocoder during both training and inference. As presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T2\" title=\"TABLE II &#8227; V-A Evaluating Prompt-Induced Prosody Similarity in MaskGCT &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">II</span></a>, CoMelSinger achieves the highest scores across both subjective and objective metrics, demonstrating its capability to synthesize natural and expressive singing voices from seen singers.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "comelsinger",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first note that the performance gap between the Ground Truth (GT) and GT with Acoustic Codec is minimal across all metrics, confirming that the discrete acoustic token representation introduces negligible degradation and establishing a strong upper bound for token-based SVS systems. CoMelSinger approaches this bound closely, suggesting that its improvements arise from architectural designs&#8212;particularly the disentangled modeling of melody and timbre&#8212;rather than signal-level enhancements. Among all models, CoMelSinger achieves the highest SMOS and SECS scores, indicating strong timbre consistency and accurate preservation of speaker identity, which validates the effectiveness of the in-context prompting mechanism. It also attains the lowest F0-RMSE and one of the highest SingMOS scores, reflecting precise melody reproduction and high perceptual naturalness. Furthermore, its competitive MCD score demonstrates the model&#8217;s ability to reconstruct spectral features with smooth and consistent vocal quality, confirming the effectiveness of the proposed structured melody control strategy.</p>\n\n",
                "matched_terms": [
                    "strategy",
                    "comelsinger",
                    "f0rmse",
                    "mcd",
                    "secs",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further evaluate CoMelSinger in a zero-shot setting, where the model synthesizes singing voices from speakers not seen during training. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T3\" title=\"TABLE III &#8227; V-C Zero-Shot Singing Voice Synthesis &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">III</span></a>, CoMelSinger maintains strong performance across all subjective and objective metrics, exhibiting only minimal degradation compared to the seen condition.</p>\n\n",
                "matched_terms": [
                    "where",
                    "model",
                    "comelsinger",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, baseline systems show notable declines in key timbre- and melody-related metrics such as SMOS, SECS, and F0-RMSE, underscoring their limited ability to generalize to unseen vocal identities. CoMelSinger&#8217;s robustness in zero-shot scenarios is attributed to the synergy between in-context prompting&#8212;which leverages short acoustic references to anchor timbre&#8212;and large-scale speech pretraining, which imparts transferable prosodic priors.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "f0rmse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the inherent challenge of handling unseen timbres, CoMelSinger continues to achieve high speaker similarity while preserving accurate pitch trajectories. This balance between identity retention and melodic fidelity demonstrates the model&#8217;s strong generalization capacity. While many existing approaches face trade-offs between controllability and naturalness, CoMelSinger effectively reconciles both through its structured architecture and explicit conditioning scheme. These findings position CoMelSinger as a strong baseline for zero-shot singing voice synthesis with discrete representations.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "comelsinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To isolate the effects of each contrastive branch, we further ablate sequence contrastive learning (SCL) and frame-level contrastive learning (FCL) individually. Excluding SCL moderately affects MCD and SECS, suggesting its role in maintaining global speaker identity. In contrast, removing FCL causes a larger drop in F0-RMSE and SingMOS, confirming its effectiveness in modeling fine-grained pitch details and promoting melodic continuity. These results validate the hierarchical design of our contrastive learning framework.</p>\n\n",
                "matched_terms": [
                    "scl",
                    "learning",
                    "pitch",
                    "sequence",
                    "f0rmse",
                    "mcd",
                    "fcl",
                    "results",
                    "secs",
                    "contrastive",
                    "framelevel",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate the impact of the singing voice transcription (SVT) module, which provides auxiliary pitch supervision. Excluding SVT results in higher F0-RMSE and lower SingMOS, confirming the benefit of explicit alignment signals for structured melody control. The most severe degradation occurs when both CL and SVT are removed, indicating their complementary roles in pitch&#8211;timbre disentanglement and temporal stability.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "svt",
                    "f0rmse",
                    "results",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.F5\" title=\"Figure 5 &#8227; Component Analysis &#8227; V-D Ablation Study &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> visualizes mel-spectrograms and pitch contours for the ground truth, our model, and two ablated variants. Predicted pitch trajectories (red) are overlaid with ground-truth pitch (blue), with word-level boundaries marked by dashed lines and pinyin annotations. Compared to the ablated models, CoMelSinger achieves better pitch alignment and smoother contours, illustrating the effectiveness of both CL and SVT in preserving melodic structure.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "model",
                    "comelsinger",
                    "svt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T5\" title=\"TABLE V &#8227; Comparison of Fine-Tuning Strategies &#8227; V-D Ablation Study &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> presents a comparison of six fine-tuning strategies in terms of both objective and subjective performance, along with their respective trainable parameter ratios. FT-LoRA delivers the best overall performance, achieving the lowest F0-RMSE, highest SingMOS, and highest SECS, while updating only 4.81% of the parameters&#8212;highlighting the effectiveness of low-rank adaptation for efficient model tuning. FT-PGS achieves the lowest MCD, suggesting enhanced spectral fidelity through gradual unfreezing, though its pitch accuracy is affected by delayed optimization of lower layers. FT-Prefix and FT-Pitch yield consistent results with minimal overhead, demonstrating the utility of lightweight adaptation modules. In contrast, FT-LLRD and FT-Full fine-tune all parameters yet underperform across most metrics, indicating that full-capacity adaptation may lead to overfitting or instability in data-scarce settings. These results underscore that parameter-efficient strategies, particularly LoRA, can match or surpass full-model fine-tuning while substantially reducing computational cost.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "f0rmse",
                    "mcd",
                    "objective",
                    "results",
                    "secs",
                    "model",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we present CoMelSinger, a zero-shot singing voice synthesis framework that extends discrete token-based TTS models to support structured and controllable melody generation. Built upon the non-autoregressive MaskGCT architecture, CoMelSinger incorporates lyrics and pitch tokens as inputs, enabling fine-grained alignment between the musical score and the generated voice. To address the challenge of prosody leakage from prompt-based conditioning&#8212;an issue unique to singing synthesis&#8212;we propose a coarse-to-fine contrastive learning strategy that explicitly disentangles pitch information from the timbre prompt. Furthermore, we introduce a lightweight singing voice transcription (SVT) module to provide frame-level pitch and duration supervision, enhancing the model&#8217;s ability to follow the intended melody with precision. Extensive experiments on both seen and unseen singers demonstrate that CoMelSinger achieves strong zero-shot generalization, consistently outperforming competitive SVS baselines in pitch accuracy, timbre consistency, and subjective quality. Our results confirm that structured melody control and contrastive disentanglement are essential for scalable and expressive singing synthesis. We believe CoMelSinger opens new possibilities for discrete token-based SVS, enabling scalable and zero-shot singing generation.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "pitch",
                    "strategy",
                    "comelsinger",
                    "svt",
                    "results",
                    "contrastive",
                    "coarsetofine",
                    "framelevel"
                ]
            }
        ]
    },
    "S5.T5": {
        "source_file": "CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance",
        "caption": "TABLE V: Performance of various fine-tuning strategies with differing trainable parameter ratios.",
        "body": "Method\nMCD ↓\\downarrow\n\nF0-RMSE ↓\\downarrow\n\nSingMOS ↑\\uparrow\n\nSECS ↑\\uparrow\n\nTrainable (%)\n\n\n\n\nFT-LoRA\n4.26\n0.053\n4.34\n0.920\n6.51%\n\n\nFT-LLRD\n4.33\n0.069\n4.31\n0.894\n100.00%\n\n\nFT-Pitch\n4.47\n0.062\n5.95\n0.902\n4.46%\n\n\nFT-Prefix\n4.52\n0.059\n6.13\n0.911\n5.25%\n\n\nFT-PGS\n4.21\n0.084\n4.26\n0.887\n16.63%∼\\sim16.63%\n\n\nFT-Full\n4.41\n0.099\n4.13\n0.859\n100.00%",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MCD <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F0-RMSE <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SingMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SECS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Trainable (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">FT-LoRA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.053</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.920</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.51%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FT-LLRD</th>\n<td class=\"ltx_td ltx_align_center\">4.33</td>\n<td class=\"ltx_td ltx_align_center\">0.069</td>\n<td class=\"ltx_td ltx_align_center\">4.31</td>\n<td class=\"ltx_td ltx_align_center\">0.894</td>\n<td class=\"ltx_td ltx_align_center\">100.00%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FT-Pitch</th>\n<td class=\"ltx_td ltx_align_center\">4.47</td>\n<td class=\"ltx_td ltx_align_center\">0.062</td>\n<td class=\"ltx_td ltx_align_center\">5.95</td>\n<td class=\"ltx_td ltx_align_center\">0.902</td>\n<td class=\"ltx_td ltx_align_center\">4.46%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FT-Prefix</th>\n<td class=\"ltx_td ltx_align_center\">4.52</td>\n<td class=\"ltx_td ltx_align_center\">0.059</td>\n<td class=\"ltx_td ltx_align_center\">6.13</td>\n<td class=\"ltx_td ltx_align_center\">0.911</td>\n<td class=\"ltx_td ltx_align_center\">5.25%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FT-PGS</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.21</span></td>\n<td class=\"ltx_td ltx_align_center\">0.084</td>\n<td class=\"ltx_td ltx_align_center\">4.26</td>\n<td class=\"ltx_td ltx_align_center\">0.887</td>\n<td class=\"ltx_td ltx_align_center\">16.63%<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m5\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>16.63%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">FT-Full</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.099</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.859</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">100.00%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "f0rmse",
            "ftpgs",
            "↑uparrow",
            "1663∼sim1663",
            "mcd",
            "ftprefix",
            "ftfull",
            "various",
            "method",
            "performance",
            "↓downarrow",
            "ratios",
            "secs",
            "strategies",
            "ftlora",
            "parameter",
            "ftpitch",
            "trainable",
            "finetuning",
            "ftllrd",
            "differing",
            "singmos"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T5\" title=\"TABLE V &#8227; Comparison of Fine-Tuning Strategies &#8227; V-D Ablation Study &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">V</span></a> presents a comparison of six fine-tuning strategies in terms of both objective and subjective performance, along with their respective trainable parameter ratios. FT-LoRA delivers the best overall performance, achieving the lowest F0-RMSE, highest SingMOS, and highest SECS, while updating only 4.81% of the parameters&#8212;highlighting the effectiveness of low-rank adaptation for efficient model tuning. FT-PGS achieves the lowest MCD, suggesting enhanced spectral fidelity through gradual unfreezing, though its pitch accuracy is affected by delayed optimization of lower layers. FT-Prefix and FT-Pitch yield consistent results with minimal overhead, demonstrating the utility of lightweight adaptation modules. In contrast, FT-LLRD and FT-Full fine-tune all parameters yet underperform across most metrics, indicating that full-capacity adaptation may lead to overfitting or instability in data-scarce settings. These results underscore that parameter-efficient strategies, particularly LoRA, can match or surpass full-model fine-tuning while substantially reducing computational cost.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To improve adaptation efficiency and reduce overfitting, we apply Low-Rank Adaptation (LoRA)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#bib.bib64\" title=\"\">64</a>]</cite> to fine-tune the S2A model&#8217;s diffusion estimator module, which is implemented using a DiffLlama-style architecture. LoRA introduces trainable low-rank matrices into the linear layers of pretrained models, enabling efficient fine-tuning by updating only a small subset of parameters while keeping the original weights frozen. This allows our model to retain prior knowledge from the TTS domain while adapting to the stylistic nuances of singing voice synthesis with limited data.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "trainable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first note that the performance gap between the Ground Truth (GT) and GT with Acoustic Codec is minimal across all metrics, confirming that the discrete acoustic token representation introduces negligible degradation and establishing a strong upper bound for token-based SVS systems. CoMelSinger approaches this bound closely, suggesting that its improvements arise from architectural designs&#8212;particularly the disentangled modeling of melody and timbre&#8212;rather than signal-level enhancements. Among all models, CoMelSinger achieves the highest SMOS and SECS scores, indicating strong timbre consistency and accurate preservation of speaker identity, which validates the effectiveness of the in-context prompting mechanism. It also attains the lowest F0-RMSE and one of the highest SingMOS scores, reflecting precise melody reproduction and high perceptual naturalness. Furthermore, its competitive MCD score demonstrates the model&#8217;s ability to reconstruct spectral features with smooth and consistent vocal quality, confirming the effectiveness of the proposed structured melody control strategy.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "f0rmse",
                    "mcd",
                    "secs",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, baseline systems show notable declines in key timbre- and melody-related metrics such as SMOS, SECS, and F0-RMSE, underscoring their limited ability to generalize to unseen vocal identities. CoMelSinger&#8217;s robustness in zero-shot scenarios is attributed to the synergy between in-context prompting&#8212;which leverages short acoustic references to anchor timbre&#8212;and large-scale speech pretraining, which imparts transferable prosodic priors.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "f0rmse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the contribution of each component in CoMelSinger, we perform ablation studies by systematically disabling key modules. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.19883v1#S5.T4\" title=\"TABLE IV &#8227; V-D Ablation Study &#8227; V Experimental Results &#8227; CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance\"><span class=\"ltx_text ltx_ref_tag\">IV</span></a> reports results on four objective metrics: MCD, F0-RMSE, SingMOS, and SECS. Removing the entire coarse-to-fine contrastive learning (CL) framework leads to substantial degradation across all metrics, indicating reduced pitch accuracy and speaker consistency. This highlights the importance of contrastive objectives in disentangling pitch from timbre and improving input&#8211;output alignment.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "f0rmse",
                    "mcd",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To isolate the effects of each contrastive branch, we further ablate sequence contrastive learning (SCL) and frame-level contrastive learning (FCL) individually. Excluding SCL moderately affects MCD and SECS, suggesting its role in maintaining global speaker identity. In contrast, removing FCL causes a larger drop in F0-RMSE and SingMOS, confirming its effectiveness in modeling fine-grained pitch details and promoting melodic continuity. These results validate the hierarchical design of our contrastive learning framework.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "f0rmse",
                    "mcd",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate the impact of the singing voice transcription (SVT) module, which provides auxiliary pitch supervision. Excluding SVT results in higher F0-RMSE and lower SingMOS, confirming the benefit of explicit alignment signals for structured melody control. The most severe degradation occurs when both CL and SVT are removed, indicating their complementary roles in pitch&#8211;timbre disentanglement and temporal stability.</p>\n\n",
                "matched_terms": [
                    "f0rmse",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate six representative fine-tuning strategies on the DiffLlama backbone, each trained for 1000 epochs under identical schedules. The comparison highlights trade-offs between parameter efficiency, adaptation capacity, and overfitting risk under limited SVS data.</p>\n\n",
                "matched_terms": [
                    "parameter",
                    "finetuning",
                    "strategies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FT-LoRA</span>: Applies LoRA to self-attention projections with <math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=32\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=32</annotation></semantics></math>, and dropout 0.1. Only the pitch encoder, output head, and cond module are trainable.</p>\n\n",
                "matched_terms": [
                    "ftlora",
                    "trainable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FT-PGS</span>: Unfreezes two upper DiffLlama layers every 200 epochs, progressively increasing trainable capacity.</p>\n\n",
                "matched_terms": [
                    "ftpgs",
                    "trainable"
                ]
            }
        ]
    }
}