{
    "S3.T1": {
        "caption": "Table 1: Comparison of EchoFake with existing audio deepfake detection datasets. “#utt” denotes the number of utterances, “#spk” the number of speakers, and “#gen” the number of generation methods.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Year</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">#utt</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">#spk</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">#gen</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">ASVspoof 2019 LA</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">2019</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">122,299</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">107</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">19</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">WaveFake</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2021</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">117,985</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">ASVspoof 2021 LA</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2021</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">181,566</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">67</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">13</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">ASVspoof 2021 DF</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2021</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">611,829</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">100+</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">In-the-Wild</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2022</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">31,779</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">58</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">MLAAD</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2024</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">82,000</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">101</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">ASVspoof 5</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2024</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1,211,186</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1,922</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">28</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">EchoFake (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">2025</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">81,890</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">13,005</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">11</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "“utt”",
            "detection",
            "denotes",
            "comparison",
            "spk",
            "mlaad",
            "“gen”",
            "audio",
            "generation",
            "speakers",
            "methods",
            "“spk”",
            "inthewild",
            "echofake",
            "wavefake",
            "year",
            "deepfake",
            "datasets",
            "number",
            "ours",
            "existing",
            "gen",
            "asvspoof",
            "utt",
            "dataset",
            "utterances"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The proposed </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">EchoFake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset comprises 81,890 utterances from 13,005 speakers and covers 11 different TTS systems. A comprehensive comparison of EchoFake with established datasets is summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1.2 TTS Model Selection &#8227; 3.1 Dataset Construction &#8227; 3 EchoFake Dataset &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. While EchoFake contains fewer utterances than mainstream datasets, it outperforms them in speaker diversity, ensuring broader representation of vocal characteristics and accent variations.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The growing prevalence of speech deepfakes has raised serious concerns, particularly in real-world scenarios such as telephone fraud and identity theft. While many anti-spoofing systems have demonstrated promising performance on lab-generated synthetic speech, they often fail when confronted with physical replay attacks&#8212;a common and low-cost form of attack used in practical settings. Our experiments show that models trained on existing datasets exhibit severe performance degradation, with average accuracy dropping to 59.6% when evaluated on replayed audio. To bridge this gap, we present EchoFake, a comprehensive dataset comprising more than 120 hours of audio from over 13,000 speakers, featuring both cutting-edge zero-shot text-to-speech (TTS) speech and physical replay recordings collected under varied devices and real-world environmental settings. Additionally, we evaluate three baseline detection models and show that models trained on EchoFake achieve lower average EERs across datasets, indicating better generalization. By introducing more practical challenges relevant to real-world deployment, EchoFake offers a more realistic foundation for advancing spoofing detection methods.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "existing",
                    "audio",
                    "detection",
                    "speakers",
                    "methods",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nAnti-spoofing dataset, speech deepfake detection, replay attack</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "deepfake",
                    "dataset",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The recent advances in zero-shot text-to-speech (TTS) and large-scale audio language models (ALM) have dramatically lowered the barrier to generating high-quality synthetic speech. With only a few seconds of reference audio, these models can convincingly clone a speaker&#8217;s voice, producing speech that is perceptually indistinguishable from genuine utterances. While these technologies offer exciting applications in personalized voice assistants, voiceovers, and accessibility, they also pose a serious threat to security and public trust. The proliferation of voice cloning tools has opened up new avenues for audio-based forgery, impersonation, and misinformation.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While many audio deepfake detection (ADD) systems achieve strong performance on in-domain benchmarks, their robustness in real-world conditions remains a major concern. A key reason is overfitting&#8212;many ADD models are trained on clean, studio-quality datasets such as ASVspoof2019 logical access (LA) track, which leads to poor generalization when deployed in the wild. To mitigate overfitting, several datasets have been proposed to simulate more realistic conditions. ASVspoof 2021</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LA track focuses on telephone-channel scenarios, while ASVspoof 2021 deepfake (DF) track addresses challenges from lossy compression and encoding. The InTheWild </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset further collects fake and genuine samples from social media platforms to increase data diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dataset",
                    "detection",
                    "asvspoof",
                    "deepfake",
                    "inthewild",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite recent progress, three major challenges persist for audio deepfake detection (ADD). First, models often misclassify bona fide speech recorded on consumer devices as spoofed, raising serious usability concerns (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">a). Second, many systems are trained on outdated spoofing methods (e.g., ASVspoof 2019 LA), failing to reflect advances in neural TTS and voice cloning. Most critically, detection remains highly vulnerable to replay attacks: by playing synthetic audio through a speaker and re-recording it, attackers can mask artifacts and deceive models into accepting it as genuine speech (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">b). In high-stakes applications such as telephone fraud, adversaries may even replay authentic voice snippets from prior conversations or social media, making detection especially difficult since the audio truly originates from the victim.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "detection",
                    "asvspoof",
                    "methods",
                    "deepfake"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Existing benchmarks like ASVspoof 2019 physical access (PA) and 2021 PA tracks have laid the groundwork for replay attack detection, yet their datasets rely on software-simulated playback rather than actual physical recordings. More critically, while the PA tracks contain replayed bona fide speech, they lack any instances of replayed deepfake speech. This omission limits their relevance to emerging threats, where attackers may clone a victim&#8217;s voice and physically replay it to bypass anti-spoofing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "existing",
                    "detection",
                    "asvspoof",
                    "deepfake",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these challenges, we introduce EchoFake, a novel dataset that integrates zero-shot TTS deepfakes with varied physical replay conditions, offering a more realistic benchmark for spoofing detection. Using EchoFake, we reveal that existing anti-spoofing models suffer severe performance drops under diverse replay scenarios and remain vulnerable even after training, especially when distinguishing replayed bona fide speech. Nevertheless, incorporating replay diversity improves generalization: models trained on EchoFake achieve lower average EERs across multiple benchmarks, highlighting both the weaknesses of prior datasets and the benefits of realistic replay data for robust detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "existing",
                    "detection",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Audio deepfake detection aims to distinguish bona fide utterances from spoofed or synthesized ones. Existing approaches can be broadly categorized into pipeline-based and end-to-end paradigms. 1) Pipeline-based detectors&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> typically adopt a two-stage strategy: handcrafted or pretrained features&#8212;such as mel-frequency cepstral coefficients (MFCCs), linear frequency cepstral coefficients (LFCCs), or wav2vec2 embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;are first extracted, followed by a separate classifier for final prediction. This modular design allows flexible integration of domain knowledge and facilitates interpretability and transferability across datasets and spoofing conditions. 2) End-to-end methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, on the other hand, learn task-specific representations directly from raw waveforms by jointly optimizing feature extraction and classification. These models benefit from strong representation learning capabilities and can automatically adapt to data variations without manual feature engineering.</span>\n</p>\n\n",
                "matched_terms": [
                    "existing",
                    "audio",
                    "detection",
                    "methods",
                    "deepfake",
                    "utterances",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A variety of datasets have been developed to support research in audio deepfake detection. Classic benchmarks such as ASVspoof 2019 LA and ASVspoof 2021&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib1\" title=\"\">1</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focused on spoofing attacks targeting automatic speaker verification systems. The In-the-Wild&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset collects synthetic speech samples of celebrities from the Internet, offering a more realistic setting for deepfake detection. The ADD challenge series&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ASVspoof 5&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> incorporates codec artifacts and multi-domain content for broader coverage. However, most datasets focus solely on either synthesis or replay. Our proposed EchoFake dataset fills this gap by integrating TTS-generated and replayed speech under diverse recording configurations, supporting more comprehensive model evaluation under realistic deployment conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "audio",
                    "dataset",
                    "detection",
                    "asvspoof",
                    "deepfake",
                    "inthewild",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To tackle the mismatch between lab-generated synthetic datasets and real-world spoofing scenarios involving replay attacks, we construct EchoFake, a new dataset that broadens the scope of audio deepfake detection beyond synthetic samples alone.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "audio",
                    "dataset",
                    "detection",
                    "deepfake",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">EchoFake consists of four subsets: training, development, closed-set evaluation, and open-set evaluation. The first three share the same pool of speakers and TTS systems to support in-distribution training and tuning, while the open-set set introduces unseen speakers, new spoofing systems, and diverse replay conditions, providing a rigorous test of model generalization in realistic settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S3.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 EchoFake Dataset &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, bona fide speech samples are directly extracted from the CommonVoice 17.0 dataset. Half of these samples were replayed to construct the replayed bona fide subset. For fake speech, we randomly sampled source texts and reference speech clips from CommonVoice and then use zero-shot TTS models to synthesize new utterances with the target speaker&#8217;s voice cloned from the reference speech clips. Similarly, 50% of the generated utterances are replayed to obtain the replayed fake subset. All sampling operations are strictly non-redundant to maximize diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the open-set evaluation, we introduced previously unseen configurations to test model generalization. Specifically, playback was performed using Edifier MR4 powered studio monitor speakers and a Xiaomi 13 Ultra smartphone, while recording was carried out with another Xiaomi 13 Ultra or a pair of wired earbuds with a built-in microphone. Recording was conducted in a larger office room (18.6 &#215; 13.2 &#215; 3.2 m) at a microphone&#8211;speaker distance of 30 cm. These settings produce four additional unseen replay conditions that further increase the diversity and challenge of EchoFake.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "speakers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All four subsets undergo the same preprocessing pipeline. First, volume normalization is performed using </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">ffmpeg</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with the integrated loudness adjusted to </span>\n  <math alttext=\"-23\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">23</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-23</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LUFS, a loudness range of </span>\n  <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">7</mn>\n      <annotation encoding=\"application/x-tex\">7</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LU, and a true peak limit of </span>\n  <math alttext=\"-2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dBTP. Then, MP3 compression is applied with a bitrate of 64 kbps, a sampling rate of 16 kHz, and mono-channel output to maintain compatibility with the CommonVoice dataset and simulate real-world audio degradation in social media platforms.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">EchoFake is partitioned into four subsets: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">train</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">dev</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">closed-set evaluation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">open-set evaluation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The distribution of utterances across these splits is detailed in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.1.2 TTS Model Selection &#8227; 3.1 Dataset Construction &#8227; 3 EchoFake Dataset &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We maintain a balanced number of samples across four data categories&#8212;</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">bona fide</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">replayed bona fide</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">fake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">replayed fake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;resulting in a total of 126.4 hours of audio data. In addition, detailed metadata is provided for each utterance, including speaker identity, reference speech information, synthesis method, and recording environment. All metadata and dataset construction scripts has been released alongside the dataset to facilitate reproducible research. </span>\n  <span class=\"ltx_note ltx_role_footnotemark\" id=\"footnotex3\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_note_type\">footnotemark: </span>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "audio",
                    "dataset",
                    "utterances",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate model robustness under realistic spoofing scenarios, we examine detection performance on the EchoFake dataset under two classification settings: a four-class detection setup and a conventional binary classification task.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "dataset",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In both four-class and binary detection tasks, models achieve strong performance in closed-set conditions but degrade sharply in open-set scenarios, with replayed samples being the main source of errors. Replayed bona fide (RB) speech is particularly difficult to detect due to its lack of synthetic artifacts and close resemblance to genuine speech, while replayed fake (RF) samples further complicate classification by combining synthesis traces with channel distortions. In contrast, purely synthetic speech remains the easiest to identify, suggesting that existing models overly rely on spectral artifacts. These results reveal a critical weakness of current anti-spoofing systems: high-fidelity replay attacks can effectively mask or distort discriminative cues, leading to substantial misclassification and undermining robustness in real-world conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "existing",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To assess cross-dataset generalization capabilities, we conducted systematic evaluations of spoofing detection systems across multiple benchmarks. In this experiment, the RB samples from EchoFake is treated as spoofed audio, alongside F and RF samples, to align with the binary classification setting of existing datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "existing",
                    "audio",
                    "detection",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To examine the impact of replay data, we retrained all baseline models on the EchoFake training set after removing replay samples and compared their performance with models trained on the full dataset (For fair comparison, we downsampled the original training set to match the replay-excluded set&#8217;s sample size). When evaluated across five conventional benchmarks (ASV19LA, ASV21LA, ASV21DF, In-the-Wild, and WaveFake), the EER changes after removing replay data were marginal: </span>\n  <math alttext=\"-0.03\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">0.03</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-0.03\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (RawNet), </span>\n  <math alttext=\"+1.67\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">1.67</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+1.67\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (AASIST), and </span>\n  <math alttext=\"+4.07\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">4.07</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+4.07\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (wav2vec2). However, when including the EchoFake-open set for evaluation (six datasets in total), average EERs increased by </span>\n  <math alttext=\"+1.54\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">1.54</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+1.54\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"+5.26\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">5.26</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+5.26\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"+6.22\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">6.22</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+6.22\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively. These results demonstrate that incorporating replay data in training provides a favorable trade-off: it does not significantly degrade performance on conventional benchmarks while substantially enhancing robustness against replay-based attacks.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "datasets",
                    "wavefake",
                    "dataset",
                    "inthewild",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we present EchoFake, a novel and comprehensive dataset designed to advance ADD system development under realistic conditions. By integrating both zero-shot TTS speech and diverse physical replay recordings, EchoFake captures spoofing patterns overlooked in existing datasets. Evaluations on EchoFake reveal that current models suffer significant performance degradation in realistic replay scenarios, highlighting critical weaknesses in both model robustness and dataset coverage. Meanwhile, incorporating EchoFake during training improves generalization across multiple benchmarks, suggesting the benefit of modeling real-world attack variability. We hope that EchoFake will serve as a valuable benchmark and contribute toward building more resilient and deployable anti-spoofing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "existing",
                    "datasets",
                    "dataset"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Data composition of EchoFake dataset. The dataset includes four categories of audio: bona fide (B), replayed bona fide (RB), fake (F), and replayed fake (RF). It is divided into two evaluation sets—closed-set (Eval-C) and open-set (Eval-O)—with corresponding durations (Dur) reported in hours.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Split</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">#B</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">#RB</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">#F</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">#RF</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">#Total</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Dur</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">Train</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">10,000</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">9,955</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">10,004</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">9,967</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">39,926</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">62.5h</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">Dev</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:80%;\">1,000</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:80%;\">987</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:80%;\">996</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:80%;\">990</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:80%;\">3,973</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.2h</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">Eval-C</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1,500</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1,498</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1,500</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">1,493</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">5,991</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.4h</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">Eval-O</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">12,800</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">6,400</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">6,400</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">6,400</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">32,000</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">48.3h</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">Total</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">25,300</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">18,840</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">18,900</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">18,850</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">81,890</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">126.4h</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "categories",
            "sets—closedset",
            "dur",
            "four",
            "divided",
            "evaluation",
            "fide",
            "durations",
            "two",
            "openset",
            "483h",
            "hours",
            "corresponding",
            "audio",
            "625h",
            "reported",
            "fake",
            "composition",
            "1264h",
            "replayed",
            "evalo—with",
            "echofake",
            "94h",
            "evalo",
            "bona",
            "into",
            "includes",
            "evalc",
            "dev",
            "total",
            "train",
            "data",
            "62h",
            "dataset",
            "split"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">EchoFake is partitioned into four subsets: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">train</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">dev</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">closed-set evaluation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">open-set evaluation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The distribution of utterances across these splits is detailed in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.1.2 TTS Model Selection &#8227; 3.1 Dataset Construction &#8227; 3 EchoFake Dataset &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We maintain a balanced number of samples across four data categories&#8212;</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">bona fide</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">replayed bona fide</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">fake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">replayed fake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;resulting in a total of 126.4 hours of audio data. In addition, detailed metadata is provided for each utterance, including speaker identity, reference speech information, synthesis method, and recording environment. All metadata and dataset construction scripts has been released alongside the dataset to facilitate reproducible research. </span>\n  <span class=\"ltx_note ltx_role_footnotemark\" id=\"footnotex3\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_note_type\">footnotemark: </span>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The growing prevalence of speech deepfakes has raised serious concerns, particularly in real-world scenarios such as telephone fraud and identity theft. While many anti-spoofing systems have demonstrated promising performance on lab-generated synthetic speech, they often fail when confronted with physical replay attacks&#8212;a common and low-cost form of attack used in practical settings. Our experiments show that models trained on existing datasets exhibit severe performance degradation, with average accuracy dropping to 59.6% when evaluated on replayed audio. To bridge this gap, we present EchoFake, a comprehensive dataset comprising more than 120 hours of audio from over 13,000 speakers, featuring both cutting-edge zero-shot text-to-speech (TTS) speech and physical replay recordings collected under varied devices and real-world environmental settings. Additionally, we evaluate three baseline detection models and show that models trained on EchoFake achieve lower average EERs across datasets, indicating better generalization. By introducing more practical challenges relevant to real-world deployment, EchoFake offers a more realistic foundation for advancing spoofing detection methods.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "hours",
                    "audio",
                    "dataset",
                    "replayed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While many audio deepfake detection (ADD) systems achieve strong performance on in-domain benchmarks, their robustness in real-world conditions remains a major concern. A key reason is overfitting&#8212;many ADD models are trained on clean, studio-quality datasets such as ASVspoof2019 logical access (LA) track, which leads to poor generalization when deployed in the wild. To mitigate overfitting, several datasets have been proposed to simulate more realistic conditions. ASVspoof 2021</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LA track focuses on telephone-channel scenarios, while ASVspoof 2021 deepfake (DF) track addresses challenges from lossy compression and encoding. The InTheWild </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset further collects fake and genuine samples from social media platforms to increase data diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "data",
                    "fake",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite recent progress, three major challenges persist for audio deepfake detection (ADD). First, models often misclassify bona fide speech recorded on consumer devices as spoofed, raising serious usability concerns (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">a). Second, many systems are trained on outdated spoofing methods (e.g., ASVspoof 2019 LA), failing to reflect advances in neural TTS and voice cloning. Most critically, detection remains highly vulnerable to replay attacks: by playing synthetic audio through a speaker and re-recording it, attackers can mask artifacts and deceive models into accepting it as genuine speech (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">b). In high-stakes applications such as telephone fraud, adversaries may even replay authentic voice snippets from prior conversations or social media, making detection especially difficult since the audio truly originates from the victim.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "into",
                    "bona",
                    "fide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Existing benchmarks like ASVspoof 2019 physical access (PA) and 2021 PA tracks have laid the groundwork for replay attack detection, yet their datasets rely on software-simulated playback rather than actual physical recordings. More critically, while the PA tracks contain replayed bona fide speech, they lack any instances of replayed deepfake speech. This omission limits their relevance to emerging threats, where attackers may clone a victim&#8217;s voice and physically replay it to bypass anti-spoofing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "bona",
                    "replayed",
                    "fide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these challenges, we introduce EchoFake, a novel dataset that integrates zero-shot TTS deepfakes with varied physical replay conditions, offering a more realistic benchmark for spoofing detection. Using EchoFake, we reveal that existing anti-spoofing models suffer severe performance drops under diverse replay scenarios and remain vulnerable even after training, especially when distinguishing replayed bona fide speech. Nevertheless, incorporating replay diversity improves generalization: models trained on EchoFake achieve lower average EERs across multiple benchmarks, highlighting both the weaknesses of prior datasets and the benefits of realistic replay data for robust detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "bona",
                    "data",
                    "fide",
                    "dataset",
                    "replayed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Audio deepfake detection aims to distinguish bona fide utterances from spoofed or synthesized ones. Existing approaches can be broadly categorized into pipeline-based and end-to-end paradigms. 1) Pipeline-based detectors&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> typically adopt a two-stage strategy: handcrafted or pretrained features&#8212;such as mel-frequency cepstral coefficients (MFCCs), linear frequency cepstral coefficients (LFCCs), or wav2vec2 embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;are first extracted, followed by a separate classifier for final prediction. This modular design allows flexible integration of domain knowledge and facilitates interpretability and transferability across datasets and spoofing conditions. 2) End-to-end methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, on the other hand, learn task-specific representations directly from raw waveforms by jointly optimizing feature extraction and classification. These models benefit from strong representation learning capabilities and can automatically adapt to data variations without manual feature engineering.</span>\n</p>\n\n",
                "matched_terms": [
                    "into",
                    "bona",
                    "audio",
                    "data",
                    "fide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A variety of datasets have been developed to support research in audio deepfake detection. Classic benchmarks such as ASVspoof 2019 LA and ASVspoof 2021&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib1\" title=\"\">1</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focused on spoofing attacks targeting automatic speaker verification systems. The In-the-Wild&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset collects synthetic speech samples of celebrities from the Internet, offering a more realistic setting for deepfake detection. The ADD challenge series&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ASVspoof 5&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> incorporates codec artifacts and multi-domain content for broader coverage. However, most datasets focus solely on either synthesis or replay. Our proposed EchoFake dataset fills this gap by integrating TTS-generated and replayed speech under diverse recording configurations, supporting more comprehensive model evaluation under realistic deployment conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "audio",
                    "evaluation",
                    "dataset",
                    "replayed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To tackle the mismatch between lab-generated synthetic datasets and real-world spoofing scenarios involving replay attacks, we construct EchoFake, a new dataset that broadens the scope of audio deepfake detection beyond synthetic samples alone.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "echofake",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">EchoFake consists of four subsets: training, development, closed-set evaluation, and open-set evaluation. The first three share the same pool of speakers and TTS systems to support in-distribution training and tuning, while the open-set set introduces unseen speakers, new spoofing systems, and diverse replay conditions, providing a rigorous test of model generalization in realistic settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "evaluation",
                    "four",
                    "openset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S3.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 EchoFake Dataset &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, bona fide speech samples are directly extracted from the CommonVoice 17.0 dataset. Half of these samples were replayed to construct the replayed bona fide subset. For fake speech, we randomly sampled source texts and reference speech clips from CommonVoice and then use zero-shot TTS models to synthesize new utterances with the target speaker&#8217;s voice cloned from the reference speech clips. Similarly, 50% of the generated utterances are replayed to obtain the replayed fake subset. All sampling operations are strictly non-redundant to maximize diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "bona",
                    "fake",
                    "fide",
                    "dataset",
                    "replayed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Based on the aforementioned selection criteria, six state-of-the-art TTS models were adopted for generating fake speech across three subsets: the training set, development set, and closed-set evaluation set. Specifically, we employed: (1) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">XTTSv2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Multilingual; VQ-VAE architecture with GPT-2 encoder and HiFi-GAN vocoder; (2) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F5-TTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Flow matching using diffusion transformers, with sway sampling strategy; (3) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SpeechT5</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Unified encoder-decoder; Multi-task pretraining; Cross-modal vector quantization; (4) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LLaSA-1B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Large language model (LLM) with codec-based audio adapter; (5) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">OpenAudio-S1</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: LLM and dual autoregressive architecture, with FFGAN vocoder; (6) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">StyleTTS2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Style diffusion and adversarial training with speech language models.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "fake"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate cross-model robustness against diverse spoofing threats, five additional models were incorporated exclusively for open-set evaluation: (1) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CosyVoice2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Streaming TTS; Progressive semantic decoding with LLMs and flow matching; (2) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">IndexTTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Bilingual TTS; Conformer-based speech conditional encoder; BigVGAN vocoder; (3) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MaskGCT</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Masked generative transformers with discrete speech semantic representation; (4) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">OpenVoice-V2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: TTS with flexible voice style control using normalizing flows; (5) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FireRedTTS-1</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Semantic-aware speech tokenizer; Decoder-only transformers with flow matching.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "openset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A core contribution of EchoFake is the inclusion of replayed speech, which introduces stronger distortions than lossy compression or telephone-channel encoding. Replay not only reduces fidelity through playback and recording hardware but also adds reverberation and background noise. To capture this variability, we vary playback devices, recording devices, environments, and microphone&#8211;speaker distances, yielding 16 closed-set combinations and 4 unseen open-set conditions. Data collection is automated through a WebRTC-based application that synchronizes playback and recording across LAN-connected devices, emulating real-world scenarios such as conference calls. The implementation is open-sourced under the MIT License for reproducibility. </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnotex1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <span class=\"ltx_text\" style=\"font-size:90%;\">Datasets, codes and automated recording tools: </span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/EchoFake/EchoFake/\" style=\"font-size:90%;\" title=\"\">https://github.com/EchoFake/EchoFake/</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "data",
                    "replayed",
                    "openset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To construct these replay conditions, we systematically varied playback and recording devices, environments, and microphone&#8211;speaker distances. For the training, development, and closed-set evaluation subsets, replayed speech was generated using two playback devices (a MacBook Pro 14-inch 2021 and an iPad Mini, 7th generation) and two recording devices (an iPhone 13 mini and a Samsung Galaxy A54). Recordings were captured in both a meeting room (12.8 &#215; 9.3 &#215; 3.2 m) and a smaller home room (4.8 &#215; 3.2 &#215; 3.2 m), with microphone&#8211;speaker distances of 15 cm and 50 cm. These four variables yield 16 distinct replay conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "four",
                    "replayed",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the open-set evaluation, we introduced previously unseen configurations to test model generalization. Specifically, playback was performed using Edifier MR4 powered studio monitor speakers and a Xiaomi 13 Ultra smartphone, while recording was carried out with another Xiaomi 13 Ultra or a pair of wired earbuds with a built-in microphone. Recording was conducted in a larger office room (18.6 &#215; 13.2 &#215; 3.2 m) at a microphone&#8211;speaker distance of 30 cm. These settings produce four additional unseen replay conditions that further increase the diversity and challenge of EchoFake.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "evaluation",
                    "four",
                    "openset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All four subsets undergo the same preprocessing pipeline. First, volume normalization is performed using </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">ffmpeg</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with the integrated loudness adjusted to </span>\n  <math alttext=\"-23\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">23</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-23</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LUFS, a loudness range of </span>\n  <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">7</mn>\n      <annotation encoding=\"application/x-tex\">7</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LU, and a true peak limit of </span>\n  <math alttext=\"-2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dBTP. Then, MP3 compression is applied with a bitrate of 64 kbps, a sampling rate of 16 kHz, and mono-channel output to maintain compatibility with the CommonVoice dataset and simulate real-world audio degradation in social media platforms.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "four",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The proposed </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">EchoFake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset comprises 81,890 utterances from 13,005 speakers and covers 11 different TTS systems. A comprehensive comparison of EchoFake with established datasets is summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1.2 TTS Model Selection &#8227; 3.1 Dataset Construction &#8227; 3 EchoFake Dataset &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. While EchoFake contains fewer utterances than mainstream datasets, it outperforms them in speaker diversity, ensuring broader representation of vocal characteristics and accent variations.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the proposed dataset using three representative baseline systems: RawNet2, AASIST, and Wav2Vec2. For RawNet2, the model is trained for 100 epochs with a batch size of 64, an initial learning rate of </span>\n  <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">4</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For AASIST, the model is trained for 60 epochs with a batch size of 32, an initial learning rate of </span>\n  <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">4</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For wav2vec2, we append a multi-layer perceptron (MLP) as backend classifier and train the model in an end-to-end manner for 20 epochs with a batch size of 32, an initial learning rate of </span>\n  <math alttext=\"10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">5</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. All models are optimized using the Adam optimizer (</span>\n  <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.999</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) with a weight decay of </span>\n  <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">4</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For all models, we select the checkpoint with the best performance on the development set for final evaluation. All experiments were conducted on a single NVIDIA RTX 4090 GPU.</span>\n</p>\n\n",
                "matched_terms": [
                    "train",
                    "evaluation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate model robustness under realistic spoofing scenarios, we examine detection performance on the EchoFake dataset under two classification settings: a four-class detection setup and a conventional binary classification task.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "dataset",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In both four-class and binary detection tasks, models achieve strong performance in closed-set conditions but degrade sharply in open-set scenarios, with replayed samples being the main source of errors. Replayed bona fide (RB) speech is particularly difficult to detect due to its lack of synthetic artifacts and close resemblance to genuine speech, while replayed fake (RF) samples further complicate classification by combining synthesis traces with channel distortions. In contrast, purely synthetic speech remains the easiest to identify, suggesting that existing models overly rely on spectral artifacts. These results reveal a critical weakness of current anti-spoofing systems: high-fidelity replay attacks can effectively mask or distort discriminative cues, leading to substantial misclassification and undermining robustness in real-world conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "bona",
                    "fake",
                    "fide",
                    "replayed",
                    "openset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To assess cross-dataset generalization capabilities, we conducted systematic evaluations of spoofing detection systems across multiple benchmarks. In this experiment, the RB samples from EchoFake is treated as spoofed audio, alongside F and RF samples, to align with the binary classification setting of existing datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "echofake"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4 Experiments &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, EchoFake exposes a critical vulnerability of current ADD models to realistic replay attacks, with all three baseline models exhibiting significant performance degradation on the open-set evaluation subset (average EER: 48%). This deterioration highlights the challenge of developing generalizable detectors under diverse attack types and realistic acoustic conditions. Notably, models achieve their best cross-dataset performance when trained on EchoFake, suggesting its enhanced diversity in spoofing scenarios enables more effective generalization. These results establish EchoFake as a rigorous benchmark and emphasize the value of training sets that explicitly model real-world heterogeneity.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "evaluation",
                    "openset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To examine the impact of replay data, we retrained all baseline models on the EchoFake training set after removing replay samples and compared their performance with models trained on the full dataset (For fair comparison, we downsampled the original training set to match the replay-excluded set&#8217;s sample size). When evaluated across five conventional benchmarks (ASV19LA, ASV21LA, ASV21DF, In-the-Wild, and WaveFake), the EER changes after removing replay data were marginal: </span>\n  <math alttext=\"-0.03\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">0.03</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-0.03\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (RawNet), </span>\n  <math alttext=\"+1.67\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">1.67</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+1.67\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (AASIST), and </span>\n  <math alttext=\"+4.07\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">4.07</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+4.07\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (wav2vec2). However, when including the EchoFake-open set for evaluation (six datasets in total), average EERs increased by </span>\n  <math alttext=\"+1.54\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">1.54</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+1.54\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"+5.26\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">5.26</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+5.26\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"+6.22\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">6.22</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+6.22\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively. These results demonstrate that incorporating replay data in training provides a favorable trade-off: it does not significantly degrade performance on conventional benchmarks while substantially enhancing robustness against replay-based attacks.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "total",
                    "evaluation",
                    "data",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we present EchoFake, a novel and comprehensive dataset designed to advance ADD system development under realistic conditions. By integrating both zero-shot TTS speech and diverse physical replay recordings, EchoFake captures spoofing patterns overlooked in existing datasets. Evaluations on EchoFake reveal that current models suffer significant performance degradation in realistic replay scenarios, highlighting critical weaknesses in both model robustness and dataset coverage. Meanwhile, incorporating EchoFake during training improves generalization across multiple benchmarks, suggesting the benefit of modeling real-world attack variability. We hope that EchoFake will serve as a valuable benchmark and contribute toward building more resilient and deployable anti-spoofing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "dataset"
                ]
            }
        ]
    },
    "S4.T3": {
        "caption": "Table 3: EER(%) of three baseline models trained on different datasets and evaluated on various benchmarks. The final column (w. avg.) reports the weighted average EER, computed by first averaging the results on 19LA-eval, 21LA, and 21 DF—since they share similar synthesis methods—before combining with the remaining benchmarks.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt\" colspan=\"2\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"7\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Test Set EER%(<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Train set</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">ASV19LA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">ASV21LA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">ASV21DF</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">In-the-Wild</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WaveFake</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">EchoFake-open</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">w. avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:80%;\">RawNet2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">ASV19LA-train</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">6.773</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">7.979</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">22.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">43.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">56.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">46.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">39.57</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">In-the-Wild</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">46.58</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">48.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">43.88</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">54.38</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">49.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">50.24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">WaveFake</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">63.07</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">59.02</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">53.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">41.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">56.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">52.25</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">EchoFake-train</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">34.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">36.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">37.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">37.52</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">34.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">21.13</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">32.49</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:80%;\">AASIST</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">ASV19LA-train</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.8295</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.820</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">17.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">43.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">48.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">43.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">35.83</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">In-the-Wild</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">36.42</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">44.07</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">37.28</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">35.27</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">49.27</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">41.27</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">WaveFake</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">40.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">38.52</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">46.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">29.13</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">37.81</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">36.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">EchoFake-train</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">31.49</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">32.39</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">39.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">37.75</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">33.59</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">14.88</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">30.15</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:80%;\">Wav2Vec2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">ASV19LA-train</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">11.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">8.673</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">5.244</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">11.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">10.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">42.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">18.25</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">In-the-Wild</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">11.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">24.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">15.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">16.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">51.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">28.21</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">WaveFake</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">63.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">64.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">56.69</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">49.37</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#8211;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">55.52</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">55.46</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:80%;\">EchoFake-train</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">14.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">13.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">17.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:80%;\">16.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">23.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">11.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">16.79</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "evaluated",
            "column",
            "asv19latrain",
            "final",
            "averaging",
            "asv21la",
            "df—since",
            "echofaketrain",
            "methods—before",
            "various",
            "baseline",
            "synthesis",
            "combining",
            "average",
            "eer↓downarrow",
            "computed",
            "test",
            "19laeval",
            "trained",
            "first",
            "remaining",
            "inthewild",
            "avg",
            "wavefake",
            "eer",
            "asv19la",
            "aasist",
            "rawnet2",
            "results",
            "echofakeopen",
            "asv21df",
            "datasets",
            "wav2vec2",
            "reports",
            "set",
            "share",
            "21la",
            "three",
            "model",
            "train",
            "weighted",
            "similar",
            "benchmarks",
            "different",
            "they"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4 Experiments &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, EchoFake exposes a critical vulnerability of current ADD models to realistic replay attacks, with all three baseline models exhibiting significant performance degradation on the open-set evaluation subset (average EER: 48%). This deterioration highlights the challenge of developing generalizable detectors under diverse attack types and realistic acoustic conditions. Notably, models achieve their best cross-dataset performance when trained on EchoFake, suggesting its enhanced diversity in spoofing scenarios enables more effective generalization. These results establish EchoFake as a rigorous benchmark and emphasize the value of training sets that explicitly model real-world heterogeneity.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The growing prevalence of speech deepfakes has raised serious concerns, particularly in real-world scenarios such as telephone fraud and identity theft. While many anti-spoofing systems have demonstrated promising performance on lab-generated synthetic speech, they often fail when confronted with physical replay attacks&#8212;a common and low-cost form of attack used in practical settings. Our experiments show that models trained on existing datasets exhibit severe performance degradation, with average accuracy dropping to 59.6% when evaluated on replayed audio. To bridge this gap, we present EchoFake, a comprehensive dataset comprising more than 120 hours of audio from over 13,000 speakers, featuring both cutting-edge zero-shot text-to-speech (TTS) speech and physical replay recordings collected under varied devices and real-world environmental settings. Additionally, we evaluate three baseline detection models and show that models trained on EchoFake achieve lower average EERs across datasets, indicating better generalization. By introducing more practical challenges relevant to real-world deployment, EchoFake offers a more realistic foundation for advancing spoofing detection methods.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluated",
                    "average",
                    "three",
                    "baseline",
                    "trained",
                    "datasets",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The recent advances in zero-shot text-to-speech (TTS) and large-scale audio language models (ALM) have dramatically lowered the barrier to generating high-quality synthetic speech. With only a few seconds of reference audio, these models can convincingly clone a speaker&#8217;s voice, producing speech that is perceptually indistinguishable from genuine utterances. While these technologies offer exciting applications in personalized voice assistants, voiceovers, and accessibility, they also pose a serious threat to security and public trust. The proliferation of voice cloning tools has opened up new avenues for audio-based forgery, impersonation, and misinformation.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">While many audio deepfake detection (ADD) systems achieve strong performance on in-domain benchmarks, their robustness in real-world conditions remains a major concern. A key reason is overfitting&#8212;many ADD models are trained on clean, studio-quality datasets such as ASVspoof2019 logical access (LA) track, which leads to poor generalization when deployed in the wild. To mitigate overfitting, several datasets have been proposed to simulate more realistic conditions. ASVspoof 2021</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LA track focuses on telephone-channel scenarios, while ASVspoof 2021 deepfake (DF) track addresses challenges from lossy compression and encoding. The InTheWild </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset further collects fake and genuine samples from social media platforms to increase data diversity.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "inthewild",
                    "benchmarks",
                    "trained",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Despite recent progress, three major challenges persist for audio deepfake detection (ADD). First, models often misclassify bona fide speech recorded on consumer devices as spoofed, raising serious usability concerns (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">a). Second, many systems are trained on outdated spoofing methods (e.g., ASVspoof 2019 LA), failing to reflect advances in neural TTS and voice cloning. Most critically, detection remains highly vulnerable to replay attacks: by playing synthetic audio through a speaker and re-recording it, attackers can mask artifacts and deceive models into accepting it as genuine speech (Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">b). In high-stakes applications such as telephone fraud, adversaries may even replay authentic voice snippets from prior conversations or social media, making detection especially difficult since the audio truly originates from the victim.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "three",
                    "trained",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Existing benchmarks like ASVspoof 2019 physical access (PA) and 2021 PA tracks have laid the groundwork for replay attack detection, yet their datasets rely on software-simulated playback rather than actual physical recordings. More critically, while the PA tracks contain replayed bona fide speech, they lack any instances of replayed deepfake speech. This omission limits their relevance to emerging threats, where attackers may clone a victim&#8217;s voice and physically replay it to bypass anti-spoofing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "datasets",
                    "they"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these challenges, we introduce EchoFake, a novel dataset that integrates zero-shot TTS deepfakes with varied physical replay conditions, offering a more realistic benchmark for spoofing detection. Using EchoFake, we reveal that existing anti-spoofing models suffer severe performance drops under diverse replay scenarios and remain vulnerable even after training, especially when distinguishing replayed bona fide speech. Nevertheless, incorporating replay diversity improves generalization: models trained on EchoFake achieve lower average EERs across multiple benchmarks, highlighting both the weaknesses of prior datasets and the benefits of realistic replay data for robust detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "average",
                    "benchmarks",
                    "trained",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Audio deepfake detection aims to distinguish bona fide utterances from spoofed or synthesized ones. Existing approaches can be broadly categorized into pipeline-based and end-to-end paradigms. 1) Pipeline-based detectors&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> typically adopt a two-stage strategy: handcrafted or pretrained features&#8212;such as mel-frequency cepstral coefficients (MFCCs), linear frequency cepstral coefficients (LFCCs), or wav2vec2 embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;are first extracted, followed by a separate classifier for final prediction. This modular design allows flexible integration of domain knowledge and facilitates interpretability and transferability across datasets and spoofing conditions. 2) End-to-end methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, on the other hand, learn task-specific representations directly from raw waveforms by jointly optimizing feature extraction and classification. These models benefit from strong representation learning capabilities and can automatically adapt to data variations without manual feature engineering.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "datasets",
                    "final",
                    "first",
                    "wav2vec2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A variety of datasets have been developed to support research in audio deepfake detection. Classic benchmarks such as ASVspoof 2019 LA and ASVspoof 2021&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib1\" title=\"\">1</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focused on spoofing attacks targeting automatic speaker verification systems. The In-the-Wild&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset collects synthetic speech samples of celebrities from the Internet, offering a more realistic setting for deepfake detection. The ADD challenge series&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ASVspoof 5&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> incorporates codec artifacts and multi-domain content for broader coverage. However, most datasets focus solely on either synthesis or replay. Our proposed EchoFake dataset fills this gap by integrating TTS-generated and replayed speech under diverse recording configurations, supporting more comprehensive model evaluation under realistic deployment conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "synthesis",
                    "model",
                    "benchmarks",
                    "inthewild",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">EchoFake consists of four subsets: training, development, closed-set evaluation, and open-set evaluation. The first three share the same pool of speakers and TTS systems to support in-distribution training and tuning, while the open-set set introduces unseen speakers, new spoofing systems, and diverse replay conditions, providing a rigorous test of model generalization in realistic settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "share",
                    "three",
                    "model",
                    "test",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For TTS model selection, we considered three key criteria: (1) widespread adoption or popularity in TTS communities (e.g., GitHub, HuggingFace); (2) employing state-of-the-art techniques in speech synthesis with high intelligibility, naturalness and fidelity; and (3) support for zero-shot voice cloning, which enables speaker mimicry, a common strategy in voice-based spoofing attacks.</span>\n</p>\n\n",
                "matched_terms": [
                    "three",
                    "model",
                    "synthesis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Based on the aforementioned selection criteria, six state-of-the-art TTS models were adopted for generating fake speech across three subsets: the training set, development set, and closed-set evaluation set. Specifically, we employed: (1) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">XTTSv2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Multilingual; VQ-VAE architecture with GPT-2 encoder and HiFi-GAN vocoder; (2) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F5-TTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Flow matching using diffusion transformers, with sway sampling strategy; (3) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SpeechT5</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Unified encoder-decoder; Multi-task pretraining; Cross-modal vector quantization; (4) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LLaSA-1B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Large language model (LLM) with codec-based audio adapter; (5) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">OpenAudio-S1</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: LLM and dual autoregressive architecture, with FFGAN vocoder; (6) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">StyleTTS2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Style diffusion and adversarial training with speech language models.</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "models",
                    "three",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the open-set evaluation, we introduced previously unseen configurations to test model generalization. Specifically, playback was performed using Edifier MR4 powered studio monitor speakers and a Xiaomi 13 Ultra smartphone, while recording was carried out with another Xiaomi 13 Ultra or a pair of wired earbuds with a built-in microphone. Recording was conducted in a larger office room (18.6 &#215; 13.2 &#215; 3.2 m) at a microphone&#8211;speaker distance of 30 cm. These settings produce four additional unseen replay conditions that further increase the diversity and challenge of EchoFake.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The proposed </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">EchoFake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset comprises 81,890 utterances from 13,005 speakers and covers 11 different TTS systems. A comprehensive comparison of EchoFake with established datasets is summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1.2 TTS Model Selection &#8227; 3.1 Dataset Construction &#8227; 3 EchoFake Dataset &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. While EchoFake contains fewer utterances than mainstream datasets, it outperforms them in speaker diversity, ensuring broader representation of vocal characteristics and accent variations.</span>\n</p>\n\n",
                "matched_terms": [
                    "different",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">EchoFake is partitioned into four subsets: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">train</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">dev</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">closed-set evaluation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">open-set evaluation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The distribution of utterances across these splits is detailed in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.1.2 TTS Model Selection &#8227; 3.1 Dataset Construction &#8227; 3 EchoFake Dataset &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We maintain a balanced number of samples across four data categories&#8212;</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">bona fide</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">replayed bona fide</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">fake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">replayed fake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;resulting in a total of 126.4 hours of audio data. In addition, detailed metadata is provided for each utterance, including speaker identity, reference speech information, synthesis method, and recording environment. All metadata and dataset construction scripts has been released alongside the dataset to facilitate reproducible research. </span>\n  <span class=\"ltx_note ltx_role_footnotemark\" id=\"footnotex3\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_note_type\">footnotemark: </span>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "train",
                    "synthesis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the proposed dataset using three representative baseline systems: RawNet2, AASIST, and Wav2Vec2. For RawNet2, the model is trained for 100 epochs with a batch size of 64, an initial learning rate of </span>\n  <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">4</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For AASIST, the model is trained for 60 epochs with a batch size of 32, an initial learning rate of </span>\n  <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">4</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For wav2vec2, we append a multi-layer perceptron (MLP) as backend classifier and train the model in an end-to-end manner for 20 epochs with a batch size of 32, an initial learning rate of </span>\n  <math alttext=\"10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">5</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. All models are optimized using the Adam optimizer (</span>\n  <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.999</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) with a weight decay of </span>\n  <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">4</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For all models, we select the checkpoint with the best performance on the development set for final evaluation. All experiments were conducted on a single NVIDIA RTX 4090 GPU.</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "models",
                    "train",
                    "three",
                    "final",
                    "model",
                    "aasist",
                    "rawnet2",
                    "baseline",
                    "trained",
                    "wav2vec2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In both four-class and binary detection tasks, models achieve strong performance in closed-set conditions but degrade sharply in open-set scenarios, with replayed samples being the main source of errors. Replayed bona fide (RB) speech is particularly difficult to detect due to its lack of synthetic artifacts and close resemblance to genuine speech, while replayed fake (RF) samples further complicate classification by combining synthesis traces with channel distortions. In contrast, purely synthetic speech remains the easiest to identify, suggesting that existing models overly rely on spectral artifacts. These results reveal a critical weakness of current anti-spoofing systems: high-fidelity replay attacks can effectively mask or distort discriminative cues, leading to substantial misclassification and undermining robustness in real-world conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "synthesis",
                    "results",
                    "combining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To assess cross-dataset generalization capabilities, we conducted systematic evaluations of spoofing detection systems across multiple benchmarks. In this experiment, the RB samples from EchoFake is treated as spoofed audio, alongside F and RF samples, to align with the binary classification setting of existing datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "benchmarks",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To examine the impact of replay data, we retrained all baseline models on the EchoFake training set after removing replay samples and compared their performance with models trained on the full dataset (For fair comparison, we downsampled the original training set to match the replay-excluded set&#8217;s sample size). When evaluated across five conventional benchmarks (ASV19LA, ASV21LA, ASV21DF, In-the-Wild, and WaveFake), the EER changes after removing replay data were marginal: </span>\n  <math alttext=\"-0.03\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">0.03</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-0.03\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (RawNet), </span>\n  <math alttext=\"+1.67\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">1.67</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+1.67\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (AASIST), and </span>\n  <math alttext=\"+4.07\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">4.07</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+4.07\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (wav2vec2). However, when including the EchoFake-open set for evaluation (six datasets in total), average EERs increased by </span>\n  <math alttext=\"+1.54\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">1.54</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+1.54\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"+5.26\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">5.26</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+5.26\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"+6.22\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">6.22</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+6.22\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively. These results demonstrate that incorporating replay data in training provides a favorable trade-off: it does not significantly degrade performance on conventional benchmarks while substantially enhancing robustness against replay-based attacks.</span>\n</p>\n\n",
                "matched_terms": [
                    "set",
                    "models",
                    "benchmarks",
                    "evaluated",
                    "wavefake",
                    "inthewild",
                    "eer",
                    "average",
                    "wav2vec2",
                    "asv21la",
                    "asv19la",
                    "aasist",
                    "results",
                    "echofakeopen",
                    "asv21df",
                    "baseline",
                    "trained",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we present EchoFake, a novel and comprehensive dataset designed to advance ADD system development under realistic conditions. By integrating both zero-shot TTS speech and diverse physical replay recordings, EchoFake captures spoofing patterns overlooked in existing datasets. Evaluations on EchoFake reveal that current models suffer significant performance degradation in realistic replay scenarios, highlighting critical weaknesses in both model robustness and dataset coverage. Meanwhile, incorporating EchoFake during training improves generalization across multiple benchmarks, suggesting the benefit of modeling real-world attack variability. We hope that EchoFake will serve as a valuable benchmark and contribute toward building more resilient and deployable anti-spoofing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "benchmarks",
                    "model",
                    "datasets"
                ]
            }
        ]
    },
    "S4.T4": {
        "caption": "Table 4: F1-score (%) for four-class classification and EER/Accuracy (%) for binary classification on EchoFake test sets. Detailed performance breakdown (F1/Accuracy) across all four classes is included.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">F1-score%(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Cond.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Avg. F1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">RB</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">F</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">RF</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">RawNet2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Closed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">94.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">93.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">94.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">93.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">94.13</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Open</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">53.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">73.23</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">27.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">72.79</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">41.35</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">AASIST</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Closed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">97.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">99.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">96.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">96.21</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Open</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">51.07</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">70.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">26.30</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">79.90</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">27.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Wav2Vec2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Closed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">99.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">99.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.02</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Open</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">60.99</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">78.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">40.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">75.64</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">49.34</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">EER%(<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Accuracy%(<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Cond.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">All</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">RB</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">F</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">RF</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">RawNet2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Closed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">96.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">99.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">88.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">99.87</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Open</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">21.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">78.88</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">65.89</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">94.30</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">76.42</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">AASIST</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Closed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">99.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">99.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">100.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Open</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">14.88</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">85.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">66.92</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.66</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">89.78</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Wav2Vec2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Closed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">99.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">99.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">99.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">99.93</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Open</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">88.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">67.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">99.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">97.13</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "four",
            "closed",
            "breakdown",
            "classes",
            "open",
            "eer↓downarrow",
            "accuracy↑uparrow",
            "cond",
            "test",
            "fourclass",
            "performance",
            "echofake",
            "across",
            "avg",
            "eeraccuracy",
            "sets",
            "f1score",
            "classification",
            "aasist",
            "binary",
            "rawnet2",
            "wav2vec2",
            "included",
            "f1accuracy",
            "model",
            "all",
            "detailed",
            "f1score↑uparrow"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The growing prevalence of speech deepfakes has raised serious concerns, particularly in real-world scenarios such as telephone fraud and identity theft. While many anti-spoofing systems have demonstrated promising performance on lab-generated synthetic speech, they often fail when confronted with physical replay attacks&#8212;a common and low-cost form of attack used in practical settings. Our experiments show that models trained on existing datasets exhibit severe performance degradation, with average accuracy dropping to 59.6% when evaluated on replayed audio. To bridge this gap, we present EchoFake, a comprehensive dataset comprising more than 120 hours of audio from over 13,000 speakers, featuring both cutting-edge zero-shot text-to-speech (TTS) speech and physical replay recordings collected under varied devices and real-world environmental settings. Additionally, we evaluate three baseline detection models and show that models trained on EchoFake achieve lower average EERs across datasets, indicating better generalization. By introducing more practical challenges relevant to real-world deployment, EchoFake offers a more realistic foundation for advancing spoofing detection methods.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these challenges, we introduce EchoFake, a novel dataset that integrates zero-shot TTS deepfakes with varied physical replay conditions, offering a more realistic benchmark for spoofing detection. Using EchoFake, we reveal that existing anti-spoofing models suffer severe performance drops under diverse replay scenarios and remain vulnerable even after training, especially when distinguishing replayed bona fide speech. Nevertheless, incorporating replay diversity improves generalization: models trained on EchoFake achieve lower average EERs across multiple benchmarks, highlighting both the weaknesses of prior datasets and the benefits of realistic replay data for robust detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Audio deepfake detection aims to distinguish bona fide utterances from spoofed or synthesized ones. Existing approaches can be broadly categorized into pipeline-based and end-to-end paradigms. 1) Pipeline-based detectors&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib5\" title=\"\">5</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> typically adopt a two-stage strategy: handcrafted or pretrained features&#8212;such as mel-frequency cepstral coefficients (MFCCs), linear frequency cepstral coefficients (LFCCs), or wav2vec2 embeddings&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;are first extracted, followed by a separate classifier for final prediction. This modular design allows flexible integration of domain knowledge and facilitates interpretability and transferability across datasets and spoofing conditions. 2) End-to-end methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, on the other hand, learn task-specific representations directly from raw waveforms by jointly optimizing feature extraction and classification. These models benefit from strong representation learning capabilities and can automatically adapt to data variations without manual feature engineering.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "wav2vec2",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A variety of datasets have been developed to support research in audio deepfake detection. Classic benchmarks such as ASVspoof 2019 LA and ASVspoof 2021&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib1\" title=\"\">1</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focused on spoofing attacks targeting automatic speaker verification systems. The In-the-Wild&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset collects synthetic speech samples of celebrities from the Internet, offering a more realistic setting for deepfake detection. The ADD challenge series&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ASVspoof 5&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> incorporates codec artifacts and multi-domain content for broader coverage. However, most datasets focus solely on either synthesis or replay. Our proposed EchoFake dataset fills this gap by integrating TTS-generated and replayed speech under diverse recording configurations, supporting more comprehensive model evaluation under realistic deployment conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">EchoFake consists of four subsets: training, development, closed-set evaluation, and open-set evaluation. The first three share the same pool of speakers and TTS systems to support in-distribution training and tuning, while the open-set set introduces unseen speakers, new spoofing systems, and diverse replay conditions, providing a rigorous test of model generalization in realistic settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "four",
                    "model",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Based on the aforementioned selection criteria, six state-of-the-art TTS models were adopted for generating fake speech across three subsets: the training set, development set, and closed-set evaluation set. Specifically, we employed: (1) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">XTTSv2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Multilingual; VQ-VAE architecture with GPT-2 encoder and HiFi-GAN vocoder; (2) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">F5-TTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Flow matching using diffusion transformers, with sway sampling strategy; (3) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SpeechT5</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Unified encoder-decoder; Multi-task pretraining; Cross-modal vector quantization; (4) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LLaSA-1B</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Large language model (LLM) with codec-based audio adapter; (5) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">OpenAudio-S1</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: LLM and dual autoregressive architecture, with FFGAN vocoder; (6) </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">StyleTTS2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Style diffusion and adversarial training with speech language models.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A core contribution of EchoFake is the inclusion of replayed speech, which introduces stronger distortions than lossy compression or telephone-channel encoding. Replay not only reduces fidelity through playback and recording hardware but also adds reverberation and background noise. To capture this variability, we vary playback devices, recording devices, environments, and microphone&#8211;speaker distances, yielding 16 closed-set combinations and 4 unseen open-set conditions. Data collection is automated through a WebRTC-based application that synchronizes playback and recording across LAN-connected devices, emulating real-world scenarios such as conference calls. The implementation is open-sourced under the MIT License for reproducibility. </span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnotex1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <span class=\"ltx_text\" style=\"font-size:90%;\">Datasets, codes and automated recording tools: </span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/EchoFake/EchoFake/\" style=\"font-size:90%;\" title=\"\">https://github.com/EchoFake/EchoFake/</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the open-set evaluation, we introduced previously unseen configurations to test model generalization. Specifically, playback was performed using Edifier MR4 powered studio monitor speakers and a Xiaomi 13 Ultra smartphone, while recording was carried out with another Xiaomi 13 Ultra or a pair of wired earbuds with a built-in microphone. Recording was conducted in a larger office room (18.6 &#215; 13.2 &#215; 3.2 m) at a microphone&#8211;speaker distance of 30 cm. These settings produce four additional unseen replay conditions that further increase the diversity and challenge of EchoFake.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "four",
                    "model",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">All four subsets undergo the same preprocessing pipeline. First, volume normalization is performed using </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">ffmpeg</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with the integrated loudness adjusted to </span>\n  <math alttext=\"-23\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">23</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-23</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LUFS, a loudness range of </span>\n  <math alttext=\"7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">7</mn>\n      <annotation encoding=\"application/x-tex\">7</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> LU, and a true peak limit of </span>\n  <math alttext=\"-2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">2</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-2</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dBTP. Then, MP3 compression is applied with a bitrate of 64 kbps, a sampling rate of 16 kHz, and mono-channel output to maintain compatibility with the CommonVoice dataset and simulate real-world audio degradation in social media platforms.</span>\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "four"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">EchoFake is partitioned into four subsets: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">train</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">dev</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">closed-set evaluation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">open-set evaluation</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The distribution of utterances across these splits is detailed in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.1.2 TTS Model Selection &#8227; 3.1 Dataset Construction &#8227; 3 EchoFake Dataset &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We maintain a balanced number of samples across four data categories&#8212;</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">bona fide</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">replayed bona fide</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">fake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">replayed fake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;resulting in a total of 126.4 hours of audio data. In addition, detailed metadata is provided for each utterance, including speaker identity, reference speech information, synthesis method, and recording environment. All metadata and dataset construction scripts has been released alongside the dataset to facilitate reproducible research. </span>\n  <span class=\"ltx_note ltx_role_footnotemark\" id=\"footnotex3\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_note_type\">footnotemark: </span>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "across",
                    "four",
                    "all",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the proposed dataset using three representative baseline systems: RawNet2, AASIST, and Wav2Vec2. For RawNet2, the model is trained for 100 epochs with a batch size of 64, an initial learning rate of </span>\n  <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">4</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For AASIST, the model is trained for 60 epochs with a batch size of 32, an initial learning rate of </span>\n  <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">4</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For wav2vec2, we append a multi-layer perceptron (MLP) as backend classifier and train the model in an end-to-end manner for 20 epochs with a batch size of 32, an initial learning rate of </span>\n  <math alttext=\"10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">5</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. All models are optimized using the Adam optimizer (</span>\n  <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.9</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0.999</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) with a weight decay of </span>\n  <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">10</mn>\n        <mrow>\n          <mo mathsize=\"0.900em\">&#8722;</mo>\n          <mn mathsize=\"0.900em\">4</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For all models, we select the checkpoint with the best performance on the development set for final evaluation. All experiments were conducted on a single NVIDIA RTX 4090 GPU.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "all",
                    "aasist",
                    "rawnet2",
                    "wav2vec2",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate system performance using two standard metrics: F1-score for four-class classification (B/RB/F/RF) and equal error rate (EER) for binary spoofing detection (spoof/bonafide). The F1-score provides balanced accuracy measurement, while EER optimizes the trade-off between false acceptance and rejection rates.</span>\n</p>\n\n",
                "matched_terms": [
                    "f1score",
                    "classification",
                    "binary",
                    "fourclass",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate model robustness under realistic spoofing scenarios, we examine detection performance on the EchoFake dataset under two classification settings: a four-class detection setup and a conventional binary classification task.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "model",
                    "classification",
                    "binary",
                    "fourclass",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In both four-class and binary detection tasks, models achieve strong performance in closed-set conditions but degrade sharply in open-set scenarios, with replayed samples being the main source of errors. Replayed bona fide (RB) speech is particularly difficult to detect due to its lack of synthetic artifacts and close resemblance to genuine speech, while replayed fake (RF) samples further complicate classification by combining synthesis traces with channel distortions. In contrast, purely synthetic speech remains the easiest to identify, suggesting that existing models overly rely on spectral artifacts. These results reveal a critical weakness of current anti-spoofing systems: high-fidelity replay attacks can effectively mask or distort discriminative cues, leading to substantial misclassification and undermining robustness in real-world conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "classification",
                    "binary",
                    "fourclass",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To assess cross-dataset generalization capabilities, we conducted systematic evaluations of spoofing detection systems across multiple benchmarks. In this experiment, the RB samples from EchoFake is treated as spoofed audio, alongside F and RF samples, to align with the binary classification setting of existing datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "across",
                    "binary",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19414v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4 Experiments &#8227; ECHOFAKE: A REPLAY-AWARE DATASET FOR PRACTICAL SPEECH DEEPFAKE DETECTION\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, EchoFake exposes a critical vulnerability of current ADD models to realistic replay attacks, with all three baseline models exhibiting significant performance degradation on the open-set evaluation subset (average EER: 48%). This deterioration highlights the challenge of developing generalizable detectors under diverse attack types and realistic acoustic conditions. Notably, models achieve their best cross-dataset performance when trained on EchoFake, suggesting its enhanced diversity in spoofing scenarios enables more effective generalization. These results establish EchoFake as a rigorous benchmark and emphasize the value of training sets that explicitly model real-world heterogeneity.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "sets",
                    "model",
                    "all",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To examine the impact of replay data, we retrained all baseline models on the EchoFake training set after removing replay samples and compared their performance with models trained on the full dataset (For fair comparison, we downsampled the original training set to match the replay-excluded set&#8217;s sample size). When evaluated across five conventional benchmarks (ASV19LA, ASV21LA, ASV21DF, In-the-Wild, and WaveFake), the EER changes after removing replay data were marginal: </span>\n  <math alttext=\"-0.03\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">0.03</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-0.03\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (RawNet), </span>\n  <math alttext=\"+1.67\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">1.67</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+1.67\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (AASIST), and </span>\n  <math alttext=\"+4.07\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">4.07</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+4.07\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (wav2vec2). However, when including the EchoFake-open set for evaluation (six datasets in total), average EERs increased by </span>\n  <math alttext=\"+1.54\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">1.54</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+1.54\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"+5.26\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">5.26</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+5.26\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"+6.22\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">+</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">6.22</mn>\n          <mo mathsize=\"0.900em\">%</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">+6.22\\%</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively. These results demonstrate that incorporating replay data in training provides a favorable trade-off: it does not significantly degrade performance on conventional benchmarks while substantially enhancing robustness against replay-based attacks.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "across",
                    "all",
                    "aasist",
                    "wav2vec2",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we present EchoFake, a novel and comprehensive dataset designed to advance ADD system development under realistic conditions. By integrating both zero-shot TTS speech and diverse physical replay recordings, EchoFake captures spoofing patterns overlooked in existing datasets. Evaluations on EchoFake reveal that current models suffer significant performance degradation in realistic replay scenarios, highlighting critical weaknesses in both model robustness and dataset coverage. Meanwhile, incorporating EchoFake during training improves generalization across multiple benchmarks, suggesting the benefit of modeling real-world attack variability. We hope that EchoFake will serve as a valuable benchmark and contribute toward building more resilient and deployable anti-spoofing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "echofake",
                    "across",
                    "model",
                    "performance"
                ]
            }
        ]
    }
}