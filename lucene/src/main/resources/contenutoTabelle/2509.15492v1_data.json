{
    "S3.T1": {
        "source_file": "Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech",
        "caption": "Table 1: Model comparison of video to audio with context-aware speech. VoiceLDM is a text-guided context-aware TTS model.",
        "body": "Method\nDataset\n\nWER ↓\\downarrow\n\n\nΔ\\DeltaWER ↓\\downarrow\n\n\nFAD ↓\\downarrow\n\n\nDeSync ↓\\downarrow\n\n\nLPAPS ↓\\downarrow\n\n\n\nGT\n-\n27.4 %\n3.2 %\n-\n-\n-\n\n\nVoiceLDM\nMultiple\n\n17.2 %\n\n22.5 %\n0.36\n1.49\n6.48\n\n\nVATT\nVGGSound\n98.9 %\n75.6 %\n0.43\n1.33\n6.65\n\n\nBVS\nAS-Speech\n\n26.4 %\n\n\n21.9 %\n\n0.41\n1.33\n6.38",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">WER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">WER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FAD </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DeSync </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">LPAPS </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">GT</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.4 %</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.2 %</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VoiceLDM</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Multiple</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">17.2</span><span class=\"ltx_text\" style=\"font-size:90%;\"> %</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.5 %</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.36</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.49</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.48</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VATT</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VGGSound</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.9 %</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">75.6 %</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.43</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.33</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.65</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BVS</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">AS-Speech</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">26.4</span><span class=\"ltx_text\" style=\"font-size:90%;\"> %</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">21.9</span><span class=\"ltx_text\" style=\"font-size:90%;\"> %</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">0.41</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.33</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.38</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "asspeech",
            "↓downarrow",
            "video",
            "voiceldm",
            "speech",
            "desync",
            "tts",
            "vatt",
            "vggsound",
            "δdeltawer",
            "wer",
            "bvs",
            "model",
            "multiple",
            "dataset",
            "fad",
            "textguided",
            "method",
            "contextaware",
            "lpaps",
            "comparison",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the effectiveness of BVS on video-to-audio with context-aware speech by incorporating a pretrained text-to-speech SSL generator in MaskGCT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As there are no existing methods that directly address this task, we select two most relevant baselines: VoiceLDM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a text-to-context-aware TTS model that generates speech aligned with the semantic content of text prompts, and VATT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a V2A model that produces ambient sounds temporally synchronized with visual input through audio captions.\nWe exclude the concurrent work, DualDub&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as it focuses on generating speech from videos featuring talking heads.\nWe evaluate both baseline methods with their official checkpoints, and the results are reported in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Video to Audio with Context-Aware Speech &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Although VoiceLDM focuses on synthesizing context-aware speech given text prompts and is trained on both AudioSet-Speech and multiple synthesized noisy speech datasets with a CLAP text encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, its worse DeSync and LPAPS scores suggest that it struggles to generate audios with synchronized sound effects given prompts that contain events in temporal order. Conversely, VATT is designed to produce temporally aligned audio from videos; however, the high WER and </span>\n  <math alttext=\"\\Delta\\textnormal{WER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mtext mathsize=\"0.900em\">WER</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta\\textnormal{WER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicate VATT fails to generate intelligible speech. Compared to both baselines, BVS overcomes these limitations by generating temporally and semantically synchronized audio with intelligible speech given transcripts and videos. Notably, despite being trained solely on AudioSet-Speech, BVS achieves WER values closer to the ground truth with a lower </span>\n  <math alttext=\"\\Delta\\textnormal{WER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mtext mathsize=\"0.900em\">WER</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta\\textnormal{WER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which demonstrating the robustness of BVS in generating synchronized audio with context-aware intelligible speech.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The generation of realistic, context-aware audio is important in real-world applications such as video game development. While existing video-to-audio (V2A) methods mainly focus on Foley sound generation, they struggle to produce intelligible speech. Meanwhile, current environmental speech synthesis approaches remain text-driven and fail to temporally align with dynamic video content. In this paper, we propose Beyond Video-to-SFX (BVS), a method to generate synchronized audio with environmentally aware intelligible speech for given videos. We introduce a two-stage modeling method: (1) stage one is a video-guided audio semantic (V2AS) model to predict unified audio semantic tokens conditioned on phonetic cues; (2) stage two is a video-conditioned semantic-to-acoustic (VS2A) model that refines semantic tokens into detailed acoustic tokens. Experiments demonstrate the effectiveness of BVS in scenarios such as video-to-context-aware speech synthesis and immersive audio background conversion, with ablation studies further validating our design. Our demonstration is available at&#160;<a class=\"ltx_ref ltx_href\" href=\"https://xinleiniu.github.io/BVS-demo/\" title=\"\">BVS-Demo</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "video",
                    "method",
                    "contextaware",
                    "audio",
                    "bvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The generation of realistic, high-fidelity audio, including sound effects and speech, has become increasingly important for real-world applications such as film production, virtual reality (VR), and game development. To this end, useful generative models must generate audio that is coherent, expressive, and contextually aligned with the content of the given condition. Within this domain, video-to-audio (V2A) focuses on generating audio that is temporally and semantically aligned with visual cues in videos. In many scenarios, the audio track includes not only visually grounded sounds but also speech, and the speaker may not be directly observable. For example, a video clip of someone washing dishes might include background conversations that are not visually depicted. Motivated by this challenge, our work aims to develop a method capable of synthesizing audio that incorporates environmentally aware speech. The goal is to generate audio that is temporally aligned with the video while ensuring semantic consistency and incorporating context-aware spoken content.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "video",
                    "method",
                    "contextaware",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Previous V2A studies&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib6\" title=\"\">6</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focus on Foley sound generation, aiming to synthesize ambient sounds that are temporally and semantically aligned with the video. More recent approaches&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> extend this paradigm by incorporating text prompts alongside video input. The inclusion of textual context enables models to capture more complex scenarios, such as the sound scene of &#8216;a man speaking while he is playing tennis&#8217;. However, these methods remain limited to sound event-level alignment driven by visual and/or textual cues and lack the capability to generate coherent and intelligible speech.\nConsequently, they often fail to generate natural spoken content, which restricts their real-world application where context-aware spoken content is essential.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "contextaware",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A closely related task is video-to-speech (V2S)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which focuses on generating speech from videos with talking heads. However, V2S methods primarily address clean speech synthesis and do not handle sound effect generation, making them complementary but distinct from V2A approaches. A concurrent work by&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> demonstrates V2S generation with integrated sound effects; however, their approach is primarily designed for videos with talking heads.\nIn parallel, environmental context-aware speech synthesis&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focuses on generating speech that incorporates background noise consistent with the given text prompts. These methods typically leverage a complex training strategy to learn the speech information. They first pretrain on synthetic noisy speech datasets and then fine-tune on real-world datasets transcribed by automatic speech recognition (ASR) models to improve speech modeling. Other related approaches&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> explore mixing clean speech with ambient sounds to create contextually consistent audio environments. While these techniques are effective in achieving semantic consistency between speech and background context, they generally lack fine-grained temporal control over sound events, a capability that is critical for synchronizing sound effects with dynamic video content.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "contextaware",
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We identify two major limitations in previous works. Firstly, a key challenge is the scarcity of speech datasets that simultaneously provide context-rich environmental sounds, paired video streams, and ground-truth transcripts. Previous methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> first train their models with synthesized noisy speech by mixing clean speech data for TTS training with environmental sound data, and then fine-tune using real-world unlabeled audio data containing speech with transcripts obtained from ASR models.\nHowever, ASR transcripts often suffer from inaccuracies and unstable segmentation in noisy speech, which introduces incorrect labels during speech learning. Meanwhile, synthetic noisy speech lacks natural video grounding since corresponding video streams cannot be created, limiting multimodal consistency. Secondly, current V2A models often fail to generate intelligible and natural speech, and often lack control over the speech content. As a result, additional post-production is often needed to remove unintelligible vocals and mix clean speech with ambient sounds for contextual alignment, which reduces their practicality in real-world applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts",
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we propose Beyond Video-to-SFX (BVS), a method for synthesizing video-synchronized audio that incorporates environmental-aware speech. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, BVS generates soundtracks that are temporally and semantically aligned with visual content while integrating context-aware intelligible speech, guided by provided transcripts or audios containing spoken content.\nWe make the following contributions:\n(1) We propose BVS, an improved V2A framework enabling intelligible speech synthesis, addressing a core limitation of previous approaches.\n(2) Our method is trained directly on raw audio data, eliminating the need for complex preprocessing steps such as synthetic noisy datasets or filtering ASR-generated transcripts.\n(3) We demonstrate that BVS is a flexible framework for various downstream tasks, including audio background conversion and video to context-aware speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "video",
                    "method",
                    "contextaware",
                    "audio",
                    "bvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Generating real-world audio that matches a visual scene is challenging. Audio tracks such as speech, sound effects, and ambient sounds are each influenced by distinct factors. For example, speech often occurs with irregular onset and offset times. It is primarily human-related, frequently off-screen, and relies more on phonetic cues than on visual context. In contrast, sound effects and ambient sounds are closely linked to visual objects and scene context, exhibiting highly variable temporal alignment with the video. These fundamental differences make it difficult for a single model to jointly generate both intelligible speech and natural ambient sounds directly from phonetic and visual cues. Moreover, to create a convincing audio experience for a given video, all audio tracks must remain coherent and share consistent environmental characteristics.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "video",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this, we introduce an audio semantic token space as a unified representation of speech and ambient sounds inspired from two-stage TTS pipelines&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The semantic tokens capture abstract audio information, reducing the gap between multi-modal conditions and acoustic outputs. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our BVS pipeline operates in two stages: (1) fuse information across speech, visual, and contextual modalities into audio semantic tokens, and (2) refine these audio semantic tokens into acoustic outputs. This separation reduces the burden of a single-stage model that needs to learn both multimodal fusion and acoustic generation simultaneously.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts",
                    "audio",
                    "bvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We design a video-to-audio semantic (V2AS) masked generative transformer, which generates audio semantic tokens by jointly integrating the speech and sound effects from video and phonetic cues. As mentioned before, speech, sound effects, and ambient sounds are each related to different objects. We split the audio semantic token generation into two components: (1) clean speech semantic token generation in the form of speech SSL token; (2) audio semantic fusion according to speech and video cues.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Noisy Audio to Speech Semantic Token:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe aim to mitigate inaccurate transcription in previous methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and directly train models on raw noisy audios containing speech. To guide the model in identifying speech regions within raw audio,\nwe leverage a pretrained speech SSL token generator&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> trained for the speech enhancement (SE) task to extract speech information from noisy inputs. Rather than employing the full model to directly separate clean speech waveform, we use its first-stage generative transformer to predict speech self-supervised learning (SSL) token sequences in the quantized W2V-BERT feature space&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, obtained via a VQ-VAE&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Formally, given a raw audio input </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the pretrained SE masked generative transformer captures speech SSL tokens </span>\n  <math alttext=\"S_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">S_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from </span>\n  <math alttext=\"p_{\\phi}(S_{s}|x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mi mathsize=\"0.900em\">&#981;</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">S</mi>\n              <mi mathsize=\"0.900em\">s</mi>\n            </msub>\n            <mo fence=\"false\" mathsize=\"0.900em\">|</mo>\n            <mi mathsize=\"0.900em\">x</mi>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p_{\\phi}(S_{s}|x)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#981;</mi>\n      <annotation encoding=\"application/x-tex\">\\phi</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the parameter of the pretrained SE model.\nThis design emphasizes capturing of speech information, rather than reconstructing the clean speech waveform, which offers two key advantages: (1) it disentangles speech information more effectively under noisy conditions of the raw audio, and (2) it reduces downstream decoding errors by using compact speech SSL tokens.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audio Semantic Fusion:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWhile speech semantic tokens allow the model to be aware of speech regions directly from raw audio, the remaining challenge is to fuse the speech information with video context while generating synchronized audios from visual cues. To address this, we make use of the quantized W2V-BERT feature space as the unified audio semantic token space, which is motivated by our empirical findings: Although trained primarily on speech, SSL models also encode non-speech acoustic cues. This is also pointed out in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We leveraged a pretrained semantic to acoustic generative model to reconstruct the waveform for a given SSL token sequence including both speech and background noise. We empirically observed that the reconstructed audio contains both intelligible speech and matching background noise. Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2 Method &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further illustrates this empirical finding. We also provide the audio samples on our &#160;</span>\n  <a class=\"ltx_ref ltx_href\" href=\"https://xinleiniu.github.io/BVS-demo/\" style=\"font-size:90%;\" title=\"\">Demo-page</a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Reconstructions of the W2V-BERT SSL tokens preserve both speech and non-speech information. Our empirical analysis further shows that the residual difference between speech SSL embeddings and full audio semantic embeddings potentially corresponds to non-speech components such as background noise or ambient sounds. This suggests that speech and non-speech information can be disentangled and recombined in the SSL embedding space.\nMotivated by this observation, the V2AS model predicts audio semantic token sequences in the same quantized W2V-BERT feature space that fuses non-speech acoustic information with speech information into audio semantic tokens using complementary cues from speech SSL tokens and videos. This process is shown in the right-hand side of stage 1 in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nBy predicting semantic tokens rather than raw acoustic outputs, the model focuses on learning cross-modal relationships between speech, ambient sounds, and visual context.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "video",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Objective:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe use the mask-and-predict learning strategy from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and mask the ground truth audio semantic token sequence </span>\n  <math alttext=\"S_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">S_{a}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> at each time step </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with a corresponding binary mask </span>\n  <math alttext=\"M_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">M</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">M_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Items in </span>\n  <math alttext=\"S_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">S_{a}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are replaced with a special [MASK] token if </span>\n  <math alttext=\"m_{t}^{i}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">m</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msubsup>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m_{t}^{i}=1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, otherwise </span>\n  <math alttext=\"S_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m6\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">S_{a}^{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> remain unchanged. The resulting masked token sequence is denoted as </span>\n  <math alttext=\"S_{a}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m7\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n        <mi mathsize=\"0.900em\">M</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">S_{a}^{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Each </span>\n  <math alttext=\"m_{t}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m8\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">m_{t}^{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is i.i.d to a Bernoulli distribution with parameter </span>\n  <math alttext=\"\\gamma(t)\\in(0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">&#947;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\gamma(t)\\in(0,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Thus, the V2AS model is formulated as </span>\n  <math alttext=\"p_{\\theta_{\\text{V2AS}}}(S_{a}|S^{M}_{a},V,S_{s})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">p</mi>\n          <msub>\n            <mi mathsize=\"0.900em\">&#952;</mi>\n            <mtext mathsize=\"0.900em\">V2AS</mtext>\n          </msub>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">S</mi>\n              <mi mathsize=\"0.900em\">a</mi>\n            </msub>\n            <mo fence=\"false\" mathsize=\"0.900em\">|</mo>\n            <mrow>\n              <msubsup>\n                <mi mathsize=\"0.900em\">S</mi>\n                <mi mathsize=\"0.900em\">a</mi>\n                <mi mathsize=\"0.900em\">M</mi>\n              </msubsup>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mi mathsize=\"0.900em\">V</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <msub>\n                <mi mathsize=\"0.900em\">S</mi>\n                <mi mathsize=\"0.900em\">s</mi>\n              </msub>\n            </mrow>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p_{\\theta_{\\text{V2AS}}}(S_{a}|S^{M}_{a},V,S_{s})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m11\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">V</mi>\n      <annotation encoding=\"application/x-tex\">V</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents video conditions. We train the model using cross-entropy loss between predicted audio semantic tokens </span>\n  <math alttext=\"\\hat{S}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m12\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">S</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">a</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\hat{S}_{a}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ground truth </span>\n  <math alttext=\"S_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m13\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">S_{a}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as</span>\n</p>\n\n",
                "matched_terms": [
                    "video",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Architecture:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nTo further enhance the model&#8217;s video understanding ability, we incorporate visual representations enriched by audio captions generated with a V2Cap large language model (LLM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which we denote as </span>\n  <math alttext=\"E_{\\text{VCap}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m14\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mtext mathsize=\"0.900em\">VCap</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">E_{\\text{VCap}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The detailed V2AS architecture is shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (in the yellow dash box), which extends a V2A transformer framework&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by injecting speech SSL embeddings into extra cross-attention blocks as </span>\n  <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m15\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">K</mi>\n      <annotation encoding=\"application/x-tex\">K</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m16\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">V</mi>\n      <annotation encoding=\"application/x-tex\">V</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"N_{1}=12\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m17\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">12</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N_{1}=12</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"N_{2}=14\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m18\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">14</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N_{2}=14</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This integration improves speech localization and intelligibility, and captures non-speech details guided by video context. During training, we concatenate the masked audio semantic embeddings </span>\n  <math alttext=\"E_{a}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m19\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n        <mi mathsize=\"0.900em\">M</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E_{a}^{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"E_{\\text{VCap}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m20\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mtext mathsize=\"0.900em\">VCap</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">E_{\\text{VCap}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> along the temporal axis. During inference, the V2AS model generates audio semantic token sequences conditioned on the video prompt and speech SSL tokens. The speech SSL token sequences can be obtained by a unified speech generative transformer such as Metis stage 1&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which takes transcripts, audio, or other inputs.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "video",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We then train a video-guided semantic-to-acoustic (VS2A) model to obtain acoustic token sequences conditioned on audio semantic token sequences and </span>\n  <math alttext=\"E_{\\text{VCap}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mtext mathsize=\"0.900em\">VCap</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">E_{\\text{VCap}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The VS2A model refines generation by mapping abstract audio semantic representations into acoustic tokens while leveraging video context to enhance the alignment. Following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the VS2A model is a masked generative transformer that produces a multi-layer acoustic </span>\n  <math alttext=\"A^{1:K}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">A</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <mi mathsize=\"0.900em\">K</mi>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">A^{1:K}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">K</mi>\n      <annotation encoding=\"application/x-tex\">K</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the number of codebooks, using iterative parallel decoding within each layer. For simplicity, we adopt separate VS2A models: one dedicated to predicting the first-layer acoustic codec and another for the subsequent layers. We denote the </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-th acoustic token layer as </span>\n  <math alttext=\"A^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">A</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">A^{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and formulate the model as </span>\n  <math alttext=\"p_{\\theta_{\\text{VS2A}}}(A^{i}|V,A^{1:i-1})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">p</mi>\n          <msub>\n            <mi mathsize=\"0.900em\">&#952;</mi>\n            <mtext mathsize=\"0.900em\">VS2A</mtext>\n          </msub>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <msup>\n              <mi mathsize=\"0.900em\">A</mi>\n              <mi mathsize=\"0.900em\">i</mi>\n            </msup>\n            <mo fence=\"false\" mathsize=\"0.900em\">|</mo>\n            <mrow>\n              <mi mathsize=\"0.900em\">V</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <msup>\n                <mi mathsize=\"0.900em\">A</mi>\n                <mrow>\n                  <mn mathsize=\"0.900em\">1</mn>\n                  <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n                  <mrow>\n                    <mi mathsize=\"0.900em\">i</mi>\n                    <mo mathsize=\"0.900em\">&#8722;</mo>\n                    <mn mathsize=\"0.900em\">1</mn>\n                  </mrow>\n                </mrow>\n              </msup>\n            </mrow>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p_{\\theta_{\\text{VS2A}}}(A^{i}|V,A^{1:i-1})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. During training, we simply sum the embeddings of the audio semantic tokens and the embeddings of the acoustic codec tokens from layer 1 to </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is denoted as </span>\n  <math alttext=\"E_{C_{i}}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <msub>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mi mathsize=\"0.900em\">M</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E_{C_{i}}^{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> . Then we concatenate </span>\n  <math alttext=\"E_{\\text{VCap}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mtext mathsize=\"0.900em\">VCap</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">E_{\\text{VCap}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"E_{C_{i}}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <msub>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mi mathsize=\"0.900em\">M</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E_{C_{i}}^{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as input of the model. We use cross-entropy loss between predicted acoustic tokens and ground truth acoustic tokens as</span>\n</p>\n\n",
                "matched_terms": [
                    "video",
                    "model",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe train our model on the filtered English subset of AudioSet-Speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (AS-Speech), which contains approximately 580k audio&#8211;video pairs. Each sample has a 10-second duration and contains both speech and various environmental sound events. For evaluation, we adopt AC-filtered&#160;</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_href\" href=\"https://github.com/glory20h/voiceldm-data\" title=\"\">https://github.com/glory20h/voiceldm-data</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which includes audio sourced from AudioCap&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, captions, and transcripts. As AudioCaps is derived from AudioSet, we retrieve the corresponding video streams via the YouTube IDs, resulting in a curated set of 152 samples, which we refer to as AS-filtered dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "asspeech",
                    "dataset",
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Implementation Details:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe train V2AS and VS2A models on three NVIDIA A100 (80GB) GPUs for four and three days, respectively, using a batch size of 32, a learning rate of </span>\n  <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, 20k and 32k warm-up steps, for approximately 80 and 34 epochs, respectively. We optimize these models with the AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer. We adopt VATT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> transformer with extra cross-attention blocks for our V2AS model, and we adopt the S2A model architecture in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the VS2A model. For the video modality, we employ the EVA-CLIP image encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to extract mean-pooled frame features sampled at 5 fps, yielding a visual sequence of size </span>\n  <math alttext=\"50\\times 768\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">50</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"0.900em\">768</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">50\\times 768</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for a 10-second video. Following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the visual sequence passes through a pretrained V2Cap LLaMa and we extract its hidden state as input features. We add two learnable modality-specific embeddings to the concatenated inputs in the V2AS model. We use Metis SE stage 1 model to obtain speech SSL tokens&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Audio SSL features are extracted from W2V-BERT 2.0&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which are then quantized into discrete tokens at 50 tokens rate per second by the pretrained VQ-VAE in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Acoustic tokens are obtained from 16kHz Encodec&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with four codebooks at a 50 Hz token rate. During inference, we use 16 steps for the V2AS model and [20, 10, 1, 1] steps for the VS2A model. The classifier-free guidance (CFG) scale is set to 5.0 for V2AS and 2.5 for VS2A.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "vatt",
                    "video",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Metrics:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate our method along three key dimensions: speech intelligibility, temporal alignment, and semantic perceptual similarity. For speech intelligibility, we report both the word error rate (WER) and </span>\n  <math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n      <annotation encoding=\"application/x-tex\">\\Delta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">WER. WER is computed between the ground truth transcripts and those obtained from transcribing our generated audio using the whisper-large-v3. Since ASR models are trained on transcribing clean speech, it may produce inaccurate transcripts on noisy inputs. We additionally report </span>\n  <math alttext=\"\\Delta\\textnormal{WER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mtext mathsize=\"0.900em\">WER</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta\\textnormal{WER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which measures the absolute difference in WER between the ground truth and generated results, as </span>\n  <math alttext=\"\\Delta\\text{WER}=\\frac{1}{N}\\sum^{N}_{i}|\\text{WER}(\\text{gt}_{i},t_{i})-\\text{WER}(\\text{pred}_{i},t_{i})|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mtext mathsize=\"0.900em\">WER</mtext>\n        </mrow>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mfrac>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mi mathsize=\"0.900em\">N</mi>\n          </mfrac>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <msubsup>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" rspace=\"0em\" stretchy=\"true\">&#8721;</mo>\n              <mi mathsize=\"0.900em\">i</mi>\n              <mi mathsize=\"0.900em\">N</mi>\n            </msubsup>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n              <mrow>\n                <mrow>\n                  <mtext mathsize=\"0.900em\">WER</mtext>\n                  <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                  <mrow>\n                    <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                    <msub>\n                      <mtext mathsize=\"0.900em\">gt</mtext>\n                      <mi mathsize=\"0.900em\">i</mi>\n                    </msub>\n                    <mo mathsize=\"0.900em\">,</mo>\n                    <msub>\n                      <mi mathsize=\"0.900em\">t</mi>\n                      <mi mathsize=\"0.900em\">i</mi>\n                    </msub>\n                    <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n                  </mrow>\n                </mrow>\n                <mo mathsize=\"0.900em\">&#8722;</mo>\n                <mrow>\n                  <mtext mathsize=\"0.900em\">WER</mtext>\n                  <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                  <mrow>\n                    <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                    <msub>\n                      <mtext mathsize=\"0.900em\">pred</mtext>\n                      <mi mathsize=\"0.900em\">i</mi>\n                    </msub>\n                    <mo mathsize=\"0.900em\">,</mo>\n                    <msub>\n                      <mi mathsize=\"0.900em\">t</mi>\n                      <mi mathsize=\"0.900em\">i</mi>\n                    </msub>\n                    <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n                  </mrow>\n                </mrow>\n              </mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n            </mrow>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta\\text{WER}=\\frac{1}{N}\\sum^{N}_{i}|\\text{WER}(\\text{gt}_{i},t_{i})-\\text{WER}(\\text{pred}_{i},t_{i})|</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">t_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents the transcripts for audio sample </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo assess temporal alignment between video and generated audio, we follow&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and report the DeSync&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For semantic and perceptual similarity, we follow&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and calculate the FAD and LPAPS scores, which are computed using the clap-laion-audio model in music-speech-audioset.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "desync",
                    "model",
                    "fad",
                    "video",
                    "δdeltawer",
                    "method",
                    "wer",
                    "lpaps",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We then demonstrate the effectiveness of BVS in immersive audio background conversion. Unlike the setting in&#160;Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Video to Audio with Context-Aware Speech &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, this task takes noisy speech as input and converts it into a new context-aware speech that is naturally integrated within the given video&#8217;s content. We randomly sample 500 video&#8211;audio pairs from the AS-filtered dataset, which serve as target video and source audio. Since no direct baselines exist, we construct a proxy baseline by first applying a Metis speech enhancement model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to obtain clean speech, which is then directly mixed with sound effects generated by VATT. To prevent VATT from generating unintelligible human voices, we exclude caption inputs and make the model focus on generating sound effects.\nTo measure the consistency of converted audio between source and target audio, we report LPAPS</span>\n  <math alttext=\"{}_{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">T</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{T}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and LPAPS</span>\n  <math alttext=\"{}_{\\text{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">S</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{S}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where LPAPS</span>\n  <math alttext=\"{}_{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">T</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{T}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> measures similarity to the audio track of target video, and LPAPS</span>\n  <math alttext=\"{}_{\\text{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">S</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{S}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> measures similarity to the source audio.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.1 Video to Audio with Context-Aware Speech &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, although the baseline achieves a lower DeSync score, our method outperforms it in terms of LPAPS and FAD, reflecting higher perceptual similarity to ground truth. In contrast, simple post hoc mixing of clean speech with generated sound effects often results in poor semantic alignment between speech and environmental context, whereas BVS produces coherent and context-aware audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "desync",
                    "model",
                    "dataset",
                    "fad",
                    "vatt",
                    "video",
                    "method",
                    "contextaware",
                    "lpaps",
                    "audio",
                    "bvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We first validate whether the audio semantic tokens obtained from W2V-BERT have the ability to represent non-speech information. As shown in the first row of Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.3 Ablation study &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we evaluate reconstruction results on AS-filtered using our VS2A model conditioned on ground-truth audio semantic tokens and videos. The metrics of reconstructed samples reflect that the VS2A model can generate audio that contains both speech and non-speech sounds, with better scores than generation results in Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Video to Audio with Context-Aware Speech &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Immersive Audio Background Conversion &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe then conduct ablation studies to evaluate the importance of the speech semantic token from the SE model and the audio semantic tokens in our two-stage method.\nFirstly, we remove the audio semantic tokens from BVS, converting it into a single-stage model that directly predicts acoustic tokens, which we denote as BVS-Single. Secondly, we remove speech SSL tokens obtained from raw audios by replacing the SE speech SSL generator with a transcript-based speech SSL generator used in MaskGCT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We use Whisper&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to enable alignment of speech onset and offset times. We use filtered transcripts obtained from an ASR model by&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to guide phonetic information. We refer to this as BVS-Text.\nAs illustrated in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.3 Ablation study &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, both methods fail to learn speech information effectively with significantly higher WER and FAD. In BVS-Text, although the model is given transcripts with speech onset-offset timestamps, the model still fails to learn the complex alignment of speech and sound effects from video and transcripts. This indicates that the models fail to capture the relationship between video, speech, and context information. In contrast, our method with both speech SSL tokens obtained directly from raw audios, and a two-stage design explicitly guides the model to disentangle, fuse, and refine the information of speech and sound effects.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "fad",
                    "video",
                    "method",
                    "wer",
                    "audio",
                    "bvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we introduce BVS, which addresses the challenge of video-to-audio generation with intelligible and environmentally aware speech. By leveraging a pretrained speech SSL generator during training, BVS disentangles speech information from noisy audio, and then effectively fuses cues from video, speech, and audio modalities, and generates audio with a two-stage design. Our experiments demonstrate that BVS can be applied to diverse real-world scenarios, including both generation and conversion tasks. Despite these promising results, we observed some limitations. Since BVS relies on speech SSL models for the unified audio semantic token space, which performance on non-speech sound effects remains constrained. Moreover, due to limitations in the available datasets, accurate lip&#8211;speech synchronization remains challenging. These limitations point to important directions for future research. In particular, exploring more powerful unified speech&#8211;audio SSL representations may improve sound effect generation, while incorporating richer multimodal datasets could enhance synchronization and further advance the quality of video-to-audio synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "video",
                    "audio",
                    "bvs"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech",
        "caption": "Table 2: Experiment on immersive audio background conversion.",
        "body": "Method\n\nΔ\\DeltaWER ↓\\downarrow\n\n\nFAD ↓\\downarrow\n\n\nDeSync ↓\\downarrow\n\n\nLPAPS↓S{}_{\\text{S}}\\downarrow\n\n\nLPAPS↓T{}_{\\text{T}}\\downarrow\n\n\n\n\n\nSE + VATT\n27.8 %\n0.39\n1.22\n6.69\n6.47\n\n\nBVS\n\n26.9 %\n\n0.38\n1.32\n6.42\n6.28",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">WER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FAD </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DeSync </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">LPAPS</span><math alttext=\"{}_{\\text{S}}\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><mmultiscripts><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><mprescripts/><mtext mathsize=\"0.900em\">S</mtext><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{\\text{S}}\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">LPAPS</span><math alttext=\"{}_{\\text{T}}\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><mmultiscripts><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><mprescripts/><mtext mathsize=\"0.900em\">T</mtext><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{\\text{T}}\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SE + VATT</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.8 %</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.39</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.22</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.69</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.47</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BVS</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">26.9</span><span class=\"ltx_text\" style=\"font-size:90%;\"> %</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.38</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.32</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.42</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.28</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "bvs",
            "desync",
            "lpaps↓ttexttdownarrow",
            "conversion",
            "↓downarrow",
            "fad",
            "lpaps↓stextsdownarrow",
            "vatt",
            "δdeltawer",
            "immersive",
            "experiment",
            "method",
            "audio",
            "background"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We then demonstrate the effectiveness of BVS in immersive audio background conversion. Unlike the setting in&#160;Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Video to Audio with Context-Aware Speech &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, this task takes noisy speech as input and converts it into a new context-aware speech that is naturally integrated within the given video&#8217;s content. We randomly sample 500 video&#8211;audio pairs from the AS-filtered dataset, which serve as target video and source audio. Since no direct baselines exist, we construct a proxy baseline by first applying a Metis speech enhancement model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to obtain clean speech, which is then directly mixed with sound effects generated by VATT. To prevent VATT from generating unintelligible human voices, we exclude caption inputs and make the model focus on generating sound effects.\nTo measure the consistency of converted audio between source and target audio, we report LPAPS</span>\n  <math alttext=\"{}_{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">T</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{T}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and LPAPS</span>\n  <math alttext=\"{}_{\\text{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">S</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{S}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where LPAPS</span>\n  <math alttext=\"{}_{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">T</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{T}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> measures similarity to the audio track of target video, and LPAPS</span>\n  <math alttext=\"{}_{\\text{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">S</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{S}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> measures similarity to the source audio.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.1 Video to Audio with Context-Aware Speech &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, although the baseline achieves a lower DeSync score, our method outperforms it in terms of LPAPS and FAD, reflecting higher perceptual similarity to ground truth. In contrast, simple post hoc mixing of clean speech with generated sound effects often results in poor semantic alignment between speech and environmental context, whereas BVS produces coherent and context-aware audio.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The generation of realistic, context-aware audio is important in real-world applications such as video game development. While existing video-to-audio (V2A) methods mainly focus on Foley sound generation, they struggle to produce intelligible speech. Meanwhile, current environmental speech synthesis approaches remain text-driven and fail to temporally align with dynamic video content. In this paper, we propose Beyond Video-to-SFX (BVS), a method to generate synchronized audio with environmentally aware intelligible speech for given videos. We introduce a two-stage modeling method: (1) stage one is a video-guided audio semantic (V2AS) model to predict unified audio semantic tokens conditioned on phonetic cues; (2) stage two is a video-conditioned semantic-to-acoustic (VS2A) model that refines semantic tokens into detailed acoustic tokens. Experiments demonstrate the effectiveness of BVS in scenarios such as video-to-context-aware speech synthesis and immersive audio background conversion, with ablation studies further validating our design. Our demonstration is available at&#160;<a class=\"ltx_ref ltx_href\" href=\"https://xinleiniu.github.io/BVS-demo/\" title=\"\">BVS-Demo</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "method",
                    "immersive",
                    "background",
                    "audio",
                    "bvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The generation of realistic, high-fidelity audio, including sound effects and speech, has become increasingly important for real-world applications such as film production, virtual reality (VR), and game development. To this end, useful generative models must generate audio that is coherent, expressive, and contextually aligned with the content of the given condition. Within this domain, video-to-audio (V2A) focuses on generating audio that is temporally and semantically aligned with visual cues in videos. In many scenarios, the audio track includes not only visually grounded sounds but also speech, and the speaker may not be directly observable. For example, a video clip of someone washing dishes might include background conversations that are not visually depicted. Motivated by this challenge, our work aims to develop a method capable of synthesizing audio that incorporates environmentally aware speech. The goal is to generate audio that is temporally aligned with the video while ensuring semantic consistency and incorporating context-aware spoken content.</span>\n</p>\n\n",
                "matched_terms": [
                    "method",
                    "audio",
                    "background"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A closely related task is video-to-speech (V2S)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which focuses on generating speech from videos with talking heads. However, V2S methods primarily address clean speech synthesis and do not handle sound effect generation, making them complementary but distinct from V2A approaches. A concurrent work by&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> demonstrates V2S generation with integrated sound effects; however, their approach is primarily designed for videos with talking heads.\nIn parallel, environmental context-aware speech synthesis&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focuses on generating speech that incorporates background noise consistent with the given text prompts. These methods typically leverage a complex training strategy to learn the speech information. They first pretrain on synthetic noisy speech datasets and then fine-tune on real-world datasets transcribed by automatic speech recognition (ASR) models to improve speech modeling. Other related approaches&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> explore mixing clean speech with ambient sounds to create contextually consistent audio environments. While these techniques are effective in achieving semantic consistency between speech and background context, they generally lack fine-grained temporal control over sound events, a capability that is critical for synchronizing sound effects with dynamic video content.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "background"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we propose Beyond Video-to-SFX (BVS), a method for synthesizing video-synchronized audio that incorporates environmental-aware speech. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, BVS generates soundtracks that are temporally and semantically aligned with visual content while integrating context-aware intelligible speech, guided by provided transcripts or audios containing spoken content.\nWe make the following contributions:\n(1) We propose BVS, an improved V2A framework enabling intelligible speech synthesis, addressing a core limitation of previous approaches.\n(2) Our method is trained directly on raw audio data, eliminating the need for complex preprocessing steps such as synthetic noisy datasets or filtering ASR-generated transcripts.\n(3) We demonstrate that BVS is a flexible framework for various downstream tasks, including audio background conversion and video to context-aware speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "method",
                    "background",
                    "audio",
                    "bvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this, we introduce an audio semantic token space as a unified representation of speech and ambient sounds inspired from two-stage TTS pipelines&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The semantic tokens capture abstract audio information, reducing the gap between multi-modal conditions and acoustic outputs. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our BVS pipeline operates in two stages: (1) fuse information across speech, visual, and contextual modalities into audio semantic tokens, and (2) refine these audio semantic tokens into acoustic outputs. This separation reduces the burden of a single-stage model that needs to learn both multimodal fusion and acoustic generation simultaneously.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "bvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audio Semantic Fusion:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWhile speech semantic tokens allow the model to be aware of speech regions directly from raw audio, the remaining challenge is to fuse the speech information with video context while generating synchronized audios from visual cues. To address this, we make use of the quantized W2V-BERT feature space as the unified audio semantic token space, which is motivated by our empirical findings: Although trained primarily on speech, SSL models also encode non-speech acoustic cues. This is also pointed out in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We leveraged a pretrained semantic to acoustic generative model to reconstruct the waveform for a given SSL token sequence including both speech and background noise. We empirically observed that the reconstructed audio contains both intelligible speech and matching background noise. Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2 Method &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further illustrates this empirical finding. We also provide the audio samples on our &#160;</span>\n  <a class=\"ltx_ref ltx_href\" href=\"https://xinleiniu.github.io/BVS-demo/\" style=\"font-size:90%;\" title=\"\">Demo-page</a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Reconstructions of the W2V-BERT SSL tokens preserve both speech and non-speech information. Our empirical analysis further shows that the residual difference between speech SSL embeddings and full audio semantic embeddings potentially corresponds to non-speech components such as background noise or ambient sounds. This suggests that speech and non-speech information can be disentangled and recombined in the SSL embedding space.\nMotivated by this observation, the V2AS model predicts audio semantic token sequences in the same quantized W2V-BERT feature space that fuses non-speech acoustic information with speech information into audio semantic tokens using complementary cues from speech SSL tokens and videos. This process is shown in the right-hand side of stage 1 in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nBy predicting semantic tokens rather than raw acoustic outputs, the model focuses on learning cross-modal relationships between speech, ambient sounds, and visual context.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "background"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Implementation Details:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe train V2AS and VS2A models on three NVIDIA A100 (80GB) GPUs for four and three days, respectively, using a batch size of 32, a learning rate of </span>\n  <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, 20k and 32k warm-up steps, for approximately 80 and 34 epochs, respectively. We optimize these models with the AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer. We adopt VATT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> transformer with extra cross-attention blocks for our V2AS model, and we adopt the S2A model architecture in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the VS2A model. For the video modality, we employ the EVA-CLIP image encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to extract mean-pooled frame features sampled at 5 fps, yielding a visual sequence of size </span>\n  <math alttext=\"50\\times 768\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">50</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"0.900em\">768</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">50\\times 768</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for a 10-second video. Following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the visual sequence passes through a pretrained V2Cap LLaMa and we extract its hidden state as input features. We add two learnable modality-specific embeddings to the concatenated inputs in the V2AS model. We use Metis SE stage 1 model to obtain speech SSL tokens&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Audio SSL features are extracted from W2V-BERT 2.0&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which are then quantized into discrete tokens at 50 tokens rate per second by the pretrained VQ-VAE in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Acoustic tokens are obtained from 16kHz Encodec&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with four codebooks at a 50 Hz token rate. During inference, we use 16 steps for the V2AS model and [20, 10, 1, 1] steps for the VS2A model. The classifier-free guidance (CFG) scale is set to 5.0 for V2AS and 2.5 for VS2A.</span>\n</p>\n\n",
                "matched_terms": [
                    "vatt",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Metrics:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate our method along three key dimensions: speech intelligibility, temporal alignment, and semantic perceptual similarity. For speech intelligibility, we report both the word error rate (WER) and </span>\n  <math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n      <annotation encoding=\"application/x-tex\">\\Delta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">WER. WER is computed between the ground truth transcripts and those obtained from transcribing our generated audio using the whisper-large-v3. Since ASR models are trained on transcribing clean speech, it may produce inaccurate transcripts on noisy inputs. We additionally report </span>\n  <math alttext=\"\\Delta\\textnormal{WER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mtext mathsize=\"0.900em\">WER</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta\\textnormal{WER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which measures the absolute difference in WER between the ground truth and generated results, as </span>\n  <math alttext=\"\\Delta\\text{WER}=\\frac{1}{N}\\sum^{N}_{i}|\\text{WER}(\\text{gt}_{i},t_{i})-\\text{WER}(\\text{pred}_{i},t_{i})|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mtext mathsize=\"0.900em\">WER</mtext>\n        </mrow>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mfrac>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mi mathsize=\"0.900em\">N</mi>\n          </mfrac>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <msubsup>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" rspace=\"0em\" stretchy=\"true\">&#8721;</mo>\n              <mi mathsize=\"0.900em\">i</mi>\n              <mi mathsize=\"0.900em\">N</mi>\n            </msubsup>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n              <mrow>\n                <mrow>\n                  <mtext mathsize=\"0.900em\">WER</mtext>\n                  <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                  <mrow>\n                    <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                    <msub>\n                      <mtext mathsize=\"0.900em\">gt</mtext>\n                      <mi mathsize=\"0.900em\">i</mi>\n                    </msub>\n                    <mo mathsize=\"0.900em\">,</mo>\n                    <msub>\n                      <mi mathsize=\"0.900em\">t</mi>\n                      <mi mathsize=\"0.900em\">i</mi>\n                    </msub>\n                    <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n                  </mrow>\n                </mrow>\n                <mo mathsize=\"0.900em\">&#8722;</mo>\n                <mrow>\n                  <mtext mathsize=\"0.900em\">WER</mtext>\n                  <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                  <mrow>\n                    <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                    <msub>\n                      <mtext mathsize=\"0.900em\">pred</mtext>\n                      <mi mathsize=\"0.900em\">i</mi>\n                    </msub>\n                    <mo mathsize=\"0.900em\">,</mo>\n                    <msub>\n                      <mi mathsize=\"0.900em\">t</mi>\n                      <mi mathsize=\"0.900em\">i</mi>\n                    </msub>\n                    <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n                  </mrow>\n                </mrow>\n              </mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n            </mrow>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta\\text{WER}=\\frac{1}{N}\\sum^{N}_{i}|\\text{WER}(\\text{gt}_{i},t_{i})-\\text{WER}(\\text{pred}_{i},t_{i})|</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">t_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents the transcripts for audio sample </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo assess temporal alignment between video and generated audio, we follow&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and report the DeSync&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For semantic and perceptual similarity, we follow&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and calculate the FAD and LPAPS scores, which are computed using the clap-laion-audio model in music-speech-audioset.</span>\n</p>\n\n",
                "matched_terms": [
                    "desync",
                    "fad",
                    "δdeltawer",
                    "method",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the effectiveness of BVS on video-to-audio with context-aware speech by incorporating a pretrained text-to-speech SSL generator in MaskGCT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As there are no existing methods that directly address this task, we select two most relevant baselines: VoiceLDM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a text-to-context-aware TTS model that generates speech aligned with the semantic content of text prompts, and VATT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a V2A model that produces ambient sounds temporally synchronized with visual input through audio captions.\nWe exclude the concurrent work, DualDub&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as it focuses on generating speech from videos featuring talking heads.\nWe evaluate both baseline methods with their official checkpoints, and the results are reported in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Video to Audio with Context-Aware Speech &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Although VoiceLDM focuses on synthesizing context-aware speech given text prompts and is trained on both AudioSet-Speech and multiple synthesized noisy speech datasets with a CLAP text encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, its worse DeSync and LPAPS scores suggest that it struggles to generate audios with synchronized sound effects given prompts that contain events in temporal order. Conversely, VATT is designed to produce temporally aligned audio from videos; however, the high WER and </span>\n  <math alttext=\"\\Delta\\textnormal{WER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mtext mathsize=\"0.900em\">WER</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta\\textnormal{WER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicate VATT fails to generate intelligible speech. Compared to both baselines, BVS overcomes these limitations by generating temporally and semantically synchronized audio with intelligible speech given transcripts and videos. Notably, despite being trained solely on AudioSet-Speech, BVS achieves WER values closer to the ground truth with a lower </span>\n  <math alttext=\"\\Delta\\textnormal{WER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mtext mathsize=\"0.900em\">WER</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta\\textnormal{WER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which demonstrating the robustness of BVS in generating synchronized audio with context-aware intelligible speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "desync",
                    "audio",
                    "bvs",
                    "vatt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We first validate whether the audio semantic tokens obtained from W2V-BERT have the ability to represent non-speech information. As shown in the first row of Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.3 Ablation study &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we evaluate reconstruction results on AS-filtered using our VS2A model conditioned on ground-truth audio semantic tokens and videos. The metrics of reconstructed samples reflect that the VS2A model can generate audio that contains both speech and non-speech sounds, with better scores than generation results in Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Video to Audio with Context-Aware Speech &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Immersive Audio Background Conversion &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe then conduct ablation studies to evaluate the importance of the speech semantic token from the SE model and the audio semantic tokens in our two-stage method.\nFirstly, we remove the audio semantic tokens from BVS, converting it into a single-stage model that directly predicts acoustic tokens, which we denote as BVS-Single. Secondly, we remove speech SSL tokens obtained from raw audios by replacing the SE speech SSL generator with a transcript-based speech SSL generator used in MaskGCT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We use Whisper&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to enable alignment of speech onset and offset times. We use filtered transcripts obtained from an ASR model by&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to guide phonetic information. We refer to this as BVS-Text.\nAs illustrated in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.3 Ablation study &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, both methods fail to learn speech information effectively with significantly higher WER and FAD. In BVS-Text, although the model is given transcripts with speech onset-offset timestamps, the model still fails to learn the complex alignment of speech and sound effects from video and transcripts. This indicates that the models fail to capture the relationship between video, speech, and context information. In contrast, our method with both speech SSL tokens obtained directly from raw audios, and a two-stage design explicitly guides the model to disentangle, fuse, and refine the information of speech and sound effects.</span>\n</p>\n\n",
                "matched_terms": [
                    "method",
                    "audio",
                    "bvs",
                    "fad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we introduce BVS, which addresses the challenge of video-to-audio generation with intelligible and environmentally aware speech. By leveraging a pretrained speech SSL generator during training, BVS disentangles speech information from noisy audio, and then effectively fuses cues from video, speech, and audio modalities, and generates audio with a two-stage design. Our experiments demonstrate that BVS can be applied to diverse real-world scenarios, including both generation and conversion tasks. Despite these promising results, we observed some limitations. Since BVS relies on speech SSL models for the unified audio semantic token space, which performance on non-speech sound effects remains constrained. Moreover, due to limitations in the available datasets, accurate lip&#8211;speech synchronization remains challenging. These limitations point to important directions for future research. In particular, exploring more powerful unified speech&#8211;audio SSL representations may improve sound effect generation, while incorporating richer multimodal datasets could enhance synchronization and further advance the quality of video-to-audio synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "conversion",
                    "audio",
                    "bvs"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech",
        "caption": "Table 3: Ablation study. SP-SSL stands for speech semantic tokens, and audio SSL stands for unified audio semantic tokens.",
        "body": "Method\nSP-SSL\nAudio SSL\n\nΔ\\DeltaWER ↓\\downarrow\n\n\nFAD ↓\\downarrow\n\n\nDeSync ↓\\downarrow\n\n\nLPAPS ↓\\downarrow\n\n\n\nVS2A\n-\n-\n19.8 %\n0.25\n1.02\n5.12\n\n\n\n\nBVS-Single\n✓\\checkmark\n\n99.1 %\n1.18\n1.38\n6.78\n\n\nBVS-Text\n\n✓\\checkmark\n98.7 %\n1.12\n1.49\n7.01\n\n\nBVS (Ours)\n✓\\checkmark\n✓\\checkmark\n\n21.9 %\n\n0.41\n1.33\n6.38",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SP-SSL</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Audio SSL</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">WER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FAD </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">DeSync </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">LPAPS </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VS2A</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.8 %</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.25</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.02</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.12</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BVS-Single</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m6\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">99.1 %</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.18</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.38</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.78</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BVS-Text</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m7\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">98.7 %</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.12</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.49</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BVS (Ours)</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m8\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m9\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">21.9</span><span class=\"ltx_text\" style=\"font-size:90%;\"> %</span>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.41</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.33</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">6.38</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "unified",
            "ablation",
            "bvssingle",
            "tokens",
            "↓downarrow",
            "ours",
            "speech",
            "desync",
            "δdeltawer",
            "bvs",
            "ssl",
            "stands",
            "fad",
            "bvstext",
            "semantic",
            "spssl",
            "study",
            "vs2a",
            "✓checkmark",
            "method",
            "lpaps",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We first validate whether the audio semantic tokens obtained from W2V-BERT have the ability to represent non-speech information. As shown in the first row of Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.3 Ablation study &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we evaluate reconstruction results on AS-filtered using our VS2A model conditioned on ground-truth audio semantic tokens and videos. The metrics of reconstructed samples reflect that the VS2A model can generate audio that contains both speech and non-speech sounds, with better scores than generation results in Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Video to Audio with Context-Aware Speech &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Immersive Audio Background Conversion &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe then conduct ablation studies to evaluate the importance of the speech semantic token from the SE model and the audio semantic tokens in our two-stage method.\nFirstly, we remove the audio semantic tokens from BVS, converting it into a single-stage model that directly predicts acoustic tokens, which we denote as BVS-Single. Secondly, we remove speech SSL tokens obtained from raw audios by replacing the SE speech SSL generator with a transcript-based speech SSL generator used in MaskGCT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We use Whisper&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to enable alignment of speech onset and offset times. We use filtered transcripts obtained from an ASR model by&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to guide phonetic information. We refer to this as BVS-Text.\nAs illustrated in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.3 Ablation study &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, both methods fail to learn speech information effectively with significantly higher WER and FAD. In BVS-Text, although the model is given transcripts with speech onset-offset timestamps, the model still fails to learn the complex alignment of speech and sound effects from video and transcripts. This indicates that the models fail to capture the relationship between video, speech, and context information. In contrast, our method with both speech SSL tokens obtained directly from raw audios, and a two-stage design explicitly guides the model to disentangle, fuse, and refine the information of speech and sound effects.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The generation of realistic, context-aware audio is important in real-world applications such as video game development. While existing video-to-audio (V2A) methods mainly focus on Foley sound generation, they struggle to produce intelligible speech. Meanwhile, current environmental speech synthesis approaches remain text-driven and fail to temporally align with dynamic video content. In this paper, we propose Beyond Video-to-SFX (BVS), a method to generate synchronized audio with environmentally aware intelligible speech for given videos. We introduce a two-stage modeling method: (1) stage one is a video-guided audio semantic (V2AS) model to predict unified audio semantic tokens conditioned on phonetic cues; (2) stage two is a video-conditioned semantic-to-acoustic (VS2A) model that refines semantic tokens into detailed acoustic tokens. Experiments demonstrate the effectiveness of BVS in scenarios such as video-to-context-aware speech synthesis and immersive audio background conversion, with ablation studies further validating our design. Our demonstration is available at&#160;<a class=\"ltx_ref ltx_href\" href=\"https://xinleiniu.github.io/BVS-demo/\" title=\"\">BVS-Demo</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "unified",
                    "ablation",
                    "semantic",
                    "vs2a",
                    "method",
                    "tokens",
                    "audio",
                    "bvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The generation of realistic, high-fidelity audio, including sound effects and speech, has become increasingly important for real-world applications such as film production, virtual reality (VR), and game development. To this end, useful generative models must generate audio that is coherent, expressive, and contextually aligned with the content of the given condition. Within this domain, video-to-audio (V2A) focuses on generating audio that is temporally and semantically aligned with visual cues in videos. In many scenarios, the audio track includes not only visually grounded sounds but also speech, and the speaker may not be directly observable. For example, a video clip of someone washing dishes might include background conversations that are not visually depicted. Motivated by this challenge, our work aims to develop a method capable of synthesizing audio that incorporates environmentally aware speech. The goal is to generate audio that is temporally aligned with the video while ensuring semantic consistency and incorporating context-aware spoken content.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "method",
                    "audio",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">A closely related task is video-to-speech (V2S)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which focuses on generating speech from videos with talking heads. However, V2S methods primarily address clean speech synthesis and do not handle sound effect generation, making them complementary but distinct from V2A approaches. A concurrent work by&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> demonstrates V2S generation with integrated sound effects; however, their approach is primarily designed for videos with talking heads.\nIn parallel, environmental context-aware speech synthesis&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focuses on generating speech that incorporates background noise consistent with the given text prompts. These methods typically leverage a complex training strategy to learn the speech information. They first pretrain on synthetic noisy speech datasets and then fine-tune on real-world datasets transcribed by automatic speech recognition (ASR) models to improve speech modeling. Other related approaches&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> explore mixing clean speech with ambient sounds to create contextually consistent audio environments. While these techniques are effective in achieving semantic consistency between speech and background context, they generally lack fine-grained temporal control over sound events, a capability that is critical for synchronizing sound effects with dynamic video content.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We identify two major limitations in previous works. Firstly, a key challenge is the scarcity of speech datasets that simultaneously provide context-rich environmental sounds, paired video streams, and ground-truth transcripts. Previous methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> first train their models with synthesized noisy speech by mixing clean speech data for TTS training with environmental sound data, and then fine-tune using real-world unlabeled audio data containing speech with transcripts obtained from ASR models.\nHowever, ASR transcripts often suffer from inaccuracies and unstable segmentation in noisy speech, which introduces incorrect labels during speech learning. Meanwhile, synthetic noisy speech lacks natural video grounding since corresponding video streams cannot be created, limiting multimodal consistency. Secondly, current V2A models often fail to generate intelligible and natural speech, and often lack control over the speech content. As a result, additional post-production is often needed to remove unintelligible vocals and mix clean speech with ambient sounds for contextual alignment, which reduces their practicality in real-world applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we propose Beyond Video-to-SFX (BVS), a method for synthesizing video-synchronized audio that incorporates environmental-aware speech. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, BVS generates soundtracks that are temporally and semantically aligned with visual content while integrating context-aware intelligible speech, guided by provided transcripts or audios containing spoken content.\nWe make the following contributions:\n(1) We propose BVS, an improved V2A framework enabling intelligible speech synthesis, addressing a core limitation of previous approaches.\n(2) Our method is trained directly on raw audio data, eliminating the need for complex preprocessing steps such as synthetic noisy datasets or filtering ASR-generated transcripts.\n(3) We demonstrate that BVS is a flexible framework for various downstream tasks, including audio background conversion and video to context-aware speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "method",
                    "audio",
                    "bvs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Generating real-world audio that matches a visual scene is challenging. Audio tracks such as speech, sound effects, and ambient sounds are each influenced by distinct factors. For example, speech often occurs with irregular onset and offset times. It is primarily human-related, frequently off-screen, and relies more on phonetic cues than on visual context. In contrast, sound effects and ambient sounds are closely linked to visual objects and scene context, exhibiting highly variable temporal alignment with the video. These fundamental differences make it difficult for a single model to jointly generate both intelligible speech and natural ambient sounds directly from phonetic and visual cues. Moreover, to create a convincing audio experience for a given video, all audio tracks must remain coherent and share consistent environmental characteristics.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this, we introduce an audio semantic token space as a unified representation of speech and ambient sounds inspired from two-stage TTS pipelines&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The semantic tokens capture abstract audio information, reducing the gap between multi-modal conditions and acoustic outputs. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our BVS pipeline operates in two stages: (1) fuse information across speech, visual, and contextual modalities into audio semantic tokens, and (2) refine these audio semantic tokens into acoustic outputs. This separation reduces the burden of a single-stage model that needs to learn both multimodal fusion and acoustic generation simultaneously.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "bvs",
                    "unified",
                    "tokens",
                    "audio",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We design a video-to-audio semantic (V2AS) masked generative transformer, which generates audio semantic tokens by jointly integrating the speech and sound effects from video and phonetic cues. As mentioned before, speech, sound effects, and ambient sounds are each related to different objects. We split the audio semantic token generation into two components: (1) clean speech semantic token generation in the form of speech SSL token; (2) audio semantic fusion according to speech and video cues.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ssl",
                    "tokens",
                    "audio",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Noisy Audio to Speech Semantic Token:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe aim to mitigate inaccurate transcription in previous methods&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib13\" title=\"\">13</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and directly train models on raw noisy audios containing speech. To guide the model in identifying speech regions within raw audio,\nwe leverage a pretrained speech SSL token generator&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> trained for the speech enhancement (SE) task to extract speech information from noisy inputs. Rather than employing the full model to directly separate clean speech waveform, we use its first-stage generative transformer to predict speech self-supervised learning (SSL) token sequences in the quantized W2V-BERT feature space&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, obtained via a VQ-VAE&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Formally, given a raw audio input </span>\n  <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">x</mi>\n      <annotation encoding=\"application/x-tex\">x</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the pretrained SE masked generative transformer captures speech SSL tokens </span>\n  <math alttext=\"S_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">S_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from </span>\n  <math alttext=\"p_{\\phi}(S_{s}|x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mi mathsize=\"0.900em\">&#981;</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">S</mi>\n              <mi mathsize=\"0.900em\">s</mi>\n            </msub>\n            <mo fence=\"false\" mathsize=\"0.900em\">|</mo>\n            <mi mathsize=\"0.900em\">x</mi>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p_{\\phi}(S_{s}|x)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"\\phi\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#981;</mi>\n      <annotation encoding=\"application/x-tex\">\\phi</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the parameter of the pretrained SE model.\nThis design emphasizes capturing of speech information, rather than reconstructing the clean speech waveform, which offers two key advantages: (1) it disentangles speech information more effectively under noisy conditions of the raw audio, and (2) it reduces downstream decoding errors by using compact speech SSL tokens.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ssl",
                    "tokens",
                    "audio",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audio Semantic Fusion:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWhile speech semantic tokens allow the model to be aware of speech regions directly from raw audio, the remaining challenge is to fuse the speech information with video context while generating synchronized audios from visual cues. To address this, we make use of the quantized W2V-BERT feature space as the unified audio semantic token space, which is motivated by our empirical findings: Although trained primarily on speech, SSL models also encode non-speech acoustic cues. This is also pointed out in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We leveraged a pretrained semantic to acoustic generative model to reconstruct the waveform for a given SSL token sequence including both speech and background noise. We empirically observed that the reconstructed audio contains both intelligible speech and matching background noise. Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2 Method &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further illustrates this empirical finding. We also provide the audio samples on our &#160;</span>\n  <a class=\"ltx_ref ltx_href\" href=\"https://xinleiniu.github.io/BVS-demo/\" style=\"font-size:90%;\" title=\"\">Demo-page</a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Reconstructions of the W2V-BERT SSL tokens preserve both speech and non-speech information. Our empirical analysis further shows that the residual difference between speech SSL embeddings and full audio semantic embeddings potentially corresponds to non-speech components such as background noise or ambient sounds. This suggests that speech and non-speech information can be disentangled and recombined in the SSL embedding space.\nMotivated by this observation, the V2AS model predicts audio semantic token sequences in the same quantized W2V-BERT feature space that fuses non-speech acoustic information with speech information into audio semantic tokens using complementary cues from speech SSL tokens and videos. This process is shown in the right-hand side of stage 1 in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nBy predicting semantic tokens rather than raw acoustic outputs, the model focuses on learning cross-modal relationships between speech, ambient sounds, and visual context.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ssl",
                    "unified",
                    "tokens",
                    "audio",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training Objective:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe use the mask-and-predict learning strategy from&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and mask the ground truth audio semantic token sequence </span>\n  <math alttext=\"S_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">S_{a}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> at each time step </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with a corresponding binary mask </span>\n  <math alttext=\"M_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">M</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">M_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Items in </span>\n  <math alttext=\"S_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">S_{a}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are replaced with a special [MASK] token if </span>\n  <math alttext=\"m_{t}^{i}=1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msubsup>\n          <mi mathsize=\"0.900em\">m</mi>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msubsup>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m_{t}^{i}=1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, otherwise </span>\n  <math alttext=\"S_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m6\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">S_{a}^{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> remain unchanged. The resulting masked token sequence is denoted as </span>\n  <math alttext=\"S_{a}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m7\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n        <mi mathsize=\"0.900em\">M</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">S_{a}^{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Each </span>\n  <math alttext=\"m_{t}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m8\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">m_{t}^{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is i.i.d to a Bernoulli distribution with parameter </span>\n  <math alttext=\"\\gamma(t)\\in(0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">&#947;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\gamma(t)\\in(0,1]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Thus, the V2AS model is formulated as </span>\n  <math alttext=\"p_{\\theta_{\\text{V2AS}}}(S_{a}|S^{M}_{a},V,S_{s})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">p</mi>\n          <msub>\n            <mi mathsize=\"0.900em\">&#952;</mi>\n            <mtext mathsize=\"0.900em\">V2AS</mtext>\n          </msub>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">S</mi>\n              <mi mathsize=\"0.900em\">a</mi>\n            </msub>\n            <mo fence=\"false\" mathsize=\"0.900em\">|</mo>\n            <mrow>\n              <msubsup>\n                <mi mathsize=\"0.900em\">S</mi>\n                <mi mathsize=\"0.900em\">a</mi>\n                <mi mathsize=\"0.900em\">M</mi>\n              </msubsup>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mi mathsize=\"0.900em\">V</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <msub>\n                <mi mathsize=\"0.900em\">S</mi>\n                <mi mathsize=\"0.900em\">s</mi>\n              </msub>\n            </mrow>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p_{\\theta_{\\text{V2AS}}}(S_{a}|S^{M}_{a},V,S_{s})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m11\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">V</mi>\n      <annotation encoding=\"application/x-tex\">V</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents video conditions. We train the model using cross-entropy loss between predicted audio semantic tokens </span>\n  <math alttext=\"\\hat{S}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m12\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">S</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">a</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\hat{S}_{a}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and ground truth </span>\n  <math alttext=\"S_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m13\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">S</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">S_{a}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as</span>\n</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "audio",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Architecture:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nTo further enhance the model&#8217;s video understanding ability, we incorporate visual representations enriched by audio captions generated with a V2Cap large language model (LLM)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which we denote as </span>\n  <math alttext=\"E_{\\text{VCap}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m14\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mtext mathsize=\"0.900em\">VCap</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">E_{\\text{VCap}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The detailed V2AS architecture is shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (in the yellow dash box), which extends a V2A transformer framework&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by injecting speech SSL embeddings into extra cross-attention blocks as </span>\n  <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m15\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">K</mi>\n      <annotation encoding=\"application/x-tex\">K</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"V\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m16\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">V</mi>\n      <annotation encoding=\"application/x-tex\">V</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"N_{1}=12\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m17\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">12</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N_{1}=12</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"N_{2}=14\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m18\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">14</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">N_{2}=14</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This integration improves speech localization and intelligibility, and captures non-speech details guided by video context. During training, we concatenate the masked audio semantic embeddings </span>\n  <math alttext=\"E_{a}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m19\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mi mathsize=\"0.900em\">a</mi>\n        <mi mathsize=\"0.900em\">M</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E_{a}^{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"E_{\\text{VCap}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m20\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mtext mathsize=\"0.900em\">VCap</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">E_{\\text{VCap}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> along the temporal axis. During inference, the V2AS model generates audio semantic token sequences conditioned on the video prompt and speech SSL tokens. The speech SSL token sequences can be obtained by a unified speech generative transformer such as Metis stage 1&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which takes transcripts, audio, or other inputs.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ssl",
                    "unified",
                    "tokens",
                    "audio",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We then train a video-guided semantic-to-acoustic (VS2A) model to obtain acoustic token sequences conditioned on audio semantic token sequences and </span>\n  <math alttext=\"E_{\\text{VCap}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mtext mathsize=\"0.900em\">VCap</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">E_{\\text{VCap}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The VS2A model refines generation by mapping abstract audio semantic representations into acoustic tokens while leveraging video context to enhance the alignment. Following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the VS2A model is a masked generative transformer that produces a multi-layer acoustic </span>\n  <math alttext=\"A^{1:K}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">A</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <mi mathsize=\"0.900em\">K</mi>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">A^{1:K}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">K</mi>\n      <annotation encoding=\"application/x-tex\">K</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the number of codebooks, using iterative parallel decoding within each layer. For simplicity, we adopt separate VS2A models: one dedicated to predicting the first-layer acoustic codec and another for the subsequent layers. We denote the </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-th acoustic token layer as </span>\n  <math alttext=\"A^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">A</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">A^{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and formulate the model as </span>\n  <math alttext=\"p_{\\theta_{\\text{VS2A}}}(A^{i}|V,A^{1:i-1})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">p</mi>\n          <msub>\n            <mi mathsize=\"0.900em\">&#952;</mi>\n            <mtext mathsize=\"0.900em\">VS2A</mtext>\n          </msub>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <msup>\n              <mi mathsize=\"0.900em\">A</mi>\n              <mi mathsize=\"0.900em\">i</mi>\n            </msup>\n            <mo fence=\"false\" mathsize=\"0.900em\">|</mo>\n            <mrow>\n              <mi mathsize=\"0.900em\">V</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <msup>\n                <mi mathsize=\"0.900em\">A</mi>\n                <mrow>\n                  <mn mathsize=\"0.900em\">1</mn>\n                  <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n                  <mrow>\n                    <mi mathsize=\"0.900em\">i</mi>\n                    <mo mathsize=\"0.900em\">&#8722;</mo>\n                    <mn mathsize=\"0.900em\">1</mn>\n                  </mrow>\n                </mrow>\n              </msup>\n            </mrow>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p_{\\theta_{\\text{VS2A}}}(A^{i}|V,A^{1:i-1})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. During training, we simply sum the embeddings of the audio semantic tokens and the embeddings of the acoustic codec tokens from layer 1 to </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is denoted as </span>\n  <math alttext=\"E_{C_{i}}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <msub>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mi mathsize=\"0.900em\">M</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E_{C_{i}}^{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> . Then we concatenate </span>\n  <math alttext=\"E_{\\text{VCap}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">E</mi>\n        <mtext mathsize=\"0.900em\">VCap</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">E_{\\text{VCap}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"E_{C_{i}}^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mi mathsize=\"0.900em\">E</mi>\n        <msub>\n          <mi mathsize=\"0.900em\">C</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mi mathsize=\"0.900em\">M</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">E_{C_{i}}^{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as input of the model. We use cross-entropy loss between predicted acoustic tokens and ground truth acoustic tokens as</span>\n</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "vs2a",
                    "audio",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">During inference, the VS2A model generates acoustic tokens progressively from coarse to fine across layers, employing iterative parallel decoding within each layer.</span>\n</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "vs2a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe train our model on the filtered English subset of AudioSet-Speech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (AS-Speech), which contains approximately 580k audio&#8211;video pairs. Each sample has a 10-second duration and contains both speech and various environmental sound events. For evaluation, we adopt AC-filtered&#160;</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_href\" href=\"https://github.com/glory20h/voiceldm-data\" title=\"\">https://github.com/glory20h/voiceldm-data</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which includes audio sourced from AudioCap&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, captions, and transcripts. As AudioCaps is derived from AudioSet, we retrieve the corresponding video streams via the YouTube IDs, resulting in a curated set of 152 samples, which we refer to as AS-filtered dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Implementation Details:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe train V2AS and VS2A models on three NVIDIA A100 (80GB) GPUs for four and three days, respectively, using a batch size of 32, a learning rate of </span>\n  <math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, 20k and 32k warm-up steps, for approximately 80 and 34 epochs, respectively. We optimize these models with the AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> optimizer. We adopt VATT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> transformer with extra cross-attention blocks for our V2AS model, and we adopt the S2A model architecture in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the VS2A model. For the video modality, we employ the EVA-CLIP image encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to extract mean-pooled frame features sampled at 5 fps, yielding a visual sequence of size </span>\n  <math alttext=\"50\\times 768\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">50</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"0.900em\">768</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">50\\times 768</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for a 10-second video. Following&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the visual sequence passes through a pretrained V2Cap LLaMa and we extract its hidden state as input features. We add two learnable modality-specific embeddings to the concatenated inputs in the V2AS model. We use Metis SE stage 1 model to obtain speech SSL tokens&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Audio SSL features are extracted from W2V-BERT 2.0&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which are then quantized into discrete tokens at 50 tokens rate per second by the pretrained VQ-VAE in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Acoustic tokens are obtained from 16kHz Encodec&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with four codebooks at a 50 Hz token rate. During inference, we use 16 steps for the V2AS model and [20, 10, 1, 1] steps for the VS2A model. The classifier-free guidance (CFG) scale is set to 5.0 for V2AS and 2.5 for VS2A.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "ssl",
                    "vs2a",
                    "tokens",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Metrics:</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe evaluate our method along three key dimensions: speech intelligibility, temporal alignment, and semantic perceptual similarity. For speech intelligibility, we report both the word error rate (WER) and </span>\n  <math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n      <annotation encoding=\"application/x-tex\">\\Delta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">WER. WER is computed between the ground truth transcripts and those obtained from transcribing our generated audio using the whisper-large-v3. Since ASR models are trained on transcribing clean speech, it may produce inaccurate transcripts on noisy inputs. We additionally report </span>\n  <math alttext=\"\\Delta\\textnormal{WER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mtext mathsize=\"0.900em\">WER</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta\\textnormal{WER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which measures the absolute difference in WER between the ground truth and generated results, as </span>\n  <math alttext=\"\\Delta\\text{WER}=\\frac{1}{N}\\sum^{N}_{i}|\\text{WER}(\\text{gt}_{i},t_{i})-\\text{WER}(\\text{pred}_{i},t_{i})|\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mtext mathsize=\"0.900em\">WER</mtext>\n        </mrow>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mfrac>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mi mathsize=\"0.900em\">N</mi>\n          </mfrac>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <msubsup>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" rspace=\"0em\" stretchy=\"true\">&#8721;</mo>\n              <mi mathsize=\"0.900em\">i</mi>\n              <mi mathsize=\"0.900em\">N</mi>\n            </msubsup>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n              <mrow>\n                <mrow>\n                  <mtext mathsize=\"0.900em\">WER</mtext>\n                  <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                  <mrow>\n                    <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                    <msub>\n                      <mtext mathsize=\"0.900em\">gt</mtext>\n                      <mi mathsize=\"0.900em\">i</mi>\n                    </msub>\n                    <mo mathsize=\"0.900em\">,</mo>\n                    <msub>\n                      <mi mathsize=\"0.900em\">t</mi>\n                      <mi mathsize=\"0.900em\">i</mi>\n                    </msub>\n                    <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n                  </mrow>\n                </mrow>\n                <mo mathsize=\"0.900em\">&#8722;</mo>\n                <mrow>\n                  <mtext mathsize=\"0.900em\">WER</mtext>\n                  <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                  <mrow>\n                    <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n                    <msub>\n                      <mtext mathsize=\"0.900em\">pred</mtext>\n                      <mi mathsize=\"0.900em\">i</mi>\n                    </msub>\n                    <mo mathsize=\"0.900em\">,</mo>\n                    <msub>\n                      <mi mathsize=\"0.900em\">t</mi>\n                      <mi mathsize=\"0.900em\">i</mi>\n                    </msub>\n                    <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n                  </mrow>\n                </mrow>\n              </mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n            </mrow>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta\\text{WER}=\\frac{1}{N}\\sum^{N}_{i}|\\text{WER}(\\text{gt}_{i},t_{i})-\\text{WER}(\\text{pred}_{i},t_{i})|</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mi mathsize=\"0.900em\">i</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">t_{i}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents the transcripts for audio sample </span>\n  <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">i</mi>\n      <annotation encoding=\"application/x-tex\">i</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nTo assess temporal alignment between video and generated audio, we follow&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and report the DeSync&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For semantic and perceptual similarity, we follow&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and calculate the FAD and LPAPS scores, which are computed using the clap-laion-audio model in music-speech-audioset.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "desync",
                    "fad",
                    "δdeltawer",
                    "method",
                    "lpaps",
                    "audio",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the effectiveness of BVS on video-to-audio with context-aware speech by incorporating a pretrained text-to-speech SSL generator in MaskGCT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As there are no existing methods that directly address this task, we select two most relevant baselines: VoiceLDM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a text-to-context-aware TTS model that generates speech aligned with the semantic content of text prompts, and VATT&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a V2A model that produces ambient sounds temporally synchronized with visual input through audio captions.\nWe exclude the concurrent work, DualDub&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as it focuses on generating speech from videos featuring talking heads.\nWe evaluate both baseline methods with their official checkpoints, and the results are reported in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Video to Audio with Context-Aware Speech &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Although VoiceLDM focuses on synthesizing context-aware speech given text prompts and is trained on both AudioSet-Speech and multiple synthesized noisy speech datasets with a CLAP text encoder&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, its worse DeSync and LPAPS scores suggest that it struggles to generate audios with synchronized sound effects given prompts that contain events in temporal order. Conversely, VATT is designed to produce temporally aligned audio from videos; however, the high WER and </span>\n  <math alttext=\"\\Delta\\textnormal{WER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mtext mathsize=\"0.900em\">WER</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta\\textnormal{WER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicate VATT fails to generate intelligible speech. Compared to both baselines, BVS overcomes these limitations by generating temporally and semantically synchronized audio with intelligible speech given transcripts and videos. Notably, despite being trained solely on AudioSet-Speech, BVS achieves WER values closer to the ground truth with a lower </span>\n  <math alttext=\"\\Delta\\textnormal{WER}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#916;</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mtext mathsize=\"0.900em\">WER</mtext>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\Delta\\textnormal{WER}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which demonstrating the robustness of BVS in generating synchronized audio with context-aware intelligible speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "desync",
                    "ssl",
                    "bvs",
                    "lpaps",
                    "audio",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We then demonstrate the effectiveness of BVS in immersive audio background conversion. Unlike the setting in&#160;Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Video to Audio with Context-Aware Speech &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, this task takes noisy speech as input and converts it into a new context-aware speech that is naturally integrated within the given video&#8217;s content. We randomly sample 500 video&#8211;audio pairs from the AS-filtered dataset, which serve as target video and source audio. Since no direct baselines exist, we construct a proxy baseline by first applying a Metis speech enhancement model&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to obtain clean speech, which is then directly mixed with sound effects generated by VATT. To prevent VATT from generating unintelligible human voices, we exclude caption inputs and make the model focus on generating sound effects.\nTo measure the consistency of converted audio between source and target audio, we report LPAPS</span>\n  <math alttext=\"{}_{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">T</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{T}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and LPAPS</span>\n  <math alttext=\"{}_{\\text{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">S</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{S}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where LPAPS</span>\n  <math alttext=\"{}_{\\text{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">T</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{T}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> measures similarity to the audio track of target video, and LPAPS</span>\n  <math alttext=\"{}_{\\text{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi/>\n        <mtext mathsize=\"0.900em\">S</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">{}_{\\text{S}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> measures similarity to the source audio.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15492v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.1 Video to Audio with Context-Aware Speech &#8227; 3 Experiment &#8227; Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, although the baseline achieves a lower DeSync score, our method outperforms it in terms of LPAPS and FAD, reflecting higher perceptual similarity to ground truth. In contrast, simple post hoc mixing of clean speech with generated sound effects often results in poor semantic alignment between speech and environmental context, whereas BVS produces coherent and context-aware audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "desync",
                    "bvs",
                    "fad",
                    "method",
                    "lpaps",
                    "audio",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we introduce BVS, which addresses the challenge of video-to-audio generation with intelligible and environmentally aware speech. By leveraging a pretrained speech SSL generator during training, BVS disentangles speech information from noisy audio, and then effectively fuses cues from video, speech, and audio modalities, and generates audio with a two-stage design. Our experiments demonstrate that BVS can be applied to diverse real-world scenarios, including both generation and conversion tasks. Despite these promising results, we observed some limitations. Since BVS relies on speech SSL models for the unified audio semantic token space, which performance on non-speech sound effects remains constrained. Moreover, due to limitations in the available datasets, accurate lip&#8211;speech synchronization remains challenging. These limitations point to important directions for future research. In particular, exploring more powerful unified speech&#8211;audio SSL representations may improve sound effect generation, while incorporating richer multimodal datasets could enhance synchronization and further advance the quality of video-to-audio synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "bvs",
                    "ssl",
                    "unified",
                    "audio",
                    "semantic"
                ]
            }
        ]
    }
}