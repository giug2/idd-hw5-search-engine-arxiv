{
    "S5.T1": {
        "caption": "Table 1: Results of objective evaluations. The best results among commercial models are highlighted in bold, while the best results among open-source models are underlined.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">PER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Mulan-T <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Mulan-A <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Audio Aesthetics <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">SongEval <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">CE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">CU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">PC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">PQ</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">CO</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">MU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ME</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">CL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">NA</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">SUNO V4.5</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">7.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">7.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">8.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.89</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mureka-O1</th>\n<td class=\"ltx_td ltx_align_center\">0.09</td>\n<td class=\"ltx_td ltx_align_center\">0.37</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">7.65</td>\n<td class=\"ltx_td ltx_align_center\">7.81</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">6.31</span></td>\n<td class=\"ltx_td ltx_align_center\">8.35</td>\n<td class=\"ltx_td ltx_align_center\">4.14</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.06</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4.07</span></td>\n<td class=\"ltx_td ltx_align_center\">3.93</td>\n<td class=\"ltx_td ltx_align_center\">3.85</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">DiffRhythm+</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ACE-Step</th>\n<td class=\"ltx_td ltx_align_center\">0.23</td>\n<td class=\"ltx_td ltx_align_center\">0.28</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">7.26</td>\n<td class=\"ltx_td ltx_align_center\">7.51</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">6.25</span></td>\n<td class=\"ltx_td ltx_align_center\">7.79</td>\n<td class=\"ltx_td ltx_align_center\">3.77</td>\n<td class=\"ltx_td ltx_align_center\">3.46</td>\n<td class=\"ltx_td ltx_align_center\">3.58</td>\n<td class=\"ltx_td ltx_align_center\">3.56</td>\n<td class=\"ltx_td ltx_align_center\">3.38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LeVo</th>\n<td class=\"ltx_td ltx_align_center\">0.19</td>\n<td class=\"ltx_td ltx_align_center\">0.35</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.81</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">7.51</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">7.78</span></td>\n<td class=\"ltx_td ltx_align_center\">5.68</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">8.12</span></td>\n<td class=\"ltx_td ltx_align_center\">3.74</td>\n<td class=\"ltx_td ltx_align_center\">3.56</td>\n<td class=\"ltx_td ltx_align_center\">3.62</td>\n<td class=\"ltx_td ltx_align_center\">3.55</td>\n<td class=\"ltx_td ltx_align_center\">3.40</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">DiffRhythm 2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">7.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">7.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">6.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">7.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.78</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "while",
            "mulana",
            "↓downarrow",
            "commercial",
            "aesthetics",
            "evaluations",
            "audio",
            "objective",
            "mulant",
            "songeval",
            "suno",
            "levo",
            "bold",
            "diffrhythm",
            "highlighted",
            "murekao1",
            "results",
            "underlined",
            "among",
            "↑uparrow",
            "model",
            "best",
            "acestep",
            "v45",
            "opensource"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Objective Results &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, DiffRhythm 2 significantly outperforms other open-source models across music quality metrics. However, in aspects such as musicality, it still shows a clear gap compared to commercial systems like SUNO V4.5. In terms of audio quality, it performs slightly worse than LeVo but remains superior to most other models. This result demonstrates that employing a lower frame rate is feasible for compressed song reconstruction, although the increased reconstruction burden makes it difficult to achieve perfect fidelity. DiffRhythm 2 achieves the highest scores on Mulan-T (0.40) and PER (0.13), substantially exceeding other models. However, its Mulan-A score is lower than that of LeVo and similar systems, which we attribute to our choice of modeling audio style prompts with global representations. A global style embedding cannot adequately capture the stylistic content present in songs.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generating full-length, high-quality songs is challenging, as it requires maintaining long-term coherence both across text and music modalities and within the music modality itself. Existing non-autoregressive (NAR) frameworks, while capable of producing high-quality songs, often struggle with the alignment between lyrics and vocal. Concurrently, catering to diverse musical preferences necessitates reinforcement learning from human feedback (RLHF). However, existing methods often rely on merging multiple models during multi-preference optimization, which results in significant performance degradation.\nTo address these challenges, we introduce DiffRhythm 2, an end-to-end framework designed for high-fidelity, controllable song generation. To tackle the lyric alignment problem, DiffRhythm 2 employs a semi-autoregressive architecture based on block flow matching. This design enables faithful alignment of lyrics to singing vocals without relying on external labels and constraints, all while preserving the high generation quality and efficiency of NAR models. To make this framework computationally tractable for long sequences, we implement a music variational autoencoder (VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity audio reconstruction.\nIn addition, to overcome the limitations of multi-preference optimization in RLHF, we propose cross-pair preference optimization. This method effectively mitigates the performance drop typically associated with model merging, allowing for more robust optimization across diverse human preferences. We further enhance musicality and structural coherence by introducing stochastic block representation alignment loss.\nExperimental results demonstrate that DiffRhythm 2 can generate complete songs up to 210 seconds in length, consistently outperforming existing open-source models in both subjective and objective evaluations, while maintaining efficient generation speed. To encourage reproducibility and further exploration, we release the inference code and model checkpoints.\nCode and weights are availble at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/xiaomi-research/diffrhythm2\" title=\"\">https://github.com/xiaomi-research/diffrhythm2</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "evaluations",
                    "audio",
                    "model",
                    "diffrhythm",
                    "objective",
                    "results",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early approaches, such as Jukebox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dhariwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib7\" title=\"\">2020</a>)</cite> and SongCreator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib17\" title=\"\">2024</a>)</cite>, generate songs by predicting the acoustic tokens of vocals and accompaniment mixed into a single track. However, the limited information density of these tokens hampers high-quality song generation. Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib36\" title=\"\">2025</a>)</cite> improves generation quality by modeling the vocals and accompaniment on separate tracks, while SongGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib22\" title=\"\">2025</a>)</cite> further enhances quality by introducing interleaved dual-track prediction. Despite these improvements, Yue and SongGen predict the tracks independently, which often leads to inconsistencies between the vocals and accompaniment. LeVo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> addresses this issue by first generating a mixed track and using it to guide the separate prediction of vocals and accompaniment, thereby reducing mismatches. However, the overall quality is still limited by the information compression in the mixed track tokenization, and it is difficult to generate complex and rich accompaniment. Most of these methods are based on autoregressive frameworks, which result in slow generation speeds. This hinders real-time applications and user interactivity.</p>\n\n",
                "matched_terms": [
                    "levo",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike the aforementioned approaches, DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> takes a bold approach within a non-autoregressive (NAR) framework. By leveraging the generation ability of diffusion models and continuous representations, it achieves high-quality mixed-track generation of vocals and accompaniment at over 50 times faster than autoregressive methods. However, NAR models struggle with lyric alignment in long sequences. DiffRhythm addresses this issue by conditioning on sentence-level timestamps. However, this solution significantly reduces creativity and diversity, and also imposes higher requirements on the training data. ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> solved the alignment problem without timestamps by introducing representation alignment (REPA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib35\" title=\"\">2024</a>)</cite> loss with mHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boito et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib1\" title=\"\">2024</a>)</cite> as semantic constraints. However, the addition of these constraints significantly reduced musicality, creating a delicate trade-off between lyric alignment and generation quality.</p>\n\n",
                "matched_terms": [
                    "acestep",
                    "models",
                    "bold",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, pretrained models often fail to align with human preferences due to the gap between training objectives and desired song quality. Therefore, a typical method is to introduce reinforcement learning from human feedback (RLHF) in the post-training process to refine generation preferences across various musical dimensions. DiffRhythm+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib3\" title=\"\">2025</a>)</cite> demonstrated the feasibility of improving overall musicality with Direct Preference Optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib26\" title=\"\">2023</a>)</cite>. LeVo extended this approach by applying DPO to different dimensions separately and interpolating model weights to enhance overall performance. While this strategy improved balance across dimensions, it inevitably limited the upper bound of each individual capability due to the averaging effect.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "levo",
                    "model",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we present DiffRhythm 2, a semi-autoregressive end-to-end framework for song generation based on block flow matching. This framework partitions latent representations into fixed-length blocks and generates each block non-autoregressively using flow matching, while maintaining autoregressive dependencies between blocks. This design enables faithful lyric alignment without requiring additional labels or constraints. The semi-autoregressive structure provides rich bidirectional context within blocks, ensuring long-sequence consistency, and supports fast inference for songs up to 210 seconds long. To accommodate block flow matching during training, we introduce stochastic block REPA loss that ensures efficient training and provides accurate representation guidance for the current block. We group similar preferences and perform pairwise optimization across groups to improve the performance of multi-preference alignment. Finally, as the block flow matching training strategy significantly increases the sequence length, we implement a high-compression music variational autoencoder (VAE), achieving high compression while still enabling high-quality audio reconstruction. To encourage reproducibility and further exploration, we will release the inference code and model checkpoints. The main contributions of this paper are summarized below:</p>\n\n",
                "matched_terms": [
                    "model",
                    "audio",
                    "while",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a cross-pair preference optimization strategy to efficiently handle multi-preference alignment, which enhances final performance while reducing the number of optimized models through a group-based optimization approach.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design a music VAE with low-frame-rate compression at 5 Hz that preserves reconstruction quality while reducing sequence length, which not only accelerates inference but also makes long-sequence modeling in DiffRhythm 2 both feasible and effective.</p>\n\n",
                "matched_terms": [
                    "while",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Song generation aims to produce coherent vocals and accompaniment given lyrics and style specifications, requiring joint modeling of melody, harmony, and lyric alignment. Early methods adopted sequential pipelines: Melodist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib12\" title=\"\">2024</a>)</cite> and MelodyLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib19\" title=\"\">2024</a>)</cite> first generated vocals from lyrics, then synthesized accompaniment. However, since vocals and accompaniment are inherently interdependent, this sequential approach often produces suboptimal results. To address these limitations, parallel generation methods emerged. SongCreator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib17\" title=\"\">2024</a>)</cite> and Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib36\" title=\"\">2025</a>)</cite> generate vocals and accompaniment independently in parallel, while SongGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib22\" title=\"\">2025</a>)</cite> introduces interleaved prediction across tracks to improve quality. Despite these advances, weak coupling between vocal and instrumental tracks often leads to harmonic inconsistencies. LeVo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> mitigates this by first generating a mixed track, then using it to guide separate vocal and accompaniment synthesis, achieving better harmony at the cost of reduced accompaniment complexity.\nRecent work has explored alternative architectures to improve both quality and efficiency. MusicCoT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib15\" title=\"\">2025</a>)</cite> and Songbloom&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib33\" title=\"\">2025</a>)</cite> leverage Chain-of-Thought reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib30\" title=\"\">2022</a>)</cite> to enhance vocal-accompaniment coordination and overall generation quality. However, these language-model-based approaches suffer from computational overhead and struggle with style consistency across long sequences due to their autoregressive nature.\nNon-autoregressive approaches offer promising alternatives. DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> and ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> employ diffusion models to achieve superior long-sequence consistency and vocal-accompaniment harmony while maintaining faster inference. However, both methods face challenges in lyric alignment for extended sequences. While each proposes specific solutions, these fixes come at the cost of reduced creativity or musicality. Specifically, DiffRhythm relies on timestamp conditioning while ACE-Step employs representation alignment, highlighting the need for more sophisticated alignment strategies.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "levo",
                    "diffrhythm",
                    "acestep",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture of DiffRhythm 2, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S3.F1\" title=\"Figure 1 &#8227; 3.1 Overview &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, comprises of a Music VAE and a Diffusion Transformer enhanced with block flow matching. The Diffusion Transformer is conditioned on lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> together with either text style prompt <math alttext=\"S_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">S_{t}</annotation></semantics></math> or audio style prompt <math alttext=\"S_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">S_{a}</annotation></semantics></math>, and generates VAE latents block by block. These latents are then passed to the Music VAE decoder to reconstruct the waveform.\nFor training, in addition to the block flow matching loss, we introduce the stochastic block REPA loss combined with MuQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib39\" title=\"\">2025</a>)</cite> representations to enhance the model&#8217;s musicality and structural modeling. Moreover, to address the degraded average performance caused by multi-preference optimization, we introduce a cross-pair preference optimization strategy. The following subsections will provide detailed descriptions of the DiffRhythm 2 modules and training procedure.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of training DiffRhythm 2 on extremely long sequences, we design a customized Music VAE with a frame rate as low as 5 Hz. The VAE processes 24 kHz input audio as input and reconstructs it at 48 kHz, achieving compression ratios of 4800&#215; during encoding and 9600&#215; during decoding. The Music VAE consists of an encoder, a transformer block, and a decoder. The encoder adopts the same architecture as Stable Audio 2 VAE&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/Stability-AI/stable-audio-tools</span></span></span><cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib8\" title=\"\">2025</a>)</cite>. A transformer block is inserted before the decoder to alleviate reconstruction pressure. To maximize reconstruction quality, we employ BigVGAN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib16\" title=\"\">2023</a>)</cite> as the decoder. For the training loss, we combine the multi-scale mel loss from BigVGAN with the multi-scale STFT loss from Stable Audio 2 VAE to jointly optimize vocals and accompaniment. In addition, we employ a set of discriminators, including the multi-period discriminator, multi-scale discriminator, and CQT discriminator from BigVGAN.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the definition, block flow matching requires access to clean block. While this is naturally available during inference, the training input sequence typically consists only of <math alttext=\"(S,L,Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>L</mi><mo>,</mo><msub><mi>Z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,L,Z_{t})</annotation></semantics></math> without the clean blocks. To address this, we concatenate the clean sequence to the noisy sequence, forming <math alttext=\"(S,L,Z,Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo>,</mo><msub><mi>Z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,L,Z,Z_{t})</annotation></semantics></math>, and apply an attention mask to enforce autoregressive dependencies.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S3.F2\" title=\"Figure 2 &#8227; 3.3.2 Details of Application &#8227; 3.3 Block Flow Matching &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our attention mask design: the style prompt <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> and lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> can be attended by any block; for the clean sequence, the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block can only attend to blocks <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> through <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>; for the noisy sequence, the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block can only attend to clean blocks <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m9\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> through <math alttext=\"i-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">i-1</annotation></semantics></math> and its own noisy block.\nThis design enforces autoregressive training but results in very long input sequences. To alleviate this, we employ a 5 Hz music VAE to substantially compress the training sequence length.</p>\n\n",
                "matched_terms": [
                    "while",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To design the EOP frame, we experimented with several distributions. When the mean of the EOP distribution deviates significantly from that of the latent, such as in <math alttext=\"\\mathcal{N}(1,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m8\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(1,1)</annotation></semantics></math>, the model tends to generate strong noise near the end of the sequence. When the two distributions are numerically close, such as in <math alttext=\"\\mathcal{N}(0,2)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m9\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,2)</annotation></semantics></math>, prediction becomes unreliable since the model struggles to recognize the stopping position. Moreover, because no KL constraint is applied to the latent, enforcing a specific probabilistic form for EOP further increases the modeling difficulty. Overall, we find that <math alttext=\"\\mathcal{N}(1,0)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m10\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(1,0)</annotation></semantics></math>, i.e., a constant vector of ones, provides the most effective EOP representation, as it ensures clear numerical separation from latent features while remaining easy for the model to learn.</p>\n\n",
                "matched_terms": [
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As REPA loss is no longer required for lyric alignment constraints, we repurpose it to improve the model&#8217;s musicality and structural modeling. For the target SSL representations, extracting them with the same block size as DiffRhythm 2 hinders the capture of larger-scale musical structures. Moreover, SSL representations are typically downsampled by multi-layer convolution, which often introduces several frame shifts. The misalignment between <math alttext=\"z^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msup><mi>z</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">z^{i}</annotation></semantics></math> and SSL features caused by these shifts makes it unreliable to calculate REPA loss on target block directly. A better solution is to compute the loss on the entire sequence, which leverages the contextual receptive field to mitigate misalignment and enables the model to learn musical structures more effectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib3\" title=\"\">2025</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> have demonstrated the effectiveness and necessity of applying DPO for post-training in song generation tasks. As song generation involves multiple preference dimensions, multi-preference alignment optimization has become increasingly important. Existing approaches typically rely on merging separately optimized models. However, as the number of preferences increases, model merging often significantly reduces the average performance across preferences, severely limiting the overall efficiency of DPO.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We focus on four preference dimensions: musicality, style similarity, lyric alignment accuracy, and audio quality. Independent DPO experiments reveal complex interactions among these dimensions: improving lyric alignment often compromises musicality; optimizing audio quality may enhance alignment but weaken style similarity; while reinforcing style similarity tends to benefit musicality. These observations indicate that preferences can be either conflicting or synergistic.</p>\n\n",
                "matched_terms": [
                    "among",
                    "audio",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, we propose a cross-pair preference optimization strategy. Preferences are grouped by similarity and paired across groups for pairwise optimization. Conflicting preferences are paired to mitigate trade-offs, while synergistic preferences are leveraged to enhance consistency across optimized models. This design not only reduces the number of optimized models required but also substantially improves the performance of the merged model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In DiffRhythm 2, we pair (musicality, lyric alignment accuracy) and (style similarity, audio quality). During DPO training, the winning sample must satisfy both preferences within a pair, whereas the losing sample is required to satisfy at least one. To ensure balanced optimization, the three possible losing cases are preserved in equal proportions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DiffRhythm 2 is trained on a large-scale dataset comprising approximately 1.4 million songs, with a total duration of about 70,000 hours. The dataset spans three categories: Chinese, English and instrumental music, with a distribution ratio of roughly 4:5:1. To ensure data quality, we implement an efficient preprocessing pipeline. First, Audiobox-Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib28\" title=\"\">2025</a>)</cite> is employed to filter audio based on quality. Then, Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib2\" title=\"\">2012</a>)</cite> and FireRedASR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib32\" title=\"\">2025b</a>)</cite> are then used to transcribe the vocal tracks, and the two transcriptions are cross-validated against the original lyrics to ensure accuracy. Finally, All-in-One&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim &amp; Nam, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib14\" title=\"\">2023</a>)</cite> and Qwen2.5-omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib31\" title=\"\">2025a</a>)</cite> are used to annotate four key dimensions of the songs: structure, style, instrumentation, and emotion.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the objective evaluation, four aspects are considered: music quality, audio quality, style similarity, and lyric accuracy. To evaluate lyric accuracy, we transcribe the generated songs using Qwen3 ASR&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_href\" href=\"https://qwen.ai/blog?id=e199227023e8ebaac5f348f97fa804d1858ffc8a&amp;from=research.research-list\" title=\"\">https://qwen.ai/blog?id=e199227023e8ebaac5f348f97fa804d1858ffc8a&amp;from=research.research-list</a></span></span></span> and compute the phoneme error rate (PER). Style similarity is evaluated with MuQ-Mulan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib39\" title=\"\">2025</a>)</cite>, which measures the similarity of both textual prompts (Mulan-T) and audio prompts (Mulan-A). Music quality is assessed using SongEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib34\" title=\"\">2025</a>)</cite> in terms of overall coherence (CO), memorability (ME), naturalness of vocal breathing and phrasing (NA), clarity of song structure (CL), and overall musicality (MU). Audio quality is evaluated with Audiobox-Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib28\" title=\"\">2025</a>)</cite>, which considers content enjoyment (CE), content usefulness (CU), production complexity (PC), and production quality (PQ). We further explore the generation speed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A2\" title=\"Appendix B Generation Speed &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "mulant",
                    "songeval",
                    "audio",
                    "objective",
                    "mulana"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive comparison of DiffRhythm 2 against multiple systems. For benchmarking, we select two industry-leading commercial systems: Suno V4.5&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://suno.com/blog/introducing-v4-5\" title=\"\">https://suno.com/blog/introducing-v4-5</a></span></span></span> and Mureka-O1&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_href\" href=\"https://www.mureka.ai\" title=\"\">https://www.mureka.ai</a></span></span></span>. In addition, we include three open-source systems for evaluation: DiffRhythm+&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>DiffRhythm+ is tested using DiffRhythm+-full released at https://github.com/ASLP-lab/DiffRhythm</span></span></span>, ACE-Step&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>ACE-Step is tested using the code released at https://github.com/ace-step/ACE-Step</span></span></span>, and LeVo&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>LeVo is tested using the code released at https://github.com/tencent-ailab/songgeneration</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "commercial",
                    "suno",
                    "levo",
                    "diffrhythm",
                    "acestep",
                    "murekao1",
                    "v45",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T2\" title=\"Table 2 &#8227; 5.1 Objective Results &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, DiffRhythm 2 performs notably better than other open-source models in musicality, vocal-accompaniment harmony, and overall quality. Its accompaniment performance is comparable to that of ACE-Step and far exceeds that of LeVo, demonstrating that continuous representations can effectively enhance accompaniment generation. However, in vocal quality, since our system neither leverages semantic constraints nor separately models the vocal track, it falls slightly behind ACE-Step and LeVo. Overall, though, commercial models still maintain a clear advantage over open-source systems.</p>\n\n",
                "matched_terms": [
                    "commercial",
                    "models",
                    "levo",
                    "diffrhythm",
                    "acestep",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T3\" title=\"Table 3 &#8227; 5.3 Ablation Study &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, removing REPA loss leads to a clear drop in SongEval scores. Moreover, listening tests reveal that music structure become noticeably misaligned or even fail entirely. Similarly, without cross-pair preference optimization, all evaluation metrics decline significantly compared to DiffRhythm 2.</p>\n\n",
                "matched_terms": [
                    "diffrhythm",
                    "songeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present DiffRhythm 2, a semi-autoregressive end-to-end music generation framework based on block flow matching. By leveraging this approach, we achieve high-quality lyric alignment without relying on semantic constraints, while generating mixed-track songs with high fidelity. We further introduce stochastic block REPA loss for semi-autoregressive training, which enhances musicality and structural modeling. In addition, we propose cross-pair preference optimization, which effectively addresses the challenge of degraded average performance when optimizing across multiple preferences. Our experiments demonstrate the superior song generation capabilities of DiffRhythm 2.</p>\n\n",
                "matched_terms": [
                    "while",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, experiments also reveal certain limitations. The low-frame-rate VAE imposes an upper bound on the fidelity of reconstructed audio, making it difficult to match real audio quality. Furthermore, improving vocal modeling without compromising model creativity remains a key challenge for enhancing mixed-track generation. More broadly, it is evident that open-source models still fall short of commercial systems in overall performance, necessitating for further advancements in data and generation strategies.</p>\n\n",
                "matched_terms": [
                    "commercial",
                    "models",
                    "audio",
                    "model",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As DiffRhythm 2 is capable of generating complete songs, it could potentially be misused to create disinformation, deepfake audio, or other harmful content. Being committed to the responsible advancement of this field, we will provide appropriate usage restrictions and guidelines when releasing the open-source code and model checkpoints.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "diffrhythm",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the compression and reconstruction performance of Stable Audio 2 VAE, the DCAE from ACE-Step, the dual-track MuCodec from LeVo and our implemented Music VAE on a testset of 50 songs covering diverse genres. It should also be noted that Music VAE supports only mono audio, whereas all the other models support stereo audio. As shown in the table, our Music VAE achieves comparable or even higher PESQ and STOI scores despite operating at a substantially lower frame rate than the other models. However, the WER of Music VAE is slightly higher than other models, indicating that the extreme low frame rate still results in some loss of fine-grained reconstruction details. The single codebook design of MuCodec significantly compromises the quality of the accompaniment and leads to inferior overall reconstruction performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "levo",
                    "acestep",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By examining the outputs of the generative model, we find that the reconstruction quality of generated audio is generally superior to that of audio obtained through direct encoding and decoding with codec or VAE. In reconstruction tests, issues such as pronounced pronunciation errors or blurred accompaniment frequently occur, whereas their occurrence is greatly reduced in generated audio. Consequently, objective metrics based solely on codec or VAE reconstructions may fail to accurately reflect the quality achievable by the generative model.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "objective",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the generation speed of LeVo, Yue, DiffRhythm+, and DiffRhythm 2. To ensure fairness, all models are required to generate a fixed sequence of two minutes in length, regardless of generation quality. For both DiffRhythm+ and DiffRhythm 2, the number of sampling steps is set to 32. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A2.T5\" title=\"Table 5 &#8227; Appendix B Generation Speed &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, DiffRhythm 2 is only slightly slower than DiffRhythm+ and ACE-Step, while being significantly faster than LeVo. It is worth noting that DiffRhythm+ does not incorporate any attention acceleration framework, which explains why it is slower than ACE-Step despite having a smaller model size. ACE-Step leverages linear attention to reduce computational complexity, whereas LeVo adopts Flash Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/Dao-AILab/flash-attention\" title=\"\">https://github.com/Dao-AILab/flash-attention</a></span></span></span>, and DiffRhythm 2 employs Flex Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_href\" href=\"https://pytorch.ac.cn/blog/flexattention/\" title=\"\">https://pytorch.ac.cn/blog/flexattention/</a></span></span></span> for acceleration.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "levo",
                    "model",
                    "diffrhythm",
                    "acestep"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Results of subjective evaluations. The best results among commercial models are highlighted in bold, while the best results among open-source models are underlined.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">MUS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">HAR <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">VOC <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">ACC <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">OVP <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">SUNO V4.5</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">4.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">3.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">3.92</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">Mureka-O1</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">3.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.99</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">3.63</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.87</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">DiffRhythm+</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">2.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">ACE-Step</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.75</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.60</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.55</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">LeVo</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.48</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.46</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">DiffRhythm 2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.77</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "while",
            "commercial",
            "evaluations",
            "voc",
            "suno",
            "levo",
            "bold",
            "ovp",
            "diffrhythm",
            "highlighted",
            "murekao1",
            "results",
            "underlined",
            "mus",
            "among",
            "↑uparrow",
            "har",
            "model",
            "best",
            "acc",
            "acestep",
            "v45",
            "opensource",
            "subjective"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T2\" title=\"Table 2 &#8227; 5.1 Objective Results &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, DiffRhythm 2 performs notably better than other open-source models in musicality, vocal-accompaniment harmony, and overall quality. Its accompaniment performance is comparable to that of ACE-Step and far exceeds that of LeVo, demonstrating that continuous representations can effectively enhance accompaniment generation. However, in vocal quality, since our system neither leverages semantic constraints nor separately models the vocal track, it falls slightly behind ACE-Step and LeVo. Overall, though, commercial models still maintain a clear advantage over open-source systems.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generating full-length, high-quality songs is challenging, as it requires maintaining long-term coherence both across text and music modalities and within the music modality itself. Existing non-autoregressive (NAR) frameworks, while capable of producing high-quality songs, often struggle with the alignment between lyrics and vocal. Concurrently, catering to diverse musical preferences necessitates reinforcement learning from human feedback (RLHF). However, existing methods often rely on merging multiple models during multi-preference optimization, which results in significant performance degradation.\nTo address these challenges, we introduce DiffRhythm 2, an end-to-end framework designed for high-fidelity, controllable song generation. To tackle the lyric alignment problem, DiffRhythm 2 employs a semi-autoregressive architecture based on block flow matching. This design enables faithful alignment of lyrics to singing vocals without relying on external labels and constraints, all while preserving the high generation quality and efficiency of NAR models. To make this framework computationally tractable for long sequences, we implement a music variational autoencoder (VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity audio reconstruction.\nIn addition, to overcome the limitations of multi-preference optimization in RLHF, we propose cross-pair preference optimization. This method effectively mitigates the performance drop typically associated with model merging, allowing for more robust optimization across diverse human preferences. We further enhance musicality and structural coherence by introducing stochastic block representation alignment loss.\nExperimental results demonstrate that DiffRhythm 2 can generate complete songs up to 210 seconds in length, consistently outperforming existing open-source models in both subjective and objective evaluations, while maintaining efficient generation speed. To encourage reproducibility and further exploration, we release the inference code and model checkpoints.\nCode and weights are availble at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/xiaomi-research/diffrhythm2\" title=\"\">https://github.com/xiaomi-research/diffrhythm2</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "evaluations",
                    "model",
                    "diffrhythm",
                    "results",
                    "opensource",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early approaches, such as Jukebox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dhariwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib7\" title=\"\">2020</a>)</cite> and SongCreator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib17\" title=\"\">2024</a>)</cite>, generate songs by predicting the acoustic tokens of vocals and accompaniment mixed into a single track. However, the limited information density of these tokens hampers high-quality song generation. Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib36\" title=\"\">2025</a>)</cite> improves generation quality by modeling the vocals and accompaniment on separate tracks, while SongGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib22\" title=\"\">2025</a>)</cite> further enhances quality by introducing interleaved dual-track prediction. Despite these improvements, Yue and SongGen predict the tracks independently, which often leads to inconsistencies between the vocals and accompaniment. LeVo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> addresses this issue by first generating a mixed track and using it to guide the separate prediction of vocals and accompaniment, thereby reducing mismatches. However, the overall quality is still limited by the information compression in the mixed track tokenization, and it is difficult to generate complex and rich accompaniment. Most of these methods are based on autoregressive frameworks, which result in slow generation speeds. This hinders real-time applications and user interactivity.</p>\n\n",
                "matched_terms": [
                    "levo",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike the aforementioned approaches, DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> takes a bold approach within a non-autoregressive (NAR) framework. By leveraging the generation ability of diffusion models and continuous representations, it achieves high-quality mixed-track generation of vocals and accompaniment at over 50 times faster than autoregressive methods. However, NAR models struggle with lyric alignment in long sequences. DiffRhythm addresses this issue by conditioning on sentence-level timestamps. However, this solution significantly reduces creativity and diversity, and also imposes higher requirements on the training data. ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> solved the alignment problem without timestamps by introducing representation alignment (REPA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib35\" title=\"\">2024</a>)</cite> loss with mHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boito et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib1\" title=\"\">2024</a>)</cite> as semantic constraints. However, the addition of these constraints significantly reduced musicality, creating a delicate trade-off between lyric alignment and generation quality.</p>\n\n",
                "matched_terms": [
                    "acestep",
                    "models",
                    "bold",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, pretrained models often fail to align with human preferences due to the gap between training objectives and desired song quality. Therefore, a typical method is to introduce reinforcement learning from human feedback (RLHF) in the post-training process to refine generation preferences across various musical dimensions. DiffRhythm+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib3\" title=\"\">2025</a>)</cite> demonstrated the feasibility of improving overall musicality with Direct Preference Optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib26\" title=\"\">2023</a>)</cite>. LeVo extended this approach by applying DPO to different dimensions separately and interpolating model weights to enhance overall performance. While this strategy improved balance across dimensions, it inevitably limited the upper bound of each individual capability due to the averaging effect.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "levo",
                    "model",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we present DiffRhythm 2, a semi-autoregressive end-to-end framework for song generation based on block flow matching. This framework partitions latent representations into fixed-length blocks and generates each block non-autoregressively using flow matching, while maintaining autoregressive dependencies between blocks. This design enables faithful lyric alignment without requiring additional labels or constraints. The semi-autoregressive structure provides rich bidirectional context within blocks, ensuring long-sequence consistency, and supports fast inference for songs up to 210 seconds long. To accommodate block flow matching during training, we introduce stochastic block REPA loss that ensures efficient training and provides accurate representation guidance for the current block. We group similar preferences and perform pairwise optimization across groups to improve the performance of multi-preference alignment. Finally, as the block flow matching training strategy significantly increases the sequence length, we implement a high-compression music variational autoencoder (VAE), achieving high compression while still enabling high-quality audio reconstruction. To encourage reproducibility and further exploration, we will release the inference code and model checkpoints. The main contributions of this paper are summarized below:</p>\n\n",
                "matched_terms": [
                    "model",
                    "while",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a cross-pair preference optimization strategy to efficiently handle multi-preference alignment, which enhances final performance while reducing the number of optimized models through a group-based optimization approach.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design a music VAE with low-frame-rate compression at 5 Hz that preserves reconstruction quality while reducing sequence length, which not only accelerates inference but also makes long-sequence modeling in DiffRhythm 2 both feasible and effective.</p>\n\n",
                "matched_terms": [
                    "while",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Song generation aims to produce coherent vocals and accompaniment given lyrics and style specifications, requiring joint modeling of melody, harmony, and lyric alignment. Early methods adopted sequential pipelines: Melodist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib12\" title=\"\">2024</a>)</cite> and MelodyLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib19\" title=\"\">2024</a>)</cite> first generated vocals from lyrics, then synthesized accompaniment. However, since vocals and accompaniment are inherently interdependent, this sequential approach often produces suboptimal results. To address these limitations, parallel generation methods emerged. SongCreator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib17\" title=\"\">2024</a>)</cite> and Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib36\" title=\"\">2025</a>)</cite> generate vocals and accompaniment independently in parallel, while SongGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib22\" title=\"\">2025</a>)</cite> introduces interleaved prediction across tracks to improve quality. Despite these advances, weak coupling between vocal and instrumental tracks often leads to harmonic inconsistencies. LeVo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> mitigates this by first generating a mixed track, then using it to guide separate vocal and accompaniment synthesis, achieving better harmony at the cost of reduced accompaniment complexity.\nRecent work has explored alternative architectures to improve both quality and efficiency. MusicCoT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib15\" title=\"\">2025</a>)</cite> and Songbloom&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib33\" title=\"\">2025</a>)</cite> leverage Chain-of-Thought reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib30\" title=\"\">2022</a>)</cite> to enhance vocal-accompaniment coordination and overall generation quality. However, these language-model-based approaches suffer from computational overhead and struggle with style consistency across long sequences due to their autoregressive nature.\nNon-autoregressive approaches offer promising alternatives. DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> and ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> employ diffusion models to achieve superior long-sequence consistency and vocal-accompaniment harmony while maintaining faster inference. However, both methods face challenges in lyric alignment for extended sequences. While each proposes specific solutions, these fixes come at the cost of reduced creativity or musicality. Specifically, DiffRhythm relies on timestamp conditioning while ACE-Step employs representation alignment, highlighting the need for more sophisticated alignment strategies.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "levo",
                    "diffrhythm",
                    "acestep",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the definition, block flow matching requires access to clean block. While this is naturally available during inference, the training input sequence typically consists only of <math alttext=\"(S,L,Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>L</mi><mo>,</mo><msub><mi>Z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,L,Z_{t})</annotation></semantics></math> without the clean blocks. To address this, we concatenate the clean sequence to the noisy sequence, forming <math alttext=\"(S,L,Z,Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo>,</mo><msub><mi>Z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,L,Z,Z_{t})</annotation></semantics></math>, and apply an attention mask to enforce autoregressive dependencies.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S3.F2\" title=\"Figure 2 &#8227; 3.3.2 Details of Application &#8227; 3.3 Block Flow Matching &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our attention mask design: the style prompt <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> and lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> can be attended by any block; for the clean sequence, the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block can only attend to blocks <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> through <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>; for the noisy sequence, the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block can only attend to clean blocks <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m9\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> through <math alttext=\"i-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">i-1</annotation></semantics></math> and its own noisy block.\nThis design enforces autoregressive training but results in very long input sequences. To alleviate this, we employ a 5 Hz music VAE to substantially compress the training sequence length.</p>\n\n",
                "matched_terms": [
                    "while",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To design the EOP frame, we experimented with several distributions. When the mean of the EOP distribution deviates significantly from that of the latent, such as in <math alttext=\"\\mathcal{N}(1,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m8\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(1,1)</annotation></semantics></math>, the model tends to generate strong noise near the end of the sequence. When the two distributions are numerically close, such as in <math alttext=\"\\mathcal{N}(0,2)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m9\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,2)</annotation></semantics></math>, prediction becomes unreliable since the model struggles to recognize the stopping position. Moreover, because no KL constraint is applied to the latent, enforcing a specific probabilistic form for EOP further increases the modeling difficulty. Overall, we find that <math alttext=\"\\mathcal{N}(1,0)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m10\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(1,0)</annotation></semantics></math>, i.e., a constant vector of ones, provides the most effective EOP representation, as it ensures clear numerical separation from latent features while remaining easy for the model to learn.</p>\n\n",
                "matched_terms": [
                    "model",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As REPA loss is no longer required for lyric alignment constraints, we repurpose it to improve the model&#8217;s musicality and structural modeling. For the target SSL representations, extracting them with the same block size as DiffRhythm 2 hinders the capture of larger-scale musical structures. Moreover, SSL representations are typically downsampled by multi-layer convolution, which often introduces several frame shifts. The misalignment between <math alttext=\"z^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msup><mi>z</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">z^{i}</annotation></semantics></math> and SSL features caused by these shifts makes it unreliable to calculate REPA loss on target block directly. A better solution is to compute the loss on the entire sequence, which leverages the contextual receptive field to mitigate misalignment and enables the model to learn musical structures more effectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib3\" title=\"\">2025</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> have demonstrated the effectiveness and necessity of applying DPO for post-training in song generation tasks. As song generation involves multiple preference dimensions, multi-preference alignment optimization has become increasingly important. Existing approaches typically rely on merging separately optimized models. However, as the number of preferences increases, model merging often significantly reduces the average performance across preferences, severely limiting the overall efficiency of DPO.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We focus on four preference dimensions: musicality, style similarity, lyric alignment accuracy, and audio quality. Independent DPO experiments reveal complex interactions among these dimensions: improving lyric alignment often compromises musicality; optimizing audio quality may enhance alignment but weaken style similarity; while reinforcing style similarity tends to benefit musicality. These observations indicate that preferences can be either conflicting or synergistic.</p>\n\n",
                "matched_terms": [
                    "among",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, we propose a cross-pair preference optimization strategy. Preferences are grouped by similarity and paired across groups for pairwise optimization. Conflicting preferences are paired to mitigate trade-offs, while synergistic preferences are leveraged to enhance consistency across optimized models. This design not only reduces the number of optimized models required but also substantially improves the performance of the merged model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the subjective evaluation, we invite ten listeners with professional music backgrounds to provide Mean Opinion Score (MOS) score. Each generated song is rated along four dimensions: musicality (MUS), harmony between vocals and accompaniment (HAR), vocal performance (VOC), accompaniment performance (ACC), and overall performance (OVP).</p>\n\n",
                "matched_terms": [
                    "har",
                    "voc",
                    "ovp",
                    "acc",
                    "subjective",
                    "mus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive comparison of DiffRhythm 2 against multiple systems. For benchmarking, we select two industry-leading commercial systems: Suno V4.5&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://suno.com/blog/introducing-v4-5\" title=\"\">https://suno.com/blog/introducing-v4-5</a></span></span></span> and Mureka-O1&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_href\" href=\"https://www.mureka.ai\" title=\"\">https://www.mureka.ai</a></span></span></span>. In addition, we include three open-source systems for evaluation: DiffRhythm+&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>DiffRhythm+ is tested using DiffRhythm+-full released at https://github.com/ASLP-lab/DiffRhythm</span></span></span>, ACE-Step&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>ACE-Step is tested using the code released at https://github.com/ace-step/ACE-Step</span></span></span>, and LeVo&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>LeVo is tested using the code released at https://github.com/tencent-ailab/songgeneration</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "commercial",
                    "suno",
                    "levo",
                    "diffrhythm",
                    "acestep",
                    "murekao1",
                    "v45",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Objective Results &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, DiffRhythm 2 significantly outperforms other open-source models across music quality metrics. However, in aspects such as musicality, it still shows a clear gap compared to commercial systems like SUNO V4.5. In terms of audio quality, it performs slightly worse than LeVo but remains superior to most other models. This result demonstrates that employing a lower frame rate is feasible for compressed song reconstruction, although the increased reconstruction burden makes it difficult to achieve perfect fidelity. DiffRhythm 2 achieves the highest scores on Mulan-T (0.40) and PER (0.13), substantially exceeding other models. However, its Mulan-A score is lower than that of LeVo and similar systems, which we attribute to our choice of modeling audio style prompts with global representations. A global style embedding cannot adequately capture the stylistic content present in songs.</p>\n\n",
                "matched_terms": [
                    "commercial",
                    "models",
                    "suno",
                    "levo",
                    "diffrhythm",
                    "v45",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present DiffRhythm 2, a semi-autoregressive end-to-end music generation framework based on block flow matching. By leveraging this approach, we achieve high-quality lyric alignment without relying on semantic constraints, while generating mixed-track songs with high fidelity. We further introduce stochastic block REPA loss for semi-autoregressive training, which enhances musicality and structural modeling. In addition, we propose cross-pair preference optimization, which effectively addresses the challenge of degraded average performance when optimizing across multiple preferences. Our experiments demonstrate the superior song generation capabilities of DiffRhythm 2.</p>\n\n",
                "matched_terms": [
                    "while",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, experiments also reveal certain limitations. The low-frame-rate VAE imposes an upper bound on the fidelity of reconstructed audio, making it difficult to match real audio quality. Furthermore, improving vocal modeling without compromising model creativity remains a key challenge for enhancing mixed-track generation. More broadly, it is evident that open-source models still fall short of commercial systems in overall performance, necessitating for further advancements in data and generation strategies.</p>\n\n",
                "matched_terms": [
                    "commercial",
                    "models",
                    "model",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As DiffRhythm 2 is capable of generating complete songs, it could potentially be misused to create disinformation, deepfake audio, or other harmful content. Being committed to the responsible advancement of this field, we will provide appropriate usage restrictions and guidelines when releasing the open-source code and model checkpoints.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "model",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the compression and reconstruction performance of Stable Audio 2 VAE, the DCAE from ACE-Step, the dual-track MuCodec from LeVo and our implemented Music VAE on a testset of 50 songs covering diverse genres. It should also be noted that Music VAE supports only mono audio, whereas all the other models support stereo audio. As shown in the table, our Music VAE achieves comparable or even higher PESQ and STOI scores despite operating at a substantially lower frame rate than the other models. However, the WER of Music VAE is slightly higher than other models, indicating that the extreme low frame rate still results in some loss of fine-grained reconstruction details. The single codebook design of MuCodec significantly compromises the quality of the accompaniment and leads to inferior overall reconstruction performance.</p>\n\n",
                "matched_terms": [
                    "acestep",
                    "models",
                    "levo",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the generation speed of LeVo, Yue, DiffRhythm+, and DiffRhythm 2. To ensure fairness, all models are required to generate a fixed sequence of two minutes in length, regardless of generation quality. For both DiffRhythm+ and DiffRhythm 2, the number of sampling steps is set to 32. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A2.T5\" title=\"Table 5 &#8227; Appendix B Generation Speed &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, DiffRhythm 2 is only slightly slower than DiffRhythm+ and ACE-Step, while being significantly faster than LeVo. It is worth noting that DiffRhythm+ does not incorporate any attention acceleration framework, which explains why it is slower than ACE-Step despite having a smaller model size. ACE-Step leverages linear attention to reduce computational complexity, whereas LeVo adopts Flash Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/Dao-AILab/flash-attention\" title=\"\">https://github.com/Dao-AILab/flash-attention</a></span></span></span>, and DiffRhythm 2 employs Flex Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_href\" href=\"https://pytorch.ac.cn/blog/flexattention/\" title=\"\">https://pytorch.ac.cn/blog/flexattention/</a></span></span></span> for acceleration.</p>\n\n",
                "matched_terms": [
                    "models",
                    "while",
                    "levo",
                    "model",
                    "diffrhythm",
                    "acestep"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Results of ablation study on DiffRhythm 2.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">PER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Mulan-T <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Mulan-A <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Audio Aesthetics <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">SongEval <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">CE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">CU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">PC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">PQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">CO</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">MU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">ME</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">CL</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\">NA</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">DiffRhythm 2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">7.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">7.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">6.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">7.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">4.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">4.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.78</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">&#8194;&#8202;&#8195;w/o DPO</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.34</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">7.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">7.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">5.46</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">7.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.46</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.53</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">&#8194;&#8202;&#8195;w/o CPPO</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.69</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">7.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">7.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">5.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">7.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.79</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.57</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.65</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.43</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">&#8194;&#8202;&#8195;w/o REPA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">0.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">7.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">7.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">6.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">7.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">3.61</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "aesthetics",
            "↑uparrow",
            "mulant",
            "repa",
            "songeval",
            "study",
            "audio",
            "model",
            "diffrhythm",
            "ablation",
            "mulana",
            "cppo",
            "results",
            "dpo",
            "↓downarrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T3\" title=\"Table 3 &#8227; 5.3 Ablation Study &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, removing REPA loss leads to a clear drop in SongEval scores. Moreover, listening tests reveal that music structure become noticeably misaligned or even fail entirely. Similarly, without cross-pair preference optimization, all evaluation metrics decline significantly compared to DiffRhythm 2.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generating full-length, high-quality songs is challenging, as it requires maintaining long-term coherence both across text and music modalities and within the music modality itself. Existing non-autoregressive (NAR) frameworks, while capable of producing high-quality songs, often struggle with the alignment between lyrics and vocal. Concurrently, catering to diverse musical preferences necessitates reinforcement learning from human feedback (RLHF). However, existing methods often rely on merging multiple models during multi-preference optimization, which results in significant performance degradation.\nTo address these challenges, we introduce DiffRhythm 2, an end-to-end framework designed for high-fidelity, controllable song generation. To tackle the lyric alignment problem, DiffRhythm 2 employs a semi-autoregressive architecture based on block flow matching. This design enables faithful alignment of lyrics to singing vocals without relying on external labels and constraints, all while preserving the high generation quality and efficiency of NAR models. To make this framework computationally tractable for long sequences, we implement a music variational autoencoder (VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity audio reconstruction.\nIn addition, to overcome the limitations of multi-preference optimization in RLHF, we propose cross-pair preference optimization. This method effectively mitigates the performance drop typically associated with model merging, allowing for more robust optimization across diverse human preferences. We further enhance musicality and structural coherence by introducing stochastic block representation alignment loss.\nExperimental results demonstrate that DiffRhythm 2 can generate complete songs up to 210 seconds in length, consistently outperforming existing open-source models in both subjective and objective evaluations, while maintaining efficient generation speed. To encourage reproducibility and further exploration, we release the inference code and model checkpoints.\nCode and weights are availble at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/xiaomi-research/diffrhythm2\" title=\"\">https://github.com/xiaomi-research/diffrhythm2</a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "results",
                    "model",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike the aforementioned approaches, DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> takes a bold approach within a non-autoregressive (NAR) framework. By leveraging the generation ability of diffusion models and continuous representations, it achieves high-quality mixed-track generation of vocals and accompaniment at over 50 times faster than autoregressive methods. However, NAR models struggle with lyric alignment in long sequences. DiffRhythm addresses this issue by conditioning on sentence-level timestamps. However, this solution significantly reduces creativity and diversity, and also imposes higher requirements on the training data. ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> solved the alignment problem without timestamps by introducing representation alignment (REPA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib35\" title=\"\">2024</a>)</cite> loss with mHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boito et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib1\" title=\"\">2024</a>)</cite> as semantic constraints. However, the addition of these constraints significantly reduced musicality, creating a delicate trade-off between lyric alignment and generation quality.</p>\n\n",
                "matched_terms": [
                    "diffrhythm",
                    "repa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, pretrained models often fail to align with human preferences due to the gap between training objectives and desired song quality. Therefore, a typical method is to introduce reinforcement learning from human feedback (RLHF) in the post-training process to refine generation preferences across various musical dimensions. DiffRhythm+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib3\" title=\"\">2025</a>)</cite> demonstrated the feasibility of improving overall musicality with Direct Preference Optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib26\" title=\"\">2023</a>)</cite>. LeVo extended this approach by applying DPO to different dimensions separately and interpolating model weights to enhance overall performance. While this strategy improved balance across dimensions, it inevitably limited the upper bound of each individual capability due to the averaging effect.</p>\n\n",
                "matched_terms": [
                    "dpo",
                    "model",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we present DiffRhythm 2, a semi-autoregressive end-to-end framework for song generation based on block flow matching. This framework partitions latent representations into fixed-length blocks and generates each block non-autoregressively using flow matching, while maintaining autoregressive dependencies between blocks. This design enables faithful lyric alignment without requiring additional labels or constraints. The semi-autoregressive structure provides rich bidirectional context within blocks, ensuring long-sequence consistency, and supports fast inference for songs up to 210 seconds long. To accommodate block flow matching during training, we introduce stochastic block REPA loss that ensures efficient training and provides accurate representation guidance for the current block. We group similar preferences and perform pairwise optimization across groups to improve the performance of multi-preference alignment. Finally, as the block flow matching training strategy significantly increases the sequence length, we implement a high-compression music variational autoencoder (VAE), achieving high compression while still enabling high-quality audio reconstruction. To encourage reproducibility and further exploration, we will release the inference code and model checkpoints. The main contributions of this paper are summarized below:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "diffrhythm",
                    "model",
                    "repa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Song generation aims to produce coherent vocals and accompaniment given lyrics and style specifications, requiring joint modeling of melody, harmony, and lyric alignment. Early methods adopted sequential pipelines: Melodist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib12\" title=\"\">2024</a>)</cite> and MelodyLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib19\" title=\"\">2024</a>)</cite> first generated vocals from lyrics, then synthesized accompaniment. However, since vocals and accompaniment are inherently interdependent, this sequential approach often produces suboptimal results. To address these limitations, parallel generation methods emerged. SongCreator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib17\" title=\"\">2024</a>)</cite> and Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib36\" title=\"\">2025</a>)</cite> generate vocals and accompaniment independently in parallel, while SongGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib22\" title=\"\">2025</a>)</cite> introduces interleaved prediction across tracks to improve quality. Despite these advances, weak coupling between vocal and instrumental tracks often leads to harmonic inconsistencies. LeVo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> mitigates this by first generating a mixed track, then using it to guide separate vocal and accompaniment synthesis, achieving better harmony at the cost of reduced accompaniment complexity.\nRecent work has explored alternative architectures to improve both quality and efficiency. MusicCoT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib15\" title=\"\">2025</a>)</cite> and Songbloom&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib33\" title=\"\">2025</a>)</cite> leverage Chain-of-Thought reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib30\" title=\"\">2022</a>)</cite> to enhance vocal-accompaniment coordination and overall generation quality. However, these language-model-based approaches suffer from computational overhead and struggle with style consistency across long sequences due to their autoregressive nature.\nNon-autoregressive approaches offer promising alternatives. DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> and ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> employ diffusion models to achieve superior long-sequence consistency and vocal-accompaniment harmony while maintaining faster inference. However, both methods face challenges in lyric alignment for extended sequences. While each proposes specific solutions, these fixes come at the cost of reduced creativity or musicality. Specifically, DiffRhythm relies on timestamp conditioning while ACE-Step employs representation alignment, highlighting the need for more sophisticated alignment strategies.</p>\n\n",
                "matched_terms": [
                    "results",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture of DiffRhythm 2, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S3.F1\" title=\"Figure 1 &#8227; 3.1 Overview &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, comprises of a Music VAE and a Diffusion Transformer enhanced with block flow matching. The Diffusion Transformer is conditioned on lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> together with either text style prompt <math alttext=\"S_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">S_{t}</annotation></semantics></math> or audio style prompt <math alttext=\"S_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">S_{a}</annotation></semantics></math>, and generates VAE latents block by block. These latents are then passed to the Music VAE decoder to reconstruct the waveform.\nFor training, in addition to the block flow matching loss, we introduce the stochastic block REPA loss combined with MuQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib39\" title=\"\">2025</a>)</cite> representations to enhance the model&#8217;s musicality and structural modeling. Moreover, to address the degraded average performance caused by multi-preference optimization, we introduce a cross-pair preference optimization strategy. The following subsections will provide detailed descriptions of the DiffRhythm 2 modules and training procedure.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "diffrhythm",
                    "repa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of training DiffRhythm 2 on extremely long sequences, we design a customized Music VAE with a frame rate as low as 5 Hz. The VAE processes 24 kHz input audio as input and reconstructs it at 48 kHz, achieving compression ratios of 4800&#215; during encoding and 9600&#215; during decoding. The Music VAE consists of an encoder, a transformer block, and a decoder. The encoder adopts the same architecture as Stable Audio 2 VAE&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/Stability-AI/stable-audio-tools</span></span></span><cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib8\" title=\"\">2025</a>)</cite>. A transformer block is inserted before the decoder to alleviate reconstruction pressure. To maximize reconstruction quality, we employ BigVGAN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib16\" title=\"\">2023</a>)</cite> as the decoder. For the training loss, we combine the multi-scale mel loss from BigVGAN with the multi-scale STFT loss from Stable Audio 2 VAE to jointly optimize vocals and accompaniment. In addition, we employ a set of discriminators, including the multi-period discriminator, multi-scale discriminator, and CQT discriminator from BigVGAN.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As REPA loss is no longer required for lyric alignment constraints, we repurpose it to improve the model&#8217;s musicality and structural modeling. For the target SSL representations, extracting them with the same block size as DiffRhythm 2 hinders the capture of larger-scale musical structures. Moreover, SSL representations are typically downsampled by multi-layer convolution, which often introduces several frame shifts. The misalignment between <math alttext=\"z^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msup><mi>z</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">z^{i}</annotation></semantics></math> and SSL features caused by these shifts makes it unreliable to calculate REPA loss on target block directly. A better solution is to compute the loss on the entire sequence, which leverages the contextual receptive field to mitigate misalignment and enables the model to learn musical structures more effectively.</p>\n\n",
                "matched_terms": [
                    "diffrhythm",
                    "model",
                    "repa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib3\" title=\"\">2025</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> have demonstrated the effectiveness and necessity of applying DPO for post-training in song generation tasks. As song generation involves multiple preference dimensions, multi-preference alignment optimization has become increasingly important. Existing approaches typically rely on merging separately optimized models. However, as the number of preferences increases, model merging often significantly reduces the average performance across preferences, severely limiting the overall efficiency of DPO.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We focus on four preference dimensions: musicality, style similarity, lyric alignment accuracy, and audio quality. Independent DPO experiments reveal complex interactions among these dimensions: improving lyric alignment often compromises musicality; optimizing audio quality may enhance alignment but weaken style similarity; while reinforcing style similarity tends to benefit musicality. These observations indicate that preferences can be either conflicting or synergistic.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In DiffRhythm 2, we pair (musicality, lyric alignment accuracy) and (style similarity, audio quality). During DPO training, the winning sample must satisfy both preferences within a pair, whereas the losing sample is required to satisfy at least one. To ensure balanced optimization, the three possible losing cases are preserved in equal proportions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dpo",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DiffRhythm 2 is trained on a large-scale dataset comprising approximately 1.4 million songs, with a total duration of about 70,000 hours. The dataset spans three categories: Chinese, English and instrumental music, with a distribution ratio of roughly 4:5:1. To ensure data quality, we implement an efficient preprocessing pipeline. First, Audiobox-Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib28\" title=\"\">2025</a>)</cite> is employed to filter audio based on quality. Then, Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib2\" title=\"\">2012</a>)</cite> and FireRedASR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib32\" title=\"\">2025b</a>)</cite> are then used to transcribe the vocal tracks, and the two transcriptions are cross-validated against the original lyrics to ensure accuracy. Finally, All-in-One&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim &amp; Nam, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib14\" title=\"\">2023</a>)</cite> and Qwen2.5-omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib31\" title=\"\">2025a</a>)</cite> are used to annotate four key dimensions of the songs: structure, style, instrumentation, and emotion.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the objective evaluation, four aspects are considered: music quality, audio quality, style similarity, and lyric accuracy. To evaluate lyric accuracy, we transcribe the generated songs using Qwen3 ASR&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_href\" href=\"https://qwen.ai/blog?id=e199227023e8ebaac5f348f97fa804d1858ffc8a&amp;from=research.research-list\" title=\"\">https://qwen.ai/blog?id=e199227023e8ebaac5f348f97fa804d1858ffc8a&amp;from=research.research-list</a></span></span></span> and compute the phoneme error rate (PER). Style similarity is evaluated with MuQ-Mulan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib39\" title=\"\">2025</a>)</cite>, which measures the similarity of both textual prompts (Mulan-T) and audio prompts (Mulan-A). Music quality is assessed using SongEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib34\" title=\"\">2025</a>)</cite> in terms of overall coherence (CO), memorability (ME), naturalness of vocal breathing and phrasing (NA), clarity of song structure (CL), and overall musicality (MU). Audio quality is evaluated with Audiobox-Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib28\" title=\"\">2025</a>)</cite>, which considers content enjoyment (CE), content usefulness (CU), production complexity (PC), and production quality (PQ). We further explore the generation speed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A2\" title=\"Appendix B Generation Speed &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mulana",
                    "mulant",
                    "songeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Objective Results &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, DiffRhythm 2 significantly outperforms other open-source models across music quality metrics. However, in aspects such as musicality, it still shows a clear gap compared to commercial systems like SUNO V4.5. In terms of audio quality, it performs slightly worse than LeVo but remains superior to most other models. This result demonstrates that employing a lower frame rate is feasible for compressed song reconstruction, although the increased reconstruction burden makes it difficult to achieve perfect fidelity. DiffRhythm 2 achieves the highest scores on Mulan-T (0.40) and PER (0.13), substantially exceeding other models. However, its Mulan-A score is lower than that of LeVo and similar systems, which we attribute to our choice of modeling audio style prompts with global representations. A global style embedding cannot adequately capture the stylistic content present in songs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "mulana",
                    "mulant",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed methods, we conduct ablation studies focusing on stochastic block REPA loss and cross-pair preference optimization. For the cross-pair preference optimization ablation (w/o CPPO), we replaced it with separate DPO on four preferences followed by model merging. For the stochastic block REPA loss ablation (w/o REPA), we simply removed the corresponding loss during training. In addition, as a comparison, we included an experiment without applying any DPO (w/o DPO).</p>\n\n",
                "matched_terms": [
                    "repa",
                    "model",
                    "ablation",
                    "cppo",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present DiffRhythm 2, a semi-autoregressive end-to-end music generation framework based on block flow matching. By leveraging this approach, we achieve high-quality lyric alignment without relying on semantic constraints, while generating mixed-track songs with high fidelity. We further introduce stochastic block REPA loss for semi-autoregressive training, which enhances musicality and structural modeling. In addition, we propose cross-pair preference optimization, which effectively addresses the challenge of degraded average performance when optimizing across multiple preferences. Our experiments demonstrate the superior song generation capabilities of DiffRhythm 2.</p>\n\n",
                "matched_terms": [
                    "diffrhythm",
                    "repa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, experiments also reveal certain limitations. The low-frame-rate VAE imposes an upper bound on the fidelity of reconstructed audio, making it difficult to match real audio quality. Furthermore, improving vocal modeling without compromising model creativity remains a key challenge for enhancing mixed-track generation. More broadly, it is evident that open-source models still fall short of commercial systems in overall performance, necessitating for further advancements in data and generation strategies.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As DiffRhythm 2 is capable of generating complete songs, it could potentially be misused to create disinformation, deepfake audio, or other harmful content. Being committed to the responsible advancement of this field, we will provide appropriate usage restrictions and guidelines when releasing the open-source code and model checkpoints.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the compression and reconstruction performance of Stable Audio 2 VAE, the DCAE from ACE-Step, the dual-track MuCodec from LeVo and our implemented Music VAE on a testset of 50 songs covering diverse genres. It should also be noted that Music VAE supports only mono audio, whereas all the other models support stereo audio. As shown in the table, our Music VAE achieves comparable or even higher PESQ and STOI scores despite operating at a substantially lower frame rate than the other models. However, the WER of Music VAE is slightly higher than other models, indicating that the extreme low frame rate still results in some loss of fine-grained reconstruction details. The single codebook design of MuCodec significantly compromises the quality of the accompaniment and leads to inferior overall reconstruction performance.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By examining the outputs of the generative model, we find that the reconstruction quality of generated audio is generally superior to that of audio obtained through direct encoding and decoding with codec or VAE. In reconstruction tests, issues such as pronounced pronunciation errors or blurred accompaniment frequently occur, whereas their occurrence is greatly reduced in generated audio. Consequently, objective metrics based solely on codec or VAE reconstructions may fail to accurately reflect the quality achievable by the generative model.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the generation speed of LeVo, Yue, DiffRhythm+, and DiffRhythm 2. To ensure fairness, all models are required to generate a fixed sequence of two minutes in length, regardless of generation quality. For both DiffRhythm+ and DiffRhythm 2, the number of sampling steps is set to 32. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A2.T5\" title=\"Table 5 &#8227; Appendix B Generation Speed &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, DiffRhythm 2 is only slightly slower than DiffRhythm+ and ACE-Step, while being significantly faster than LeVo. It is worth noting that DiffRhythm+ does not incorporate any attention acceleration framework, which explains why it is slower than ACE-Step despite having a smaller model size. ACE-Step leverages linear attention to reduce computational complexity, whereas LeVo adopts Flash Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/Dao-AILab/flash-attention\" title=\"\">https://github.com/Dao-AILab/flash-attention</a></span></span></span>, and DiffRhythm 2 employs Flex Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_href\" href=\"https://pytorch.ac.cn/blog/flexattention/\" title=\"\">https://pytorch.ac.cn/blog/flexattention/</a></span></span></span> for acceleration.</p>\n\n",
                "matched_terms": [
                    "model",
                    "diffrhythm"
                ]
            }
        ]
    },
    "A1.T4": {
        "caption": "Table 4: Comparison of reconstruction performance across different music compression models",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Frame Rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Sample Rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PESQ <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">STOI <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Stable Audio 2 VAE</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21.5Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44100Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.981</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.634</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.148</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">DCAE (ACE-Step)</th>\n<td class=\"ltx_td ltx_align_center\">10Hz</td>\n<td class=\"ltx_td ltx_align_center\">44100Hz</td>\n<td class=\"ltx_td ltx_align_center\">2.176</td>\n<td class=\"ltx_td ltx_align_center\">0.647</td>\n<td class=\"ltx_td ltx_align_center\">0.117</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MuCodec (Dual-Track)</th>\n<td class=\"ltx_td ltx_align_center\">25Hz</td>\n<td class=\"ltx_td ltx_align_center\">48000Hz</td>\n<td class=\"ltx_td ltx_align_center\">1.876</td>\n<td class=\"ltx_td ltx_align_center\">0.561</td>\n<td class=\"ltx_td ltx_align_center\">0.174</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Music VAE</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">5Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">48000Hz</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.477</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.683</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.121</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sample",
            "models",
            "48000hz",
            "pesq",
            "5hz",
            "reconstruction",
            "215hz",
            "44100hz",
            "dualtrack",
            "↓downarrow",
            "comparison",
            "frame",
            "compression",
            "rate",
            "audio",
            "25hz",
            "performance",
            "across",
            "mucodec",
            "stoi",
            "dcae",
            "10hz",
            "vae",
            "stable",
            "↑uparrow",
            "model",
            "acestep",
            "music",
            "different"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generating full-length, high-quality songs is challenging, as it requires maintaining long-term coherence both across text and music modalities and within the music modality itself. Existing non-autoregressive (NAR) frameworks, while capable of producing high-quality songs, often struggle with the alignment between lyrics and vocal. Concurrently, catering to diverse musical preferences necessitates reinforcement learning from human feedback (RLHF). However, existing methods often rely on merging multiple models during multi-preference optimization, which results in significant performance degradation.\nTo address these challenges, we introduce DiffRhythm 2, an end-to-end framework designed for high-fidelity, controllable song generation. To tackle the lyric alignment problem, DiffRhythm 2 employs a semi-autoregressive architecture based on block flow matching. This design enables faithful alignment of lyrics to singing vocals without relying on external labels and constraints, all while preserving the high generation quality and efficiency of NAR models. To make this framework computationally tractable for long sequences, we implement a music variational autoencoder (VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity audio reconstruction.\nIn addition, to overcome the limitations of multi-preference optimization in RLHF, we propose cross-pair preference optimization. This method effectively mitigates the performance drop typically associated with model merging, allowing for more robust optimization across diverse human preferences. We further enhance musicality and structural coherence by introducing stochastic block representation alignment loss.\nExperimental results demonstrate that DiffRhythm 2 can generate complete songs up to 210 seconds in length, consistently outperforming existing open-source models in both subjective and objective evaluations, while maintaining efficient generation speed. To encourage reproducibility and further exploration, we release the inference code and model checkpoints.\nCode and weights are availble at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/xiaomi-research/diffrhythm2\" title=\"\">https://github.com/xiaomi-research/diffrhythm2</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "rate",
                    "audio",
                    "reconstruction",
                    "model",
                    "music",
                    "performance",
                    "vae",
                    "frame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Music serves as an abstract expression of human emotion and culture. Songs represent a unique art form, combining music and language to convey emotions and stories through melody and lyrics. With the rapid development of deep learning, music generation has progressed beyond symbolic music generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib29\" title=\"\">2025</a>; Qu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib25\" title=\"\">2024</a>)</cite> and singing voice synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib38\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib4\" title=\"\">2020</a>; Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib24\" title=\"\">2025b</a>)</cite>, moving towards more challenging tasks such as instrumental music generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib5\" title=\"\">2023</a>; Schneider et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib27\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib37\" title=\"\">2025</a>)</cite> and song generation. Unlike singing voice synthesis, which produces vocals with predefined melodies, or instrumental music generation, which only models melody and instrumentation, song generation requires joint modeling of lyrics, structure, singing vocal and accompaniment. This is further complicated by the typical length of songs, which often exceeds three minutes, making lyric alignment and stable style modeling particularly challenging.</p>\n\n",
                "matched_terms": [
                    "models",
                    "music",
                    "stable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early approaches, such as Jukebox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dhariwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib7\" title=\"\">2020</a>)</cite> and SongCreator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib17\" title=\"\">2024</a>)</cite>, generate songs by predicting the acoustic tokens of vocals and accompaniment mixed into a single track. However, the limited information density of these tokens hampers high-quality song generation. Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib36\" title=\"\">2025</a>)</cite> improves generation quality by modeling the vocals and accompaniment on separate tracks, while SongGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib22\" title=\"\">2025</a>)</cite> further enhances quality by introducing interleaved dual-track prediction. Despite these improvements, Yue and SongGen predict the tracks independently, which often leads to inconsistencies between the vocals and accompaniment. LeVo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> addresses this issue by first generating a mixed track and using it to guide the separate prediction of vocals and accompaniment, thereby reducing mismatches. However, the overall quality is still limited by the information compression in the mixed track tokenization, and it is difficult to generate complex and rich accompaniment. Most of these methods are based on autoregressive frameworks, which result in slow generation speeds. This hinders real-time applications and user interactivity.</p>\n\n",
                "matched_terms": [
                    "compression",
                    "dualtrack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike the aforementioned approaches, DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> takes a bold approach within a non-autoregressive (NAR) framework. By leveraging the generation ability of diffusion models and continuous representations, it achieves high-quality mixed-track generation of vocals and accompaniment at over 50 times faster than autoregressive methods. However, NAR models struggle with lyric alignment in long sequences. DiffRhythm addresses this issue by conditioning on sentence-level timestamps. However, this solution significantly reduces creativity and diversity, and also imposes higher requirements on the training data. ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> solved the alignment problem without timestamps by introducing representation alignment (REPA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib35\" title=\"\">2024</a>)</cite> loss with mHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boito et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib1\" title=\"\">2024</a>)</cite> as semantic constraints. However, the addition of these constraints significantly reduced musicality, creating a delicate trade-off between lyric alignment and generation quality.</p>\n\n",
                "matched_terms": [
                    "acestep",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, pretrained models often fail to align with human preferences due to the gap between training objectives and desired song quality. Therefore, a typical method is to introduce reinforcement learning from human feedback (RLHF) in the post-training process to refine generation preferences across various musical dimensions. DiffRhythm+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib3\" title=\"\">2025</a>)</cite> demonstrated the feasibility of improving overall musicality with Direct Preference Optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib26\" title=\"\">2023</a>)</cite>. LeVo extended this approach by applying DPO to different dimensions separately and interpolating model weights to enhance overall performance. While this strategy improved balance across dimensions, it inevitably limited the upper bound of each individual capability due to the averaging effect.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "model",
                    "different",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we present DiffRhythm 2, a semi-autoregressive end-to-end framework for song generation based on block flow matching. This framework partitions latent representations into fixed-length blocks and generates each block non-autoregressively using flow matching, while maintaining autoregressive dependencies between blocks. This design enables faithful lyric alignment without requiring additional labels or constraints. The semi-autoregressive structure provides rich bidirectional context within blocks, ensuring long-sequence consistency, and supports fast inference for songs up to 210 seconds long. To accommodate block flow matching during training, we introduce stochastic block REPA loss that ensures efficient training and provides accurate representation guidance for the current block. We group similar preferences and perform pairwise optimization across groups to improve the performance of multi-preference alignment. Finally, as the block flow matching training strategy significantly increases the sequence length, we implement a high-compression music variational autoencoder (VAE), achieving high compression while still enabling high-quality audio reconstruction. To encourage reproducibility and further exploration, we will release the inference code and model checkpoints. The main contributions of this paper are summarized below:</p>\n\n",
                "matched_terms": [
                    "across",
                    "compression",
                    "audio",
                    "reconstruction",
                    "model",
                    "music",
                    "vae",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a cross-pair preference optimization strategy to efficiently handle multi-preference alignment, which enhances final performance while reducing the number of optimized models through a group-based optimization approach.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design a music VAE with low-frame-rate compression at 5 Hz that preserves reconstruction quality while reducing sequence length, which not only accelerates inference but also makes long-sequence modeling in DiffRhythm 2 both feasible and effective.</p>\n\n",
                "matched_terms": [
                    "compression",
                    "vae",
                    "music",
                    "reconstruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Song generation aims to produce coherent vocals and accompaniment given lyrics and style specifications, requiring joint modeling of melody, harmony, and lyric alignment. Early methods adopted sequential pipelines: Melodist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib12\" title=\"\">2024</a>)</cite> and MelodyLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib19\" title=\"\">2024</a>)</cite> first generated vocals from lyrics, then synthesized accompaniment. However, since vocals and accompaniment are inherently interdependent, this sequential approach often produces suboptimal results. To address these limitations, parallel generation methods emerged. SongCreator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib17\" title=\"\">2024</a>)</cite> and Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib36\" title=\"\">2025</a>)</cite> generate vocals and accompaniment independently in parallel, while SongGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib22\" title=\"\">2025</a>)</cite> introduces interleaved prediction across tracks to improve quality. Despite these advances, weak coupling between vocal and instrumental tracks often leads to harmonic inconsistencies. LeVo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> mitigates this by first generating a mixed track, then using it to guide separate vocal and accompaniment synthesis, achieving better harmony at the cost of reduced accompaniment complexity.\nRecent work has explored alternative architectures to improve both quality and efficiency. MusicCoT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib15\" title=\"\">2025</a>)</cite> and Songbloom&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib33\" title=\"\">2025</a>)</cite> leverage Chain-of-Thought reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib30\" title=\"\">2022</a>)</cite> to enhance vocal-accompaniment coordination and overall generation quality. However, these language-model-based approaches suffer from computational overhead and struggle with style consistency across long sequences due to their autoregressive nature.\nNon-autoregressive approaches offer promising alternatives. DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> and ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> employ diffusion models to achieve superior long-sequence consistency and vocal-accompaniment harmony while maintaining faster inference. However, both methods face challenges in lyric alignment for extended sequences. While each proposes specific solutions, these fixes come at the cost of reduced creativity or musicality. Specifically, DiffRhythm relies on timestamp conditioning while ACE-Step employs representation alignment, highlighting the need for more sophisticated alignment strategies.</p>\n\n",
                "matched_terms": [
                    "acestep",
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture of DiffRhythm 2, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S3.F1\" title=\"Figure 1 &#8227; 3.1 Overview &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, comprises of a Music VAE and a Diffusion Transformer enhanced with block flow matching. The Diffusion Transformer is conditioned on lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> together with either text style prompt <math alttext=\"S_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">S_{t}</annotation></semantics></math> or audio style prompt <math alttext=\"S_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">S_{a}</annotation></semantics></math>, and generates VAE latents block by block. These latents are then passed to the Music VAE decoder to reconstruct the waveform.\nFor training, in addition to the block flow matching loss, we introduce the stochastic block REPA loss combined with MuQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib39\" title=\"\">2025</a>)</cite> representations to enhance the model&#8217;s musicality and structural modeling. Moreover, to address the degraded average performance caused by multi-preference optimization, we introduce a cross-pair preference optimization strategy. The following subsections will provide detailed descriptions of the DiffRhythm 2 modules and training procedure.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "audio",
                    "music",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of training DiffRhythm 2 on extremely long sequences, we design a customized Music VAE with a frame rate as low as 5 Hz. The VAE processes 24 kHz input audio as input and reconstructs it at 48 kHz, achieving compression ratios of 4800&#215; during encoding and 9600&#215; during decoding. The Music VAE consists of an encoder, a transformer block, and a decoder. The encoder adopts the same architecture as Stable Audio 2 VAE&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/Stability-AI/stable-audio-tools</span></span></span><cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib8\" title=\"\">2025</a>)</cite>. A transformer block is inserted before the decoder to alleviate reconstruction pressure. To maximize reconstruction quality, we employ BigVGAN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib16\" title=\"\">2023</a>)</cite> as the decoder. For the training loss, we combine the multi-scale mel loss from BigVGAN with the multi-scale STFT loss from Stable Audio 2 VAE to jointly optimize vocals and accompaniment. In addition, we employ a set of discriminators, including the multi-period discriminator, multi-scale discriminator, and CQT discriminator from BigVGAN.</p>\n\n",
                "matched_terms": [
                    "compression",
                    "audio",
                    "rate",
                    "reconstruction",
                    "music",
                    "vae",
                    "stable",
                    "frame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike MIMI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib6\" title=\"\">2024</a>)</cite>, we deliberately avoid adding a similar transformer block after the encoder for two reasons: (1) The limited future information introduced by convolution helps enhance content continuity between consecutive blocks. (2) The global bidirectional receptive field of transformer introduces excessive future information, substantially increasing the burden on the generative model. We explore the reconstruction performance of our VAE in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A1\" title=\"Appendix A Results of the Music Reconstruction &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "vae",
                    "reconstruction",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the definition, block flow matching requires access to clean block. While this is naturally available during inference, the training input sequence typically consists only of <math alttext=\"(S,L,Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>L</mi><mo>,</mo><msub><mi>Z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,L,Z_{t})</annotation></semantics></math> without the clean blocks. To address this, we concatenate the clean sequence to the noisy sequence, forming <math alttext=\"(S,L,Z,Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo>,</mo><msub><mi>Z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,L,Z,Z_{t})</annotation></semantics></math>, and apply an attention mask to enforce autoregressive dependencies.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S3.F2\" title=\"Figure 2 &#8227; 3.3.2 Details of Application &#8227; 3.3 Block Flow Matching &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our attention mask design: the style prompt <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> and lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> can be attended by any block; for the clean sequence, the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block can only attend to blocks <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> through <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>; for the noisy sequence, the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block can only attend to clean blocks <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m9\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> through <math alttext=\"i-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">i-1</annotation></semantics></math> and its own noisy block.\nThis design enforces autoregressive training but results in very long input sequences. To alleviate this, we employ a 5 Hz music VAE to substantially compress the training sequence length.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The introduction of clean sequences requires a mechanism to distinguish them from noisy sequences to facilitate model training. In conventional autoregressive frameworks, this is typically achieved by inserting special tokens as delimiters. However, such an approach is not applicable here. Since the receptive field is controlled by the attention mask, inserting delimiters for each block is impractical and would also affect positional encoding.\nTo address this in a simple yet effective way, we leverage the timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> to differentiate between sequences. Specifically, we set the style prompt and lyrics to a fixed timestep of <math alttext=\"-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m2\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math>, the clean sequence to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and assign the noisy sequence a timestep sampled from <math alttext=\"\\mathcal{U}[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{U}[0,1]</annotation></semantics></math>. To further enhance training stability, different blocks within the same sequence are assigned independently sampled timesteps.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To design the EOP frame, we experimented with several distributions. When the mean of the EOP distribution deviates significantly from that of the latent, such as in <math alttext=\"\\mathcal{N}(1,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m8\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(1,1)</annotation></semantics></math>, the model tends to generate strong noise near the end of the sequence. When the two distributions are numerically close, such as in <math alttext=\"\\mathcal{N}(0,2)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m9\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,2)</annotation></semantics></math>, prediction becomes unreliable since the model struggles to recognize the stopping position. Moreover, because no KL constraint is applied to the latent, enforcing a specific probabilistic form for EOP further increases the modeling difficulty. Overall, we find that <math alttext=\"\\mathcal{N}(1,0)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m10\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(1,0)</annotation></semantics></math>, i.e., a constant vector of ones, provides the most effective EOP representation, as it ensures clear numerical separation from latent features while remaining easy for the model to learn.</p>\n\n",
                "matched_terms": [
                    "model",
                    "frame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As REPA loss is no longer required for lyric alignment constraints, we repurpose it to improve the model&#8217;s musicality and structural modeling. For the target SSL representations, extracting them with the same block size as DiffRhythm 2 hinders the capture of larger-scale musical structures. Moreover, SSL representations are typically downsampled by multi-layer convolution, which often introduces several frame shifts. The misalignment between <math alttext=\"z^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msup><mi>z</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">z^{i}</annotation></semantics></math> and SSL features caused by these shifts makes it unreliable to calculate REPA loss on target block directly. A better solution is to compute the loss on the entire sequence, which leverages the contextual receptive field to mitigate misalignment and enables the model to learn musical structures more effectively.</p>\n\n",
                "matched_terms": [
                    "model",
                    "frame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib3\" title=\"\">2025</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> have demonstrated the effectiveness and necessity of applying DPO for post-training in song generation tasks. As song generation involves multiple preference dimensions, multi-preference alignment optimization has become increasingly important. Existing approaches typically rely on merging separately optimized models. However, as the number of preferences increases, model merging often significantly reduces the average performance across preferences, severely limiting the overall efficiency of DPO.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by these findings, we propose a cross-pair preference optimization strategy. Preferences are grouped by similarity and paired across groups for pairwise optimization. Conflicting preferences are paired to mitigate trade-offs, while synergistic preferences are leveraged to enhance consistency across optimized models. This design not only reduces the number of optimized models required but also substantially improves the performance of the merged model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In DiffRhythm 2, we pair (musicality, lyric alignment accuracy) and (style similarity, audio quality). During DPO training, the winning sample must satisfy both preferences within a pair, whereas the losing sample is required to satisfy at least one. To ensure balanced optimization, the three possible losing cases are preserved in equal proportions.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DiffRhythm 2 is trained on a large-scale dataset comprising approximately 1.4 million songs, with a total duration of about 70,000 hours. The dataset spans three categories: Chinese, English and instrumental music, with a distribution ratio of roughly 4:5:1. To ensure data quality, we implement an efficient preprocessing pipeline. First, Audiobox-Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib28\" title=\"\">2025</a>)</cite> is employed to filter audio based on quality. Then, Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib2\" title=\"\">2012</a>)</cite> and FireRedASR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib32\" title=\"\">2025b</a>)</cite> are then used to transcribe the vocal tracks, and the two transcriptions are cross-validated against the original lyrics to ensure accuracy. Finally, All-in-One&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim &amp; Nam, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib14\" title=\"\">2023</a>)</cite> and Qwen2.5-omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib31\" title=\"\">2025a</a>)</cite> are used to annotate four key dimensions of the songs: structure, style, instrumentation, and emotion.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, the test set consists of 50 real lyrics and 50 generated lyrics. For each lyric, we randomly select three style prompts, resulting in a total of 300 test cases. Across the entire test set, text prompts and audio prompts are balanced in equal proportion.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the subjective evaluation, we invite ten listeners with professional music backgrounds to provide Mean Opinion Score (MOS) score. Each generated song is rated along four dimensions: musicality (MUS), harmony between vocals and accompaniment (HAR), vocal performance (VOC), accompaniment performance (ACC), and overall performance (OVP).</p>\n\n",
                "matched_terms": [
                    "music",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the objective evaluation, four aspects are considered: music quality, audio quality, style similarity, and lyric accuracy. To evaluate lyric accuracy, we transcribe the generated songs using Qwen3 ASR&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_href\" href=\"https://qwen.ai/blog?id=e199227023e8ebaac5f348f97fa804d1858ffc8a&amp;from=research.research-list\" title=\"\">https://qwen.ai/blog?id=e199227023e8ebaac5f348f97fa804d1858ffc8a&amp;from=research.research-list</a></span></span></span> and compute the phoneme error rate (PER). Style similarity is evaluated with MuQ-Mulan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib39\" title=\"\">2025</a>)</cite>, which measures the similarity of both textual prompts (Mulan-T) and audio prompts (Mulan-A). Music quality is assessed using SongEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib34\" title=\"\">2025</a>)</cite> in terms of overall coherence (CO), memorability (ME), naturalness of vocal breathing and phrasing (NA), clarity of song structure (CL), and overall musicality (MU). Audio quality is evaluated with Audiobox-Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib28\" title=\"\">2025</a>)</cite>, which considers content enjoyment (CE), content usefulness (CU), production complexity (PC), and production quality (PQ). We further explore the generation speed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A2\" title=\"Appendix B Generation Speed &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "audio",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive comparison of DiffRhythm 2 against multiple systems. For benchmarking, we select two industry-leading commercial systems: Suno V4.5&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://suno.com/blog/introducing-v4-5\" title=\"\">https://suno.com/blog/introducing-v4-5</a></span></span></span> and Mureka-O1&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_href\" href=\"https://www.mureka.ai\" title=\"\">https://www.mureka.ai</a></span></span></span>. In addition, we include three open-source systems for evaluation: DiffRhythm+&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>DiffRhythm+ is tested using DiffRhythm+-full released at https://github.com/ASLP-lab/DiffRhythm</span></span></span>, ACE-Step&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>ACE-Step is tested using the code released at https://github.com/ace-step/ACE-Step</span></span></span>, and LeVo&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>LeVo is tested using the code released at https://github.com/tencent-ailab/songgeneration</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "acestep",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Objective Results &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, DiffRhythm 2 significantly outperforms other open-source models across music quality metrics. However, in aspects such as musicality, it still shows a clear gap compared to commercial systems like SUNO V4.5. In terms of audio quality, it performs slightly worse than LeVo but remains superior to most other models. This result demonstrates that employing a lower frame rate is feasible for compressed song reconstruction, although the increased reconstruction burden makes it difficult to achieve perfect fidelity. DiffRhythm 2 achieves the highest scores on Mulan-T (0.40) and PER (0.13), substantially exceeding other models. However, its Mulan-A score is lower than that of LeVo and similar systems, which we attribute to our choice of modeling audio style prompts with global representations. A global style embedding cannot adequately capture the stylistic content present in songs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "rate",
                    "audio",
                    "reconstruction",
                    "music",
                    "frame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T2\" title=\"Table 2 &#8227; 5.1 Objective Results &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, DiffRhythm 2 performs notably better than other open-source models in musicality, vocal-accompaniment harmony, and overall quality. Its accompaniment performance is comparable to that of ACE-Step and far exceeds that of LeVo, demonstrating that continuous representations can effectively enhance accompaniment generation. However, in vocal quality, since our system neither leverages semantic constraints nor separately models the vocal track, it falls slightly behind ACE-Step and LeVo. Overall, though, commercial models still maintain a clear advantage over open-source systems.</p>\n\n",
                "matched_terms": [
                    "acestep",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed methods, we conduct ablation studies focusing on stochastic block REPA loss and cross-pair preference optimization. For the cross-pair preference optimization ablation (w/o CPPO), we replaced it with separate DPO on four preferences followed by model merging. For the stochastic block REPA loss ablation (w/o REPA), we simply removed the corresponding loss during training. In addition, as a comparison, we included an experiment without applying any DPO (w/o DPO).</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present DiffRhythm 2, a semi-autoregressive end-to-end music generation framework based on block flow matching. By leveraging this approach, we achieve high-quality lyric alignment without relying on semantic constraints, while generating mixed-track songs with high fidelity. We further introduce stochastic block REPA loss for semi-autoregressive training, which enhances musicality and structural modeling. In addition, we propose cross-pair preference optimization, which effectively addresses the challenge of degraded average performance when optimizing across multiple preferences. Our experiments demonstrate the superior song generation capabilities of DiffRhythm 2.</p>\n\n",
                "matched_terms": [
                    "music",
                    "across",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, experiments also reveal certain limitations. The low-frame-rate VAE imposes an upper bound on the fidelity of reconstructed audio, making it difficult to match real audio quality. Furthermore, improving vocal modeling without compromising model creativity remains a key challenge for enhancing mixed-track generation. More broadly, it is evident that open-source models still fall short of commercial systems in overall performance, necessitating for further advancements in data and generation strategies.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "model",
                    "vae",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As DiffRhythm 2 is capable of generating complete songs, it could potentially be misused to create disinformation, deepfake audio, or other harmful content. Being committed to the responsible advancement of this field, we will provide appropriate usage restrictions and guidelines when releasing the open-source code and model checkpoints.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the compression and reconstruction performance of Stable Audio 2 VAE, the DCAE from ACE-Step, the dual-track MuCodec from LeVo and our implemented Music VAE on a testset of 50 songs covering diverse genres. It should also be noted that Music VAE supports only mono audio, whereas all the other models support stereo audio. As shown in the table, our Music VAE achieves comparable or even higher PESQ and STOI scores despite operating at a substantially lower frame rate than the other models. However, the WER of Music VAE is slightly higher than other models, indicating that the extreme low frame rate still results in some loss of fine-grained reconstruction details. The single codebook design of MuCodec significantly compromises the quality of the accompaniment and leads to inferior overall reconstruction performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pesq",
                    "compression",
                    "rate",
                    "audio",
                    "mucodec",
                    "reconstruction",
                    "stoi",
                    "acestep",
                    "dualtrack",
                    "music",
                    "dcae",
                    "performance",
                    "vae",
                    "stable",
                    "frame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By examining the outputs of the generative model, we find that the reconstruction quality of generated audio is generally superior to that of audio obtained through direct encoding and decoding with codec or VAE. In reconstruction tests, issues such as pronounced pronunciation errors or blurred accompaniment frequently occur, whereas their occurrence is greatly reduced in generated audio. Consequently, objective metrics based solely on codec or VAE reconstructions may fail to accurately reflect the quality achievable by the generative model.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "audio",
                    "reconstruction",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the generation speed of LeVo, Yue, DiffRhythm+, and DiffRhythm 2. To ensure fairness, all models are required to generate a fixed sequence of two minutes in length, regardless of generation quality. For both DiffRhythm+ and DiffRhythm 2, the number of sampling steps is set to 32. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A2.T5\" title=\"Table 5 &#8227; Appendix B Generation Speed &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, DiffRhythm 2 is only slightly slower than DiffRhythm+ and ACE-Step, while being significantly faster than LeVo. It is worth noting that DiffRhythm+ does not incorporate any attention acceleration framework, which explains why it is slower than ACE-Step despite having a smaller model size. ACE-Step leverages linear attention to reduce computational complexity, whereas LeVo adopts Flash Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/Dao-AILab/flash-attention\" title=\"\">https://github.com/Dao-AILab/flash-attention</a></span></span></span>, and DiffRhythm 2 employs Flex Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_href\" href=\"https://pytorch.ac.cn/blog/flexattention/\" title=\"\">https://pytorch.ac.cn/blog/flexattention/</a></span></span></span> for acceleration.</p>\n\n",
                "matched_terms": [
                    "acestep",
                    "models",
                    "model"
                ]
            }
        ]
    },
    "A2.T5": {
        "caption": "Table 5: Generation speed comparison on RTX 4090.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Architecture</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Model Size</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Time Cost <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">RTF <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">DiffRhythm+</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Flow Matching + VAE Decoder</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1B + 150M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">18.3s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.153</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ACE-Step</th>\n<td class=\"ltx_td ltx_align_center\">Flow Matching + DACE + Vocoder</td>\n<td class=\"ltx_td ltx_align_center\">3.5B + 150M</td>\n<td class=\"ltx_td ltx_align_center\">15.2s</td>\n<td class=\"ltx_td ltx_align_center\">0.127</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LeVo</th>\n<td class=\"ltx_td ltx_align_center\">LM + Diffusion Decoder</td>\n<td class=\"ltx_td ltx_align_center\">2B + 0.7B</td>\n<td class=\"ltx_td ltx_align_center\">147s</td>\n<td class=\"ltx_td ltx_align_center\">1.225</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">DiffRhythm 2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Block Flow Matching + Music VAE Decoder</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1B + 170M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">25.6s</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.213</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "152s",
            "matching",
            "147s",
            "size",
            "↓downarrow",
            "comparison",
            "07b",
            "time",
            "150m",
            "architecture",
            "generation",
            "speed",
            "dace",
            "rtx",
            "cost",
            "diffusion",
            "flow",
            "183s",
            "levo",
            "256s",
            "diffrhythm",
            "35b",
            "block",
            "vae",
            "170m",
            "model",
            "vocoder",
            "acestep",
            "music",
            "rtf",
            "decoder"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We compare the generation speed of LeVo, Yue, DiffRhythm+, and DiffRhythm 2. To ensure fairness, all models are required to generate a fixed sequence of two minutes in length, regardless of generation quality. For both DiffRhythm+ and DiffRhythm 2, the number of sampling steps is set to 32. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A2.T5\" title=\"Table 5 &#8227; Appendix B Generation Speed &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, DiffRhythm 2 is only slightly slower than DiffRhythm+ and ACE-Step, while being significantly faster than LeVo. It is worth noting that DiffRhythm+ does not incorporate any attention acceleration framework, which explains why it is slower than ACE-Step despite having a smaller model size. ACE-Step leverages linear attention to reduce computational complexity, whereas LeVo adopts Flash Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/Dao-AILab/flash-attention\" title=\"\">https://github.com/Dao-AILab/flash-attention</a></span></span></span>, and DiffRhythm 2 employs Flex Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_href\" href=\"https://pytorch.ac.cn/blog/flexattention/\" title=\"\">https://pytorch.ac.cn/blog/flexattention/</a></span></span></span> for acceleration.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generating full-length, high-quality songs is challenging, as it requires maintaining long-term coherence both across text and music modalities and within the music modality itself. Existing non-autoregressive (NAR) frameworks, while capable of producing high-quality songs, often struggle with the alignment between lyrics and vocal. Concurrently, catering to diverse musical preferences necessitates reinforcement learning from human feedback (RLHF). However, existing methods often rely on merging multiple models during multi-preference optimization, which results in significant performance degradation.\nTo address these challenges, we introduce DiffRhythm 2, an end-to-end framework designed for high-fidelity, controllable song generation. To tackle the lyric alignment problem, DiffRhythm 2 employs a semi-autoregressive architecture based on block flow matching. This design enables faithful alignment of lyrics to singing vocals without relying on external labels and constraints, all while preserving the high generation quality and efficiency of NAR models. To make this framework computationally tractable for long sequences, we implement a music variational autoencoder (VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity audio reconstruction.\nIn addition, to overcome the limitations of multi-preference optimization in RLHF, we propose cross-pair preference optimization. This method effectively mitigates the performance drop typically associated with model merging, allowing for more robust optimization across diverse human preferences. We further enhance musicality and structural coherence by introducing stochastic block representation alignment loss.\nExperimental results demonstrate that DiffRhythm 2 can generate complete songs up to 210 seconds in length, consistently outperforming existing open-source models in both subjective and objective evaluations, while maintaining efficient generation speed. To encourage reproducibility and further exploration, we release the inference code and model checkpoints.\nCode and weights are availble at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/xiaomi-research/diffrhythm2\" title=\"\">https://github.com/xiaomi-research/diffrhythm2</a>.</p>\n\n",
                "matched_terms": [
                    "matching",
                    "flow",
                    "model",
                    "diffrhythm",
                    "architecture",
                    "block",
                    "generation",
                    "music",
                    "speed",
                    "vae"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Music serves as an abstract expression of human emotion and culture. Songs represent a unique art form, combining music and language to convey emotions and stories through melody and lyrics. With the rapid development of deep learning, music generation has progressed beyond symbolic music generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib29\" title=\"\">2025</a>; Qu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib25\" title=\"\">2024</a>)</cite> and singing voice synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib38\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib4\" title=\"\">2020</a>; Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib24\" title=\"\">2025b</a>)</cite>, moving towards more challenging tasks such as instrumental music generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib5\" title=\"\">2023</a>; Schneider et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib27\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib37\" title=\"\">2025</a>)</cite> and song generation. Unlike singing voice synthesis, which produces vocals with predefined melodies, or instrumental music generation, which only models melody and instrumentation, song generation requires joint modeling of lyrics, structure, singing vocal and accompaniment. This is further complicated by the typical length of songs, which often exceeds three minutes, making lyric alignment and stable style modeling particularly challenging.</p>\n\n",
                "matched_terms": [
                    "music",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early approaches, such as Jukebox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dhariwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib7\" title=\"\">2020</a>)</cite> and SongCreator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib17\" title=\"\">2024</a>)</cite>, generate songs by predicting the acoustic tokens of vocals and accompaniment mixed into a single track. However, the limited information density of these tokens hampers high-quality song generation. Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib36\" title=\"\">2025</a>)</cite> improves generation quality by modeling the vocals and accompaniment on separate tracks, while SongGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib22\" title=\"\">2025</a>)</cite> further enhances quality by introducing interleaved dual-track prediction. Despite these improvements, Yue and SongGen predict the tracks independently, which often leads to inconsistencies between the vocals and accompaniment. LeVo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> addresses this issue by first generating a mixed track and using it to guide the separate prediction of vocals and accompaniment, thereby reducing mismatches. However, the overall quality is still limited by the information compression in the mixed track tokenization, and it is difficult to generate complex and rich accompaniment. Most of these methods are based on autoregressive frameworks, which result in slow generation speeds. This hinders real-time applications and user interactivity.</p>\n\n",
                "matched_terms": [
                    "levo",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike the aforementioned approaches, DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> takes a bold approach within a non-autoregressive (NAR) framework. By leveraging the generation ability of diffusion models and continuous representations, it achieves high-quality mixed-track generation of vocals and accompaniment at over 50 times faster than autoregressive methods. However, NAR models struggle with lyric alignment in long sequences. DiffRhythm addresses this issue by conditioning on sentence-level timestamps. However, this solution significantly reduces creativity and diversity, and also imposes higher requirements on the training data. ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> solved the alignment problem without timestamps by introducing representation alignment (REPA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib35\" title=\"\">2024</a>)</cite> loss with mHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boito et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib1\" title=\"\">2024</a>)</cite> as semantic constraints. However, the addition of these constraints significantly reduced musicality, creating a delicate trade-off between lyric alignment and generation quality.</p>\n\n",
                "matched_terms": [
                    "acestep",
                    "diffusion",
                    "generation",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, pretrained models often fail to align with human preferences due to the gap between training objectives and desired song quality. Therefore, a typical method is to introduce reinforcement learning from human feedback (RLHF) in the post-training process to refine generation preferences across various musical dimensions. DiffRhythm+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib3\" title=\"\">2025</a>)</cite> demonstrated the feasibility of improving overall musicality with Direct Preference Optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib26\" title=\"\">2023</a>)</cite>. LeVo extended this approach by applying DPO to different dimensions separately and interpolating model weights to enhance overall performance. While this strategy improved balance across dimensions, it inevitably limited the upper bound of each individual capability due to the averaging effect.</p>\n\n",
                "matched_terms": [
                    "levo",
                    "generation",
                    "model",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we present DiffRhythm 2, a semi-autoregressive end-to-end framework for song generation based on block flow matching. This framework partitions latent representations into fixed-length blocks and generates each block non-autoregressively using flow matching, while maintaining autoregressive dependencies between blocks. This design enables faithful lyric alignment without requiring additional labels or constraints. The semi-autoregressive structure provides rich bidirectional context within blocks, ensuring long-sequence consistency, and supports fast inference for songs up to 210 seconds long. To accommodate block flow matching during training, we introduce stochastic block REPA loss that ensures efficient training and provides accurate representation guidance for the current block. We group similar preferences and perform pairwise optimization across groups to improve the performance of multi-preference alignment. Finally, as the block flow matching training strategy significantly increases the sequence length, we implement a high-compression music variational autoencoder (VAE), achieving high compression while still enabling high-quality audio reconstruction. To encourage reproducibility and further exploration, we will release the inference code and model checkpoints. The main contributions of this paper are summarized below:</p>\n\n",
                "matched_terms": [
                    "matching",
                    "flow",
                    "model",
                    "diffrhythm",
                    "block",
                    "generation",
                    "music",
                    "vae"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose DiffRhythm 2, a novel semi-autoregressive song generation framework based on block flow matching, which is capable of producing high-quality songs with faithful lyric alignment and excellent accompaniment performance.</p>\n\n",
                "matched_terms": [
                    "matching",
                    "flow",
                    "diffrhythm",
                    "block",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce stochastic block REPA loss to enable representation alignment in block flow matching, thereby enhancing structural modeling and achieving remarkable coherence and hierarchical structure in long-form song generation.</p>\n\n",
                "matched_terms": [
                    "matching",
                    "block",
                    "generation",
                    "flow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design a music VAE with low-frame-rate compression at 5 Hz that preserves reconstruction quality while reducing sequence length, which not only accelerates inference but also makes long-sequence modeling in DiffRhythm 2 both feasible and effective.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "music",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Flow matching (FM)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lipman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib20\" title=\"\">2023</a>)</cite> trains continuous normalizing flows (CNFs) without simulation by directly regressing a time-dependent vector field <math alttext=\"v_{t}(x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>v</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">v_{t}(x)</annotation></semantics></math> that transports samples from a simple prior <math alttext=\"p_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>p</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">p_{0}</annotation></semantics></math> to a target distribution <math alttext=\"p_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>p</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">p_{1}</annotation></semantics></math> along paths <math alttext=\"p_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">p_{t}</annotation></semantics></math>. This yields simpler objectives, faster convergence, and more stable training than score matching. Conditional flow matching (CFM) further conditions the vector field on external information, enabling guided generation. CFM is widely used in the field of image generation and has also been applied to TTS tasks in recent years.</p>\n\n",
                "matched_terms": [
                    "matching",
                    "generation",
                    "flow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Autoregressive diffusion models aim to combine the strengths of both autoregressive and non-autoregressive paradigms, achieving complementary advantages. These approaches can generally be categorized into two groups: diffusion-backbone and language-model-backbone designs. For diffusion-based backbones, the SSD series&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib10\" title=\"\">2022</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib11\" title=\"\">2023</a>)</cite> introduced this idea and applied it to text generation tasks, with Block Diffusion further optimizing the framework. ARDiT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib21\" title=\"\">2024</a>)</cite> extended this line of work to text-to-speech (TTS). On the other hand, language-model-based backbones include DiTAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib13\" title=\"\">2025</a>)</cite>, which integrates diffusion mechanisms into autoregressive language models.</p>\n\n",
                "matched_terms": [
                    "block",
                    "diffusion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Song generation aims to produce coherent vocals and accompaniment given lyrics and style specifications, requiring joint modeling of melody, harmony, and lyric alignment. Early methods adopted sequential pipelines: Melodist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib12\" title=\"\">2024</a>)</cite> and MelodyLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib19\" title=\"\">2024</a>)</cite> first generated vocals from lyrics, then synthesized accompaniment. However, since vocals and accompaniment are inherently interdependent, this sequential approach often produces suboptimal results. To address these limitations, parallel generation methods emerged. SongCreator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib17\" title=\"\">2024</a>)</cite> and Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib36\" title=\"\">2025</a>)</cite> generate vocals and accompaniment independently in parallel, while SongGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib22\" title=\"\">2025</a>)</cite> introduces interleaved prediction across tracks to improve quality. Despite these advances, weak coupling between vocal and instrumental tracks often leads to harmonic inconsistencies. LeVo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> mitigates this by first generating a mixed track, then using it to guide separate vocal and accompaniment synthesis, achieving better harmony at the cost of reduced accompaniment complexity.\nRecent work has explored alternative architectures to improve both quality and efficiency. MusicCoT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib15\" title=\"\">2025</a>)</cite> and Songbloom&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib33\" title=\"\">2025</a>)</cite> leverage Chain-of-Thought reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib30\" title=\"\">2022</a>)</cite> to enhance vocal-accompaniment coordination and overall generation quality. However, these language-model-based approaches suffer from computational overhead and struggle with style consistency across long sequences due to their autoregressive nature.\nNon-autoregressive approaches offer promising alternatives. DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> and ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> employ diffusion models to achieve superior long-sequence consistency and vocal-accompaniment harmony while maintaining faster inference. However, both methods face challenges in lyric alignment for extended sequences. While each proposes specific solutions, these fixes come at the cost of reduced creativity or musicality. Specifically, DiffRhythm relies on timestamp conditioning while ACE-Step employs representation alignment, highlighting the need for more sophisticated alignment strategies.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "levo",
                    "diffrhythm",
                    "acestep",
                    "generation",
                    "cost"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture of DiffRhythm 2, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S3.F1\" title=\"Figure 1 &#8227; 3.1 Overview &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, comprises of a Music VAE and a Diffusion Transformer enhanced with block flow matching. The Diffusion Transformer is conditioned on lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> together with either text style prompt <math alttext=\"S_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">S_{t}</annotation></semantics></math> or audio style prompt <math alttext=\"S_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">S_{a}</annotation></semantics></math>, and generates VAE latents block by block. These latents are then passed to the Music VAE decoder to reconstruct the waveform.\nFor training, in addition to the block flow matching loss, we introduce the stochastic block REPA loss combined with MuQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib39\" title=\"\">2025</a>)</cite> representations to enhance the model&#8217;s musicality and structural modeling. Moreover, to address the degraded average performance caused by multi-preference optimization, we introduce a cross-pair preference optimization strategy. The following subsections will provide detailed descriptions of the DiffRhythm 2 modules and training procedure.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "matching",
                    "flow",
                    "diffrhythm",
                    "architecture",
                    "block",
                    "music",
                    "vae",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of training DiffRhythm 2 on extremely long sequences, we design a customized Music VAE with a frame rate as low as 5 Hz. The VAE processes 24 kHz input audio as input and reconstructs it at 48 kHz, achieving compression ratios of 4800&#215; during encoding and 9600&#215; during decoding. The Music VAE consists of an encoder, a transformer block, and a decoder. The encoder adopts the same architecture as Stable Audio 2 VAE&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/Stability-AI/stable-audio-tools</span></span></span><cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib8\" title=\"\">2025</a>)</cite>. A transformer block is inserted before the decoder to alleviate reconstruction pressure. To maximize reconstruction quality, we employ BigVGAN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib16\" title=\"\">2023</a>)</cite> as the decoder. For the training loss, we combine the multi-scale mel loss from BigVGAN with the multi-scale STFT loss from Stable Audio 2 VAE to jointly optimize vocals and accompaniment. In addition, we employ a set of discriminators, including the multi-period discriminator, multi-scale discriminator, and CQT discriminator from BigVGAN.</p>\n\n",
                "matched_terms": [
                    "diffrhythm",
                    "architecture",
                    "block",
                    "music",
                    "vae",
                    "decoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike MIMI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib6\" title=\"\">2024</a>)</cite>, we deliberately avoid adding a similar transformer block after the encoder for two reasons: (1) The limited future information introduced by convolution helps enhance content continuity between consecutive blocks. (2) The global bidirectional receptive field of transformer introduces excessive future information, substantially increasing the burden on the generative model. We explore the reconstruction performance of our VAE in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A1\" title=\"Appendix A Results of the Music Reconstruction &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "block",
                    "vae",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first establish the mathematical foundation of flow matching before extending it to the block-wise setting. Given the target latent <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math>, flow matching defines a linear probability path that transports Gaussian noise <math alttext=\"Z_{0}\\sim\\mathcal{N}(0,I)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mn>0</mn></msub><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">Z_{0}\\sim\\mathcal{N}(0,I)</annotation></semantics></math> to the target data distribution <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math>. During training, the timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is sampled from <math alttext=\"\\mathcal{U}[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{U}[0,1]</annotation></semantics></math>, and the noisy latent <math alttext=\"Z_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">Z_{t}</annotation></semantics></math> is obtained by linear interpolation:</p>\n\n",
                "matched_terms": [
                    "matching",
                    "flow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math> conditions on <math alttext=\"Z_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">Z_{t}</annotation></semantics></math>, style prompt <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>, lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m10\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>, and timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m11\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> to estimate the velocity field <math alttext=\"\\hat{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m12\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>v</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{v}</annotation></semantics></math>. Since the path is linear, the ground truth velocity field <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m13\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math> is <math alttext=\"Z-Z_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m14\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>&#8722;</mo><msub><mi>Z</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">Z-Z_{0}</annotation></semantics></math>. The flow matching loss is therefore defined as</p>\n\n",
                "matched_terms": [
                    "matching",
                    "model",
                    "flow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In block flow matching, the core idea is that each block is generated with flow matching, while the dependency across blocks is handled autoregressively. Let the block size be <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, and split the target latent <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> of length <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> into <math alttext=\"Z=\\{z^{1},z^{2},\\cdots z^{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msup><mi>z</mi><mn>1</mn></msup><mo>,</mo><msup><mi>z</mi><mn>2</mn></msup><mo>,</mo><mrow><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>z</mi><mi>k</mi></msup></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=\\{z^{1},z^{2},\\cdots z^{k}\\}</annotation></semantics></math>, where <math alttext=\"k=\\lceil l/b\\rceil\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mrow><mo stretchy=\"false\">&#8968;</mo><mrow><mi>l</mi><mo>/</mo><mi>b</mi></mrow><mo stretchy=\"false\">&#8969;</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k=\\lceil l/b\\rceil</annotation></semantics></math> denotes the total number of blocks. Since the generation of each block depends on all previously generated blocks, the velocity field for the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m6\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block is estimated as</p>\n\n",
                "matched_terms": [
                    "matching",
                    "flow",
                    "block",
                    "size",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"z^{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m7\" intent=\":literal\"><semantics><msup><mi>z</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">z^{&lt;i}</annotation></semantics></math> represents all preceding blocks. <math alttext=\"z^{i}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">z^{i}_{t}</annotation></semantics></math>is the noisy latent for the current block obtained via linear interpolation, and <math alttext=\"t^{i}\\sim\\mathcal{U}[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m9\" intent=\":literal\"><semantics><mrow><msup><mi>t</mi><mi>i</mi></msup><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">t^{i}\\sim\\mathcal{U}[0,1]</annotation></semantics></math> is sampled independently for each block. The corresponding ground truth velocity for the i-th block is <math alttext=\"v^{i}=z^{i}-z^{i}_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m10\" intent=\":literal\"><semantics><mrow><msup><mi>v</mi><mi>i</mi></msup><mo>=</mo><mrow><msup><mi>z</mi><mi>i</mi></msup><mo>&#8722;</mo><msubsup><mi>z</mi><mn>0</mn><mi>i</mi></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">v^{i}=z^{i}-z^{i}_{0}</annotation></semantics></math>, where <math alttext=\"z^{i}_{0}\\sim\\mathcal{N}(0,I)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m11\" intent=\":literal\"><semantics><mrow><msubsup><mi>z</mi><mn>0</mn><mi>i</mi></msubsup><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z^{i}_{0}\\sim\\mathcal{N}(0,I)</annotation></semantics></math> is the standard Gaussian noise for this block. The overall block flow matching loss is then defined as</p>\n\n",
                "matched_terms": [
                    "matching",
                    "block",
                    "flow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the definition, block flow matching requires access to clean block. While this is naturally available during inference, the training input sequence typically consists only of <math alttext=\"(S,L,Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>L</mi><mo>,</mo><msub><mi>Z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,L,Z_{t})</annotation></semantics></math> without the clean blocks. To address this, we concatenate the clean sequence to the noisy sequence, forming <math alttext=\"(S,L,Z,Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo>,</mo><msub><mi>Z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,L,Z,Z_{t})</annotation></semantics></math>, and apply an attention mask to enforce autoregressive dependencies.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S3.F2\" title=\"Figure 2 &#8227; 3.3.2 Details of Application &#8227; 3.3 Block Flow Matching &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our attention mask design: the style prompt <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> and lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> can be attended by any block; for the clean sequence, the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block can only attend to blocks <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> through <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>; for the noisy sequence, the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block can only attend to clean blocks <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m9\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> through <math alttext=\"i-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">i-1</annotation></semantics></math> and its own noisy block.\nThis design enforces autoregressive training but results in very long input sequences. To alleviate this, we employ a 5 Hz music VAE to substantially compress the training sequence length.</p>\n\n",
                "matched_terms": [
                    "matching",
                    "flow",
                    "block",
                    "music",
                    "vae"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The introduction of clean sequences requires a mechanism to distinguish them from noisy sequences to facilitate model training. In conventional autoregressive frameworks, this is typically achieved by inserting special tokens as delimiters. However, such an approach is not applicable here. Since the receptive field is controlled by the attention mask, inserting delimiters for each block is impractical and would also affect positional encoding.\nTo address this in a simple yet effective way, we leverage the timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> to differentiate between sequences. Specifically, we set the style prompt and lyrics to a fixed timestep of <math alttext=\"-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m2\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math>, the clean sequence to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and assign the noisy sequence a timestep sampled from <math alttext=\"\\mathcal{U}[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{U}[0,1]</annotation></semantics></math>. To further enhance training stability, different blocks within the same sequence are assigned independently sampled timesteps.</p>\n\n",
                "matched_terms": [
                    "block",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike prior works that predetermine the total sequence length before generation, DiffRhythm 2 incorporates the End-of-Prediction (EOP) frame as part of the prediction target to enable variable length generation. Since generation proceeds block by block, we pad <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> EOP frames at the end of the sequence rather than a single frame. Specifically, <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is determined by the length of the final block <math alttext=\"l^{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m3\" intent=\":literal\"><semantics><msup><mi>l</mi><mi>k</mi></msup><annotation encoding=\"application/x-tex\">l^{k}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "block",
                    "generation",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If <math alttext=\"l^{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m4\" intent=\":literal\"><semantics><msup><mi>l</mi><mi>k</mi></msup><annotation encoding=\"application/x-tex\">l^{k}</annotation></semantics></math> is shorter than the block size <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m5\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, EOPs are padded to complete the block; if <math alttext=\"l^{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m6\" intent=\":literal\"><semantics><msup><mi>l</mi><mi>k</mi></msup><annotation encoding=\"application/x-tex\">l^{k}</annotation></semantics></math> equals <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m7\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, an additional full block of EOPs is padded. Formally,</p>\n\n",
                "matched_terms": [
                    "block",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training and inference procedures are described in Algorithms&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#alg1\" title=\"Algorithm 1 &#8227; 3.3.4 Training and Inference Procedures &#8227; 3.3 Block Flow Matching &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#alg2\" title=\"Algorithm 2 &#8227; 3.3.4 Training and Inference Procedures &#8227; 3.3 Block Flow Matching &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. It is worth noting that the incorporation of the autoregressive mechanism allows us to enable KV cache at the block level during inference, which substantially accelerates the generation speed.</p>\n\n",
                "matched_terms": [
                    "block",
                    "generation",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As REPA loss is no longer required for lyric alignment constraints, we repurpose it to improve the model&#8217;s musicality and structural modeling. For the target SSL representations, extracting them with the same block size as DiffRhythm 2 hinders the capture of larger-scale musical structures. Moreover, SSL representations are typically downsampled by multi-layer convolution, which often introduces several frame shifts. The misalignment between <math alttext=\"z^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msup><mi>z</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">z^{i}</annotation></semantics></math> and SSL features caused by these shifts makes it unreliable to calculate REPA loss on target block directly. A better solution is to compute the loss on the entire sequence, which leverages the contextual receptive field to mitigate misalignment and enables the model to learn musical structures more effectively.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model",
                    "diffrhythm",
                    "block"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib3\" title=\"\">2025</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> have demonstrated the effectiveness and necessity of applying DPO for post-training in song generation tasks. As song generation involves multiple preference dimensions, multi-preference alignment optimization has become increasingly important. Existing approaches typically rely on merging separately optimized models. However, as the number of preferences increases, model merging often significantly reduces the average performance across preferences, severely limiting the overall efficiency of DPO.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DiffRhythm 2 is trained on a large-scale dataset comprising approximately 1.4 million songs, with a total duration of about 70,000 hours. The dataset spans three categories: Chinese, English and instrumental music, with a distribution ratio of roughly 4:5:1. To ensure data quality, we implement an efficient preprocessing pipeline. First, Audiobox-Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib28\" title=\"\">2025</a>)</cite> is employed to filter audio based on quality. Then, Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib2\" title=\"\">2012</a>)</cite> and FireRedASR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib32\" title=\"\">2025b</a>)</cite> are then used to transcribe the vocal tracks, and the two transcriptions are cross-validated against the original lyrics to ensure accuracy. Finally, All-in-One&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim &amp; Nam, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib14\" title=\"\">2023</a>)</cite> and Qwen2.5-omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib31\" title=\"\">2025a</a>)</cite> are used to annotate four key dimensions of the songs: structure, style, instrumentation, and emotion.</p>\n\n",
                "matched_terms": [
                    "music",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the objective evaluation, four aspects are considered: music quality, audio quality, style similarity, and lyric accuracy. To evaluate lyric accuracy, we transcribe the generated songs using Qwen3 ASR&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_href\" href=\"https://qwen.ai/blog?id=e199227023e8ebaac5f348f97fa804d1858ffc8a&amp;from=research.research-list\" title=\"\">https://qwen.ai/blog?id=e199227023e8ebaac5f348f97fa804d1858ffc8a&amp;from=research.research-list</a></span></span></span> and compute the phoneme error rate (PER). Style similarity is evaluated with MuQ-Mulan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib39\" title=\"\">2025</a>)</cite>, which measures the similarity of both textual prompts (Mulan-T) and audio prompts (Mulan-A). Music quality is assessed using SongEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib34\" title=\"\">2025</a>)</cite> in terms of overall coherence (CO), memorability (ME), naturalness of vocal breathing and phrasing (NA), clarity of song structure (CL), and overall musicality (MU). Audio quality is evaluated with Audiobox-Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib28\" title=\"\">2025</a>)</cite>, which considers content enjoyment (CE), content usefulness (CU), production complexity (PC), and production quality (PQ). We further explore the generation speed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A2\" title=\"Appendix B Generation Speed &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "music",
                    "generation",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive comparison of DiffRhythm 2 against multiple systems. For benchmarking, we select two industry-leading commercial systems: Suno V4.5&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_href\" href=\"https://suno.com/blog/introducing-v4-5\" title=\"\">https://suno.com/blog/introducing-v4-5</a></span></span></span> and Mureka-O1&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_href\" href=\"https://www.mureka.ai\" title=\"\">https://www.mureka.ai</a></span></span></span>. In addition, we include three open-source systems for evaluation: DiffRhythm+&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>DiffRhythm+ is tested using DiffRhythm+-full released at https://github.com/ASLP-lab/DiffRhythm</span></span></span>, ACE-Step&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>ACE-Step is tested using the code released at https://github.com/ace-step/ACE-Step</span></span></span>, and LeVo&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>LeVo is tested using the code released at https://github.com/tencent-ailab/songgeneration</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "acestep",
                    "levo",
                    "comparison",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Objective Results &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, DiffRhythm 2 significantly outperforms other open-source models across music quality metrics. However, in aspects such as musicality, it still shows a clear gap compared to commercial systems like SUNO V4.5. In terms of audio quality, it performs slightly worse than LeVo but remains superior to most other models. This result demonstrates that employing a lower frame rate is feasible for compressed song reconstruction, although the increased reconstruction burden makes it difficult to achieve perfect fidelity. DiffRhythm 2 achieves the highest scores on Mulan-T (0.40) and PER (0.13), substantially exceeding other models. However, its Mulan-A score is lower than that of LeVo and similar systems, which we attribute to our choice of modeling audio style prompts with global representations. A global style embedding cannot adequately capture the stylistic content present in songs.</p>\n\n",
                "matched_terms": [
                    "music",
                    "levo",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T2\" title=\"Table 2 &#8227; 5.1 Objective Results &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, DiffRhythm 2 performs notably better than other open-source models in musicality, vocal-accompaniment harmony, and overall quality. Its accompaniment performance is comparable to that of ACE-Step and far exceeds that of LeVo, demonstrating that continuous representations can effectively enhance accompaniment generation. However, in vocal quality, since our system neither leverages semantic constraints nor separately models the vocal track, it falls slightly behind ACE-Step and LeVo. Overall, though, commercial models still maintain a clear advantage over open-source systems.</p>\n\n",
                "matched_terms": [
                    "acestep",
                    "levo",
                    "generation",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed methods, we conduct ablation studies focusing on stochastic block REPA loss and cross-pair preference optimization. For the cross-pair preference optimization ablation (w/o CPPO), we replaced it with separate DPO on four preferences followed by model merging. For the stochastic block REPA loss ablation (w/o REPA), we simply removed the corresponding loss during training. In addition, as a comparison, we included an experiment without applying any DPO (w/o DPO).</p>\n\n",
                "matched_terms": [
                    "block",
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T3\" title=\"Table 3 &#8227; 5.3 Ablation Study &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, removing REPA loss leads to a clear drop in SongEval scores. Moreover, listening tests reveal that music structure become noticeably misaligned or even fail entirely. Similarly, without cross-pair preference optimization, all evaluation metrics decline significantly compared to DiffRhythm 2.</p>\n\n",
                "matched_terms": [
                    "music",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present DiffRhythm 2, a semi-autoregressive end-to-end music generation framework based on block flow matching. By leveraging this approach, we achieve high-quality lyric alignment without relying on semantic constraints, while generating mixed-track songs with high fidelity. We further introduce stochastic block REPA loss for semi-autoregressive training, which enhances musicality and structural modeling. In addition, we propose cross-pair preference optimization, which effectively addresses the challenge of degraded average performance when optimizing across multiple preferences. Our experiments demonstrate the superior song generation capabilities of DiffRhythm 2.</p>\n\n",
                "matched_terms": [
                    "matching",
                    "flow",
                    "diffrhythm",
                    "block",
                    "generation",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, experiments also reveal certain limitations. The low-frame-rate VAE imposes an upper bound on the fidelity of reconstructed audio, making it difficult to match real audio quality. Furthermore, improving vocal modeling without compromising model creativity remains a key challenge for enhancing mixed-track generation. More broadly, it is evident that open-source models still fall short of commercial systems in overall performance, necessitating for further advancements in data and generation strategies.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "generation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As DiffRhythm 2 is capable of generating complete songs, it could potentially be misused to create disinformation, deepfake audio, or other harmful content. Being committed to the responsible advancement of this field, we will provide appropriate usage restrictions and guidelines when releasing the open-source code and model checkpoints.</p>\n\n",
                "matched_terms": [
                    "model",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the compression and reconstruction performance of Stable Audio 2 VAE, the DCAE from ACE-Step, the dual-track MuCodec from LeVo and our implemented Music VAE on a testset of 50 songs covering diverse genres. It should also be noted that Music VAE supports only mono audio, whereas all the other models support stereo audio. As shown in the table, our Music VAE achieves comparable or even higher PESQ and STOI scores despite operating at a substantially lower frame rate than the other models. However, the WER of Music VAE is slightly higher than other models, indicating that the extreme low frame rate still results in some loss of fine-grained reconstruction details. The single codebook design of MuCodec significantly compromises the quality of the accompaniment and leads to inferior overall reconstruction performance.</p>\n\n",
                "matched_terms": [
                    "acestep",
                    "vae",
                    "levo",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By examining the outputs of the generative model, we find that the reconstruction quality of generated audio is generally superior to that of audio obtained through direct encoding and decoding with codec or VAE. In reconstruction tests, issues such as pronounced pronunciation errors or blurred accompaniment frequently occur, whereas their occurrence is greatly reduced in generated audio. Consequently, objective metrics based solely on codec or VAE reconstructions may fail to accurately reflect the quality achievable by the generative model.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "model"
                ]
            }
        ]
    },
    "A3.T6": {
        "caption": "Table 6: Music VAE training details.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Parameter</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Specification</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Dataset</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">70,000-hour music dataset and 100,000-hour speech dataset.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Latent Space</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Encoder produces a latent representation with a latent dimension of 64 frames.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Training Steps</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">1,500,000 steps.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Batch Size (Global)</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">128 (8 per GPU).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Total Duration</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Approx. 7 days.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "70000hour",
            "size",
            "days",
            "details",
            "parameter",
            "frames",
            "approx",
            "batch",
            "training",
            "encoder",
            "gpu",
            "100000hour",
            "speech",
            "vae",
            "latent",
            "global",
            "produces",
            "total",
            "representation",
            "steps",
            "music",
            "space",
            "duration",
            "dimension",
            "dataset",
            "specification"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generating full-length, high-quality songs is challenging, as it requires maintaining long-term coherence both across text and music modalities and within the music modality itself. Existing non-autoregressive (NAR) frameworks, while capable of producing high-quality songs, often struggle with the alignment between lyrics and vocal. Concurrently, catering to diverse musical preferences necessitates reinforcement learning from human feedback (RLHF). However, existing methods often rely on merging multiple models during multi-preference optimization, which results in significant performance degradation.\nTo address these challenges, we introduce DiffRhythm 2, an end-to-end framework designed for high-fidelity, controllable song generation. To tackle the lyric alignment problem, DiffRhythm 2 employs a semi-autoregressive architecture based on block flow matching. This design enables faithful alignment of lyrics to singing vocals without relying on external labels and constraints, all while preserving the high generation quality and efficiency of NAR models. To make this framework computationally tractable for long sequences, we implement a music variational autoencoder (VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity audio reconstruction.\nIn addition, to overcome the limitations of multi-preference optimization in RLHF, we propose cross-pair preference optimization. This method effectively mitigates the performance drop typically associated with model merging, allowing for more robust optimization across diverse human preferences. We further enhance musicality and structural coherence by introducing stochastic block representation alignment loss.\nExperimental results demonstrate that DiffRhythm 2 can generate complete songs up to 210 seconds in length, consistently outperforming existing open-source models in both subjective and objective evaluations, while maintaining efficient generation speed. To encourage reproducibility and further exploration, we release the inference code and model checkpoints.\nCode and weights are availble at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/xiaomi-research/diffrhythm2\" title=\"\">https://github.com/xiaomi-research/diffrhythm2</a>.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "music",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Music serves as an abstract expression of human emotion and culture. Songs represent a unique art form, combining music and language to convey emotions and stories through melody and lyrics. With the rapid development of deep learning, music generation has progressed beyond symbolic music generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib29\" title=\"\">2025</a>; Qu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib25\" title=\"\">2024</a>)</cite> and singing voice synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib38\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib4\" title=\"\">2020</a>; Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib24\" title=\"\">2025b</a>)</cite>, moving towards more challenging tasks such as instrumental music generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib5\" title=\"\">2023</a>; Schneider et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib27\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib37\" title=\"\">2025</a>)</cite> and song generation. Unlike singing voice synthesis, which produces vocals with predefined melodies, or instrumental music generation, which only models melody and instrumentation, song generation requires joint modeling of lyrics, structure, singing vocal and accompaniment. This is further complicated by the typical length of songs, which often exceeds three minutes, making lyric alignment and stable style modeling particularly challenging.</p>\n\n",
                "matched_terms": [
                    "music",
                    "produces"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike the aforementioned approaches, DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> takes a bold approach within a non-autoregressive (NAR) framework. By leveraging the generation ability of diffusion models and continuous representations, it achieves high-quality mixed-track generation of vocals and accompaniment at over 50 times faster than autoregressive methods. However, NAR models struggle with lyric alignment in long sequences. DiffRhythm addresses this issue by conditioning on sentence-level timestamps. However, this solution significantly reduces creativity and diversity, and also imposes higher requirements on the training data. ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> solved the alignment problem without timestamps by introducing representation alignment (REPA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib35\" title=\"\">2024</a>)</cite> loss with mHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boito et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib1\" title=\"\">2024</a>)</cite> as semantic constraints. However, the addition of these constraints significantly reduced musicality, creating a delicate trade-off between lyric alignment and generation quality.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we present DiffRhythm 2, a semi-autoregressive end-to-end framework for song generation based on block flow matching. This framework partitions latent representations into fixed-length blocks and generates each block non-autoregressively using flow matching, while maintaining autoregressive dependencies between blocks. This design enables faithful lyric alignment without requiring additional labels or constraints. The semi-autoregressive structure provides rich bidirectional context within blocks, ensuring long-sequence consistency, and supports fast inference for songs up to 210 seconds long. To accommodate block flow matching during training, we introduce stochastic block REPA loss that ensures efficient training and provides accurate representation guidance for the current block. We group similar preferences and perform pairwise optimization across groups to improve the performance of multi-preference alignment. Finally, as the block flow matching training strategy significantly increases the sequence length, we implement a high-compression music variational autoencoder (VAE), achieving high compression while still enabling high-quality audio reconstruction. To encourage reproducibility and further exploration, we will release the inference code and model checkpoints. The main contributions of this paper are summarized below:</p>\n\n",
                "matched_terms": [
                    "latent",
                    "representation",
                    "music",
                    "training",
                    "vae"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design a music VAE with low-frame-rate compression at 5 Hz that preserves reconstruction quality while reducing sequence length, which not only accelerates inference but also makes long-sequence modeling in DiffRhythm 2 both feasible and effective.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Song generation aims to produce coherent vocals and accompaniment given lyrics and style specifications, requiring joint modeling of melody, harmony, and lyric alignment. Early methods adopted sequential pipelines: Melodist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib12\" title=\"\">2024</a>)</cite> and MelodyLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib19\" title=\"\">2024</a>)</cite> first generated vocals from lyrics, then synthesized accompaniment. However, since vocals and accompaniment are inherently interdependent, this sequential approach often produces suboptimal results. To address these limitations, parallel generation methods emerged. SongCreator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib17\" title=\"\">2024</a>)</cite> and Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib36\" title=\"\">2025</a>)</cite> generate vocals and accompaniment independently in parallel, while SongGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib22\" title=\"\">2025</a>)</cite> introduces interleaved prediction across tracks to improve quality. Despite these advances, weak coupling between vocal and instrumental tracks often leads to harmonic inconsistencies. LeVo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> mitigates this by first generating a mixed track, then using it to guide separate vocal and accompaniment synthesis, achieving better harmony at the cost of reduced accompaniment complexity.\nRecent work has explored alternative architectures to improve both quality and efficiency. MusicCoT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib15\" title=\"\">2025</a>)</cite> and Songbloom&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib33\" title=\"\">2025</a>)</cite> leverage Chain-of-Thought reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib30\" title=\"\">2022</a>)</cite> to enhance vocal-accompaniment coordination and overall generation quality. However, these language-model-based approaches suffer from computational overhead and struggle with style consistency across long sequences due to their autoregressive nature.\nNon-autoregressive approaches offer promising alternatives. DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> and ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> employ diffusion models to achieve superior long-sequence consistency and vocal-accompaniment harmony while maintaining faster inference. However, both methods face challenges in lyric alignment for extended sequences. While each proposes specific solutions, these fixes come at the cost of reduced creativity or musicality. Specifically, DiffRhythm relies on timestamp conditioning while ACE-Step employs representation alignment, highlighting the need for more sophisticated alignment strategies.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "produces"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture of DiffRhythm 2, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S3.F1\" title=\"Figure 1 &#8227; 3.1 Overview &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, comprises of a Music VAE and a Diffusion Transformer enhanced with block flow matching. The Diffusion Transformer is conditioned on lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> together with either text style prompt <math alttext=\"S_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">S_{t}</annotation></semantics></math> or audio style prompt <math alttext=\"S_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">S_{a}</annotation></semantics></math>, and generates VAE latents block by block. These latents are then passed to the Music VAE decoder to reconstruct the waveform.\nFor training, in addition to the block flow matching loss, we introduce the stochastic block REPA loss combined with MuQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib39\" title=\"\">2025</a>)</cite> representations to enhance the model&#8217;s musicality and structural modeling. Moreover, to address the degraded average performance caused by multi-preference optimization, we introduce a cross-pair preference optimization strategy. The following subsections will provide detailed descriptions of the DiffRhythm 2 modules and training procedure.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "music",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of training DiffRhythm 2 on extremely long sequences, we design a customized Music VAE with a frame rate as low as 5 Hz. The VAE processes 24 kHz input audio as input and reconstructs it at 48 kHz, achieving compression ratios of 4800&#215; during encoding and 9600&#215; during decoding. The Music VAE consists of an encoder, a transformer block, and a decoder. The encoder adopts the same architecture as Stable Audio 2 VAE&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/Stability-AI/stable-audio-tools</span></span></span><cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib8\" title=\"\">2025</a>)</cite>. A transformer block is inserted before the decoder to alleviate reconstruction pressure. To maximize reconstruction quality, we employ BigVGAN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib16\" title=\"\">2023</a>)</cite> as the decoder. For the training loss, we combine the multi-scale mel loss from BigVGAN with the multi-scale STFT loss from Stable Audio 2 VAE to jointly optimize vocals and accompaniment. In addition, we employ a set of discriminators, including the multi-period discriminator, multi-scale discriminator, and CQT discriminator from BigVGAN.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "music",
                    "encoder",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike MIMI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib6\" title=\"\">2024</a>)</cite>, we deliberately avoid adding a similar transformer block after the encoder for two reasons: (1) The limited future information introduced by convolution helps enhance content continuity between consecutive blocks. (2) The global bidirectional receptive field of transformer introduces excessive future information, substantially increasing the burden on the generative model. We explore the reconstruction performance of our VAE in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A1\" title=\"Appendix A Results of the Music Reconstruction &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "encoder",
                    "global"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first establish the mathematical foundation of flow matching before extending it to the block-wise setting. Given the target latent <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math>, flow matching defines a linear probability path that transports Gaussian noise <math alttext=\"Z_{0}\\sim\\mathcal{N}(0,I)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mn>0</mn></msub><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">Z_{0}\\sim\\mathcal{N}(0,I)</annotation></semantics></math> to the target data distribution <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math>. During training, the timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is sampled from <math alttext=\"\\mathcal{U}[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{U}[0,1]</annotation></semantics></math>, and the noisy latent <math alttext=\"Z_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">Z_{t}</annotation></semantics></math> is obtained by linear interpolation:</p>\n\n",
                "matched_terms": [
                    "latent",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In block flow matching, the core idea is that each block is generated with flow matching, while the dependency across blocks is handled autoregressively. Let the block size be <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, and split the target latent <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> of length <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> into <math alttext=\"Z=\\{z^{1},z^{2},\\cdots z^{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msup><mi>z</mi><mn>1</mn></msup><mo>,</mo><msup><mi>z</mi><mn>2</mn></msup><mo>,</mo><mrow><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>z</mi><mi>k</mi></msup></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=\\{z^{1},z^{2},\\cdots z^{k}\\}</annotation></semantics></math>, where <math alttext=\"k=\\lceil l/b\\rceil\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mrow><mo stretchy=\"false\">&#8968;</mo><mrow><mi>l</mi><mo>/</mo><mi>b</mi></mrow><mo stretchy=\"false\">&#8969;</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k=\\lceil l/b\\rceil</annotation></semantics></math> denotes the total number of blocks. Since the generation of each block depends on all previously generated blocks, the velocity field for the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m6\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block is estimated as</p>\n\n",
                "matched_terms": [
                    "size",
                    "latent",
                    "total"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the definition, block flow matching requires access to clean block. While this is naturally available during inference, the training input sequence typically consists only of <math alttext=\"(S,L,Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>L</mi><mo>,</mo><msub><mi>Z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,L,Z_{t})</annotation></semantics></math> without the clean blocks. To address this, we concatenate the clean sequence to the noisy sequence, forming <math alttext=\"(S,L,Z,Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo>,</mo><msub><mi>Z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,L,Z,Z_{t})</annotation></semantics></math>, and apply an attention mask to enforce autoregressive dependencies.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S3.F2\" title=\"Figure 2 &#8227; 3.3.2 Details of Application &#8227; 3.3 Block Flow Matching &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our attention mask design: the style prompt <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> and lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> can be attended by any block; for the clean sequence, the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block can only attend to blocks <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> through <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>; for the noisy sequence, the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block can only attend to clean blocks <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m9\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> through <math alttext=\"i-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">i-1</annotation></semantics></math> and its own noisy block.\nThis design enforces autoregressive training but results in very long input sequences. To alleviate this, we employ a 5 Hz music VAE to substantially compress the training sequence length.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "music",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike prior works that predetermine the total sequence length before generation, DiffRhythm 2 incorporates the End-of-Prediction (EOP) frame as part of the prediction target to enable variable length generation. Since generation proceeds block by block, we pad <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> EOP frames at the end of the sequence rather than a single frame. Specifically, <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is determined by the length of the final block <math alttext=\"l^{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m3\" intent=\":literal\"><semantics><msup><mi>l</mi><mi>k</mi></msup><annotation encoding=\"application/x-tex\">l^{k}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "total",
                    "frames"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To design the EOP frame, we experimented with several distributions. When the mean of the EOP distribution deviates significantly from that of the latent, such as in <math alttext=\"\\mathcal{N}(1,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m8\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(1,1)</annotation></semantics></math>, the model tends to generate strong noise near the end of the sequence. When the two distributions are numerically close, such as in <math alttext=\"\\mathcal{N}(0,2)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m9\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,2)</annotation></semantics></math>, prediction becomes unreliable since the model struggles to recognize the stopping position. Moreover, because no KL constraint is applied to the latent, enforcing a specific probabilistic form for EOP further increases the modeling difficulty. Overall, we find that <math alttext=\"\\mathcal{N}(1,0)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m10\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(1,0)</annotation></semantics></math>, i.e., a constant vector of ones, provides the most effective EOP representation, as it ensures clear numerical separation from latent features while remaining easy for the model to learn.</p>\n\n",
                "matched_terms": [
                    "latent",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, due to the block-wise training scheme, the hidden states <math alttext=\"h^{i-1}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>t</mi><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></msubsup><annotation encoding=\"application/x-tex\">h^{i-1}_{t}</annotation></semantics></math> and <math alttext=\"h^{i}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">h^{i}_{t}</annotation></semantics></math> corresponding to <math alttext=\"z^{i-1}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mi>t</mi><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></msubsup><annotation encoding=\"application/x-tex\">z^{i-1}_{t}</annotation></semantics></math> and <math alttext=\"z^{i}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">z^{i}_{t}</annotation></semantics></math> are not continuous, making it infeasible to directly compute REPA loss from the hidden state <math alttext=\"H_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m5\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">H_{t}</annotation></semantics></math> of <math alttext=\"Z_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m6\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">Z_{t}</annotation></semantics></math>. Considering that training is performed with teacher forcing, the hidden states <math alttext=\"h^{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m7\" intent=\":literal\"><semantics><msup><mi>h</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">h^{&lt;i}</annotation></semantics></math> and <math alttext=\"h^{i}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">h^{i}_{t}</annotation></semantics></math> corresponding to <math alttext=\"z^{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m9\" intent=\":literal\"><semantics><msup><mi>z</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">z^{&lt;i}</annotation></semantics></math> and <math alttext=\"z^{i}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m10\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">z^{i}_{t}</annotation></semantics></math> remain continuous. Thus, computing REPA loss on their combination is more reasonable. Note that for any <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m11\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the target is always the ground-truth SSL representation. Therefore, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m12\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is only used to distinguish between noisy and clean sequences.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DiffRhythm 2 is trained on a large-scale dataset comprising approximately 1.4 million songs, with a total duration of about 70,000 hours. The dataset spans three categories: Chinese, English and instrumental music, with a distribution ratio of roughly 4:5:1. To ensure data quality, we implement an efficient preprocessing pipeline. First, Audiobox-Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib28\" title=\"\">2025</a>)</cite> is employed to filter audio based on quality. Then, Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib2\" title=\"\">2012</a>)</cite> and FireRedASR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib32\" title=\"\">2025b</a>)</cite> are then used to transcribe the vocal tracks, and the two transcriptions are cross-validated against the original lyrics to ensure accuracy. Finally, All-in-One&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim &amp; Nam, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib14\" title=\"\">2023</a>)</cite> and Qwen2.5-omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib31\" title=\"\">2025a</a>)</cite> are used to annotate four key dimensions of the songs: structure, style, instrumentation, and emotion.</p>\n\n",
                "matched_terms": [
                    "total",
                    "music",
                    "dataset",
                    "duration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Objective Results &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, DiffRhythm 2 significantly outperforms other open-source models across music quality metrics. However, in aspects such as musicality, it still shows a clear gap compared to commercial systems like SUNO V4.5. In terms of audio quality, it performs slightly worse than LeVo but remains superior to most other models. This result demonstrates that employing a lower frame rate is feasible for compressed song reconstruction, although the increased reconstruction burden makes it difficult to achieve perfect fidelity. DiffRhythm 2 achieves the highest scores on Mulan-T (0.40) and PER (0.13), substantially exceeding other models. However, its Mulan-A score is lower than that of LeVo and similar systems, which we attribute to our choice of modeling audio style prompts with global representations. A global style embedding cannot adequately capture the stylistic content present in songs.</p>\n\n",
                "matched_terms": [
                    "global",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present DiffRhythm 2, a semi-autoregressive end-to-end music generation framework based on block flow matching. By leveraging this approach, we achieve high-quality lyric alignment without relying on semantic constraints, while generating mixed-track songs with high fidelity. We further introduce stochastic block REPA loss for semi-autoregressive training, which enhances musicality and structural modeling. In addition, we propose cross-pair preference optimization, which effectively addresses the challenge of degraded average performance when optimizing across multiple preferences. Our experiments demonstrate the superior song generation capabilities of DiffRhythm 2.</p>\n\n",
                "matched_terms": [
                    "music",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the compression and reconstruction performance of Stable Audio 2 VAE, the DCAE from ACE-Step, the dual-track MuCodec from LeVo and our implemented Music VAE on a testset of 50 songs covering diverse genres. It should also be noted that Music VAE supports only mono audio, whereas all the other models support stereo audio. As shown in the table, our Music VAE achieves comparable or even higher PESQ and STOI scores despite operating at a substantially lower frame rate than the other models. However, the WER of Music VAE is slightly higher than other models, indicating that the extreme low frame rate still results in some loss of fine-grained reconstruction details. The single codebook design of MuCodec significantly compromises the quality of the accompaniment and leads to inferior overall reconstruction performance.</p>\n\n",
                "matched_terms": [
                    "vae",
                    "music",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the generation speed of LeVo, Yue, DiffRhythm+, and DiffRhythm 2. To ensure fairness, all models are required to generate a fixed sequence of two minutes in length, regardless of generation quality. For both DiffRhythm+ and DiffRhythm 2, the number of sampling steps is set to 32. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A2.T5\" title=\"Table 5 &#8227; Appendix B Generation Speed &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, DiffRhythm 2 is only slightly slower than DiffRhythm+ and ACE-Step, while being significantly faster than LeVo. It is worth noting that DiffRhythm+ does not incorporate any attention acceleration framework, which explains why it is slower than ACE-Step despite having a smaller model size. ACE-Step leverages linear attention to reduce computational complexity, whereas LeVo adopts Flash Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/Dao-AILab/flash-attention\" title=\"\">https://github.com/Dao-AILab/flash-attention</a></span></span></span>, and DiffRhythm 2 employs Flex Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_href\" href=\"https://pytorch.ac.cn/blog/flexattention/\" title=\"\">https://pytorch.ac.cn/blog/flexattention/</a></span></span></span> for acceleration.</p>\n\n",
                "matched_terms": [
                    "size",
                    "steps"
                ]
            }
        ]
    },
    "A3.T7": {
        "caption": "Table 7: DiffRhythm 2 training details.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Parameter</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Specification</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Model Size</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Approx. 1B</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Batch Size</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">64 (2 per GPU).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Block Size</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">10 (2 seconds per block).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Dataset</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">70,000-hour music dataset for pretrain; high-quality subset of 20,000 hours for fintune; 40,000 pairs for DPO.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Total Training Time</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">200 hours.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Optimizer Type</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">AdamW with 1e-2 weight decay and (0.8, 0.9) betas.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Gradient Clipping</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Max norm of 0.5.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Learning Rate</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">1e-4, with linear warm-up over the first 10,000 steps; 1e-5 for finetuning.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Timestep Sampling</th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Uniform scheme: <math alttext=\"t\\sim\\mathcal{U}[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T7.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">t\\sim\\mathcal{U}[0,1]</annotation></semantics></math>.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pairs",
            "type",
            "seconds",
            "70000hour",
            "optimizer",
            "1e2",
            "size",
            "details",
            "parameter",
            "fintune",
            "t∼𝒰​01tsimmathcalu01",
            "time",
            "hours",
            "approx",
            "rate",
            "sampling",
            "clipping",
            "timestep",
            "max",
            "batch",
            "decay",
            "learning",
            "1e5",
            "scheme",
            "training",
            "highquality",
            "gpu",
            "1e4",
            "first",
            "over",
            "finetuning",
            "gradient",
            "uniform",
            "diffrhythm",
            "pretrain",
            "block",
            "adamw",
            "warmup",
            "weight",
            "norm",
            "total",
            "model",
            "steps",
            "music",
            "linear",
            "subset",
            "dpo",
            "dataset",
            "specification",
            "betas"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Generating full-length, high-quality songs is challenging, as it requires maintaining long-term coherence both across text and music modalities and within the music modality itself. Existing non-autoregressive (NAR) frameworks, while capable of producing high-quality songs, often struggle with the alignment between lyrics and vocal. Concurrently, catering to diverse musical preferences necessitates reinforcement learning from human feedback (RLHF). However, existing methods often rely on merging multiple models during multi-preference optimization, which results in significant performance degradation.\nTo address these challenges, we introduce DiffRhythm 2, an end-to-end framework designed for high-fidelity, controllable song generation. To tackle the lyric alignment problem, DiffRhythm 2 employs a semi-autoregressive architecture based on block flow matching. This design enables faithful alignment of lyrics to singing vocals without relying on external labels and constraints, all while preserving the high generation quality and efficiency of NAR models. To make this framework computationally tractable for long sequences, we implement a music variational autoencoder (VAE) that achieves a low frame rate of 5 Hz while still enabling high-fidelity audio reconstruction.\nIn addition, to overcome the limitations of multi-preference optimization in RLHF, we propose cross-pair preference optimization. This method effectively mitigates the performance drop typically associated with model merging, allowing for more robust optimization across diverse human preferences. We further enhance musicality and structural coherence by introducing stochastic block representation alignment loss.\nExperimental results demonstrate that DiffRhythm 2 can generate complete songs up to 210 seconds in length, consistently outperforming existing open-source models in both subjective and objective evaluations, while maintaining efficient generation speed. To encourage reproducibility and further exploration, we release the inference code and model checkpoints.\nCode and weights are availble at: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/xiaomi-research/diffrhythm2\" title=\"\">https://github.com/xiaomi-research/diffrhythm2</a>.</p>\n\n",
                "matched_terms": [
                    "seconds",
                    "rate",
                    "model",
                    "diffrhythm",
                    "learning",
                    "block",
                    "music",
                    "highquality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Music serves as an abstract expression of human emotion and culture. Songs represent a unique art form, combining music and language to convey emotions and stories through melody and lyrics. With the rapid development of deep learning, music generation has progressed beyond symbolic music generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib29\" title=\"\">2025</a>; Qu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib25\" title=\"\">2024</a>)</cite> and singing voice synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib38\" title=\"\">2022</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib4\" title=\"\">2020</a>; Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib24\" title=\"\">2025b</a>)</cite>, moving towards more challenging tasks such as instrumental music generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Copet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib5\" title=\"\">2023</a>; Schneider et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib27\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib37\" title=\"\">2025</a>)</cite> and song generation. Unlike singing voice synthesis, which produces vocals with predefined melodies, or instrumental music generation, which only models melody and instrumentation, song generation requires joint modeling of lyrics, structure, singing vocal and accompaniment. This is further complicated by the typical length of songs, which often exceeds three minutes, making lyric alignment and stable style modeling particularly challenging.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early approaches, such as Jukebox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dhariwal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib7\" title=\"\">2020</a>)</cite> and SongCreator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib17\" title=\"\">2024</a>)</cite>, generate songs by predicting the acoustic tokens of vocals and accompaniment mixed into a single track. However, the limited information density of these tokens hampers high-quality song generation. Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib36\" title=\"\">2025</a>)</cite> improves generation quality by modeling the vocals and accompaniment on separate tracks, while SongGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib22\" title=\"\">2025</a>)</cite> further enhances quality by introducing interleaved dual-track prediction. Despite these improvements, Yue and SongGen predict the tracks independently, which often leads to inconsistencies between the vocals and accompaniment. LeVo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> addresses this issue by first generating a mixed track and using it to guide the separate prediction of vocals and accompaniment, thereby reducing mismatches. However, the overall quality is still limited by the information compression in the mixed track tokenization, and it is difficult to generate complex and rich accompaniment. Most of these methods are based on autoregressive frameworks, which result in slow generation speeds. This hinders real-time applications and user interactivity.</p>\n\n",
                "matched_terms": [
                    "first",
                    "highquality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike the aforementioned approaches, DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> takes a bold approach within a non-autoregressive (NAR) framework. By leveraging the generation ability of diffusion models and continuous representations, it achieves high-quality mixed-track generation of vocals and accompaniment at over 50 times faster than autoregressive methods. However, NAR models struggle with lyric alignment in long sequences. DiffRhythm addresses this issue by conditioning on sentence-level timestamps. However, this solution significantly reduces creativity and diversity, and also imposes higher requirements on the training data. ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> solved the alignment problem without timestamps by introducing representation alignment (REPA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib35\" title=\"\">2024</a>)</cite> loss with mHuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Boito et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib1\" title=\"\">2024</a>)</cite> as semantic constraints. However, the addition of these constraints significantly reduced musicality, creating a delicate trade-off between lyric alignment and generation quality.</p>\n\n",
                "matched_terms": [
                    "over",
                    "diffrhythm",
                    "training",
                    "highquality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition, pretrained models often fail to align with human preferences due to the gap between training objectives and desired song quality. Therefore, a typical method is to introduce reinforcement learning from human feedback (RLHF) in the post-training process to refine generation preferences across various musical dimensions. DiffRhythm+&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib3\" title=\"\">2025</a>)</cite> demonstrated the feasibility of improving overall musicality with Direct Preference Optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rafailov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib26\" title=\"\">2023</a>)</cite>. LeVo extended this approach by applying DPO to different dimensions separately and interpolating model weights to enhance overall performance. While this strategy improved balance across dimensions, it inevitably limited the upper bound of each individual capability due to the averaging effect.</p>\n\n",
                "matched_terms": [
                    "model",
                    "diffrhythm",
                    "learning",
                    "training",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges, we present DiffRhythm 2, a semi-autoregressive end-to-end framework for song generation based on block flow matching. This framework partitions latent representations into fixed-length blocks and generates each block non-autoregressively using flow matching, while maintaining autoregressive dependencies between blocks. This design enables faithful lyric alignment without requiring additional labels or constraints. The semi-autoregressive structure provides rich bidirectional context within blocks, ensuring long-sequence consistency, and supports fast inference for songs up to 210 seconds long. To accommodate block flow matching during training, we introduce stochastic block REPA loss that ensures efficient training and provides accurate representation guidance for the current block. We group similar preferences and perform pairwise optimization across groups to improve the performance of multi-preference alignment. Finally, as the block flow matching training strategy significantly increases the sequence length, we implement a high-compression music variational autoencoder (VAE), achieving high compression while still enabling high-quality audio reconstruction. To encourage reproducibility and further exploration, we will release the inference code and model checkpoints. The main contributions of this paper are summarized below:</p>\n\n",
                "matched_terms": [
                    "seconds",
                    "model",
                    "diffrhythm",
                    "block",
                    "music",
                    "training",
                    "highquality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose DiffRhythm 2, a novel semi-autoregressive song generation framework based on block flow matching, which is capable of producing high-quality songs with faithful lyric alignment and excellent accompaniment performance.</p>\n\n",
                "matched_terms": [
                    "block",
                    "diffrhythm",
                    "highquality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design a music VAE with low-frame-rate compression at 5 Hz that preserves reconstruction quality while reducing sequence length, which not only accelerates inference but also makes long-sequence modeling in DiffRhythm 2 both feasible and effective.</p>\n\n",
                "matched_terms": [
                    "music",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Song generation aims to produce coherent vocals and accompaniment given lyrics and style specifications, requiring joint modeling of melody, harmony, and lyric alignment. Early methods adopted sequential pipelines: Melodist&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib12\" title=\"\">2024</a>)</cite> and MelodyLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib19\" title=\"\">2024</a>)</cite> first generated vocals from lyrics, then synthesized accompaniment. However, since vocals and accompaniment are inherently interdependent, this sequential approach often produces suboptimal results. To address these limitations, parallel generation methods emerged. SongCreator&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib17\" title=\"\">2024</a>)</cite> and Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib36\" title=\"\">2025</a>)</cite> generate vocals and accompaniment independently in parallel, while SongGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib22\" title=\"\">2025</a>)</cite> introduces interleaved prediction across tracks to improve quality. Despite these advances, weak coupling between vocal and instrumental tracks often leads to harmonic inconsistencies. LeVo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> mitigates this by first generating a mixed track, then using it to guide separate vocal and accompaniment synthesis, achieving better harmony at the cost of reduced accompaniment complexity.\nRecent work has explored alternative architectures to improve both quality and efficiency. MusicCoT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib15\" title=\"\">2025</a>)</cite> and Songbloom&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib33\" title=\"\">2025</a>)</cite> leverage Chain-of-Thought reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib30\" title=\"\">2022</a>)</cite> to enhance vocal-accompaniment coordination and overall generation quality. However, these language-model-based approaches suffer from computational overhead and struggle with style consistency across long sequences due to their autoregressive nature.\nNon-autoregressive approaches offer promising alternatives. DiffRhythm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ning et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib23\" title=\"\">2025a</a>)</cite> and ACE-Step&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib9\" title=\"\">2025</a>)</cite> employ diffusion models to achieve superior long-sequence consistency and vocal-accompaniment harmony while maintaining faster inference. However, both methods face challenges in lyric alignment for extended sequences. While each proposes specific solutions, these fixes come at the cost of reduced creativity or musicality. Specifically, DiffRhythm relies on timestamp conditioning while ACE-Step employs representation alignment, highlighting the need for more sophisticated alignment strategies.</p>\n\n",
                "matched_terms": [
                    "first",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture of DiffRhythm 2, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S3.F1\" title=\"Figure 1 &#8227; 3.1 Overview &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, comprises of a Music VAE and a Diffusion Transformer enhanced with block flow matching. The Diffusion Transformer is conditioned on lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> together with either text style prompt <math alttext=\"S_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">S_{t}</annotation></semantics></math> or audio style prompt <math alttext=\"S_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">S_{a}</annotation></semantics></math>, and generates VAE latents block by block. These latents are then passed to the Music VAE decoder to reconstruct the waveform.\nFor training, in addition to the block flow matching loss, we introduce the stochastic block REPA loss combined with MuQ&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib39\" title=\"\">2025</a>)</cite> representations to enhance the model&#8217;s musicality and structural modeling. Moreover, to address the degraded average performance caused by multi-preference optimization, we introduce a cross-pair preference optimization strategy. The following subsections will provide detailed descriptions of the DiffRhythm 2 modules and training procedure.</p>\n\n",
                "matched_terms": [
                    "block",
                    "music",
                    "training",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of training DiffRhythm 2 on extremely long sequences, we design a customized Music VAE with a frame rate as low as 5 Hz. The VAE processes 24 kHz input audio as input and reconstructs it at 48 kHz, achieving compression ratios of 4800&#215; during encoding and 9600&#215; during decoding. The Music VAE consists of an encoder, a transformer block, and a decoder. The encoder adopts the same architecture as Stable Audio 2 VAE&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/Stability-AI/stable-audio-tools</span></span></span><cite class=\"ltx_cite ltx_citemacro_citep\">(Evans et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib8\" title=\"\">2025</a>)</cite>. A transformer block is inserted before the decoder to alleviate reconstruction pressure. To maximize reconstruction quality, we employ BigVGAN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib16\" title=\"\">2023</a>)</cite> as the decoder. For the training loss, we combine the multi-scale mel loss from BigVGAN with the multi-scale STFT loss from Stable Audio 2 VAE to jointly optimize vocals and accompaniment. In addition, we employ a set of discriminators, including the multi-period discriminator, multi-scale discriminator, and CQT discriminator from BigVGAN.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "diffrhythm",
                    "block",
                    "music",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike MIMI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib6\" title=\"\">2024</a>)</cite>, we deliberately avoid adding a similar transformer block after the encoder for two reasons: (1) The limited future information introduced by convolution helps enhance content continuity between consecutive blocks. (2) The global bidirectional receptive field of transformer introduces excessive future information, substantially increasing the burden on the generative model. We explore the reconstruction performance of our VAE in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A1\" title=\"Appendix A Results of the Music Reconstruction &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "block",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first establish the mathematical foundation of flow matching before extending it to the block-wise setting. Given the target latent <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m1\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math>, flow matching defines a linear probability path that transports Gaussian noise <math alttext=\"Z_{0}\\sim\\mathcal{N}(0,I)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>Z</mi><mn>0</mn></msub><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">Z_{0}\\sim\\mathcal{N}(0,I)</annotation></semantics></math> to the target data distribution <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m3\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math>. During training, the timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is sampled from <math alttext=\"\\mathcal{U}[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{U}[0,1]</annotation></semantics></math>, and the noisy latent <math alttext=\"Z_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">Z_{t}</annotation></semantics></math> is obtained by linear interpolation:</p>\n\n",
                "matched_terms": [
                    "first",
                    "timestep",
                    "training",
                    "linear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m7\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math> conditions on <math alttext=\"Z_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">Z_{t}</annotation></semantics></math>, style prompt <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m9\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>, lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m10\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math>, and timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m11\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> to estimate the velocity field <math alttext=\"\\hat{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m12\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>v</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{v}</annotation></semantics></math>. Since the path is linear, the ground truth velocity field <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m13\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math> is <math alttext=\"Z-Z_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p1.m14\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>&#8722;</mo><msub><mi>Z</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">Z-Z_{0}</annotation></semantics></math>. The flow matching loss is therefore defined as</p>\n\n",
                "matched_terms": [
                    "model",
                    "linear",
                    "timestep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In block flow matching, the core idea is that each block is generated with flow matching, while the dependency across blocks is handled autoregressively. Let the block size be <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m1\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, and split the target latent <math alttext=\"Z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m2\" intent=\":literal\"><semantics><mi>Z</mi><annotation encoding=\"application/x-tex\">Z</annotation></semantics></math> of length <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m3\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> into <math alttext=\"Z=\\{z^{1},z^{2},\\cdots z^{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msup><mi>z</mi><mn>1</mn></msup><mo>,</mo><msup><mi>z</mi><mn>2</mn></msup><mo>,</mo><mrow><mi mathvariant=\"normal\">&#8943;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>z</mi><mi>k</mi></msup></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=\\{z^{1},z^{2},\\cdots z^{k}\\}</annotation></semantics></math>, where <math alttext=\"k=\\lceil l/b\\rceil\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mrow><mo stretchy=\"false\">&#8968;</mo><mrow><mi>l</mi><mo>/</mo><mi>b</mi></mrow><mo stretchy=\"false\">&#8969;</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k=\\lceil l/b\\rceil</annotation></semantics></math> denotes the total number of blocks. Since the generation of each block depends on all previously generated blocks, the velocity field for the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m6\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block is estimated as</p>\n\n",
                "matched_terms": [
                    "size",
                    "total",
                    "block"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"z^{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m7\" intent=\":literal\"><semantics><msup><mi>z</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">z^{&lt;i}</annotation></semantics></math> represents all preceding blocks. <math alttext=\"z^{i}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">z^{i}_{t}</annotation></semantics></math>is the noisy latent for the current block obtained via linear interpolation, and <math alttext=\"t^{i}\\sim\\mathcal{U}[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m9\" intent=\":literal\"><semantics><mrow><msup><mi>t</mi><mi>i</mi></msup><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">t^{i}\\sim\\mathcal{U}[0,1]</annotation></semantics></math> is sampled independently for each block. The corresponding ground truth velocity for the i-th block is <math alttext=\"v^{i}=z^{i}-z^{i}_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m10\" intent=\":literal\"><semantics><mrow><msup><mi>v</mi><mi>i</mi></msup><mo>=</mo><mrow><msup><mi>z</mi><mi>i</mi></msup><mo>&#8722;</mo><msubsup><mi>z</mi><mn>0</mn><mi>i</mi></msubsup></mrow></mrow><annotation encoding=\"application/x-tex\">v^{i}=z^{i}-z^{i}_{0}</annotation></semantics></math>, where <math alttext=\"z^{i}_{0}\\sim\\mathcal{N}(0,I)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS1.p2.m11\" intent=\":literal\"><semantics><mrow><msubsup><mi>z</mi><mn>0</mn><mi>i</mi></msubsup><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">z^{i}_{0}\\sim\\mathcal{N}(0,I)</annotation></semantics></math> is the standard Gaussian noise for this block. The overall block flow matching loss is then defined as</p>\n\n",
                "matched_terms": [
                    "block",
                    "linear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the definition, block flow matching requires access to clean block. While this is naturally available during inference, the training input sequence typically consists only of <math alttext=\"(S,L,Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>L</mi><mo>,</mo><msub><mi>Z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,L,Z_{t})</annotation></semantics></math> without the clean blocks. To address this, we concatenate the clean sequence to the noisy sequence, forming <math alttext=\"(S,L,Z,Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo>,</mo><mi>L</mi><mo>,</mo><mi>Z</mi><mo>,</mo><msub><mi>Z</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(S,L,Z,Z_{t})</annotation></semantics></math>, and apply an attention mask to enforce autoregressive dependencies.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S3.F2\" title=\"Figure 2 &#8227; 3.3.2 Details of Application &#8227; 3.3 Block Flow Matching &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates our attention mask design: the style prompt <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m3\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> and lyrics <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m4\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> can be attended by any block; for the clean sequence, the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block can only attend to blocks <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m6\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> through <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m7\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>; for the noisy sequence, the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m8\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th block can only attend to clean blocks <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m9\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> through <math alttext=\"i-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p1.m10\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">i-1</annotation></semantics></math> and its own noisy block.\nThis design enforces autoregressive training but results in very long input sequences. To alleviate this, we employ a 5 Hz music VAE to substantially compress the training sequence length.</p>\n\n",
                "matched_terms": [
                    "block",
                    "music",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The introduction of clean sequences requires a mechanism to distinguish them from noisy sequences to facilitate model training. In conventional autoregressive frameworks, this is typically achieved by inserting special tokens as delimiters. However, such an approach is not applicable here. Since the receptive field is controlled by the attention mask, inserting delimiters for each block is impractical and would also affect positional encoding.\nTo address this in a simple yet effective way, we leverage the timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> to differentiate between sequences. Specifically, we set the style prompt and lyrics to a fixed timestep of <math alttext=\"-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m2\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math>, the clean sequence to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, and assign the noisy sequence a timestep sampled from <math alttext=\"\\mathcal{U}[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS2.p2.m4\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{U}[0,1]</annotation></semantics></math>. To further enhance training stability, different blocks within the same sequence are assigned independently sampled timesteps.</p>\n\n",
                "matched_terms": [
                    "block",
                    "model",
                    "timestep",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike prior works that predetermine the total sequence length before generation, DiffRhythm 2 incorporates the End-of-Prediction (EOP) frame as part of the prediction target to enable variable length generation. Since generation proceeds block by block, we pad <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> EOP frames at the end of the sequence rather than a single frame. Specifically, <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> is determined by the length of the final block <math alttext=\"l^{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m3\" intent=\":literal\"><semantics><msup><mi>l</mi><mi>k</mi></msup><annotation encoding=\"application/x-tex\">l^{k}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "block",
                    "total",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If <math alttext=\"l^{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m4\" intent=\":literal\"><semantics><msup><mi>l</mi><mi>k</mi></msup><annotation encoding=\"application/x-tex\">l^{k}</annotation></semantics></math> is shorter than the block size <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m5\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, EOPs are padded to complete the block; if <math alttext=\"l^{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m6\" intent=\":literal\"><semantics><msup><mi>l</mi><mi>k</mi></msup><annotation encoding=\"application/x-tex\">l^{k}</annotation></semantics></math> equals <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS3.p1.m7\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math>, an additional full block of EOPs is padded. Formally,</p>\n\n",
                "matched_terms": [
                    "block",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training and inference procedures are described in Algorithms&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#alg1\" title=\"Algorithm 1 &#8227; 3.3.4 Training and Inference Procedures &#8227; 3.3 Block Flow Matching &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#alg2\" title=\"Algorithm 2 &#8227; 3.3.4 Training and Inference Procedures &#8227; 3.3 Block Flow Matching &#8227; 3 Method &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. It is worth noting that the incorporation of the autoregressive mechanism allows us to enable KV cache at the block level during inference, which substantially accelerates the generation speed.</p>\n\n",
                "matched_terms": [
                    "block",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As REPA loss is no longer required for lyric alignment constraints, we repurpose it to improve the model&#8217;s musicality and structural modeling. For the target SSL representations, extracting them with the same block size as DiffRhythm 2 hinders the capture of larger-scale musical structures. Moreover, SSL representations are typically downsampled by multi-layer convolution, which often introduces several frame shifts. The misalignment between <math alttext=\"z^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msup><mi>z</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">z^{i}</annotation></semantics></math> and SSL features caused by these shifts makes it unreliable to calculate REPA loss on target block directly. A better solution is to compute the loss on the entire sequence, which leverages the contextual receptive field to mitigate misalignment and enables the model to learn musical structures more effectively.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model",
                    "diffrhythm",
                    "block"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, due to the block-wise training scheme, the hidden states <math alttext=\"h^{i-1}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>t</mi><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></msubsup><annotation encoding=\"application/x-tex\">h^{i-1}_{t}</annotation></semantics></math> and <math alttext=\"h^{i}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m2\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">h^{i}_{t}</annotation></semantics></math> corresponding to <math alttext=\"z^{i-1}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mi>t</mi><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></msubsup><annotation encoding=\"application/x-tex\">z^{i-1}_{t}</annotation></semantics></math> and <math alttext=\"z^{i}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m4\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">z^{i}_{t}</annotation></semantics></math> are not continuous, making it infeasible to directly compute REPA loss from the hidden state <math alttext=\"H_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m5\" intent=\":literal\"><semantics><msub><mi>H</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">H_{t}</annotation></semantics></math> of <math alttext=\"Z_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m6\" intent=\":literal\"><semantics><msub><mi>Z</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">Z_{t}</annotation></semantics></math>. Considering that training is performed with teacher forcing, the hidden states <math alttext=\"h^{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m7\" intent=\":literal\"><semantics><msup><mi>h</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">h^{&lt;i}</annotation></semantics></math> and <math alttext=\"h^{i}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m8\" intent=\":literal\"><semantics><msubsup><mi>h</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">h^{i}_{t}</annotation></semantics></math> corresponding to <math alttext=\"z^{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m9\" intent=\":literal\"><semantics><msup><mi>z</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msup><annotation encoding=\"application/x-tex\">z^{&lt;i}</annotation></semantics></math> and <math alttext=\"z^{i}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m10\" intent=\":literal\"><semantics><msubsup><mi>z</mi><mi>t</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">z^{i}_{t}</annotation></semantics></math> remain continuous. Thus, computing REPA loss on their combination is more reasonable. Note that for any <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m11\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the target is always the ground-truth SSL representation. Therefore, <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m12\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> is only used to distinguish between noisy and clean sequences.</p>\n\n",
                "matched_terms": [
                    "scheme",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In practice, a noisy sequence typically contains dozens of blocks, so computing REPA loss for all of them is computationally expensive. Therefore, we randomly sample 10 blocks per noisy sequence for loss computation. For clean sequences, some blocks may be used multiple times for loss calculation. In such cases, we assign weights such that the total weight of each block sums to 1.</p>\n\n",
                "matched_terms": [
                    "block",
                    "total",
                    "weight"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib3\" title=\"\">2025</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib18\" title=\"\">2025</a>)</cite> have demonstrated the effectiveness and necessity of applying DPO for post-training in song generation tasks. As song generation involves multiple preference dimensions, multi-preference alignment optimization has become increasingly important. Existing approaches typically rely on merging separately optimized models. However, as the number of preferences increases, model merging often significantly reduces the average performance across preferences, severely limiting the overall efficiency of DPO.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In DiffRhythm 2, we pair (musicality, lyric alignment accuracy) and (style similarity, audio quality). During DPO training, the winning sample must satisfy both preferences within a pair, whereas the losing sample is required to satisfy at least one. To ensure balanced optimization, the three possible losing cases are preserved in equal proportions.</p>\n\n",
                "matched_terms": [
                    "dpo",
                    "training",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DiffRhythm 2 is trained on a large-scale dataset comprising approximately 1.4 million songs, with a total duration of about 70,000 hours. The dataset spans three categories: Chinese, English and instrumental music, with a distribution ratio of roughly 4:5:1. To ensure data quality, we implement an efficient preprocessing pipeline. First, Audiobox-Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib28\" title=\"\">2025</a>)</cite> is employed to filter audio based on quality. Then, Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib2\" title=\"\">2012</a>)</cite> and FireRedASR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib32\" title=\"\">2025b</a>)</cite> are then used to transcribe the vocal tracks, and the two transcriptions are cross-validated against the original lyrics to ensure accuracy. Finally, All-in-One&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim &amp; Nam, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib14\" title=\"\">2023</a>)</cite> and Qwen2.5-omni&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib31\" title=\"\">2025a</a>)</cite> are used to annotate four key dimensions of the songs: structure, style, instrumentation, and emotion.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "total",
                    "diffrhythm",
                    "music",
                    "dataset",
                    "first"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the objective evaluation, four aspects are considered: music quality, audio quality, style similarity, and lyric accuracy. To evaluate lyric accuracy, we transcribe the generated songs using Qwen3 ASR&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_href\" href=\"https://qwen.ai/blog?id=e199227023e8ebaac5f348f97fa804d1858ffc8a&amp;from=research.research-list\" title=\"\">https://qwen.ai/blog?id=e199227023e8ebaac5f348f97fa804d1858ffc8a&amp;from=research.research-list</a></span></span></span> and compute the phoneme error rate (PER). Style similarity is evaluated with MuQ-Mulan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib39\" title=\"\">2025</a>)</cite>, which measures the similarity of both textual prompts (Mulan-T) and audio prompts (Mulan-A). Music quality is assessed using SongEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib34\" title=\"\">2025</a>)</cite> in terms of overall coherence (CO), memorability (ME), naturalness of vocal breathing and phrasing (NA), clarity of song structure (CL), and overall musicality (MU). Audio quality is evaluated with Audiobox-Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#bib.bib28\" title=\"\">2025</a>)</cite>, which considers content enjoyment (CE), content usefulness (CU), production complexity (PC), and production quality (PQ). We further explore the generation speed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A2\" title=\"Appendix B Generation Speed &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Objective Results &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, DiffRhythm 2 significantly outperforms other open-source models across music quality metrics. However, in aspects such as musicality, it still shows a clear gap compared to commercial systems like SUNO V4.5. In terms of audio quality, it performs slightly worse than LeVo but remains superior to most other models. This result demonstrates that employing a lower frame rate is feasible for compressed song reconstruction, although the increased reconstruction burden makes it difficult to achieve perfect fidelity. DiffRhythm 2 achieves the highest scores on Mulan-T (0.40) and PER (0.13), substantially exceeding other models. However, its Mulan-A score is lower than that of LeVo and similar systems, which we attribute to our choice of modeling audio style prompts with global representations. A global style embedding cannot adequately capture the stylistic content present in songs.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "music",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T2\" title=\"Table 2 &#8227; 5.1 Objective Results &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, DiffRhythm 2 performs notably better than other open-source models in musicality, vocal-accompaniment harmony, and overall quality. Its accompaniment performance is comparable to that of ACE-Step and far exceeds that of LeVo, demonstrating that continuous representations can effectively enhance accompaniment generation. However, in vocal quality, since our system neither leverages semantic constraints nor separately models the vocal track, it falls slightly behind ACE-Step and LeVo. Overall, though, commercial models still maintain a clear advantage over open-source systems.</p>\n\n",
                "matched_terms": [
                    "over",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed methods, we conduct ablation studies focusing on stochastic block REPA loss and cross-pair preference optimization. For the cross-pair preference optimization ablation (w/o CPPO), we replaced it with separate DPO on four preferences followed by model merging. For the stochastic block REPA loss ablation (w/o REPA), we simply removed the corresponding loss during training. In addition, as a comparison, we included an experiment without applying any DPO (w/o DPO).</p>\n\n",
                "matched_terms": [
                    "block",
                    "model",
                    "training",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#S5.T3\" title=\"Table 3 &#8227; 5.3 Ablation Study &#8227; 5 Evaluation Results &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, removing REPA loss leads to a clear drop in SongEval scores. Moreover, listening tests reveal that music structure become noticeably misaligned or even fail entirely. Similarly, without cross-pair preference optimization, all evaluation metrics decline significantly compared to DiffRhythm 2.</p>\n\n",
                "matched_terms": [
                    "music",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present DiffRhythm 2, a semi-autoregressive end-to-end music generation framework based on block flow matching. By leveraging this approach, we achieve high-quality lyric alignment without relying on semantic constraints, while generating mixed-track songs with high fidelity. We further introduce stochastic block REPA loss for semi-autoregressive training, which enhances musicality and structural modeling. In addition, we propose cross-pair preference optimization, which effectively addresses the challenge of degraded average performance when optimizing across multiple preferences. Our experiments demonstrate the superior song generation capabilities of DiffRhythm 2.</p>\n\n",
                "matched_terms": [
                    "diffrhythm",
                    "block",
                    "music",
                    "training",
                    "highquality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As DiffRhythm 2 is capable of generating complete songs, it could potentially be misused to create disinformation, deepfake audio, or other harmful content. Being committed to the responsible advancement of this field, we will provide appropriate usage restrictions and guidelines when releasing the open-source code and model checkpoints.</p>\n\n",
                "matched_terms": [
                    "model",
                    "diffrhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated the compression and reconstruction performance of Stable Audio 2 VAE, the DCAE from ACE-Step, the dual-track MuCodec from LeVo and our implemented Music VAE on a testset of 50 songs covering diverse genres. It should also be noted that Music VAE supports only mono audio, whereas all the other models support stereo audio. As shown in the table, our Music VAE achieves comparable or even higher PESQ and STOI scores despite operating at a substantially lower frame rate than the other models. However, the WER of Music VAE is slightly higher than other models, indicating that the extreme low frame rate still results in some loss of fine-grained reconstruction details. The single codebook design of MuCodec significantly compromises the quality of the accompaniment and leads to inferior overall reconstruction performance.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "music",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare the generation speed of LeVo, Yue, DiffRhythm+, and DiffRhythm 2. To ensure fairness, all models are required to generate a fixed sequence of two minutes in length, regardless of generation quality. For both DiffRhythm+ and DiffRhythm 2, the number of sampling steps is set to 32. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22950v2#A2.T5\" title=\"Table 5 &#8227; Appendix B Generation Speed &#8227; DiffRhythm 2: Efficient and High Fidelity Song Generation via Block Flow Matching\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, DiffRhythm 2 is only slightly slower than DiffRhythm+ and ACE-Step, while being significantly faster than LeVo. It is worth noting that DiffRhythm+ does not incorporate any attention acceleration framework, which explains why it is slower than ACE-Step despite having a smaller model size. ACE-Step leverages linear attention to reduce computational complexity, whereas LeVo adopts Flash Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/Dao-AILab/flash-attention\" title=\"\">https://github.com/Dao-AILab/flash-attention</a></span></span></span>, and DiffRhythm 2 employs Flex Attention&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_href\" href=\"https://pytorch.ac.cn/blog/flexattention/\" title=\"\">https://pytorch.ac.cn/blog/flexattention/</a></span></span></span> for acceleration.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "model",
                    "diffrhythm",
                    "size",
                    "steps",
                    "linear"
                ]
            }
        ]
    }
}