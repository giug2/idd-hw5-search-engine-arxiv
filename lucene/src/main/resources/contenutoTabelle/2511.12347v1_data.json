{
    "S4.T1": {
        "caption": "Table 1: Zero-Shot TTS performance across different models and languages. ‡Training Hours for XTTS-v2 may be an underestimation as the model is continuously updated and specific training data has not been fully disclosed. \"-\" indicates data not available or not applicable. *For Chinese, Korean and Japanese, figures in the WER columns represent Character Error Rate (CER). †Scores reported in baseline papers.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">French</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">German</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">Spanish</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Train (hrs)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">WER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SIM-o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Train (hrs)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">WER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SIM-o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Train (hrs)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">WER</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">SIM-o</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Ground Truth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.87</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">0.73</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">XTTS-v1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">38.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20.84</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">0.37</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">XTTS-v2</td>\n<td class=\"ltx_td ltx_align_center\">2216<sup class=\"ltx_sup\">&#8225;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">5.45</span></td>\n<td class=\"ltx_td ltx_align_center\">0.58</td>\n<td class=\"ltx_td ltx_align_center\">3584<sup class=\"ltx_sup\">&#8225;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">16.50</td>\n<td class=\"ltx_td ltx_align_center\">0.59</td>\n<td class=\"ltx_td ltx_align_center\">1514<sup class=\"ltx_sup\">&#8225;</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.11</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">0.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">VoiceCraft-X</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1338</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">13.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">3405</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">8.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1191</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.67</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.63</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "applicable",
            "simo",
            "fully",
            "wer",
            "columns",
            "has",
            "not",
            "underestimation",
            "error",
            "xttsv1",
            "baseline",
            "zeroshot",
            "tts",
            "hours",
            "‡training",
            "disclosed",
            "rate",
            "papers",
            "french",
            "hrs",
            "korean",
            "character",
            "figures",
            "reported",
            "voicecraftx",
            "german",
            "training",
            "1514‡",
            "updated",
            "2216‡",
            "performance",
            "truth",
            "across",
            "continuously",
            "indicates",
            "3584‡",
            "available",
            "†scores",
            "spanish",
            "languages",
            "xttsv2",
            "train",
            "model",
            "represent",
            "specific",
            "data",
            "cer",
            "japanese",
            "chinese",
            "different",
            "ground"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluated VoiceCraft-X&#8217;s zero-shot TTS performance across 11 languages, and the results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T1\" title=\"Table 1 &#8227; Baselines. &#8227; 4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. For Chinese, VoiceCraft-X was trained on a modest <math alttext=\"5K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">5K</annotation></semantics></math> hours of data, a fraction of that used by leading models (often exceeding <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> hours). Consequently, while its CER of 3.29 was higher than these specialized models, this was achieved with substantially less data, and its speaker similarity and subjective scores reflected this data disparity. In English, VoiceCraft-X, trained on <math alttext=\"14K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>14</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">14K</annotation></semantics></math> hours, showed marked improvements over its predecessor, VoiceCraft, reducing its WER from 5.28 to 4.37 and enhancing SIM-o from 0.51 to 0.54. Critically, its CMOS score of 0.63<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The generally higher English CMOS scores likely resulted from using Seed-TTS test set as prompts with atypical, exaggerated intonation (not standard read speech).</span></span></span> was the highest among compared models, indicating superior perceived naturalness. While some models trained on significantly larger datasets achieved lower WERs, VoiceCraft-X&#8217;s subjective quality in English was highly competitive.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://zhishengzheng.com/voicecraft-x/\" title=\"\">https://zhishengzheng.com/voicecraft-x/</a>.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "languages",
                    "across",
                    "zeroshot",
                    "tts",
                    "french",
                    "model",
                    "korean",
                    "japanese",
                    "voicecraftx",
                    "data",
                    "german",
                    "available",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Highly realistic speech generation is an indispensable technology for voice assistants, content dubbing, accessibility tools, and creative media. Speech generation can be broken down into several sub-problems: <em class=\"ltx_emph ltx_font_italic\">creating</em> new audio via Text-To-Speech synthesis (TTS) or <em class=\"ltx_emph ltx_font_italic\">editing</em> part of an existing recording while ensuring voice consistency with the remainder of the original speech. Despite their shared goal of producing natural speech, TTS and speech editing are typically treated as <em class=\"ltx_emph ltx_font_italic\">separate</em> problems, especially in multilingual settings, which leaves practitioners without a <em class=\"ltx_emph ltx_font_italic\">single</em> model that can both edit and synthesize speech across languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past several years, the quality of TTS models has improved significantly, particularly in the zero-shot setting in which a model generates speech in a new speaker&#8217;s voice given a short (e.g. 3 second) audio prompt. Transformer-based neural networks have been central to this progress, leading to three broad paradigms: (i) autoregressive (AR), (ii) non-autoregressive (Non-AR), and (iii) hybrid models.\nAR models, such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> and its successors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025pseudo</span>)</cite>, generate frame-level speech tokens sequentially, where the tokens are typically derived from a neural audio codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeghidour2021soundstream</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speechtokenizer</span>)</cite>. These models are able to perform voice-cloning TTS via Transformer language models&#8217; in-context learning ability, demonstrating high-quality speech synthesis. Non-AR models include flow-matching models such as F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, as well as diffusion models such as NaturalSpeech 2/3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>. These models predict all tokens representing an utterance in parallel via iterative refinement. Hybrid approaches such as Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> aim to combine the strengths of both paradigms.\nWhile these models deliver impressive zero-shot quality, most of the models are either monolingual or focus on a handful of high-resource languages such as English and Chinese. This is likely due to the fact that these models are data-hungry, often requiring 10K-100K hours of training speech for SOTA performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages",
                    "zeroshot",
                    "hours",
                    "tts",
                    "model",
                    "training",
                    "has",
                    "chinese",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quest for broader linguistic inclusivity across the world&#8217;s 7,000 spoken languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Eberhard2024</span>)</cite> has driven research in multilingual speech generation. Efforts include curating large corpora (e.g., VoxPopuliTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025voxpopulitts</span>)</cite>, Fish-Speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2024fish</span>)</cite>) and training multilingual TTS architectures like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite>, CLAM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024clam</span>)</cite> and XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite>.\nYet even the most capable multilingual systems treat <em class=\"ltx_emph ltx_font_italic\">speech editing</em> as a separate task&#8212;or ignore it altogether&#8212;leaving users without a unified solution.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "tts",
                    "training",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper we address this gap, by introducing <span class=\"ltx_text ltx_font_bold\">VoiceCraft-X</span>, a unified autoregressive neural codec language model that performs <em class=\"ltx_emph ltx_font_italic\">both</em> speech editing and zero-shot TTS in <span class=\"ltx_text ltx_font_bold\">11 languages</span>: English (en), Mandarin (zh), Korean (ko), Japanese (ja), Spanish (es), French (fr), German (de), Dutch (nl), Italian (it), Portuguese (pt) and Polish (pl).\nOur contributions are threefold:</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "languages",
                    "zeroshot",
                    "tts",
                    "french",
                    "model",
                    "korean",
                    "voicecraftx",
                    "german",
                    "japanese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, a single autoregressive model that unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) across 11 languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "zeroshot",
                    "tts",
                    "model",
                    "voicecraftx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate VoiceCraft-X&#8217;s robust generation of high-quality, natural-sounding speech across diverse languages, even with limited per-language data, and will release our code and model to the community.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot Text-to-Speech (TTS) synthesis task entails generating speech in a new speaker&#8217;s voice from a short audio prompt, without assuming that the new speaker was seen during training. Recent progress is largely driven by Transformer-based neural networks, falling into autoregressive (AR), non-autoregressive (non-AR), and hybrid.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Autoregressive (AR) models generate speech tokens sequentially. VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> pioneered neural codec language models for high-quality zero-shot TTS via in-context learning, with subsequent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kharitonov2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lajszczak2024base</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite> further refining this paradigm. Non-Autoregressive (Non-AR) models aim for faster generation by predicting tokens in parallel or using iterative refinement. Examples include flow-matching models like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> and diffusion-based models such as NaturalSpeech 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>)</cite>, NaturalSpeech 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>, and DiTTo-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lee2024ditto</span>)</cite>. Other notable non-AR approaches include Unicats&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024unicats</span>)</cite>, SimpleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech2</span>)</cite>, E2-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite> and Mega-TTS 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2025megatts</span>)</cite>. Hybrid systems combine aspects of both AR and non-AR methods. Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> uses a two-stage architecture, while CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> also represent efforts to balance quality, speed, and controllability. In this work, VoiceCraft-X follows the codec language modeling method of VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> and enables high-quality, zero-shot multilingual speech synthesis within its unified editing and generation framework.</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "tts",
                    "represent",
                    "voicecraftx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work on multilingual speech synthesis largely pursues two complementary goals: (i) expanding language coverage and (ii) achieving zero-shot robustness to unseen speakers and languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the data side, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">saeki2024extending</span></cite> show that pairing self-supervised speech representations with unsupervised text alignment scales TTS to 100&#8201;+ languages, even when only scant transcriptions exist. Large curated corpora amplify these gains: VoxPopuliTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025voxpopulitts</span>)</cite> refines 30,000 hours of English, French and Spanish speech; Fish-Speech &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2024fish</span>)</cite> goes further, training on 720,000 hours while using an LLM to sidestep language-specific G2P rules. Model architectures have evolved in parallel. VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> adopts non-autoregressive flow matching, delivering cross-lingual zero-shot TTS in six languages via in-context learning. XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite>, extending Tortoise&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">betker2023better</span>)</cite>, combines a Perceiver Resampler with a speaker-consistency loss to reach 16 languages with speaker cloning. CLAM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024clam</span>)</cite> improves codec language model compression with probabilistic residual vector quantization, enabling single-step multi-token generation. However, these models often treat synthesis as a distinct task from speech editing. The challenge of <em class=\"ltx_emph ltx_font_italic\">unifying</em> high-quality, multilingual speech editing with robust multilingual speech synthesis within a single, open-source, and fully autoregressive model architecture remains largely unaddressed.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "models",
                    "languages",
                    "zeroshot",
                    "tts",
                    "fully",
                    "hours",
                    "french",
                    "model",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X evolves VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> into a truly multilingual speech-editing and synthesis system, treating both tasks as a single sequence-generation problem over neural codec tokens. The core of this system, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Overview &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, is the Qwen3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3</span>)</cite> large language model. Qwen3 natively supports text input in <math alttext=\"119\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>119</mn><annotation encoding=\"application/x-tex\">119</annotation></semantics></math> languages and dialects, which we leverage as the cross-lingual input text tokenizer for VoiceCraft-X. This eliminates the cumbersome phoneme-conversion step that was integral to the original VoiceCraft, resulting in a simplified pipeline with a shared tokenizer across languages, without the need to curate pronunciation lexicons for each language.</p>\n\n",
                "matched_terms": [
                    "across",
                    "voicecraftx",
                    "model",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further key innovation in VoiceCraft-X is its enhanced data layout: it interleaves text tokens and speech tokens in a single, time-ordered stream, whereas VoiceCraft reordered only the speech tokens. Enforcing this alignment between linguistic content and its acoustic realization yields more consistent and natural-sounding speech.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize the EnCodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>)</cite> neural audio codec model to tokenize the input utterance. Specifically, we train a modified version of the tokenizer which outputs a sequence of four parallel token streams at a 50Hz framerate. The tokens are discretized with residual vector quantization (RVQ) with a vocabulary size of 2048 at each quantization layer.</p>\n\n",
                "matched_terms": [
                    "train",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X employs several token reordering steps, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, to unify speech editing and synthesis. We assume that our training examples consist of utterance waveforms accompanied by time-aligned word transcriptions (we use the Montreal Forced Aligner (MFA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mcauliffe2017montreal</span>)</cite> in our work). During training, a text transcription is randomly segmented into prefix, middle, and suffix portions. These are then rearranged into a \"prefix-suffix-middle\" sequence, where the \"middle\" segment serves as the prediction target. Finally, the corresponding speech tokens for each segment are reordered identically based on the alignment timings. This ensures a monotonic alignment between the text and speech tokens, even when performing speech edits which require infilling tokens in the middle of the speech sequence. This rearrangement serves to mirror the use case in which a user wishes to modify some, but not all of the words in an utterance - by using this rearrangement, the model can be trained to predict the speech tokens within the middle of an utterance, conditioned on the preceding (prefix) and following (suffix) speech tokens in addition to the desired text transcription.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voicecraftx",
                    "not",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, the model is tasked with autoregressively predicting all audio tokens: encompassing those in the prefix, suffix, and the middle (target) segments. This prediction is optimized using a standard language modeling objective, where the cross-entropy loss function is applied to every token in the sequence. By training the model to predict not only the target segment but also the known prefix and suffix segments, it receives gradients for every timestep, resulting in faster training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "not",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To model the <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> parallel token sequences output by the EnCodec tokenizer autoregressively, we incorporate the &#8220;Delay Pattern&#8221; proposed by MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">copet2023simple</span>)</cite>. Instead of predicting all <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> codebooks for a given audio timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> simultaneously or flattening all codebooks across all timesteps into one long sequence, delay patterning inserts a cumulative time delay of one timestep per RVQ layer to the EnCodec token sequences. As a result, the prediction for the speech token at codebook level <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m4\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> at timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> can be conditioned on the model&#8217;s predictions for codebook levels 1 through <math alttext=\"k-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m6\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">k-1</annotation></semantics></math> associated with the same timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m7\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens. The system then autoregressively generates the neural codec tokens for the target audio segment.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We combined speech data across public datasets over 11 languages, amounting to a total of approximately 32K hours (detailed statistics provided in Appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS1\" title=\"A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>). The sampling rate for all audio is 16 kHz. Audio segments longer than 25 seconds were discarded. For MLS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite>, misalignment issues were particularly prominent, with approximately 20% of samples having extra or missing words in the transcript at the beginning or end. We found that this negatively impacted model performance for English, and subsequently removed utterances whose transcriptions differed significantly from those produced by the Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> model. While we found similar problems with the non-English European language data in MLS, we anecdotally observed better performance on those languages without performing this filtering. We speculate that this is due to the fact that the amount of available training data for those languages is already relatively low, and the performance improvements brought by the additional training data outweigh the detriments brought by transcription noise.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "hours",
                    "rate",
                    "model",
                    "data",
                    "training",
                    "available",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluating Text-to-Speech (TTS) performance, we curated an evaluation dataset from several established benchmarks. For English, we utilized the Seed-TTS test-en set&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> (1088 samples sourced from Common Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2019common</span>)</cite>). For Mandarin, we employed the Seed-TTS test-zh set (2020 samples from DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2021didispeech</span>)</cite>). Korean and Japanese evaluations were conducted using 200 randomly selected samples from KsponSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bang2020ksponspeech</span>)</cite> and KokoroSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Iida2021Kokoro</span>)</cite>, respectively. For the remaining seven languages supported by our model (Spanish, French, German, Dutch, Italian, Portuguese, and Polish), we randomly selected 100 samples for each language from their corresponding Multilingual LibriSpeech (MLS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite> test sets. To evaluate speech editing, we randomly selected 100-300 samples per language from these TTS test datasets and then utilized Gemini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to perform insertion, deletion, or substitution operations on the textual portions of these samples, with specific details available in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS2\" title=\"A.2 Speech Editing Dataset &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. We conducted subjective evaluation over a subset of languages (English, Chinese, French, Italian, Portuguese, and Spanish) using a random subset of the evaluation set: 40 English samples, 50 Chinese, and 20 for others.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "languages",
                    "tts",
                    "french",
                    "model",
                    "korean",
                    "japanese",
                    "specific",
                    "german",
                    "available",
                    "chinese",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model utilizes Encodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>)</cite> as the speech tokenizer. We retrain the model with some modifications, namely using 4 Residual Vector Quantization (RVQ) codebooks, each containing 2048 entries, and a framerate of 50Hz on audio recorded at 16 kHz. We retrain the model with our multilingual speech data. Other than those, the training process adheres to the methodology outlined in the work by&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>)</cite>. Additional configuration specifics can be found in Section&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A2.SS1\" title=\"B.1 Encodec Model &#8227; Appendix B Implementational Details &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>. To combine the parallel speech tokens when using them as input to the Transformer LM, at each timestep we sum the embeddings of the tokens across the four codebooks.</p>\n\n",
                "matched_terms": [
                    "across",
                    "data",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use Qwen3-0.6B-Base as both the text tokenizer and the Transformer LM backbone (details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A2.SS2\" title=\"B.2 Qwen3 Base Model &#8227; Appendix B Implementational Details &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). The outputs from the final Transformer layer are then projected into four distinct linear layers, each producing the logits for one of the codec tokens. The model comprises 613 million total parameters (457 million excluding embeddings). The codebook weights <math alttext=\"\\boldsymbol{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\alpha}</annotation></semantics></math> are set to <math alttext=\"(1.0,0.8,0.6,0.4)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mn>1.0</mn><mo>,</mo><mn>0.8</mn><mo>,</mo><mn>0.6</mn><mo>,</mo><mn>0.4</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(1.0,0.8,0.6,0.4)</annotation></semantics></math>, influencing the contribution of each codebook during training (as further detailed in our loss formulation&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A2.SS3\" title=\"B.3 Loss Design &#8227; Appendix B Implementational Details &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>). For model training, we employ the AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">loshchilov2017decoupled</span>)</cite> with a learning rate of <math alttext=\"4\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-3}</annotation></semantics></math>, <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, an epsilon of <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m6\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>, and a weight decay of <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m7\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>. A learning rate scheduler is utilized, featuring a linear warm-up for the initial <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m8\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> steps, followed by a linear decay for the remainder of the <math alttext=\"5,000K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m9\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>,</mo><mrow><mn>000</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">5,000K</annotation></semantics></math> total training steps. Gradient accumulation is performed over <math alttext=\"8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m10\" intent=\":literal\"><semantics><mn>8</mn><annotation encoding=\"application/x-tex\">8</annotation></semantics></math> micro-batches. The training of the multilingual VoiceCraft-X model took approximately one week on 16 NVIDIA A100 40GB GPUs.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "voicecraftx",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens; the model then autoregressively predicts the corresponding neural codec tokens for the target audio segment. Notably, the token reordering mechanism significantly enhances inference stability. This largely prevents repeating token loops, an issue in the original VoiceCraft which could cause artifacts (e.g., excessive silences) and required multi-sample filtering. Consequently, VoiceCraft-X reliably generates high-quality speech in a single pass without needing this filtering step. In all experiments, we employ nucleus sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">holtzman2019curious</span>)</cite> with <math alttext=\"TopK=20,TopP=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><mo>=</mo><mn>20</mn></mrow><mo>,</mo><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow><mo>=</mo><mn>1.0</mn></mrow></mrow><annotation encoding=\"application/x-tex\">TopK=20,TopP=1.0</annotation></semantics></math>, and a temperature of 1.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voicecraftx",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the English and Chinese Zero-shot TTS tasks, we compared our model with FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite>, MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>)</cite>, and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite>. For English, we also included VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> in our comparison. For the remaining languages, we benchmarked our model against the multilingual XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite> model, considering both its v1 and v2 versions. For speech editing, we compared VoiceCraft-X with the original VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> model on English.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "zeroshot",
                    "tts",
                    "model",
                    "voicecraftx",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used a combination of subjective and objective measures. Objectively, we use Word Error Rate (WER) as an automatic proxy for the intelligibility of the synthesized speech; this is calculated using Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2023funasr</span>)</cite> for Chinese and Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> for other languages. Additionally, speaker similarity (SIM-o) is objectively measured by computing the cosine similarity of speaker embeddings, which are extracted from both the generated and original target speech using a WavLM-based speaker verification model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span>)</cite>. Subjective evaluations involved human annotators (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for details) who provide Comparative Mean Opinion Scores (CMOS) and Similarity Mean Opinion Scores (SMOS) for TTS, and Naturalness Mean Opinion Scores (NMOS) and Intelligibility Mean Opinion Scores (IMOS) for speech editing. For CMOS, evaluators assess the naturalness of the synthesized speech in comparison to the ground truth, while for SMOS, they directly score the similarity between the synthesized speech and the initial speech prompt. For NMOS and IMOS, evaluators respectively assess the naturalness and intelligibility of the synthesized and original speech.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "simo",
                    "tts",
                    "wer",
                    "rate",
                    "model",
                    "truth",
                    "chinese",
                    "ground",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the remaining nine languages, VoiceCraft-X, compared to XTTS (versions v1 and v2), showed strong overall performance with varying focuses. VoiceCraft-X particularly excelled in European languages like German (WER significantly better than XTTS-v2 by over 50%), Spanish (WER over 40% better than XTTS-v2 and below the ground truth), and Italian (higher data efficiency), as well as in Korean (CER reduced by over 20%). However, in languages such as Japanese and Dutch, or for those where VoiceCraft-X had considerably less training data like Portuguese and Polish, XTTS-v2 achieved lower error rates. Nevertheless, VoiceCraft-X was often favored by evaluators for its better speaker similarity, naturalness, and intelligibility. (Further results are in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>).</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "languages",
                    "xttsv2",
                    "wer",
                    "korean",
                    "truth",
                    "voicecraftx",
                    "data",
                    "german",
                    "training",
                    "performance",
                    "cer",
                    "japanese",
                    "ground",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To explore the benefits of multilingual training, especially for lower-resource languages, we fine-tuned <span class=\"ltx_text ltx_font_italic\">monolingual</span> models on individual languages starting from different pre-trained checkpoints, comparing these against training from scratch and the multilingual model (detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Transfer Learning for Multilingual TTS &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages",
                    "model",
                    "training",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The universal advantage of pre-training over &#8220;from Scratch&#8221; models is paramount, especially for languages with limited data. For instance, Italian (294 hours) and Polish (139 hours) saw their WERs plummet from over 140 and 160 to under 14 and 20 respectively, demonstrating pre-training&#8217;s crucial role in transferring foundational knowledge and overcoming data scarcity. Even higher-resource languages like Spanish, French and German benefited significantly. Fine-tuning from an English model initialization proved highly effective for European languages (Germanic, Romance, Slavic), leveraging linguistic similarities and robust acoustic modeling, with gains particularly vital for low-data scenarios (Italian, Portuguese, Polish). Korean showed better CER with a Japanese checkpoint (42.08) than Chinese (49.11), aligning with typological closeness. Conversely, Japanese experienced negative transfer from Chinese (CER 36.18 vs. 22.36 from scratch).</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "models",
                    "languages",
                    "hours",
                    "french",
                    "model",
                    "korean",
                    "data",
                    "german",
                    "cer",
                    "japanese",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, fine-tuning from the &#8220;multilingual checkpoint&#8221; frequently yielded superior WER/CER compared to an English-only checkpoint for a range of languages including Spanish, Dutch, Italian, Portuguese, Polish, and Japanese. This advantage held across varying data volumes (e.g., Polish 139 hours, Japanese 3489 hours), suggesting that pre-training on a diverse linguistic set fosters more generalized and transferable representations than exposure to English alone, capturing a broader array of phonetic and prosodic patterns.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "languages",
                    "across",
                    "hours",
                    "data",
                    "japanese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the original multilingual model&#8217;s speaker similarity is significantly higher than models fine-tuned from other checkpoints for nearly all languages. This indicates that joint training on diverse linguistic data, leveraging collective data volume, allows the model to disentangle speaker-specific characteristics from language-specific features. This robust performance across varied languages suggests it learns a more abstract, shared representation space for speech, facilitating both high-fidelity synthesis and strong cross-lingual capabilities. While fine-tuning on single language data may impact this disentanglement ability, as evidenced by SIM-o drops in many such cases.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "simo",
                    "languages",
                    "model",
                    "data",
                    "training",
                    "indicates",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English speech editing (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Speech Editing &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), VoiceCraft-X demonstrated a better Word Error Rate (WER) than VoiceCraft. Both models produced edited speech that listeners found to be highly natural (NMOS) and intelligible (IMOS), comparable to the original recordings. VoiceCraft&#8217;s slightly higher scores in these subjective tests are not surprising, given its monolingual English focus, especially considering both models have similar parameter counts and amounts of English training data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "wer",
                    "rate",
                    "voicecraftx",
                    "data",
                    "training",
                    "not",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For multilingual speech editing in other languages&#8212;a capability where comparative baselines are notably scarce as most models do not support multilingual editing&#8212;we conducted subjective MOS evaluations. These evaluations focused on a subset of languages (French, Italian, Portuguese, and Spanish) for which MTurk annotators were available, with results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T4\" title=\"Table 4 &#8227; 4.4 Speech Editing &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The evaluations demonstrate VoiceCraft-X&#8217;s effective performance in this challenging scenario. While naturalness (NMOS) scores for edited speech are, as anticipated, lower than the original recordings, intelligibility (IMOS) remains high across these languages. Particularly for Spanish and Italian, where edited NMOS and IMOS scores closely matched the original audio, these findings underscore VoiceCraft-X&#8217;s significant and unique capability for coherent, comprehensible multilingual speech editing.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "models",
                    "across",
                    "languages",
                    "french",
                    "available",
                    "not",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present VoiceCraft-X, an autoregressive neural codec language model that successfully unifies multilingual speech editing and Text-to-Speech (TTS) synthesis. Leveraging the Qwen3 LLM and a novel token reordering strategy, VoiceCraft-X supports eleven languages, producing high-quality, natural-sounding speech. Our model demonstrates robust performance across diverse conditions and shows that a unified framework can effectively advance both speech editing and synthesis in multilingual contexts, even with limited data for some languages. This work underscores the potential of autoregressive models for complex, real-world speech generation tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "languages",
                    "tts",
                    "model",
                    "voicecraftx",
                    "data",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One key limitation is the scale of our training data. Although VoiceCraft-X performs well with approximately 32,578 hours across eleven languages, this is notably less than some state-of-the-art models. This comparative data scarcity, particularly for lower-resource languages in our set, may limit the model&#8217;s capacity to capture the full spectrum of speech nuances as effectively as systems trained on more extensive datasets.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "languages",
                    "hours",
                    "voicecraftx",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, while the model&#8217;s multilingual support is a core feature, its current reach of eleven languages (with around 20-30 explored internally) only scratches the surface of global linguistic diversity. Expanding coverage to more languages, especially under-resourced ones, remains a significant challenge that would require substantial data curation and potential model adaptations to address varied linguistic features.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "data",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, further investigation into model size scalability is also warranted. The current VoiceCraft-X utilizes the Qwen3-0.6B architecture; exploring larger model variants could unlock enhanced learning capabilities and higher fidelity in speech synthesis and editing. Systematically assessing different model sizes is crucial for optimizing the balance between performance improvements and computational demands.</p>\n\n",
                "matched_terms": [
                    "different",
                    "voicecraftx",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of advanced speech models like VoiceCraft-X, which possesses strong zero-shot voice cloning and multilingual editing capabilities, carries significant ethical responsibilities. We acknowledge the potential for misuse of this technology. Malicious actors could exploit it for unauthorized voice cloning, impersonation, the creation of convincing deepfakes for fraudulent purposes, or the generation of misinformation and propaganda. These risks are particularly pronounced given the model&#8217;s ability to operate across eleven languages, broadening the potential scope for misuse on a global scale.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages",
                    "across",
                    "zeroshot",
                    "voicecraftx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot nature of VoiceCraft-X lowers the barrier to entry for creating high-fidelity synthetic audio, making it accessible to a wider range of actors beyond those with specialized technical expertise. This accessibility amplifies the dual-use nature of the technology; while it empowers creativity and accessibility, it also provides a powerful tool for deception.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate these risks, we are committed to a responsible release of our model and code. We strongly advocate for the research community to explore and develop robust safeguards, such as audio watermarking and detection tools, to help distinguish between authentic and synthesized audio. Such advancements are crucial for building a safer information ecosystem, <span class=\"ltx_text ltx_font_italic\">but are only possible if open-source versions of these models are available for researchers to utilize.</span> Our release will be accompanied by strict intended-use guidelines and a license that explicitly prohibits malicious applications, such as impersonating public figures or private individuals without their explicit consent. We believe that by fostering an open yet cautious approach, we can encourage further research into safety measures while providing a valuable tool for beneficial applications and advancing the field of speech technology responsibly.</p>\n\n",
                "matched_terms": [
                    "available",
                    "models",
                    "figures",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike RealEdit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite>, which relies on manual, sentence-by-sentence human annotation and modification, a process that limits its scalability across many languages, we employed the powerful multilingual capabilities of the Gemini language model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to systematically introduce textual modifications to the original sentences. The goal was to generate edited versions that reflect common editing scenarios. To achieve this, Gemini was instructed to perform exactly one of the following specified operations on each original sentence:</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Encodec model we employ operates with a stride of 320 samples, corresponding to a codec frame rate of 50 Hz when processing audio recorded at 16 kHz. Its encoder begins with a base channel dimension of 64, which doubles at each of the five successive convolutional layers. Following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>)</cite>, we utilize the open-source audiocraft repository<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/audiocraft/blob/main/docs/ENCODEC.md\" title=\"\">https://github.com/facebookresearch/audiocraft/blob/main/docs/ENCODEC.md</a></span></span></span> for training. Specifically, we sample one-second speech segments from the multilingual dataset (shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.T5\" title=\"Table 5 &#8227; A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) and train for 200 epochs with a batch size of 832. Optimization is performed using the Adam algorithm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kingma2014adam</span>)</cite> with a base learning rate of 5e-5.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "train",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Qwen3-0.6B-Base model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Qwen/Qwen3-0.6B-Base\" title=\"\">https://huggingface.co/Qwen/Qwen3-0.6B-Base</a></span></span></span>, foundational to VoiceCraft-X, is a causal language model with 0.6 billion total parameters, of which 0.44 billion are non-embedding parameters. It features 28 Transformer layers, a hidden dimension of 1024, and a feed-forward network (FFN) dimension of 3072, along with 16 attention heads. The model employs Grouped-Query Attention (16 query heads and 8 key/value heads) and supports a context length of 32,768 tokens. A key factor in its suitability for VoiceCraft-X&#8217;s multilingual requirements is its pre-training on 36 trillion tokens across 119 languages. This pre-training utilized a diverse, high-quality data mix that included multilingual texts, books, and synthetic data. Furthermore, the model incorporates architectural refinements such as <span class=\"ltx_text ltx_font_italic\">qk layernorm</span> and benefits from a three-stage pre-training process designed for robust long-context handling.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "model",
                    "voicecraftx",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X is trained as an autoregressive model to predict a sequence of neural codec tokens. Given the input context, which includes text tokens, speaker embeddings, and potentially prefix/suffix audio tokens, the model predicts the target audio tokens one by one. The overall training objective is a weighted cross-entropy loss, designed to enhance learning efficiency and focus on the crucial aspects of the speech generation task.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let the sequence of all ground truth speech tokens (encompassing prefix, suffix, and middle segments, and structured according to the delay pattern described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.SS5\" title=\"3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>) be denoted by <math alttext=\"Z=(z_{1},z_{2},\\dots,z_{N})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=(z_{1},z_{2},\\dots,z_{N})</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the total number of tokens in the flattened sequence. Each token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> in this sequence corresponds to an original codec token <math alttext=\"Y_{t_{i},k_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>Y</mi><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msub><annotation encoding=\"application/x-tex\">Y_{t_{i},k_{i}}</annotation></semantics></math> from timestep <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math> and the <math alttext=\"k_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>k</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">k_{i}</annotation></semantics></math>-th codebook of the EnCodec output (where <math alttext=\"K=4\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">K=4</annotation></semantics></math> is the total number of codebooks). The model predicts the probability distribution for each token <math alttext=\"\\hat{z}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\hat{z}_{i}</annotation></semantics></math> conditioned on previous tokens and the input context.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ground",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment Weighting</span>: While the model is trained to predict tokens for all three segments (prefix, middle, and suffix) to improve training efficacy and contextual understanding, the primary goal is the accurate generation of the \"middle\" (target) segment. To reflect this, we introduce segment-specific weights. Tokens belonging to the \"prefix\" and \"suffix\" segments are assigned a weight <math alttext=\"w_{seg}=1\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{seg}=1</annotation></semantics></math>. Tokens belonging to the \"middle\" segment, which is the primary target for generation or editing, are assigned a higher weight <math alttext=\"w_{seg}=3\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">w_{seg}=3</annotation></semantics></math>. Let <math alttext=\"w_{seg}(z_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">w_{seg}(z_{i})</annotation></semantics></math> denote the segment weight for token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m4\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To compute our subjective evaluation metrics (SMOS and CMOS for TTS, NMOS and IMOS for Speech Editing), for all languages except Chinese, we recruited Amazon Mechanical Turk workers with a minimum approval rate of <math alttext=\"98\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>98</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">98\\%</annotation></semantics></math> and at least <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> successful HITs. We manually recruited university students for Chinese. We filtered workers by the following countries in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3.T7\" title=\"Table 7 &#8227; C.1 Setup &#8227; Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> for each of our languages:</p>\n\n",
                "matched_terms": [
                    "rate",
                    "languages",
                    "chinese",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each sample was annotated by 3 different annotators. We display annotation UIs for our metrics in Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A6.F4\" title=\"Figure 4 &#8227; Appendix F Cross-lingual Finetuning Hours on Zero-Shot TTS &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A6.F5\" title=\"Figure 5 &#8227; Appendix F Cross-lingual Finetuning Hours on Zero-Shot TTS &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>,&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A6.F6\" title=\"Figure 6 &#8227; Appendix F Cross-lingual Finetuning Hours on Zero-Shot TTS &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A6.F7\" title=\"Figure 7 &#8227; Appendix F Cross-lingual Finetuning Hours on Zero-Shot TTS &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "figures",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A scarcity of Amazon Mechanical Turk workers for less common languages prevented us from collecting subjective evaluation results for all targeted languages. Consequently, the SMOS results for French, Italian, Portuguese, and Spanish on the Zero-Shot TTS task that we were able to gather are detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3.T8\" title=\"Table 8 &#8227; C.2 Additional Results &#8227; Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "languages",
                    "zeroshot",
                    "tts",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For this ablation study, considering the low-resource nature of most languages, we used LibriTTS-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koizumi2023libritts</span>)</cite> and the WenetSpeech4TTS Premium&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts</span>)</cite> subset as training data. LibriTTS-R contains 585 hours of speech, while the WenetSpeech4TTS Premium subset includes 601 hours<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>YouTube clips are removed.</span></span></span>. Models were trained for 15 epochs, both with and without the reordering mechanism. The final epoch was then evaluated on the Seed-TTS test set. As can be seen from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T9\" title=\"Table 9 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the model using the reordering mechanism shows significant performance improvements across all objective evaluation metrics on both the English and Chinese datasets. Specifically, the WER for English dropped dramatically from 104.02 to 11.60, and the CER for Chinese also decreased sharply from 262.25 to 19.25. Concurrently, the SIM-o scores for both languages also showed noticeable increases, indicating an improvement in the quality and naturalness of the synthesized speech. These results strongly demonstrate that the reordering mechanism is very effective in training under low-resource scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "simo",
                    "languages",
                    "hours",
                    "wer",
                    "model",
                    "data",
                    "training",
                    "cer",
                    "chinese",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The token reordering mechanism, integral to our training methodology, introduces flexibility in how prompts are structured during zero-shot Text-to-Speech (TTS) inference. To determine the optimal placement, we evaluated several configurations for incorporating the prompt text (<math alttext=\"T_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{prompt}</annotation></semantics></math>) and prompt audio (<math alttext=\"A_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{prompt}</annotation></semantics></math>) into the input sequence. These configurations are detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T10\" title=\"Table 10 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation, based on WER and SIM-o, revealed that placing the prompt at the beginning of the \"middle\" segment yields the most favorable overall performance. Specifically, structuring the input such that the prompt text precedes the target text within the middle text segment (i.e., <math alttext=\"T_{P}=\\varnothing,T_{S}=\\varnothing,T_{M}=(T_{prompt},T_{target})\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>T</mi><mi>P</mi></msub><mo>=</mo><mi mathvariant=\"normal\">&#8709;</mi></mrow><mo>,</mo><mrow><mrow><msub><mi>T</mi><mi>S</mi></msub><mo>=</mo><mi mathvariant=\"normal\">&#8709;</mi></mrow><mo>,</mo><mrow><msub><mi>T</mi><mi>M</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>T</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">T_{P}=\\varnothing,T_{S}=\\varnothing,T_{M}=(T_{prompt},T_{target})</annotation></semantics></math>, with <math alttext=\"A_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{prompt}</annotation></semantics></math> appended after the mask tokens and before where <math alttext=\"A_{target}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{target}</annotation></semantics></math> would be generated) resulted in a WER of 4.37, which is notably better than the alternative placements.</p>\n\n",
                "matched_terms": [
                    "simo",
                    "performance",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A desirable characteristic of a multilingual Text-to-Speech (TTS) model is its ability to generate code-switched speech&#8212;that is, speech that fluidly transitions between languages. Although our model was trained exclusively on monolingual data, meaning code-switched speech is an out-of-distribution phenomenon for it, the model still demonstrated a certain capacity for code-switching without needing additional language identifiers for inputs in different languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "tts",
                    "model",
                    "data",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also observed that the model tends to perform better when the initial language of the target text matches the language of the prompt. Conversely, if the starting language of the target text differs from the prompt, the model&#8217;s performance may be significantly worse. We have made code-switched samples available on our demo page.</p>\n\n",
                "matched_terms": [
                    "available",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess VoiceCraft-X&#8217;s adaptability and the impact of data quantity, we extended fine-tuning experiments across diverse languages. Building on cross-lingual transfer insights (Section&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.SS3\" title=\"4.3 Transfer Learning for Multilingual TTS &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>), we examined the correlation between per-language fine-tuning data volume and zero-shot Text-to-Speech (TTS) quality.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "across",
                    "zeroshot",
                    "tts",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A6.F3\" title=\"Figure 3 &#8227; Appendix F Cross-lingual Finetuning Hours on Zero-Shot TTS &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates these findings, plotting per-language fine-tuning data volume (x-axis) against the relative Word Error Rate (WER) from zero-shot TTS (y-axis). This relative WER, the difference between Whisper&#8217;s WER on synthesized versus ground-truth audio, offers a normalized measure of intelligibility. The figure generally shows that more fine-tuning data improves pronunciation accuracy, especially for languages sharing similarities with VoiceCraft-X&#8217;s initial training set. However, this correlation is not universally linear. For languages like Korean and Thai, a moderate data increase (around 1000 hours) did not yield significant WER improvements. This plateauing suggests that for such languages, substantial gains may require much larger or more diverse datasets, or different fine-tuning approaches.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "zeroshot",
                    "tts",
                    "hours",
                    "wer",
                    "rate",
                    "korean",
                    "data",
                    "training",
                    "different",
                    "not",
                    "error"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Cross-lingual transfer learning performance on zero-shot TTS task. Comparison of fine-tuning from different pre-trained models versus training from scratch for various target languages. Character Error Rate (CER) for Korean and Japanese, indicated by *. \"-\" indicates data not available or not applicable.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">#Hours</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Multilingual</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">from Scratch</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">from English</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">from Chinese/Japanese</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">from Multilingual</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">WER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SIM-o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">WER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SIM-o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">WER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SIM-o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">WER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SIM-o</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">WER</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SIM-o</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Korean*</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">832</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">31.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">45.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">42.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">49.11/42.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.50/0.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">41.36</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Japanese*</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3489</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">15.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">22.36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.62</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">19.35</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.67</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Spanish</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1191</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.63</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7.08</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.47</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.30</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.52</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">French</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1338</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">13.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.60</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">18.85</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">12.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.49</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">16.39</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">German</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3405</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">8.19</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.60</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">6.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.43</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">5.93</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7.25</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Dutch</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2147</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">16.28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">16.85</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">16.02</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">11.78</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Italian</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">294</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">15.46</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.54</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">142.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">13.97</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">13.93</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Portuguese</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">223</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">22.57</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.56</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">91.89</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.26</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">15.87</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.46</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">14.74</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.55</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Polish</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">139</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">24.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">163.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">20.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">19.47</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.55</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "applicable",
            "simo",
            "wer",
            "multilingual",
            "transfer",
            "indicated",
            "various",
            "comparison",
            "not",
            "error",
            "italian",
            "zeroshot",
            "tts",
            "hours",
            "rate",
            "french",
            "crosslingual",
            "character",
            "learning",
            "korean",
            "target",
            "from",
            "training",
            "german",
            "performance",
            "finetuning",
            "versus",
            "english",
            "chinesejapanese",
            "language",
            "portuguese",
            "polish",
            "indicates",
            "available",
            "spanish",
            "pretrained",
            "languages",
            "task",
            "dutch",
            "data",
            "scratch",
            "cer",
            "japanese",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To explore the benefits of multilingual training, especially for lower-resource languages, we fine-tuned <span class=\"ltx_text ltx_font_italic\">monolingual</span> models on individual languages starting from different pre-trained checkpoints, comparing these against training from scratch and the multilingual model (detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Transfer Learning for Multilingual TTS &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://zhishengzheng.com/voicecraft-x/\" title=\"\">https://zhishengzheng.com/voicecraft-x/</a>.</p>\n\n",
                "matched_terms": [
                    "multilingual",
                    "italian",
                    "zeroshot",
                    "tts",
                    "french",
                    "crosslingual",
                    "korean",
                    "german",
                    "performance",
                    "english",
                    "portuguese",
                    "language",
                    "polish",
                    "available",
                    "spanish",
                    "languages",
                    "dutch",
                    "data",
                    "japanese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Highly realistic speech generation is an indispensable technology for voice assistants, content dubbing, accessibility tools, and creative media. Speech generation can be broken down into several sub-problems: <em class=\"ltx_emph ltx_font_italic\">creating</em> new audio via Text-To-Speech synthesis (TTS) or <em class=\"ltx_emph ltx_font_italic\">editing</em> part of an existing recording while ensuring voice consistency with the remainder of the original speech. Despite their shared goal of producing natural speech, TTS and speech editing are typically treated as <em class=\"ltx_emph ltx_font_italic\">separate</em> problems, especially in multilingual settings, which leaves practitioners without a <em class=\"ltx_emph ltx_font_italic\">single</em> model that can both edit and synthesize speech across languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "multilingual",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past several years, the quality of TTS models has improved significantly, particularly in the zero-shot setting in which a model generates speech in a new speaker&#8217;s voice given a short (e.g. 3 second) audio prompt. Transformer-based neural networks have been central to this progress, leading to three broad paradigms: (i) autoregressive (AR), (ii) non-autoregressive (Non-AR), and (iii) hybrid models.\nAR models, such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> and its successors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025pseudo</span>)</cite>, generate frame-level speech tokens sequentially, where the tokens are typically derived from a neural audio codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeghidour2021soundstream</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speechtokenizer</span>)</cite>. These models are able to perform voice-cloning TTS via Transformer language models&#8217; in-context learning ability, demonstrating high-quality speech synthesis. Non-AR models include flow-matching models such as F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, as well as diffusion models such as NaturalSpeech 2/3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>. These models predict all tokens representing an utterance in parallel via iterative refinement. Hybrid approaches such as Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> aim to combine the strengths of both paradigms.\nWhile these models deliver impressive zero-shot quality, most of the models are either monolingual or focus on a handful of high-resource languages such as English and Chinese. This is likely due to the fact that these models are data-hungry, often requiring 10K-100K hours of training speech for SOTA performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages",
                    "zeroshot",
                    "hours",
                    "tts",
                    "english",
                    "language",
                    "learning",
                    "from",
                    "training",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quest for broader linguistic inclusivity across the world&#8217;s 7,000 spoken languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Eberhard2024</span>)</cite> has driven research in multilingual speech generation. Efforts include curating large corpora (e.g., VoxPopuliTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025voxpopulitts</span>)</cite>, Fish-Speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2024fish</span>)</cite>) and training multilingual TTS architectures like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite>, CLAM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024clam</span>)</cite> and XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite>.\nYet even the most capable multilingual systems treat <em class=\"ltx_emph ltx_font_italic\">speech editing</em> as a separate task&#8212;or ignore it altogether&#8212;leaving users without a unified solution.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "multilingual",
                    "tts",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper we address this gap, by introducing <span class=\"ltx_text ltx_font_bold\">VoiceCraft-X</span>, a unified autoregressive neural codec language model that performs <em class=\"ltx_emph ltx_font_italic\">both</em> speech editing and zero-shot TTS in <span class=\"ltx_text ltx_font_bold\">11 languages</span>: English (en), Mandarin (zh), Korean (ko), Japanese (ja), Spanish (es), French (fr), German (de), Dutch (nl), Italian (it), Portuguese (pt) and Polish (pl).\nOur contributions are threefold:</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "languages",
                    "zeroshot",
                    "tts",
                    "english",
                    "portuguese",
                    "language",
                    "french",
                    "polish",
                    "korean",
                    "dutch",
                    "german",
                    "japanese",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, a single autoregressive model that unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) across 11 languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "multilingual",
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach leverages the Qwen3 large language model for cross-lingual text processing, without the need for phonetic pronunciation lexicons. We also propose a novel token reordering mechanism that time-aligns text and speech, enabling a unified sequence generation approach for both editing and synthesis.</p>\n\n",
                "matched_terms": [
                    "language",
                    "crosslingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate VoiceCraft-X&#8217;s robust generation of high-quality, natural-sounding speech across diverse languages, even with limited per-language data, and will release our code and model to the community.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech editing aims to correct mispronunciations, stutters, or recording artifacts while producing speech that is indistinguishable from natural audio. Recent approaches leverage Transformer and diffusion architectures. <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">borsos2022speechpainter</span></cite> perform audio infilling with a Transformer that maintains speaker identity and prosody, generalizing to unseen speakers. &#160;<cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span></cite> use flow matching for versatile speech infilling, and <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span></cite> show that a neural-codec language model with token infilling can concurrently handle editing and synthesis. F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> extend this idea with flow-matching or diffusion, respectively. Despite these advances, most works are monolingual, motivating a unified multilingual solution.</p>\n\n",
                "matched_terms": [
                    "language",
                    "multilingual",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot Text-to-Speech (TTS) synthesis task entails generating speech in a new speaker&#8217;s voice from a short audio prompt, without assuming that the new speaker was seen during training. Recent progress is largely driven by Transformer-based neural networks, falling into autoregressive (AR), non-autoregressive (non-AR), and hybrid.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "task",
                    "from",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Autoregressive (AR) models generate speech tokens sequentially. VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> pioneered neural codec language models for high-quality zero-shot TTS via in-context learning, with subsequent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kharitonov2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lajszczak2024base</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite> further refining this paradigm. Non-Autoregressive (Non-AR) models aim for faster generation by predicting tokens in parallel or using iterative refinement. Examples include flow-matching models like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> and diffusion-based models such as NaturalSpeech 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>)</cite>, NaturalSpeech 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>, and DiTTo-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lee2024ditto</span>)</cite>. Other notable non-AR approaches include Unicats&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024unicats</span>)</cite>, SimpleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech2</span>)</cite>, E2-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite> and Mega-TTS 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2025megatts</span>)</cite>. Hybrid systems combine aspects of both AR and non-AR methods. Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> uses a two-stage architecture, while CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> also represent efforts to balance quality, speed, and controllability. In this work, VoiceCraft-X follows the codec language modeling method of VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> and enables high-quality, zero-shot multilingual speech synthesis within its unified editing and generation framework.</p>\n\n",
                "matched_terms": [
                    "models",
                    "zeroshot",
                    "tts",
                    "language",
                    "multilingual",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior work on multilingual speech synthesis largely pursues two complementary goals: (i) expanding language coverage and (ii) achieving zero-shot robustness to unseen speakers and languages.</p>\n\n",
                "matched_terms": [
                    "language",
                    "multilingual",
                    "zeroshot",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the data side, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">saeki2024extending</span></cite> show that pairing self-supervised speech representations with unsupervised text alignment scales TTS to 100&#8201;+ languages, even when only scant transcriptions exist. Large curated corpora amplify these gains: VoxPopuliTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025voxpopulitts</span>)</cite> refines 30,000 hours of English, French and Spanish speech; Fish-Speech &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2024fish</span>)</cite> goes further, training on 720,000 hours while using an LLM to sidestep language-specific G2P rules. Model architectures have evolved in parallel. VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> adopts non-autoregressive flow matching, delivering cross-lingual zero-shot TTS in six languages via in-context learning. XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite>, extending Tortoise&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">betker2023better</span>)</cite>, combines a Perceiver Resampler with a speaker-consistency loss to reach 16 languages with speaker cloning. CLAM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024clam</span>)</cite> improves codec language model compression with probabilistic residual vector quantization, enabling single-step multi-token generation. However, these models often treat synthesis as a distinct task from speech editing. The challenge of <em class=\"ltx_emph ltx_font_italic\">unifying</em> high-quality, multilingual speech editing with robust multilingual speech synthesis within a single, open-source, and fully autoregressive model architecture remains largely unaddressed.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "models",
                    "languages",
                    "zeroshot",
                    "tts",
                    "hours",
                    "english",
                    "task",
                    "language",
                    "french",
                    "multilingual",
                    "crosslingual",
                    "learning",
                    "data",
                    "from",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X evolves VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> into a truly multilingual speech-editing and synthesis system, treating both tasks as a single sequence-generation problem over neural codec tokens. The core of this system, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Overview &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, is the Qwen3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3</span>)</cite> large language model. Qwen3 natively supports text input in <math alttext=\"119\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>119</mn><annotation encoding=\"application/x-tex\">119</annotation></semantics></math> languages and dialects, which we leverage as the cross-lingual input text tokenizer for VoiceCraft-X. This eliminates the cumbersome phoneme-conversion step that was integral to the original VoiceCraft, resulting in a simplified pipeline with a shared tokenizer across languages, without the need to curate pronunciation lexicons for each language.</p>\n\n",
                "matched_terms": [
                    "language",
                    "languages",
                    "multilingual",
                    "crosslingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the speech tokens representing the prompt speech, VoiceCraft-X also takes as input a speaker embedding vector extracted from this prompt speech. We follow the approach of CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>)</cite> by using a pre-trained voiceprint model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.modelscope.cn/models/iic/CosyVoice-300M/file/view/master/campplus.onnx\" title=\"\">https://www.modelscope.cn/models/iic/CosyVoice-300M/file/view/master/campplus.onnx</a></span></span></span> to extract the speaker embedding. The resulting vector is then passed through a linear projection layer. This projection maps the speaker embedding to match Qwen3&#8217;s input dimension.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X employs several token reordering steps, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, to unify speech editing and synthesis. We assume that our training examples consist of utterance waveforms accompanied by time-aligned word transcriptions (we use the Montreal Forced Aligner (MFA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mcauliffe2017montreal</span>)</cite> in our work). During training, a text transcription is randomly segmented into prefix, middle, and suffix portions. These are then rearranged into a \"prefix-suffix-middle\" sequence, where the \"middle\" segment serves as the prediction target. Finally, the corresponding speech tokens for each segment are reordered identically based on the alignment timings. This ensures a monotonic alignment between the text and speech tokens, even when performing speech edits which require infilling tokens in the middle of the speech sequence. This rearrangement serves to mirror the use case in which a user wishes to modify some, but not all of the words in an utterance - by using this rearrangement, the model can be trained to predict the speech tokens within the middle of an utterance, conditioned on the preceding (prefix) and following (suffix) speech tokens in addition to the desired text transcription.</p>\n\n",
                "matched_terms": [
                    "target",
                    "not",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, the model is tasked with autoregressively predicting all audio tokens: encompassing those in the prefix, suffix, and the middle (target) segments. This prediction is optimized using a standard language modeling objective, where the cross-entropy loss function is applied to every token in the sequence. By training the model to predict not only the target segment but also the known prefix and suffix segments, it receives gradients for every timestep, resulting in faster training.</p>\n\n",
                "matched_terms": [
                    "language",
                    "target",
                    "not",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens. The system then autoregressively generates the neural codec tokens for the target audio segment.</p>\n\n",
                "matched_terms": [
                    "target",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If a prompt text (<math alttext=\"T_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{prompt}</annotation></semantics></math>) and its corresponding prompt speech are provided, we concatenate the prompt text and the target text (<math alttext=\"T_{target}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{target}</annotation></semantics></math>) to form the middle text segment, and a speaker embedding is extracted from the prompt speech. If no such prompt is provided, we set the prompt text (<math alttext=\"T_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{prompt}</annotation></semantics></math>) to empty and randomly generate a speaker embedding. The final input is as follows:</p>\n\n",
                "matched_terms": [
                    "target",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We combined speech data across public datasets over 11 languages, amounting to a total of approximately 32K hours (detailed statistics provided in Appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS1\" title=\"A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>). The sampling rate for all audio is 16 kHz. Audio segments longer than 25 seconds were discarded. For MLS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite>, misalignment issues were particularly prominent, with approximately 20% of samples having extra or missing words in the transcript at the beginning or end. We found that this negatively impacted model performance for English, and subsequently removed utterances whose transcriptions differed significantly from those produced by the Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> model. While we found similar problems with the non-English European language data in MLS, we anecdotally observed better performance on those languages without performing this filtering. We speculate that this is due to the fact that the amount of available training data for those languages is already relatively low, and the performance improvements brought by the additional training data outweigh the detriments brought by transcription noise.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "hours",
                    "english",
                    "rate",
                    "language",
                    "data",
                    "from",
                    "training",
                    "available",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluating Text-to-Speech (TTS) performance, we curated an evaluation dataset from several established benchmarks. For English, we utilized the Seed-TTS test-en set&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> (1088 samples sourced from Common Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2019common</span>)</cite>). For Mandarin, we employed the Seed-TTS test-zh set (2020 samples from DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2021didispeech</span>)</cite>). Korean and Japanese evaluations were conducted using 200 randomly selected samples from KsponSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bang2020ksponspeech</span>)</cite> and KokoroSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Iida2021Kokoro</span>)</cite>, respectively. For the remaining seven languages supported by our model (Spanish, French, German, Dutch, Italian, Portuguese, and Polish), we randomly selected 100 samples for each language from their corresponding Multilingual LibriSpeech (MLS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite> test sets. To evaluate speech editing, we randomly selected 100-300 samples per language from these TTS test datasets and then utilized Gemini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to perform insertion, deletion, or substitution operations on the textual portions of these samples, with specific details available in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS2\" title=\"A.2 Speech Editing Dataset &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. We conducted subjective evaluation over a subset of languages (English, Chinese, French, Italian, Portuguese, and Spanish) using a random subset of the evaluation set: 40 English samples, 50 Chinese, and 20 for others.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "languages",
                    "tts",
                    "english",
                    "portuguese",
                    "language",
                    "french",
                    "multilingual",
                    "polish",
                    "korean",
                    "dutch",
                    "japanese",
                    "from",
                    "german",
                    "available",
                    "italian",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our model utilizes Encodec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>)</cite> as the speech tokenizer. We retrain the model with some modifications, namely using 4 Residual Vector Quantization (RVQ) codebooks, each containing 2048 entries, and a framerate of 50Hz on audio recorded at 16 kHz. We retrain the model with our multilingual speech data. Other than those, the training process adheres to the methodology outlined in the work by&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>)</cite>. Additional configuration specifics can be found in Section&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A2.SS1\" title=\"B.1 Encodec Model &#8227; Appendix B Implementational Details &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>. To combine the parallel speech tokens when using them as input to the Transformer LM, at each timestep we sum the embeddings of the tokens across the four codebooks.</p>\n\n",
                "matched_terms": [
                    "data",
                    "multilingual",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use Qwen3-0.6B-Base as both the text tokenizer and the Transformer LM backbone (details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A2.SS2\" title=\"B.2 Qwen3 Base Model &#8227; Appendix B Implementational Details &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). The outputs from the final Transformer layer are then projected into four distinct linear layers, each producing the logits for one of the codec tokens. The model comprises 613 million total parameters (457 million excluding embeddings). The codebook weights <math alttext=\"\\boldsymbol{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\alpha}</annotation></semantics></math> are set to <math alttext=\"(1.0,0.8,0.6,0.4)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mn>1.0</mn><mo>,</mo><mn>0.8</mn><mo>,</mo><mn>0.6</mn><mo>,</mo><mn>0.4</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(1.0,0.8,0.6,0.4)</annotation></semantics></math>, influencing the contribution of each codebook during training (as further detailed in our loss formulation&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A2.SS3\" title=\"B.3 Loss Design &#8227; Appendix B Implementational Details &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>). For model training, we employ the AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">loshchilov2017decoupled</span>)</cite> with a learning rate of <math alttext=\"4\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-3}</annotation></semantics></math>, <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, an epsilon of <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m6\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>, and a weight decay of <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m7\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>. A learning rate scheduler is utilized, featuring a linear warm-up for the initial <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m8\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> steps, followed by a linear decay for the remainder of the <math alttext=\"5,000K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m9\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>,</mo><mrow><mn>000</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">5,000K</annotation></semantics></math> total training steps. Gradient accumulation is performed over <math alttext=\"8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m10\" intent=\":literal\"><semantics><mn>8</mn><annotation encoding=\"application/x-tex\">8</annotation></semantics></math> micro-batches. The training of the multilingual VoiceCraft-X model took approximately one week on 16 NVIDIA A100 40GB GPUs.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "multilingual",
                    "learning",
                    "from",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens; the model then autoregressively predicts the corresponding neural codec tokens for the target audio segment. Notably, the token reordering mechanism significantly enhances inference stability. This largely prevents repeating token loops, an issue in the original VoiceCraft which could cause artifacts (e.g., excessive silences) and required multi-sample filtering. Consequently, VoiceCraft-X reliably generates high-quality speech in a single pass without needing this filtering step. In all experiments, we employ nucleus sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">holtzman2019curious</span>)</cite> with <math alttext=\"TopK=20,TopP=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><mo>=</mo><mn>20</mn></mrow><mo>,</mo><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow><mo>=</mo><mn>1.0</mn></mrow></mrow><annotation encoding=\"application/x-tex\">TopK=20,TopP=1.0</annotation></semantics></math>, and a temperature of 1.</p>\n\n",
                "matched_terms": [
                    "target",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the English and Chinese Zero-shot TTS tasks, we compared our model with FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite>, MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>)</cite>, and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite>. For English, we also included VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> in our comparison. For the remaining languages, we benchmarked our model against the multilingual XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite> model, considering both its v1 and v2 versions. For speech editing, we compared VoiceCraft-X with the original VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> model on English.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "zeroshot",
                    "tts",
                    "english",
                    "multilingual",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used a combination of subjective and objective measures. Objectively, we use Word Error Rate (WER) as an automatic proxy for the intelligibility of the synthesized speech; this is calculated using Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2023funasr</span>)</cite> for Chinese and Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> for other languages. Additionally, speaker similarity (SIM-o) is objectively measured by computing the cosine similarity of speaker embeddings, which are extracted from both the generated and original target speech using a WavLM-based speaker verification model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span>)</cite>. Subjective evaluations involved human annotators (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for details) who provide Comparative Mean Opinion Scores (CMOS) and Similarity Mean Opinion Scores (SMOS) for TTS, and Naturalness Mean Opinion Scores (NMOS) and Intelligibility Mean Opinion Scores (IMOS) for speech editing. For CMOS, evaluators assess the naturalness of the synthesized speech in comparison to the ground truth, while for SMOS, they directly score the similarity between the synthesized speech and the initial speech prompt. For NMOS and IMOS, evaluators respectively assess the naturalness and intelligibility of the synthesized and original speech.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "simo",
                    "tts",
                    "wer",
                    "rate",
                    "target",
                    "from",
                    "comparison",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated VoiceCraft-X&#8217;s zero-shot TTS performance across 11 languages, and the results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T1\" title=\"Table 1 &#8227; Baselines. &#8227; 4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. For Chinese, VoiceCraft-X was trained on a modest <math alttext=\"5K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">5K</annotation></semantics></math> hours of data, a fraction of that used by leading models (often exceeding <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> hours). Consequently, while its CER of 3.29 was higher than these specialized models, this was achieved with substantially less data, and its speaker similarity and subjective scores reflected this data disparity. In English, VoiceCraft-X, trained on <math alttext=\"14K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>14</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">14K</annotation></semantics></math> hours, showed marked improvements over its predecessor, VoiceCraft, reducing its WER from 5.28 to 4.37 and enhancing SIM-o from 0.51 to 0.54. Critically, its CMOS score of 0.63<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The generally higher English CMOS scores likely resulted from using Seed-TTS test set as prompts with atypical, exaggerated intonation (not standard read speech).</span></span></span> was the highest among compared models, indicating superior perceived naturalness. While some models trained on significantly larger datasets achieved lower WERs, VoiceCraft-X&#8217;s subjective quality in English was highly competitive.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages",
                    "simo",
                    "zeroshot",
                    "tts",
                    "hours",
                    "wer",
                    "english",
                    "data",
                    "from",
                    "cer",
                    "not",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the remaining nine languages, VoiceCraft-X, compared to XTTS (versions v1 and v2), showed strong overall performance with varying focuses. VoiceCraft-X particularly excelled in European languages like German (WER significantly better than XTTS-v2 by over 50%), Spanish (WER over 40% better than XTTS-v2 and below the ground truth), and Italian (higher data efficiency), as well as in Korean (CER reduced by over 20%). However, in languages such as Japanese and Dutch, or for those where VoiceCraft-X had considerably less training data like Portuguese and Polish, XTTS-v2 achieved lower error rates. Nevertheless, VoiceCraft-X was often favored by evaluators for its better speaker similarity, naturalness, and intelligibility. (Further results are in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>).</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "languages",
                    "wer",
                    "portuguese",
                    "polish",
                    "korean",
                    "dutch",
                    "data",
                    "german",
                    "training",
                    "performance",
                    "cer",
                    "japanese",
                    "italian",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The universal advantage of pre-training over &#8220;from Scratch&#8221; models is paramount, especially for languages with limited data. For instance, Italian (294 hours) and Polish (139 hours) saw their WERs plummet from over 140 and 160 to under 14 and 20 respectively, demonstrating pre-training&#8217;s crucial role in transferring foundational knowledge and overcoming data scarcity. Even higher-resource languages like Spanish, French and German benefited significantly. Fine-tuning from an English model initialization proved highly effective for European languages (Germanic, Romance, Slavic), leveraging linguistic similarities and robust acoustic modeling, with gains particularly vital for low-data scenarios (Italian, Portuguese, Polish). Korean showed better CER with a Japanese checkpoint (42.08) than Chinese (49.11), aligning with typological closeness. Conversely, Japanese experienced negative transfer from Chinese (CER 36.18 vs. 22.36 from scratch).</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "models",
                    "languages",
                    "hours",
                    "finetuning",
                    "english",
                    "portuguese",
                    "polish",
                    "french",
                    "transfer",
                    "korean",
                    "data",
                    "from",
                    "german",
                    "scratch",
                    "cer",
                    "japanese",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, fine-tuning from the &#8220;multilingual checkpoint&#8221; frequently yielded superior WER/CER compared to an English-only checkpoint for a range of languages including Spanish, Dutch, Italian, Portuguese, Polish, and Japanese. This advantage held across varying data volumes (e.g., Polish 139 hours, Japanese 3489 hours), suggesting that pre-training on a diverse linguistic set fosters more generalized and transferable representations than exposure to English alone, capturing a broader array of phonetic and prosodic patterns.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "languages",
                    "finetuning",
                    "hours",
                    "english",
                    "portuguese",
                    "polish",
                    "dutch",
                    "data",
                    "from",
                    "japanese",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the original multilingual model&#8217;s speaker similarity is significantly higher than models fine-tuned from other checkpoints for nearly all languages. This indicates that joint training on diverse linguistic data, leveraging collective data volume, allows the model to disentangle speaker-specific characteristics from language-specific features. This robust performance across varied languages suggests it learns a more abstract, shared representation space for speech, facilitating both high-fidelity synthesis and strong cross-lingual capabilities. While fine-tuning on single language data may impact this disentanglement ability, as evidenced by SIM-o drops in many such cases.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages",
                    "simo",
                    "finetuning",
                    "language",
                    "multilingual",
                    "crosslingual",
                    "data",
                    "from",
                    "training",
                    "indicates",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English speech editing (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Speech Editing &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), VoiceCraft-X demonstrated a better Word Error Rate (WER) than VoiceCraft. Both models produced edited speech that listeners found to be highly natural (NMOS) and intelligible (IMOS), comparable to the original recordings. VoiceCraft&#8217;s slightly higher scores in these subjective tests are not surprising, given its monolingual English focus, especially considering both models have similar parameter counts and amounts of English training data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "english",
                    "wer",
                    "rate",
                    "data",
                    "training",
                    "not",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For multilingual speech editing in other languages&#8212;a capability where comparative baselines are notably scarce as most models do not support multilingual editing&#8212;we conducted subjective MOS evaluations. These evaluations focused on a subset of languages (French, Italian, Portuguese, and Spanish) for which MTurk annotators were available, with results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T4\" title=\"Table 4 &#8227; 4.4 Speech Editing &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The evaluations demonstrate VoiceCraft-X&#8217;s effective performance in this challenging scenario. While naturalness (NMOS) scores for edited speech are, as anticipated, lower than the original recordings, intelligibility (IMOS) remains high across these languages. Particularly for Spanish and Italian, where edited NMOS and IMOS scores closely matched the original audio, these findings underscore VoiceCraft-X&#8217;s significant and unique capability for coherent, comprehensible multilingual speech editing.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "models",
                    "languages",
                    "portuguese",
                    "french",
                    "multilingual",
                    "available",
                    "not",
                    "italian",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present VoiceCraft-X, an autoregressive neural codec language model that successfully unifies multilingual speech editing and Text-to-Speech (TTS) synthesis. Leveraging the Qwen3 LLM and a novel token reordering strategy, VoiceCraft-X supports eleven languages, producing high-quality, natural-sounding speech. Our model demonstrates robust performance across diverse conditions and shows that a unified framework can effectively advance both speech editing and synthesis in multilingual contexts, even with limited data for some languages. This work underscores the potential of autoregressive models for complex, real-world speech generation tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages",
                    "tts",
                    "language",
                    "multilingual",
                    "data",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One key limitation is the scale of our training data. Although VoiceCraft-X performs well with approximately 32,578 hours across eleven languages, this is notably less than some state-of-the-art models. This comparative data scarcity, particularly for lower-resource languages in our set, may limit the model&#8217;s capacity to capture the full spectrum of speech nuances as effectively as systems trained on more extensive datasets.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages",
                    "hours",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Secondly, while the model&#8217;s multilingual support is a core feature, its current reach of eleven languages (with around 20-30 explored internally) only scratches the surface of global linguistic diversity. Expanding coverage to more languages, especially under-resourced ones, remains a significant challenge that would require substantial data curation and potential model adaptations to address varied linguistic features.</p>\n\n",
                "matched_terms": [
                    "data",
                    "languages",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, further investigation into model size scalability is also warranted. The current VoiceCraft-X utilizes the Qwen3-0.6B architecture; exploring larger model variants could unlock enhanced learning capabilities and higher fidelity in speech synthesis and editing. Systematically assessing different model sizes is crucial for optimizing the balance between performance improvements and computational demands.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "different",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of advanced speech models like VoiceCraft-X, which possesses strong zero-shot voice cloning and multilingual editing capabilities, carries significant ethical responsibilities. We acknowledge the potential for misuse of this technology. Malicious actors could exploit it for unauthorized voice cloning, impersonation, the creation of convincing deepfakes for fraudulent purposes, or the generation of misinformation and propaganda. These risks are particularly pronounced given the model&#8217;s ability to operate across eleven languages, broadening the potential scope for misuse on a global scale.</p>\n\n",
                "matched_terms": [
                    "models",
                    "multilingual",
                    "zeroshot",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To mitigate these risks, we are committed to a responsible release of our model and code. We strongly advocate for the research community to explore and develop robust safeguards, such as audio watermarking and detection tools, to help distinguish between authentic and synthesized audio. Such advancements are crucial for building a safer information ecosystem, <span class=\"ltx_text ltx_font_italic\">but are only possible if open-source versions of these models are available for researchers to utilize.</span> Our release will be accompanied by strict intended-use guidelines and a license that explicitly prohibits malicious applications, such as impersonating public figures or private individuals without their explicit consent. We believe that by fostering an open yet cautious approach, we can encourage further research into safety measures while providing a valuable tool for beneficial applications and advancing the field of speech technology responsibly.</p>\n\n",
                "matched_terms": [
                    "available",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training datasets for each language are as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.T5\" title=\"Table 5 &#8227; A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. For all of them, we remove all YouTube clips.</p>\n\n",
                "matched_terms": [
                    "language",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To create a comprehensive evaluation set for speech editing, we began by selecting a subset of samples from the Text-to-Speech (TTS) evaluation datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.SS1\" title=\"4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. For each language, 100-300 original text samples were chosen.</p>\n\n",
                "matched_terms": [
                    "language",
                    "from",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike RealEdit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite>, which relies on manual, sentence-by-sentence human annotation and modification, a process that limits its scalability across many languages, we employed the powerful multilingual capabilities of the Gemini language model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to systematically introduce textual modifications to the original sentences. The goal was to generate edited versions that reflect common editing scenarios. To achieve this, Gemini was instructed to perform exactly one of the following specified operations on each original sentence:</p>\n\n",
                "matched_terms": [
                    "language",
                    "multilingual",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Encodec model we employ operates with a stride of 320 samples, corresponding to a codec frame rate of 50 Hz when processing audio recorded at 16 kHz. Its encoder begins with a base channel dimension of 64, which doubles at each of the five successive convolutional layers. Following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>)</cite>, we utilize the open-source audiocraft repository<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/audiocraft/blob/main/docs/ENCODEC.md\" title=\"\">https://github.com/facebookresearch/audiocraft/blob/main/docs/ENCODEC.md</a></span></span></span> for training. Specifically, we sample one-second speech segments from the multilingual dataset (shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.T5\" title=\"Table 5 &#8227; A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) and train for 200 epochs with a batch size of 832. Optimization is performed using the Adam algorithm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kingma2014adam</span>)</cite> with a base learning rate of 5e-5.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "multilingual",
                    "learning",
                    "from",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Qwen3-0.6B-Base model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Qwen/Qwen3-0.6B-Base\" title=\"\">https://huggingface.co/Qwen/Qwen3-0.6B-Base</a></span></span></span>, foundational to VoiceCraft-X, is a causal language model with 0.6 billion total parameters, of which 0.44 billion are non-embedding parameters. It features 28 Transformer layers, a hidden dimension of 1024, and a feed-forward network (FFN) dimension of 3072, along with 16 attention heads. The model employs Grouped-Query Attention (16 query heads and 8 key/value heads) and supports a context length of 32,768 tokens. A key factor in its suitability for VoiceCraft-X&#8217;s multilingual requirements is its pre-training on 36 trillion tokens across 119 languages. This pre-training utilized a diverse, high-quality data mix that included multilingual texts, books, and synthetic data. Furthermore, the model incorporates architectural refinements such as <span class=\"ltx_text ltx_font_italic\">qk layernorm</span> and benefits from a three-stage pre-training process designed for robust long-context handling.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "language",
                    "multilingual",
                    "data",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X is trained as an autoregressive model to predict a sequence of neural codec tokens. Given the input context, which includes text tokens, speaker embeddings, and potentially prefix/suffix audio tokens, the model predicts the target audio tokens one by one. The overall training objective is a weighted cross-entropy loss, designed to enhance learning efficiency and focus on the crucial aspects of the speech generation task.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "target",
                    "task",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment Weighting</span>: While the model is trained to predict tokens for all three segments (prefix, middle, and suffix) to improve training efficacy and contextual understanding, the primary goal is the accurate generation of the \"middle\" (target) segment. To reflect this, we introduce segment-specific weights. Tokens belonging to the \"prefix\" and \"suffix\" segments are assigned a weight <math alttext=\"w_{seg}=1\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{seg}=1</annotation></semantics></math>. Tokens belonging to the \"middle\" segment, which is the primary target for generation or editing, are assigned a higher weight <math alttext=\"w_{seg}=3\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">w_{seg}=3</annotation></semantics></math>. Let <math alttext=\"w_{seg}(z_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">w_{seg}(z_{i})</annotation></semantics></math> denote the segment weight for token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m4\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "target",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L_{CE}(\\hat{z}_{i},z_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p4.m2\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L_{CE}(\\hat{z}_{i},z_{i})</annotation></semantics></math> is the cross-entropy loss for predicting token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p4.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math>. This weighted loss function guides the model to prioritize the generation of the target audio segment while still learning from the context provided by the prefix and suffix, and appropriately valuing the contribution of each codebook.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "target",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To compute our subjective evaluation metrics (SMOS and CMOS for TTS, NMOS and IMOS for Speech Editing), for all languages except Chinese, we recruited Amazon Mechanical Turk workers with a minimum approval rate of <math alttext=\"98\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>98</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">98\\%</annotation></semantics></math> and at least <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> successful HITs. We manually recruited university students for Chinese. We filtered workers by the following countries in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3.T7\" title=\"Table 7 &#8227; C.1 Setup &#8227; Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> for each of our languages:</p>\n\n",
                "matched_terms": [
                    "rate",
                    "languages",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A scarcity of Amazon Mechanical Turk workers for less common languages prevented us from collecting subjective evaluation results for all targeted languages. Consequently, the SMOS results for French, Italian, Portuguese, and Spanish on the Zero-Shot TTS task that we were able to gather are detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3.T8\" title=\"Table 8 &#8227; C.2 Additional Results &#8227; Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "languages",
                    "zeroshot",
                    "tts",
                    "portuguese",
                    "task",
                    "french",
                    "from",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For this ablation study, considering the low-resource nature of most languages, we used LibriTTS-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koizumi2023libritts</span>)</cite> and the WenetSpeech4TTS Premium&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts</span>)</cite> subset as training data. LibriTTS-R contains 585 hours of speech, while the WenetSpeech4TTS Premium subset includes 601 hours<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>YouTube clips are removed.</span></span></span>. Models were trained for 15 epochs, both with and without the reordering mechanism. The final epoch was then evaluated on the Seed-TTS test set. As can be seen from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T9\" title=\"Table 9 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the model using the reordering mechanism shows significant performance improvements across all objective evaluation metrics on both the English and Chinese datasets. Specifically, the WER for English dropped dramatically from 104.02 to 11.60, and the CER for Chinese also decreased sharply from 262.25 to 19.25. Concurrently, the SIM-o scores for both languages also showed noticeable increases, indicating an improvement in the quality and naturalness of the synthesized speech. These results strongly demonstrate that the reordering mechanism is very effective in training under low-resource scenarios.</p>\n\n",
                "matched_terms": [
                    "models",
                    "languages",
                    "simo",
                    "hours",
                    "english",
                    "wer",
                    "data",
                    "from",
                    "training",
                    "cer",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The token reordering mechanism, integral to our training methodology, introduces flexibility in how prompts are structured during zero-shot Text-to-Speech (TTS) inference. To determine the optimal placement, we evaluated several configurations for incorporating the prompt text (<math alttext=\"T_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{prompt}</annotation></semantics></math>) and prompt audio (<math alttext=\"A_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{prompt}</annotation></semantics></math>) into the input sequence. These configurations are detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T10\" title=\"Table 10 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation, based on WER and SIM-o, revealed that placing the prompt at the beginning of the \"middle\" segment yields the most favorable overall performance. Specifically, structuring the input such that the prompt text precedes the target text within the middle text segment (i.e., <math alttext=\"T_{P}=\\varnothing,T_{S}=\\varnothing,T_{M}=(T_{prompt},T_{target})\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>T</mi><mi>P</mi></msub><mo>=</mo><mi mathvariant=\"normal\">&#8709;</mi></mrow><mo>,</mo><mrow><mrow><msub><mi>T</mi><mi>S</mi></msub><mo>=</mo><mi mathvariant=\"normal\">&#8709;</mi></mrow><mo>,</mo><mrow><msub><mi>T</mi><mi>M</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>T</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">T_{P}=\\varnothing,T_{S}=\\varnothing,T_{M}=(T_{prompt},T_{target})</annotation></semantics></math>, with <math alttext=\"A_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{prompt}</annotation></semantics></math> appended after the mask tokens and before where <math alttext=\"A_{target}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{target}</annotation></semantics></math> would be generated) resulted in a WER of 4.37, which is notably better than the alternative placements.</p>\n\n",
                "matched_terms": [
                    "target",
                    "simo",
                    "performance",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A desirable characteristic of a multilingual Text-to-Speech (TTS) model is its ability to generate code-switched speech&#8212;that is, speech that fluidly transitions between languages. Although our model was trained exclusively on monolingual data, meaning code-switched speech is an out-of-distribution phenomenon for it, the model still demonstrated a certain capacity for code-switching without needing additional language identifiers for inputs in different languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "tts",
                    "language",
                    "multilingual",
                    "data",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also observed that the model tends to perform better when the initial language of the target text matches the language of the prompt. Conversely, if the starting language of the target text differs from the prompt, the model&#8217;s performance may be significantly worse. We have made code-switched samples available on our demo page.</p>\n\n",
                "matched_terms": [
                    "language",
                    "target",
                    "from",
                    "available",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess VoiceCraft-X&#8217;s adaptability and the impact of data quantity, we extended fine-tuning experiments across diverse languages. Building on cross-lingual transfer insights (Section&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.SS3\" title=\"4.3 Transfer Learning for Multilingual TTS &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>), we examined the correlation between per-language fine-tuning data volume and zero-shot Text-to-Speech (TTS) quality.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "zeroshot",
                    "finetuning",
                    "tts",
                    "crosslingual",
                    "transfer",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A6.F3\" title=\"Figure 3 &#8227; Appendix F Cross-lingual Finetuning Hours on Zero-Shot TTS &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates these findings, plotting per-language fine-tuning data volume (x-axis) against the relative Word Error Rate (WER) from zero-shot TTS (y-axis). This relative WER, the difference between Whisper&#8217;s WER on synthesized versus ground-truth audio, offers a normalized measure of intelligibility. The figure generally shows that more fine-tuning data improves pronunciation accuracy, especially for languages sharing similarities with VoiceCraft-X&#8217;s initial training set. However, this correlation is not universally linear. For languages like Korean and Thai, a moderate data increase (around 1000 hours) did not yield significant WER improvements. This plateauing suggests that for such languages, substantial gains may require much larger or more diverse datasets, or different fine-tuning approaches.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "zeroshot",
                    "tts",
                    "hours",
                    "wer",
                    "rate",
                    "finetuning",
                    "versus",
                    "korean",
                    "data",
                    "from",
                    "training",
                    "different",
                    "not",
                    "error"
                ]
            }
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Performance on English speech editing.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\">WER</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\">NMOS</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\">IMOS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\">Original</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\">2.42</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\">3.78</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\">3.79</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\">VoiceCraft</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\">5.99</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\"><span class=\"ltx_text ltx_font_bold\">3.87</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\"><span class=\"ltx_text ltx_font_bold\">3.87</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\">VoiceCraft-X</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\"><span class=\"ltx_text ltx_font_bold\">5.62</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\">3.68</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\" style=\"padding-top:-0.75pt;padding-bottom:-0.75pt;\">3.79</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "imos",
            "editing",
            "wer",
            "english",
            "original",
            "voicecraft",
            "voicecraftx",
            "speech",
            "nmos",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For English speech editing (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Speech Editing &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), VoiceCraft-X demonstrated a better Word Error Rate (WER) than VoiceCraft. Both models produced edited speech that listeners found to be highly natural (NMOS) and intelligible (IMOS), comparable to the original recordings. VoiceCraft&#8217;s slightly higher scores in these subjective tests are not surprising, given its monolingual English focus, especially considering both models have similar parameter counts and amounts of English training data.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://zhishengzheng.com/voicecraft-x/\" title=\"\">https://zhishengzheng.com/voicecraft-x/</a>.</p>\n\n",
                "matched_terms": [
                    "english",
                    "editing",
                    "voicecraftx",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis \n<br class=\"ltx_break\"/>and Speech Editing</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraftx",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Highly realistic speech generation is an indispensable technology for voice assistants, content dubbing, accessibility tools, and creative media. Speech generation can be broken down into several sub-problems: <em class=\"ltx_emph ltx_font_italic\">creating</em> new audio via Text-To-Speech synthesis (TTS) or <em class=\"ltx_emph ltx_font_italic\">editing</em> part of an existing recording while ensuring voice consistency with the remainder of the original speech. Despite their shared goal of producing natural speech, TTS and speech editing are typically treated as <em class=\"ltx_emph ltx_font_italic\">separate</em> problems, especially in multilingual settings, which leaves practitioners without a <em class=\"ltx_emph ltx_font_italic\">single</em> model that can both edit and synthesize speech across languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "original",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past several years, the quality of TTS models has improved significantly, particularly in the zero-shot setting in which a model generates speech in a new speaker&#8217;s voice given a short (e.g. 3 second) audio prompt. Transformer-based neural networks have been central to this progress, leading to three broad paradigms: (i) autoregressive (AR), (ii) non-autoregressive (Non-AR), and (iii) hybrid models.\nAR models, such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> and its successors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025pseudo</span>)</cite>, generate frame-level speech tokens sequentially, where the tokens are typically derived from a neural audio codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeghidour2021soundstream</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speechtokenizer</span>)</cite>. These models are able to perform voice-cloning TTS via Transformer language models&#8217; in-context learning ability, demonstrating high-quality speech synthesis. Non-AR models include flow-matching models such as F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, as well as diffusion models such as NaturalSpeech 2/3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>. These models predict all tokens representing an utterance in parallel via iterative refinement. Hybrid approaches such as Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> aim to combine the strengths of both paradigms.\nWhile these models deliver impressive zero-shot quality, most of the models are either monolingual or focus on a handful of high-resource languages such as English and Chinese. This is likely due to the fact that these models are data-hungry, often requiring 10K-100K hours of training speech for SOTA performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "english",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quest for broader linguistic inclusivity across the world&#8217;s 7,000 spoken languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Eberhard2024</span>)</cite> has driven research in multilingual speech generation. Efforts include curating large corpora (e.g., VoxPopuliTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025voxpopulitts</span>)</cite>, Fish-Speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2024fish</span>)</cite>) and training multilingual TTS architectures like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite>, CLAM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024clam</span>)</cite> and XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite>.\nYet even the most capable multilingual systems treat <em class=\"ltx_emph ltx_font_italic\">speech editing</em> as a separate task&#8212;or ignore it altogether&#8212;leaving users without a unified solution.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper we address this gap, by introducing <span class=\"ltx_text ltx_font_bold\">VoiceCraft-X</span>, a unified autoregressive neural codec language model that performs <em class=\"ltx_emph ltx_font_italic\">both</em> speech editing and zero-shot TTS in <span class=\"ltx_text ltx_font_bold\">11 languages</span>: English (en), Mandarin (zh), Korean (ko), Japanese (ja), Spanish (es), French (fr), German (de), Dutch (nl), Italian (it), Portuguese (pt) and Polish (pl).\nOur contributions are threefold:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraftx",
                    "english",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, a single autoregressive model that unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) across 11 languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraftx",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach leverages the Qwen3 large language model for cross-lingual text processing, without the need for phonetic pronunciation lexicons. We also propose a novel token reordering mechanism that time-aligns text and speech, enabling a unified sequence generation approach for both editing and synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech editing aims to correct mispronunciations, stutters, or recording artifacts while producing speech that is indistinguishable from natural audio. Recent approaches leverage Transformer and diffusion architectures. <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">borsos2022speechpainter</span></cite> perform audio infilling with a Transformer that maintains speaker identity and prosody, generalizing to unseen speakers. &#160;<cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span></cite> use flow matching for versatile speech infilling, and <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span></cite> show that a neural-codec language model with token infilling can concurrently handle editing and synthesis. F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> extend this idea with flow-matching or diffusion, respectively. Despite these advances, most works are monolingual, motivating a unified multilingual solution.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Autoregressive (AR) models generate speech tokens sequentially. VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> pioneered neural codec language models for high-quality zero-shot TTS via in-context learning, with subsequent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kharitonov2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lajszczak2024base</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite> further refining this paradigm. Non-Autoregressive (Non-AR) models aim for faster generation by predicting tokens in parallel or using iterative refinement. Examples include flow-matching models like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> and diffusion-based models such as NaturalSpeech 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>)</cite>, NaturalSpeech 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>, and DiTTo-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lee2024ditto</span>)</cite>. Other notable non-AR approaches include Unicats&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024unicats</span>)</cite>, SimpleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech2</span>)</cite>, E2-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite> and Mega-TTS 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2025megatts</span>)</cite>. Hybrid systems combine aspects of both AR and non-AR methods. Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> uses a two-stage architecture, while CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> also represent efforts to balance quality, speed, and controllability. In this work, VoiceCraft-X follows the codec language modeling method of VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> and enables high-quality, zero-shot multilingual speech synthesis within its unified editing and generation framework.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraft",
                    "voicecraftx",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the data side, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">saeki2024extending</span></cite> show that pairing self-supervised speech representations with unsupervised text alignment scales TTS to 100&#8201;+ languages, even when only scant transcriptions exist. Large curated corpora amplify these gains: VoxPopuliTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025voxpopulitts</span>)</cite> refines 30,000 hours of English, French and Spanish speech; Fish-Speech &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2024fish</span>)</cite> goes further, training on 720,000 hours while using an LLM to sidestep language-specific G2P rules. Model architectures have evolved in parallel. VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> adopts non-autoregressive flow matching, delivering cross-lingual zero-shot TTS in six languages via in-context learning. XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite>, extending Tortoise&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">betker2023better</span>)</cite>, combines a Perceiver Resampler with a speaker-consistency loss to reach 16 languages with speaker cloning. CLAM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024clam</span>)</cite> improves codec language model compression with probabilistic residual vector quantization, enabling single-step multi-token generation. However, these models often treat synthesis as a distinct task from speech editing. The challenge of <em class=\"ltx_emph ltx_font_italic\">unifying</em> high-quality, multilingual speech editing with robust multilingual speech synthesis within a single, open-source, and fully autoregressive model architecture remains largely unaddressed.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "english",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X evolves VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> into a truly multilingual speech-editing and synthesis system, treating both tasks as a single sequence-generation problem over neural codec tokens. The core of this system, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Overview &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, is the Qwen3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3</span>)</cite> large language model. Qwen3 natively supports text input in <math alttext=\"119\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>119</mn><annotation encoding=\"application/x-tex\">119</annotation></semantics></math> languages and dialects, which we leverage as the cross-lingual input text tokenizer for VoiceCraft-X. This eliminates the cumbersome phoneme-conversion step that was integral to the original VoiceCraft, resulting in a simplified pipeline with a shared tokenizer across languages, without the need to curate pronunciation lexicons for each language.</p>\n\n",
                "matched_terms": [
                    "voicecraft",
                    "voicecraftx",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A further key innovation in VoiceCraft-X is its enhanced data layout: it interleaves text tokens and speech tokens in a single, time-ordered stream, whereas VoiceCraft reordered only the speech tokens. Enforcing this alignment between linguistic content and its acoustic realization yields more consistent and natural-sounding speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraft",
                    "voicecraftx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the speech tokens representing the prompt speech, VoiceCraft-X also takes as input a speaker embedding vector extracted from this prompt speech. We follow the approach of CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>)</cite> by using a pre-trained voiceprint model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.modelscope.cn/models/iic/CosyVoice-300M/file/view/master/campplus.onnx\" title=\"\">https://www.modelscope.cn/models/iic/CosyVoice-300M/file/view/master/campplus.onnx</a></span></span></span> to extract the speaker embedding. The resulting vector is then passed through a linear projection layer. This projection maps the speaker embedding to match Qwen3&#8217;s input dimension.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraftx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X employs several token reordering steps, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, to unify speech editing and synthesis. We assume that our training examples consist of utterance waveforms accompanied by time-aligned word transcriptions (we use the Montreal Forced Aligner (MFA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mcauliffe2017montreal</span>)</cite> in our work). During training, a text transcription is randomly segmented into prefix, middle, and suffix portions. These are then rearranged into a \"prefix-suffix-middle\" sequence, where the \"middle\" segment serves as the prediction target. Finally, the corresponding speech tokens for each segment are reordered identically based on the alignment timings. This ensures a monotonic alignment between the text and speech tokens, even when performing speech edits which require infilling tokens in the middle of the speech sequence. This rearrangement serves to mirror the use case in which a user wishes to modify some, but not all of the words in an utterance - by using this rearrangement, the model can be trained to predict the speech tokens within the middle of an utterance, conditioned on the preceding (prefix) and following (suffix) speech tokens in addition to the desired text transcription.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraftx",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens. The system then autoregressively generates the neural codec tokens for the target audio segment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraftx",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We combined speech data across public datasets over 11 languages, amounting to a total of approximately 32K hours (detailed statistics provided in Appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS1\" title=\"A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>). The sampling rate for all audio is 16 kHz. Audio segments longer than 25 seconds were discarded. For MLS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite>, misalignment issues were particularly prominent, with approximately 20% of samples having extra or missing words in the transcript at the beginning or end. We found that this negatively impacted model performance for English, and subsequently removed utterances whose transcriptions differed significantly from those produced by the Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> model. While we found similar problems with the non-English European language data in MLS, we anecdotally observed better performance on those languages without performing this filtering. We speculate that this is due to the fact that the amount of available training data for those languages is already relatively low, and the performance improvements brought by the additional training data outweigh the detriments brought by transcription noise.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "english",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluating Text-to-Speech (TTS) performance, we curated an evaluation dataset from several established benchmarks. For English, we utilized the Seed-TTS test-en set&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> (1088 samples sourced from Common Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2019common</span>)</cite>). For Mandarin, we employed the Seed-TTS test-zh set (2020 samples from DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2021didispeech</span>)</cite>). Korean and Japanese evaluations were conducted using 200 randomly selected samples from KsponSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bang2020ksponspeech</span>)</cite> and KokoroSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Iida2021Kokoro</span>)</cite>, respectively. For the remaining seven languages supported by our model (Spanish, French, German, Dutch, Italian, Portuguese, and Polish), we randomly selected 100 samples for each language from their corresponding Multilingual LibriSpeech (MLS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite> test sets. To evaluate speech editing, we randomly selected 100-300 samples per language from these TTS test datasets and then utilized Gemini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to perform insertion, deletion, or substitution operations on the textual portions of these samples, with specific details available in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS2\" title=\"A.2 Speech Editing Dataset &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. We conducted subjective evaluation over a subset of languages (English, Chinese, French, Italian, Portuguese, and Spanish) using a random subset of the evaluation set: 40 English samples, 50 Chinese, and 20 for others.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "english",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens; the model then autoregressively predicts the corresponding neural codec tokens for the target audio segment. Notably, the token reordering mechanism significantly enhances inference stability. This largely prevents repeating token loops, an issue in the original VoiceCraft which could cause artifacts (e.g., excessive silences) and required multi-sample filtering. Consequently, VoiceCraft-X reliably generates high-quality speech in a single pass without needing this filtering step. In all experiments, we employ nucleus sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">holtzman2019curious</span>)</cite> with <math alttext=\"TopK=20,TopP=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><mo>=</mo><mn>20</mn></mrow><mo>,</mo><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow><mo>=</mo><mn>1.0</mn></mrow></mrow><annotation encoding=\"application/x-tex\">TopK=20,TopP=1.0</annotation></semantics></math>, and a temperature of 1.</p>\n\n",
                "matched_terms": [
                    "original",
                    "editing",
                    "voicecraft",
                    "voicecraftx",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the English and Chinese Zero-shot TTS tasks, we compared our model with FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite>, MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>)</cite>, and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite>. For English, we also included VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> in our comparison. For the remaining languages, we benchmarked our model against the multilingual XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite> model, considering both its v1 and v2 versions. For speech editing, we compared VoiceCraft-X with the original VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> model on English.</p>\n\n",
                "matched_terms": [
                    "original",
                    "english",
                    "editing",
                    "voicecraft",
                    "voicecraftx",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used a combination of subjective and objective measures. Objectively, we use Word Error Rate (WER) as an automatic proxy for the intelligibility of the synthesized speech; this is calculated using Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2023funasr</span>)</cite> for Chinese and Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> for other languages. Additionally, speaker similarity (SIM-o) is objectively measured by computing the cosine similarity of speaker embeddings, which are extracted from both the generated and original target speech using a WavLM-based speaker verification model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span>)</cite>. Subjective evaluations involved human annotators (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for details) who provide Comparative Mean Opinion Scores (CMOS) and Similarity Mean Opinion Scores (SMOS) for TTS, and Naturalness Mean Opinion Scores (NMOS) and Intelligibility Mean Opinion Scores (IMOS) for speech editing. For CMOS, evaluators assess the naturalness of the synthesized speech in comparison to the ground truth, while for SMOS, they directly score the similarity between the synthesized speech and the initial speech prompt. For NMOS and IMOS, evaluators respectively assess the naturalness and intelligibility of the synthesized and original speech.</p>\n\n",
                "matched_terms": [
                    "original",
                    "editing",
                    "wer",
                    "speech",
                    "nmos",
                    "imos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated VoiceCraft-X&#8217;s zero-shot TTS performance across 11 languages, and the results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T1\" title=\"Table 1 &#8227; Baselines. &#8227; 4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. For Chinese, VoiceCraft-X was trained on a modest <math alttext=\"5K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">5K</annotation></semantics></math> hours of data, a fraction of that used by leading models (often exceeding <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> hours). Consequently, while its CER of 3.29 was higher than these specialized models, this was achieved with substantially less data, and its speaker similarity and subjective scores reflected this data disparity. In English, VoiceCraft-X, trained on <math alttext=\"14K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>14</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">14K</annotation></semantics></math> hours, showed marked improvements over its predecessor, VoiceCraft, reducing its WER from 5.28 to 4.37 and enhancing SIM-o from 0.51 to 0.54. Critically, its CMOS score of 0.63<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The generally higher English CMOS scores likely resulted from using Seed-TTS test set as prompts with atypical, exaggerated intonation (not standard read speech).</span></span></span> was the highest among compared models, indicating superior perceived naturalness. While some models trained on significantly larger datasets achieved lower WERs, VoiceCraft-X&#8217;s subjective quality in English was highly competitive.</p>\n\n",
                "matched_terms": [
                    "english",
                    "wer",
                    "voicecraft",
                    "voicecraftx",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the remaining nine languages, VoiceCraft-X, compared to XTTS (versions v1 and v2), showed strong overall performance with varying focuses. VoiceCraft-X particularly excelled in European languages like German (WER significantly better than XTTS-v2 by over 50%), Spanish (WER over 40% better than XTTS-v2 and below the ground truth), and Italian (higher data efficiency), as well as in Korean (CER reduced by over 20%). However, in languages such as Japanese and Dutch, or for those where VoiceCraft-X had considerably less training data like Portuguese and Polish, XTTS-v2 achieved lower error rates. Nevertheless, VoiceCraft-X was often favored by evaluators for its better speaker similarity, naturalness, and intelligibility. (Further results are in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>).</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "performance",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the original multilingual model&#8217;s speaker similarity is significantly higher than models fine-tuned from other checkpoints for nearly all languages. This indicates that joint training on diverse linguistic data, leveraging collective data volume, allows the model to disentangle speaker-specific characteristics from language-specific features. This robust performance across varied languages suggests it learns a more abstract, shared representation space for speech, facilitating both high-fidelity synthesis and strong cross-lingual capabilities. While fine-tuning on single language data may impact this disentanglement ability, as evidenced by SIM-o drops in many such cases.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "original",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For multilingual speech editing in other languages&#8212;a capability where comparative baselines are notably scarce as most models do not support multilingual editing&#8212;we conducted subjective MOS evaluations. These evaluations focused on a subset of languages (French, Italian, Portuguese, and Spanish) for which MTurk annotators were available, with results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T4\" title=\"Table 4 &#8227; 4.4 Speech Editing &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The evaluations demonstrate VoiceCraft-X&#8217;s effective performance in this challenging scenario. While naturalness (NMOS) scores for edited speech are, as anticipated, lower than the original recordings, intelligibility (IMOS) remains high across these languages. Particularly for Spanish and Italian, where edited NMOS and IMOS scores closely matched the original audio, these findings underscore VoiceCraft-X&#8217;s significant and unique capability for coherent, comprehensible multilingual speech editing.</p>\n\n",
                "matched_terms": [
                    "original",
                    "editing",
                    "performance",
                    "speech",
                    "nmos",
                    "imos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present VoiceCraft-X, an autoregressive neural codec language model that successfully unifies multilingual speech editing and Text-to-Speech (TTS) synthesis. Leveraging the Qwen3 LLM and a novel token reordering strategy, VoiceCraft-X supports eleven languages, producing high-quality, natural-sounding speech. Our model demonstrates robust performance across diverse conditions and shows that a unified framework can effectively advance both speech editing and synthesis in multilingual contexts, even with limited data for some languages. This work underscores the potential of autoregressive models for complex, real-world speech generation tasks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraftx",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One key limitation is the scale of our training data. Although VoiceCraft-X performs well with approximately 32,578 hours across eleven languages, this is notably less than some state-of-the-art models. This comparative data scarcity, particularly for lower-resource languages in our set, may limit the model&#8217;s capacity to capture the full spectrum of speech nuances as effectively as systems trained on more extensive datasets.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraftx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, further investigation into model size scalability is also warranted. The current VoiceCraft-X utilizes the Qwen3-0.6B architecture; exploring larger model variants could unlock enhanced learning capabilities and higher fidelity in speech synthesis and editing. Systematically assessing different model sizes is crucial for optimizing the balance between performance improvements and computational demands.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraftx",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of advanced speech models like VoiceCraft-X, which possesses strong zero-shot voice cloning and multilingual editing capabilities, carries significant ethical responsibilities. We acknowledge the potential for misuse of this technology. Malicious actors could exploit it for unauthorized voice cloning, impersonation, the creation of convincing deepfakes for fraudulent purposes, or the generation of misinformation and propaganda. These risks are particularly pronounced given the model&#8217;s ability to operate across eleven languages, broadening the potential scope for misuse on a global scale.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraftx",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To create a comprehensive evaluation set for speech editing, we began by selecting a subset of samples from the Text-to-Speech (TTS) evaluation datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.SS1\" title=\"4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. For each language, 100-300 original text samples were chosen.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "original",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike RealEdit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite>, which relies on manual, sentence-by-sentence human annotation and modification, a process that limits its scalability across many languages, we employed the powerful multilingual capabilities of the Gemini language model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to systematically introduce textual modifications to the original sentences. The goal was to generate edited versions that reflect common editing scenarios. To achieve this, Gemini was instructed to perform exactly one of the following specified operations on each original sentence:</p>\n\n",
                "matched_terms": [
                    "original",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X is trained as an autoregressive model to predict a sequence of neural codec tokens. Given the input context, which includes text tokens, speaker embeddings, and potentially prefix/suffix audio tokens, the model predicts the target audio tokens one by one. The overall training objective is a weighted cross-entropy loss, designed to enhance learning efficiency and focus on the crucial aspects of the speech generation task.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "voicecraftx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let the sequence of all ground truth speech tokens (encompassing prefix, suffix, and middle segments, and structured according to the delay pattern described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.SS5\" title=\"3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>) be denoted by <math alttext=\"Z=(z_{1},z_{2},\\dots,z_{N})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=(z_{1},z_{2},\\dots,z_{N})</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the total number of tokens in the flattened sequence. Each token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> in this sequence corresponds to an original codec token <math alttext=\"Y_{t_{i},k_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>Y</mi><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msub><annotation encoding=\"application/x-tex\">Y_{t_{i},k_{i}}</annotation></semantics></math> from timestep <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math> and the <math alttext=\"k_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>k</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">k_{i}</annotation></semantics></math>-th codebook of the EnCodec output (where <math alttext=\"K=4\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">K=4</annotation></semantics></math> is the total number of codebooks). The model predicts the probability distribution for each token <math alttext=\"\\hat{z}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\hat{z}_{i}</annotation></semantics></math> conditioned on previous tokens and the input context.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To compute our subjective evaluation metrics (SMOS and CMOS for TTS, NMOS and IMOS for Speech Editing), for all languages except Chinese, we recruited Amazon Mechanical Turk workers with a minimum approval rate of <math alttext=\"98\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>98</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">98\\%</annotation></semantics></math> and at least <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> successful HITs. We manually recruited university students for Chinese. We filtered workers by the following countries in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3.T7\" title=\"Table 7 &#8227; C.1 Setup &#8227; Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> for each of our languages:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "nmos",
                    "editing",
                    "imos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For this ablation study, considering the low-resource nature of most languages, we used LibriTTS-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koizumi2023libritts</span>)</cite> and the WenetSpeech4TTS Premium&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts</span>)</cite> subset as training data. LibriTTS-R contains 585 hours of speech, while the WenetSpeech4TTS Premium subset includes 601 hours<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>YouTube clips are removed.</span></span></span>. Models were trained for 15 epochs, both with and without the reordering mechanism. The final epoch was then evaluated on the Seed-TTS test set. As can be seen from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T9\" title=\"Table 9 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the model using the reordering mechanism shows significant performance improvements across all objective evaluation metrics on both the English and Chinese datasets. Specifically, the WER for English dropped dramatically from 104.02 to 11.60, and the CER for Chinese also decreased sharply from 262.25 to 19.25. Concurrently, the SIM-o scores for both languages also showed noticeable increases, indicating an improvement in the quality and naturalness of the synthesized speech. These results strongly demonstrate that the reordering mechanism is very effective in training under low-resource scenarios.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance",
                    "english",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation, based on WER and SIM-o, revealed that placing the prompt at the beginning of the \"middle\" segment yields the most favorable overall performance. Specifically, structuring the input such that the prompt text precedes the target text within the middle text segment (i.e., <math alttext=\"T_{P}=\\varnothing,T_{S}=\\varnothing,T_{M}=(T_{prompt},T_{target})\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>T</mi><mi>P</mi></msub><mo>=</mo><mi mathvariant=\"normal\">&#8709;</mi></mrow><mo>,</mo><mrow><mrow><msub><mi>T</mi><mi>S</mi></msub><mo>=</mo><mi mathvariant=\"normal\">&#8709;</mi></mrow><mo>,</mo><mrow><msub><mi>T</mi><mi>M</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>T</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">T_{P}=\\varnothing,T_{S}=\\varnothing,T_{M}=(T_{prompt},T_{target})</annotation></semantics></math>, with <math alttext=\"A_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{prompt}</annotation></semantics></math> appended after the mask tokens and before where <math alttext=\"A_{target}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{target}</annotation></semantics></math> would be generated) resulted in a WER of 4.37, which is notably better than the alternative placements.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "wer"
                ]
            }
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Subjective performance on speech editing.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Original</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Edited</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">NMOS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">IMOS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">NMOS</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">IMOS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">French</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.13</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">3.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Italian</span></td>\n<td class=\"ltx_td ltx_align_center\">4.38</td>\n<td class=\"ltx_td ltx_align_center\">4.78</td>\n<td class=\"ltx_td ltx_align_center\">3.77</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">4.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Portuguese</span></td>\n<td class=\"ltx_td ltx_align_center\">4.42</td>\n<td class=\"ltx_td ltx_align_center\">4.98</td>\n<td class=\"ltx_td ltx_align_center\">2.63</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">3.78</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">Spanish</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.58</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">3.78</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "spanish",
            "original",
            "editing",
            "imos",
            "edited",
            "portuguese",
            "french",
            "speech",
            "subjective",
            "nmos",
            "italian",
            "performance"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For multilingual speech editing in other languages&#8212;a capability where comparative baselines are notably scarce as most models do not support multilingual editing&#8212;we conducted subjective MOS evaluations. These evaluations focused on a subset of languages (French, Italian, Portuguese, and Spanish) for which MTurk annotators were available, with results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T4\" title=\"Table 4 &#8227; 4.4 Speech Editing &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The evaluations demonstrate VoiceCraft-X&#8217;s effective performance in this challenging scenario. While naturalness (NMOS) scores for edited speech are, as anticipated, lower than the original recordings, intelligibility (IMOS) remains high across these languages. Particularly for Spanish and Italian, where edited NMOS and IMOS scores closely matched the original audio, these findings underscore VoiceCraft-X&#8217;s significant and unique capability for coherent, comprehensible multilingual speech editing.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://zhishengzheng.com/voicecraft-x/\" title=\"\">https://zhishengzheng.com/voicecraft-x/</a>.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "editing",
                    "portuguese",
                    "french",
                    "speech",
                    "italian",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis \n<br class=\"ltx_break\"/>and Speech Editing</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Highly realistic speech generation is an indispensable technology for voice assistants, content dubbing, accessibility tools, and creative media. Speech generation can be broken down into several sub-problems: <em class=\"ltx_emph ltx_font_italic\">creating</em> new audio via Text-To-Speech synthesis (TTS) or <em class=\"ltx_emph ltx_font_italic\">editing</em> part of an existing recording while ensuring voice consistency with the remainder of the original speech. Despite their shared goal of producing natural speech, TTS and speech editing are typically treated as <em class=\"ltx_emph ltx_font_italic\">separate</em> problems, especially in multilingual settings, which leaves practitioners without a <em class=\"ltx_emph ltx_font_italic\">single</em> model that can both edit and synthesize speech across languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "original",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past several years, the quality of TTS models has improved significantly, particularly in the zero-shot setting in which a model generates speech in a new speaker&#8217;s voice given a short (e.g. 3 second) audio prompt. Transformer-based neural networks have been central to this progress, leading to three broad paradigms: (i) autoregressive (AR), (ii) non-autoregressive (Non-AR), and (iii) hybrid models.\nAR models, such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> and its successors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025pseudo</span>)</cite>, generate frame-level speech tokens sequentially, where the tokens are typically derived from a neural audio codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeghidour2021soundstream</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speechtokenizer</span>)</cite>. These models are able to perform voice-cloning TTS via Transformer language models&#8217; in-context learning ability, demonstrating high-quality speech synthesis. Non-AR models include flow-matching models such as F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, as well as diffusion models such as NaturalSpeech 2/3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>. These models predict all tokens representing an utterance in parallel via iterative refinement. Hybrid approaches such as Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> aim to combine the strengths of both paradigms.\nWhile these models deliver impressive zero-shot quality, most of the models are either monolingual or focus on a handful of high-resource languages such as English and Chinese. This is likely due to the fact that these models are data-hungry, often requiring 10K-100K hours of training speech for SOTA performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quest for broader linguistic inclusivity across the world&#8217;s 7,000 spoken languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Eberhard2024</span>)</cite> has driven research in multilingual speech generation. Efforts include curating large corpora (e.g., VoxPopuliTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025voxpopulitts</span>)</cite>, Fish-Speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2024fish</span>)</cite>) and training multilingual TTS architectures like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite>, CLAM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024clam</span>)</cite> and XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite>.\nYet even the most capable multilingual systems treat <em class=\"ltx_emph ltx_font_italic\">speech editing</em> as a separate task&#8212;or ignore it altogether&#8212;leaving users without a unified solution.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper we address this gap, by introducing <span class=\"ltx_text ltx_font_bold\">VoiceCraft-X</span>, a unified autoregressive neural codec language model that performs <em class=\"ltx_emph ltx_font_italic\">both</em> speech editing and zero-shot TTS in <span class=\"ltx_text ltx_font_bold\">11 languages</span>: English (en), Mandarin (zh), Korean (ko), Japanese (ja), Spanish (es), French (fr), German (de), Dutch (nl), Italian (it), Portuguese (pt) and Polish (pl).\nOur contributions are threefold:</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "editing",
                    "portuguese",
                    "french",
                    "speech",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, a single autoregressive model that unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) across 11 languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach leverages the Qwen3 large language model for cross-lingual text processing, without the need for phonetic pronunciation lexicons. We also propose a novel token reordering mechanism that time-aligns text and speech, enabling a unified sequence generation approach for both editing and synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech editing aims to correct mispronunciations, stutters, or recording artifacts while producing speech that is indistinguishable from natural audio. Recent approaches leverage Transformer and diffusion architectures. <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">borsos2022speechpainter</span></cite> perform audio infilling with a Transformer that maintains speaker identity and prosody, generalizing to unseen speakers. &#160;<cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span></cite> use flow matching for versatile speech infilling, and <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span></cite> show that a neural-codec language model with token infilling can concurrently handle editing and synthesis. F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> extend this idea with flow-matching or diffusion, respectively. Despite these advances, most works are monolingual, motivating a unified multilingual solution.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Autoregressive (AR) models generate speech tokens sequentially. VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> pioneered neural codec language models for high-quality zero-shot TTS via in-context learning, with subsequent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kharitonov2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lajszczak2024base</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite> further refining this paradigm. Non-Autoregressive (Non-AR) models aim for faster generation by predicting tokens in parallel or using iterative refinement. Examples include flow-matching models like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> and diffusion-based models such as NaturalSpeech 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>)</cite>, NaturalSpeech 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>, and DiTTo-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lee2024ditto</span>)</cite>. Other notable non-AR approaches include Unicats&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024unicats</span>)</cite>, SimpleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech2</span>)</cite>, E2-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite> and Mega-TTS 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2025megatts</span>)</cite>. Hybrid systems combine aspects of both AR and non-AR methods. Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> uses a two-stage architecture, while CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> also represent efforts to balance quality, speed, and controllability. In this work, VoiceCraft-X follows the codec language modeling method of VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> and enables high-quality, zero-shot multilingual speech synthesis within its unified editing and generation framework.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the data side, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">saeki2024extending</span></cite> show that pairing self-supervised speech representations with unsupervised text alignment scales TTS to 100&#8201;+ languages, even when only scant transcriptions exist. Large curated corpora amplify these gains: VoxPopuliTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025voxpopulitts</span>)</cite> refines 30,000 hours of English, French and Spanish speech; Fish-Speech &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2024fish</span>)</cite> goes further, training on 720,000 hours while using an LLM to sidestep language-specific G2P rules. Model architectures have evolved in parallel. VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> adopts non-autoregressive flow matching, delivering cross-lingual zero-shot TTS in six languages via in-context learning. XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite>, extending Tortoise&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">betker2023better</span>)</cite>, combines a Perceiver Resampler with a speaker-consistency loss to reach 16 languages with speaker cloning. CLAM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024clam</span>)</cite> improves codec language model compression with probabilistic residual vector quantization, enabling single-step multi-token generation. However, these models often treat synthesis as a distinct task from speech editing. The challenge of <em class=\"ltx_emph ltx_font_italic\">unifying</em> high-quality, multilingual speech editing with robust multilingual speech synthesis within a single, open-source, and fully autoregressive model architecture remains largely unaddressed.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "french",
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X employs several token reordering steps, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, to unify speech editing and synthesis. We assume that our training examples consist of utterance waveforms accompanied by time-aligned word transcriptions (we use the Montreal Forced Aligner (MFA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mcauliffe2017montreal</span>)</cite> in our work). During training, a text transcription is randomly segmented into prefix, middle, and suffix portions. These are then rearranged into a \"prefix-suffix-middle\" sequence, where the \"middle\" segment serves as the prediction target. Finally, the corresponding speech tokens for each segment are reordered identically based on the alignment timings. This ensures a monotonic alignment between the text and speech tokens, even when performing speech edits which require infilling tokens in the middle of the speech sequence. This rearrangement serves to mirror the use case in which a user wishes to modify some, but not all of the words in an utterance - by using this rearrangement, the model can be trained to predict the speech tokens within the middle of an utterance, conditioned on the preceding (prefix) and following (suffix) speech tokens in addition to the desired text transcription.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens. The system then autoregressively generates the neural codec tokens for the target audio segment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We combined speech data across public datasets over 11 languages, amounting to a total of approximately 32K hours (detailed statistics provided in Appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS1\" title=\"A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>). The sampling rate for all audio is 16 kHz. Audio segments longer than 25 seconds were discarded. For MLS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite>, misalignment issues were particularly prominent, with approximately 20% of samples having extra or missing words in the transcript at the beginning or end. We found that this negatively impacted model performance for English, and subsequently removed utterances whose transcriptions differed significantly from those produced by the Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> model. While we found similar problems with the non-English European language data in MLS, we anecdotally observed better performance on those languages without performing this filtering. We speculate that this is due to the fact that the amount of available training data for those languages is already relatively low, and the performance improvements brought by the additional training data outweigh the detriments brought by transcription noise.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluating Text-to-Speech (TTS) performance, we curated an evaluation dataset from several established benchmarks. For English, we utilized the Seed-TTS test-en set&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> (1088 samples sourced from Common Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2019common</span>)</cite>). For Mandarin, we employed the Seed-TTS test-zh set (2020 samples from DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2021didispeech</span>)</cite>). Korean and Japanese evaluations were conducted using 200 randomly selected samples from KsponSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bang2020ksponspeech</span>)</cite> and KokoroSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Iida2021Kokoro</span>)</cite>, respectively. For the remaining seven languages supported by our model (Spanish, French, German, Dutch, Italian, Portuguese, and Polish), we randomly selected 100 samples for each language from their corresponding Multilingual LibriSpeech (MLS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite> test sets. To evaluate speech editing, we randomly selected 100-300 samples per language from these TTS test datasets and then utilized Gemini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to perform insertion, deletion, or substitution operations on the textual portions of these samples, with specific details available in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS2\" title=\"A.2 Speech Editing Dataset &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. We conducted subjective evaluation over a subset of languages (English, Chinese, French, Italian, Portuguese, and Spanish) using a random subset of the evaluation set: 40 English samples, 50 Chinese, and 20 for others.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "editing",
                    "portuguese",
                    "french",
                    "speech",
                    "subjective",
                    "italian",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens; the model then autoregressively predicts the corresponding neural codec tokens for the target audio segment. Notably, the token reordering mechanism significantly enhances inference stability. This largely prevents repeating token loops, an issue in the original VoiceCraft which could cause artifacts (e.g., excessive silences) and required multi-sample filtering. Consequently, VoiceCraft-X reliably generates high-quality speech in a single pass without needing this filtering step. In all experiments, we employ nucleus sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">holtzman2019curious</span>)</cite> with <math alttext=\"TopK=20,TopP=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><mo>=</mo><mn>20</mn></mrow><mo>,</mo><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow><mo>=</mo><mn>1.0</mn></mrow></mrow><annotation encoding=\"application/x-tex\">TopK=20,TopP=1.0</annotation></semantics></math>, and a temperature of 1.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "original",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the English and Chinese Zero-shot TTS tasks, we compared our model with FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite>, MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>)</cite>, and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite>. For English, we also included VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> in our comparison. For the remaining languages, we benchmarked our model against the multilingual XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite> model, considering both its v1 and v2 versions. For speech editing, we compared VoiceCraft-X with the original VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> model on English.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "original",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used a combination of subjective and objective measures. Objectively, we use Word Error Rate (WER) as an automatic proxy for the intelligibility of the synthesized speech; this is calculated using Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2023funasr</span>)</cite> for Chinese and Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> for other languages. Additionally, speaker similarity (SIM-o) is objectively measured by computing the cosine similarity of speaker embeddings, which are extracted from both the generated and original target speech using a WavLM-based speaker verification model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span>)</cite>. Subjective evaluations involved human annotators (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for details) who provide Comparative Mean Opinion Scores (CMOS) and Similarity Mean Opinion Scores (SMOS) for TTS, and Naturalness Mean Opinion Scores (NMOS) and Intelligibility Mean Opinion Scores (IMOS) for speech editing. For CMOS, evaluators assess the naturalness of the synthesized speech in comparison to the ground truth, while for SMOS, they directly score the similarity between the synthesized speech and the initial speech prompt. For NMOS and IMOS, evaluators respectively assess the naturalness and intelligibility of the synthesized and original speech.</p>\n\n",
                "matched_terms": [
                    "original",
                    "editing",
                    "speech",
                    "subjective",
                    "nmos",
                    "imos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated VoiceCraft-X&#8217;s zero-shot TTS performance across 11 languages, and the results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T1\" title=\"Table 1 &#8227; Baselines. &#8227; 4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. For Chinese, VoiceCraft-X was trained on a modest <math alttext=\"5K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">5K</annotation></semantics></math> hours of data, a fraction of that used by leading models (often exceeding <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> hours). Consequently, while its CER of 3.29 was higher than these specialized models, this was achieved with substantially less data, and its speaker similarity and subjective scores reflected this data disparity. In English, VoiceCraft-X, trained on <math alttext=\"14K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>14</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">14K</annotation></semantics></math> hours, showed marked improvements over its predecessor, VoiceCraft, reducing its WER from 5.28 to 4.37 and enhancing SIM-o from 0.51 to 0.54. Critically, its CMOS score of 0.63<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The generally higher English CMOS scores likely resulted from using Seed-TTS test set as prompts with atypical, exaggerated intonation (not standard read speech).</span></span></span> was the highest among compared models, indicating superior perceived naturalness. While some models trained on significantly larger datasets achieved lower WERs, VoiceCraft-X&#8217;s subjective quality in English was highly competitive.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "subjective",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the remaining nine languages, VoiceCraft-X, compared to XTTS (versions v1 and v2), showed strong overall performance with varying focuses. VoiceCraft-X particularly excelled in European languages like German (WER significantly better than XTTS-v2 by over 50%), Spanish (WER over 40% better than XTTS-v2 and below the ground truth), and Italian (higher data efficiency), as well as in Korean (CER reduced by over 20%). However, in languages such as Japanese and Dutch, or for those where VoiceCraft-X had considerably less training data like Portuguese and Polish, XTTS-v2 achieved lower error rates. Nevertheless, VoiceCraft-X was often favored by evaluators for its better speaker similarity, naturalness, and intelligibility. (Further results are in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>).</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "italian",
                    "performance",
                    "portuguese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The universal advantage of pre-training over &#8220;from Scratch&#8221; models is paramount, especially for languages with limited data. For instance, Italian (294 hours) and Polish (139 hours) saw their WERs plummet from over 140 and 160 to under 14 and 20 respectively, demonstrating pre-training&#8217;s crucial role in transferring foundational knowledge and overcoming data scarcity. Even higher-resource languages like Spanish, French and German benefited significantly. Fine-tuning from an English model initialization proved highly effective for European languages (Germanic, Romance, Slavic), leveraging linguistic similarities and robust acoustic modeling, with gains particularly vital for low-data scenarios (Italian, Portuguese, Polish). Korean showed better CER with a Japanese checkpoint (42.08) than Chinese (49.11), aligning with typological closeness. Conversely, Japanese experienced negative transfer from Chinese (CER 36.18 vs. 22.36 from scratch).</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "french",
                    "italian",
                    "portuguese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, fine-tuning from the &#8220;multilingual checkpoint&#8221; frequently yielded superior WER/CER compared to an English-only checkpoint for a range of languages including Spanish, Dutch, Italian, Portuguese, Polish, and Japanese. This advantage held across varying data volumes (e.g., Polish 139 hours, Japanese 3489 hours), suggesting that pre-training on a diverse linguistic set fosters more generalized and transferable representations than exposure to English alone, capturing a broader array of phonetic and prosodic patterns.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "italian",
                    "portuguese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the original multilingual model&#8217;s speaker similarity is significantly higher than models fine-tuned from other checkpoints for nearly all languages. This indicates that joint training on diverse linguistic data, leveraging collective data volume, allows the model to disentangle speaker-specific characteristics from language-specific features. This robust performance across varied languages suggests it learns a more abstract, shared representation space for speech, facilitating both high-fidelity synthesis and strong cross-lingual capabilities. While fine-tuning on single language data may impact this disentanglement ability, as evidenced by SIM-o drops in many such cases.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "original",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English speech editing (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Speech Editing &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), VoiceCraft-X demonstrated a better Word Error Rate (WER) than VoiceCraft. Both models produced edited speech that listeners found to be highly natural (NMOS) and intelligible (IMOS), comparable to the original recordings. VoiceCraft&#8217;s slightly higher scores in these subjective tests are not surprising, given its monolingual English focus, especially considering both models have similar parameter counts and amounts of English training data.</p>\n\n",
                "matched_terms": [
                    "original",
                    "editing",
                    "edited",
                    "speech",
                    "subjective",
                    "nmos",
                    "imos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present VoiceCraft-X, an autoregressive neural codec language model that successfully unifies multilingual speech editing and Text-to-Speech (TTS) synthesis. Leveraging the Qwen3 LLM and a novel token reordering strategy, VoiceCraft-X supports eleven languages, producing high-quality, natural-sounding speech. Our model demonstrates robust performance across diverse conditions and shows that a unified framework can effectively advance both speech editing and synthesis in multilingual contexts, even with limited data for some languages. This work underscores the potential of autoregressive models for complex, real-world speech generation tasks.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, further investigation into model size scalability is also warranted. The current VoiceCraft-X utilizes the Qwen3-0.6B architecture; exploring larger model variants could unlock enhanced learning capabilities and higher fidelity in speech synthesis and editing. Systematically assessing different model sizes is crucial for optimizing the balance between performance improvements and computational demands.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of advanced speech models like VoiceCraft-X, which possesses strong zero-shot voice cloning and multilingual editing capabilities, carries significant ethical responsibilities. We acknowledge the potential for misuse of this technology. Malicious actors could exploit it for unauthorized voice cloning, impersonation, the creation of convincing deepfakes for fraudulent purposes, or the generation of misinformation and propaganda. These risks are particularly pronounced given the model&#8217;s ability to operate across eleven languages, broadening the potential scope for misuse on a global scale.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To create a comprehensive evaluation set for speech editing, we began by selecting a subset of samples from the Text-to-Speech (TTS) evaluation datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.SS1\" title=\"4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. For each language, 100-300 original text samples were chosen.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "original",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike RealEdit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite>, which relies on manual, sentence-by-sentence human annotation and modification, a process that limits its scalability across many languages, we employed the powerful multilingual capabilities of the Gemini language model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to systematically introduce textual modifications to the original sentences. The goal was to generate edited versions that reflect common editing scenarios. To achieve this, Gemini was instructed to perform exactly one of the following specified operations on each original sentence:</p>\n\n",
                "matched_terms": [
                    "edited",
                    "original",
                    "editing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let the sequence of all ground truth speech tokens (encompassing prefix, suffix, and middle segments, and structured according to the delay pattern described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.SS5\" title=\"3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>) be denoted by <math alttext=\"Z=(z_{1},z_{2},\\dots,z_{N})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=(z_{1},z_{2},\\dots,z_{N})</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the total number of tokens in the flattened sequence. Each token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> in this sequence corresponds to an original codec token <math alttext=\"Y_{t_{i},k_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>Y</mi><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msub><annotation encoding=\"application/x-tex\">Y_{t_{i},k_{i}}</annotation></semantics></math> from timestep <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math> and the <math alttext=\"k_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>k</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">k_{i}</annotation></semantics></math>-th codebook of the EnCodec output (where <math alttext=\"K=4\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">K=4</annotation></semantics></math> is the total number of codebooks). The model predicts the probability distribution for each token <math alttext=\"\\hat{z}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\hat{z}_{i}</annotation></semantics></math> conditioned on previous tokens and the input context.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To compute our subjective evaluation metrics (SMOS and CMOS for TTS, NMOS and IMOS for Speech Editing), for all languages except Chinese, we recruited Amazon Mechanical Turk workers with a minimum approval rate of <math alttext=\"98\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>98</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">98\\%</annotation></semantics></math> and at least <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> successful HITs. We manually recruited university students for Chinese. We filtered workers by the following countries in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3.T7\" title=\"Table 7 &#8227; C.1 Setup &#8227; Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> for each of our languages:</p>\n\n",
                "matched_terms": [
                    "editing",
                    "speech",
                    "subjective",
                    "nmos",
                    "imos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A scarcity of Amazon Mechanical Turk workers for less common languages prevented us from collecting subjective evaluation results for all targeted languages. Consequently, the SMOS results for French, Italian, Portuguese, and Spanish on the Zero-Shot TTS task that we were able to gather are detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3.T8\" title=\"Table 8 &#8227; C.2 Additional Results &#8227; Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "portuguese",
                    "french",
                    "subjective",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For this ablation study, considering the low-resource nature of most languages, we used LibriTTS-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koizumi2023libritts</span>)</cite> and the WenetSpeech4TTS Premium&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts</span>)</cite> subset as training data. LibriTTS-R contains 585 hours of speech, while the WenetSpeech4TTS Premium subset includes 601 hours<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>YouTube clips are removed.</span></span></span>. Models were trained for 15 epochs, both with and without the reordering mechanism. The final epoch was then evaluated on the Seed-TTS test set. As can be seen from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T9\" title=\"Table 9 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the model using the reordering mechanism shows significant performance improvements across all objective evaluation metrics on both the English and Chinese datasets. Specifically, the WER for English dropped dramatically from 104.02 to 11.60, and the CER for Chinese also decreased sharply from 262.25 to 19.25. Concurrently, the SIM-o scores for both languages also showed noticeable increases, indicating an improvement in the quality and naturalness of the synthesized speech. These results strongly demonstrate that the reordering mechanism is very effective in training under low-resource scenarios.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            }
        ]
    },
    "A1.T5": {
        "caption": "Table 5: Speech-corpus statistics used for training\n(total: 32 578 h).",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">MLS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">CML-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">oliveira2023cml</span>)</cite>\n</td>\n</tr>\n</table> \n",
        "informative_terms_identified": [
            "pratap2020mls",
            "cmltts",
            "total",
            "statistics",
            "mls",
            "training",
            "speechcorpus",
            "used",
            "oliveira2023cml"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The training datasets for each language are as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.T5\" title=\"Table 5 &#8227; A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. For all of them, we remove all YouTube clips.</p>\n\n",
            "<p class=\"ltx_p\">The Encodec model we employ operates with a stride of 320 samples, corresponding to a codec frame rate of 50 Hz when processing audio recorded at 16 kHz. Its encoder begins with a base channel dimension of 64, which doubles at each of the five successive convolutional layers. Following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>)</cite>, we utilize the open-source audiocraft repository<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/facebookresearch/audiocraft/blob/main/docs/ENCODEC.md\" title=\"\">https://github.com/facebookresearch/audiocraft/blob/main/docs/ENCODEC.md</a></span></span></span> for training. Specifically, we sample one-second speech segments from the multilingual dataset (shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.T5\" title=\"Table 5 &#8227; A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) and train for 200 epochs with a batch size of 832. Optimization is performed using the Adam algorithm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kingma2014adam</span>)</cite> with a base learning rate of 5e-5.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We combined speech data across public datasets over 11 languages, amounting to a total of approximately 32K hours (detailed statistics provided in Appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS1\" title=\"A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>). The sampling rate for all audio is 16 kHz. Audio segments longer than 25 seconds were discarded. For MLS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite>, misalignment issues were particularly prominent, with approximately 20% of samples having extra or missing words in the transcript at the beginning or end. We found that this negatively impacted model performance for English, and subsequently removed utterances whose transcriptions differed significantly from those produced by the Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> model. While we found similar problems with the non-English European language data in MLS, we anecdotally observed better performance on those languages without performing this filtering. We speculate that this is due to the fact that the amount of available training data for those languages is already relatively low, and the performance improvements brought by the additional training data outweigh the detriments brought by transcription noise.</p>\n\n",
                "matched_terms": [
                    "pratap2020mls",
                    "total",
                    "statistics",
                    "mls",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluating Text-to-Speech (TTS) performance, we curated an evaluation dataset from several established benchmarks. For English, we utilized the Seed-TTS test-en set&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> (1088 samples sourced from Common Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2019common</span>)</cite>). For Mandarin, we employed the Seed-TTS test-zh set (2020 samples from DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2021didispeech</span>)</cite>). Korean and Japanese evaluations were conducted using 200 randomly selected samples from KsponSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bang2020ksponspeech</span>)</cite> and KokoroSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Iida2021Kokoro</span>)</cite>, respectively. For the remaining seven languages supported by our model (Spanish, French, German, Dutch, Italian, Portuguese, and Polish), we randomly selected 100 samples for each language from their corresponding Multilingual LibriSpeech (MLS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite> test sets. To evaluate speech editing, we randomly selected 100-300 samples per language from these TTS test datasets and then utilized Gemini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to perform insertion, deletion, or substitution operations on the textual portions of these samples, with specific details available in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS2\" title=\"A.2 Speech Editing Dataset &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. We conducted subjective evaluation over a subset of languages (English, Chinese, French, Italian, Portuguese, and Spanish) using a random subset of the evaluation set: 40 English samples, 50 Chinese, and 20 for others.</p>\n\n",
                "matched_terms": [
                    "pratap2020mls",
                    "mls"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use Qwen3-0.6B-Base as both the text tokenizer and the Transformer LM backbone (details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A2.SS2\" title=\"B.2 Qwen3 Base Model &#8227; Appendix B Implementational Details &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). The outputs from the final Transformer layer are then projected into four distinct linear layers, each producing the logits for one of the codec tokens. The model comprises 613 million total parameters (457 million excluding embeddings). The codebook weights <math alttext=\"\\boldsymbol{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\alpha}</annotation></semantics></math> are set to <math alttext=\"(1.0,0.8,0.6,0.4)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mn>1.0</mn><mo>,</mo><mn>0.8</mn><mo>,</mo><mn>0.6</mn><mo>,</mo><mn>0.4</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(1.0,0.8,0.6,0.4)</annotation></semantics></math>, influencing the contribution of each codebook during training (as further detailed in our loss formulation&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A2.SS3\" title=\"B.3 Loss Design &#8227; Appendix B Implementational Details &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>). For model training, we employ the AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">loshchilov2017decoupled</span>)</cite> with a learning rate of <math alttext=\"4\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-3}</annotation></semantics></math>, <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, an epsilon of <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m6\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>, and a weight decay of <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m7\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>. A learning rate scheduler is utilized, featuring a linear warm-up for the initial <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m8\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> steps, followed by a linear decay for the remainder of the <math alttext=\"5,000K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m9\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>,</mo><mrow><mn>000</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">5,000K</annotation></semantics></math> total training steps. Gradient accumulation is performed over <math alttext=\"8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m10\" intent=\":literal\"><semantics><mn>8</mn><annotation encoding=\"application/x-tex\">8</annotation></semantics></math> micro-batches. The training of the multilingual VoiceCraft-X model took approximately one week on 16 NVIDIA A100 40GB GPUs.</p>\n\n",
                "matched_terms": [
                    "total",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For this ablation study, considering the low-resource nature of most languages, we used LibriTTS-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koizumi2023libritts</span>)</cite> and the WenetSpeech4TTS Premium&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts</span>)</cite> subset as training data. LibriTTS-R contains 585 hours of speech, while the WenetSpeech4TTS Premium subset includes 601 hours<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>YouTube clips are removed.</span></span></span>. Models were trained for 15 epochs, both with and without the reordering mechanism. The final epoch was then evaluated on the Seed-TTS test set. As can be seen from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T9\" title=\"Table 9 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the model using the reordering mechanism shows significant performance improvements across all objective evaluation metrics on both the English and Chinese datasets. Specifically, the WER for English dropped dramatically from 104.02 to 11.60, and the CER for Chinese also decreased sharply from 262.25 to 19.25. Concurrently, the SIM-o scores for both languages also showed noticeable increases, indicating an improvement in the quality and naturalness of the synthesized speech. These results strongly demonstrate that the reordering mechanism is very effective in training under low-resource scenarios.</p>\n\n",
                "matched_terms": [
                    "used",
                    "training"
                ]
            }
        ]
    },
    "A3.T7": {
        "caption": "Table 7: Countries used to filter crowdworkers for each language",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:260.2pt;\"><span class=\"ltx_text ltx_font_bold\">Countries</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">English</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:260.2pt;\">United States</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Chinese</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:260.2pt;\">China</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">French</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:260.2pt;\">Belgium, Canada, France, Luxembourg, Switzerland</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Italian</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:260.2pt;\">Italy</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Portuguese</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:260.2pt;\">Brazil, Portugal</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Spanish</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:260.2pt;\">Argentina, Chile, Colombia, Mexico, Spain, United States</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "each",
            "united",
            "italian",
            "luxembourg",
            "crowdworkers",
            "brazil",
            "switzerland",
            "french",
            "france",
            "argentina",
            "mexico",
            "used",
            "colombia",
            "filter",
            "english",
            "portuguese",
            "language",
            "china",
            "belgium",
            "countries",
            "spanish",
            "chile",
            "portugal",
            "spain",
            "canada",
            "states",
            "italy",
            "chinese"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To compute our subjective evaluation metrics (SMOS and CMOS for TTS, NMOS and IMOS for Speech Editing), for all languages except Chinese, we recruited Amazon Mechanical Turk workers with a minimum approval rate of <math alttext=\"98\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>98</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">98\\%</annotation></semantics></math> and at least <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> successful HITs. We manually recruited university students for Chinese. We filtered workers by the following countries in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3.T7\" title=\"Table 7 &#8227; C.1 Setup &#8227; Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> for each of our languages:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://zhishengzheng.com/voicecraft-x/\" title=\"\">https://zhishengzheng.com/voicecraft-x/</a>.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "english",
                    "portuguese",
                    "language",
                    "french",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past several years, the quality of TTS models has improved significantly, particularly in the zero-shot setting in which a model generates speech in a new speaker&#8217;s voice given a short (e.g. 3 second) audio prompt. Transformer-based neural networks have been central to this progress, leading to three broad paradigms: (i) autoregressive (AR), (ii) non-autoregressive (Non-AR), and (iii) hybrid models.\nAR models, such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> and its successors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025pseudo</span>)</cite>, generate frame-level speech tokens sequentially, where the tokens are typically derived from a neural audio codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeghidour2021soundstream</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speechtokenizer</span>)</cite>. These models are able to perform voice-cloning TTS via Transformer language models&#8217; in-context learning ability, demonstrating high-quality speech synthesis. Non-AR models include flow-matching models such as F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, as well as diffusion models such as NaturalSpeech 2/3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>. These models predict all tokens representing an utterance in parallel via iterative refinement. Hybrid approaches such as Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> aim to combine the strengths of both paradigms.\nWhile these models deliver impressive zero-shot quality, most of the models are either monolingual or focus on a handful of high-resource languages such as English and Chinese. This is likely due to the fact that these models are data-hungry, often requiring 10K-100K hours of training speech for SOTA performance.</p>\n\n",
                "matched_terms": [
                    "language",
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper we address this gap, by introducing <span class=\"ltx_text ltx_font_bold\">VoiceCraft-X</span>, a unified autoregressive neural codec language model that performs <em class=\"ltx_emph ltx_font_italic\">both</em> speech editing and zero-shot TTS in <span class=\"ltx_text ltx_font_bold\">11 languages</span>: English (en), Mandarin (zh), Korean (ko), Japanese (ja), Spanish (es), French (fr), German (de), Dutch (nl), Italian (it), Portuguese (pt) and Polish (pl).\nOur contributions are threefold:</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "english",
                    "portuguese",
                    "language",
                    "french",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the data side, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">saeki2024extending</span></cite> show that pairing self-supervised speech representations with unsupervised text alignment scales TTS to 100&#8201;+ languages, even when only scant transcriptions exist. Large curated corpora amplify these gains: VoxPopuliTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025voxpopulitts</span>)</cite> refines 30,000 hours of English, French and Spanish speech; Fish-Speech &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2024fish</span>)</cite> goes further, training on 720,000 hours while using an LLM to sidestep language-specific G2P rules. Model architectures have evolved in parallel. VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> adopts non-autoregressive flow matching, delivering cross-lingual zero-shot TTS in six languages via in-context learning. XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite>, extending Tortoise&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">betker2023better</span>)</cite>, combines a Perceiver Resampler with a speaker-consistency loss to reach 16 languages with speaker cloning. CLAM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024clam</span>)</cite> improves codec language model compression with probabilistic residual vector quantization, enabling single-step multi-token generation. However, these models often treat synthesis as a distinct task from speech editing. The challenge of <em class=\"ltx_emph ltx_font_italic\">unifying</em> high-quality, multilingual speech editing with robust multilingual speech synthesis within a single, open-source, and fully autoregressive model architecture remains largely unaddressed.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "language",
                    "french",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X evolves VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> into a truly multilingual speech-editing and synthesis system, treating both tasks as a single sequence-generation problem over neural codec tokens. The core of this system, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Overview &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, is the Qwen3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3</span>)</cite> large language model. Qwen3 natively supports text input in <math alttext=\"119\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>119</mn><annotation encoding=\"application/x-tex\">119</annotation></semantics></math> languages and dialects, which we leverage as the cross-lingual input text tokenizer for VoiceCraft-X. This eliminates the cumbersome phoneme-conversion step that was integral to the original VoiceCraft, resulting in a simplified pipeline with a shared tokenizer across languages, without the need to curate pronunciation lexicons for each language.</p>\n\n",
                "matched_terms": [
                    "language",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We combined speech data across public datasets over 11 languages, amounting to a total of approximately 32K hours (detailed statistics provided in Appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS1\" title=\"A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>). The sampling rate for all audio is 16 kHz. Audio segments longer than 25 seconds were discarded. For MLS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite>, misalignment issues were particularly prominent, with approximately 20% of samples having extra or missing words in the transcript at the beginning or end. We found that this negatively impacted model performance for English, and subsequently removed utterances whose transcriptions differed significantly from those produced by the Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> model. While we found similar problems with the non-English European language data in MLS, we anecdotally observed better performance on those languages without performing this filtering. We speculate that this is due to the fact that the amount of available training data for those languages is already relatively low, and the performance improvements brought by the additional training data outweigh the detriments brought by transcription noise.</p>\n\n",
                "matched_terms": [
                    "language",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluating Text-to-Speech (TTS) performance, we curated an evaluation dataset from several established benchmarks. For English, we utilized the Seed-TTS test-en set&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> (1088 samples sourced from Common Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2019common</span>)</cite>). For Mandarin, we employed the Seed-TTS test-zh set (2020 samples from DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2021didispeech</span>)</cite>). Korean and Japanese evaluations were conducted using 200 randomly selected samples from KsponSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bang2020ksponspeech</span>)</cite> and KokoroSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Iida2021Kokoro</span>)</cite>, respectively. For the remaining seven languages supported by our model (Spanish, French, German, Dutch, Italian, Portuguese, and Polish), we randomly selected 100 samples for each language from their corresponding Multilingual LibriSpeech (MLS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite> test sets. To evaluate speech editing, we randomly selected 100-300 samples per language from these TTS test datasets and then utilized Gemini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to perform insertion, deletion, or substitution operations on the textual portions of these samples, with specific details available in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS2\" title=\"A.2 Speech Editing Dataset &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. We conducted subjective evaluation over a subset of languages (English, Chinese, French, Italian, Portuguese, and Spanish) using a random subset of the evaluation set: 40 English samples, 50 Chinese, and 20 for others.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "english",
                    "portuguese",
                    "language",
                    "french",
                    "each",
                    "chinese",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the English and Chinese Zero-shot TTS tasks, we compared our model with FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite>, MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>)</cite>, and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite>. For English, we also included VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> in our comparison. For the remaining languages, we benchmarked our model against the multilingual XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite> model, considering both its v1 and v2 versions. For speech editing, we compared VoiceCraft-X with the original VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> model on English.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used a combination of subjective and objective measures. Objectively, we use Word Error Rate (WER) as an automatic proxy for the intelligibility of the synthesized speech; this is calculated using Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2023funasr</span>)</cite> for Chinese and Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> for other languages. Additionally, speaker similarity (SIM-o) is objectively measured by computing the cosine similarity of speaker embeddings, which are extracted from both the generated and original target speech using a WavLM-based speaker verification model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span>)</cite>. Subjective evaluations involved human annotators (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for details) who provide Comparative Mean Opinion Scores (CMOS) and Similarity Mean Opinion Scores (SMOS) for TTS, and Naturalness Mean Opinion Scores (NMOS) and Intelligibility Mean Opinion Scores (IMOS) for speech editing. For CMOS, evaluators assess the naturalness of the synthesized speech in comparison to the ground truth, while for SMOS, they directly score the similarity between the synthesized speech and the initial speech prompt. For NMOS and IMOS, evaluators respectively assess the naturalness and intelligibility of the synthesized and original speech.</p>\n\n",
                "matched_terms": [
                    "used",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated VoiceCraft-X&#8217;s zero-shot TTS performance across 11 languages, and the results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T1\" title=\"Table 1 &#8227; Baselines. &#8227; 4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. For Chinese, VoiceCraft-X was trained on a modest <math alttext=\"5K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">5K</annotation></semantics></math> hours of data, a fraction of that used by leading models (often exceeding <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> hours). Consequently, while its CER of 3.29 was higher than these specialized models, this was achieved with substantially less data, and its speaker similarity and subjective scores reflected this data disparity. In English, VoiceCraft-X, trained on <math alttext=\"14K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>14</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">14K</annotation></semantics></math> hours, showed marked improvements over its predecessor, VoiceCraft, reducing its WER from 5.28 to 4.37 and enhancing SIM-o from 0.51 to 0.54. Critically, its CMOS score of 0.63<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The generally higher English CMOS scores likely resulted from using Seed-TTS test set as prompts with atypical, exaggerated intonation (not standard read speech).</span></span></span> was the highest among compared models, indicating superior perceived naturalness. While some models trained on significantly larger datasets achieved lower WERs, VoiceCraft-X&#8217;s subjective quality in English was highly competitive.</p>\n\n",
                "matched_terms": [
                    "used",
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the remaining nine languages, VoiceCraft-X, compared to XTTS (versions v1 and v2), showed strong overall performance with varying focuses. VoiceCraft-X particularly excelled in European languages like German (WER significantly better than XTTS-v2 by over 50%), Spanish (WER over 40% better than XTTS-v2 and below the ground truth), and Italian (higher data efficiency), as well as in Korean (CER reduced by over 20%). However, in languages such as Japanese and Dutch, or for those where VoiceCraft-X had considerably less training data like Portuguese and Polish, XTTS-v2 achieved lower error rates. Nevertheless, VoiceCraft-X was often favored by evaluators for its better speaker similarity, naturalness, and intelligibility. (Further results are in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>).</p>\n\n",
                "matched_terms": [
                    "portuguese",
                    "italian",
                    "spanish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The universal advantage of pre-training over &#8220;from Scratch&#8221; models is paramount, especially for languages with limited data. For instance, Italian (294 hours) and Polish (139 hours) saw their WERs plummet from over 140 and 160 to under 14 and 20 respectively, demonstrating pre-training&#8217;s crucial role in transferring foundational knowledge and overcoming data scarcity. Even higher-resource languages like Spanish, French and German benefited significantly. Fine-tuning from an English model initialization proved highly effective for European languages (Germanic, Romance, Slavic), leveraging linguistic similarities and robust acoustic modeling, with gains particularly vital for low-data scenarios (Italian, Portuguese, Polish). Korean showed better CER with a Japanese checkpoint (42.08) than Chinese (49.11), aligning with typological closeness. Conversely, Japanese experienced negative transfer from Chinese (CER 36.18 vs. 22.36 from scratch).</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "english",
                    "portuguese",
                    "french",
                    "chinese",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, fine-tuning from the &#8220;multilingual checkpoint&#8221; frequently yielded superior WER/CER compared to an English-only checkpoint for a range of languages including Spanish, Dutch, Italian, Portuguese, Polish, and Japanese. This advantage held across varying data volumes (e.g., Polish 139 hours, Japanese 3489 hours), suggesting that pre-training on a diverse linguistic set fosters more generalized and transferable representations than exposure to English alone, capturing a broader array of phonetic and prosodic patterns.</p>\n\n",
                "matched_terms": [
                    "portuguese",
                    "spanish",
                    "italian",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For multilingual speech editing in other languages&#8212;a capability where comparative baselines are notably scarce as most models do not support multilingual editing&#8212;we conducted subjective MOS evaluations. These evaluations focused on a subset of languages (French, Italian, Portuguese, and Spanish) for which MTurk annotators were available, with results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T4\" title=\"Table 4 &#8227; 4.4 Speech Editing &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The evaluations demonstrate VoiceCraft-X&#8217;s effective performance in this challenging scenario. While naturalness (NMOS) scores for edited speech are, as anticipated, lower than the original recordings, intelligibility (IMOS) remains high across these languages. Particularly for Spanish and Italian, where edited NMOS and IMOS scores closely matched the original audio, these findings underscore VoiceCraft-X&#8217;s significant and unique capability for coherent, comprehensible multilingual speech editing.</p>\n\n",
                "matched_terms": [
                    "portuguese",
                    "french",
                    "italian",
                    "spanish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The training datasets for each language are as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.T5\" title=\"Table 5 &#8227; A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. For all of them, we remove all YouTube clips.</p>\n\n",
                "matched_terms": [
                    "language",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To create a comprehensive evaluation set for speech editing, we began by selecting a subset of samples from the Text-to-Speech (TTS) evaluation datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.SS1\" title=\"4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. For each language, 100-300 original text samples were chosen.</p>\n\n",
                "matched_terms": [
                    "language",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Unlike RealEdit&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite>, which relies on manual, sentence-by-sentence human annotation and modification, a process that limits its scalability across many languages, we employed the powerful multilingual capabilities of the Gemini language model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to systematically introduce textual modifications to the original sentences. The goal was to generate edited versions that reflect common editing scenarios. To achieve this, Gemini was instructed to perform exactly one of the following specified operations on each original sentence:</p>\n\n",
                "matched_terms": [
                    "language",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A scarcity of Amazon Mechanical Turk workers for less common languages prevented us from collecting subjective evaluation results for all targeted languages. Consequently, the SMOS results for French, Italian, Portuguese, and Spanish on the Zero-Shot TTS task that we were able to gather are detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3.T8\" title=\"Table 8 &#8227; C.2 Additional Results &#8227; Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "portuguese",
                    "french",
                    "italian",
                    "spanish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For this ablation study, considering the low-resource nature of most languages, we used LibriTTS-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koizumi2023libritts</span>)</cite> and the WenetSpeech4TTS Premium&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts</span>)</cite> subset as training data. LibriTTS-R contains 585 hours of speech, while the WenetSpeech4TTS Premium subset includes 601 hours<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>YouTube clips are removed.</span></span></span>. Models were trained for 15 epochs, both with and without the reordering mechanism. The final epoch was then evaluated on the Seed-TTS test set. As can be seen from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T9\" title=\"Table 9 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the model using the reordering mechanism shows significant performance improvements across all objective evaluation metrics on both the English and Chinese datasets. Specifically, the WER for English dropped dramatically from 104.02 to 11.60, and the CER for Chinese also decreased sharply from 262.25 to 19.25. Concurrently, the SIM-o scores for both languages also showed noticeable increases, indicating an improvement in the quality and naturalness of the synthesized speech. These results strongly demonstrate that the reordering mechanism is very effective in training under low-resource scenarios.</p>\n\n",
                "matched_terms": [
                    "used",
                    "chinese",
                    "english"
                ]
            }
        ]
    },
    "A3.T8": {
        "caption": "Table 8: SMOS on Zero-Shot TTS.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">French</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Italian</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Portuguese</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Spanish</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Ground Truth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.15</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">3.42</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">XTTS-v1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.63</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">2.83</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">XTTS-v2</td>\n<td class=\"ltx_td ltx_align_center\">2.23</td>\n<td class=\"ltx_td ltx_align_center\">2.75</td>\n<td class=\"ltx_td ltx_align_center\">2.48</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">3.22</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">VoiceCraft-X</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">2.87</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">3.58</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "spanish",
            "xttsv1",
            "xttsv2",
            "zeroshot",
            "tts",
            "portuguese",
            "french",
            "model",
            "truth",
            "voicecraftx",
            "smos",
            "italian",
            "ground"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">A scarcity of Amazon Mechanical Turk workers for less common languages prevented us from collecting subjective evaluation results for all targeted languages. Consequently, the SMOS results for French, Italian, Portuguese, and Spanish on the Zero-Shot TTS task that we were able to gather are detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3.T8\" title=\"Table 8 &#8227; C.2 Additional Results &#8227; Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://zhishengzheng.com/voicecraft-x/\" title=\"\">https://zhishengzheng.com/voicecraft-x/</a>.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "zeroshot",
                    "tts",
                    "portuguese",
                    "french",
                    "model",
                    "voicecraftx",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Highly realistic speech generation is an indispensable technology for voice assistants, content dubbing, accessibility tools, and creative media. Speech generation can be broken down into several sub-problems: <em class=\"ltx_emph ltx_font_italic\">creating</em> new audio via Text-To-Speech synthesis (TTS) or <em class=\"ltx_emph ltx_font_italic\">editing</em> part of an existing recording while ensuring voice consistency with the remainder of the original speech. Despite their shared goal of producing natural speech, TTS and speech editing are typically treated as <em class=\"ltx_emph ltx_font_italic\">separate</em> problems, especially in multilingual settings, which leaves practitioners without a <em class=\"ltx_emph ltx_font_italic\">single</em> model that can both edit and synthesize speech across languages.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past several years, the quality of TTS models has improved significantly, particularly in the zero-shot setting in which a model generates speech in a new speaker&#8217;s voice given a short (e.g. 3 second) audio prompt. Transformer-based neural networks have been central to this progress, leading to three broad paradigms: (i) autoregressive (AR), (ii) non-autoregressive (Non-AR), and (iii) hybrid models.\nAR models, such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> and its successors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025pseudo</span>)</cite>, generate frame-level speech tokens sequentially, where the tokens are typically derived from a neural audio codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeghidour2021soundstream</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speechtokenizer</span>)</cite>. These models are able to perform voice-cloning TTS via Transformer language models&#8217; in-context learning ability, demonstrating high-quality speech synthesis. Non-AR models include flow-matching models such as F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, as well as diffusion models such as NaturalSpeech 2/3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>. These models predict all tokens representing an utterance in parallel via iterative refinement. Hybrid approaches such as Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> aim to combine the strengths of both paradigms.\nWhile these models deliver impressive zero-shot quality, most of the models are either monolingual or focus on a handful of high-resource languages such as English and Chinese. This is likely due to the fact that these models are data-hungry, often requiring 10K-100K hours of training speech for SOTA performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper we address this gap, by introducing <span class=\"ltx_text ltx_font_bold\">VoiceCraft-X</span>, a unified autoregressive neural codec language model that performs <em class=\"ltx_emph ltx_font_italic\">both</em> speech editing and zero-shot TTS in <span class=\"ltx_text ltx_font_bold\">11 languages</span>: English (en), Mandarin (zh), Korean (ko), Japanese (ja), Spanish (es), French (fr), German (de), Dutch (nl), Italian (it), Portuguese (pt) and Polish (pl).\nOur contributions are threefold:</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "zeroshot",
                    "tts",
                    "portuguese",
                    "french",
                    "model",
                    "voicecraftx",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, a single autoregressive model that unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) across 11 languages.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voicecraftx",
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot Text-to-Speech (TTS) synthesis task entails generating speech in a new speaker&#8217;s voice from a short audio prompt, without assuming that the new speaker was seen during training. Recent progress is largely driven by Transformer-based neural networks, falling into autoregressive (AR), non-autoregressive (non-AR), and hybrid.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Autoregressive (AR) models generate speech tokens sequentially. VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> pioneered neural codec language models for high-quality zero-shot TTS via in-context learning, with subsequent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kharitonov2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lajszczak2024base</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite> further refining this paradigm. Non-Autoregressive (Non-AR) models aim for faster generation by predicting tokens in parallel or using iterative refinement. Examples include flow-matching models like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> and diffusion-based models such as NaturalSpeech 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>)</cite>, NaturalSpeech 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>, and DiTTo-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lee2024ditto</span>)</cite>. Other notable non-AR approaches include Unicats&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024unicats</span>)</cite>, SimpleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech2</span>)</cite>, E2-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite> and Mega-TTS 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2025megatts</span>)</cite>. Hybrid systems combine aspects of both AR and non-AR methods. Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> uses a two-stage architecture, while CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> also represent efforts to balance quality, speed, and controllability. In this work, VoiceCraft-X follows the codec language modeling method of VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> and enables high-quality, zero-shot multilingual speech synthesis within its unified editing and generation framework.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the data side, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">saeki2024extending</span></cite> show that pairing self-supervised speech representations with unsupervised text alignment scales TTS to 100&#8201;+ languages, even when only scant transcriptions exist. Large curated corpora amplify these gains: VoxPopuliTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025voxpopulitts</span>)</cite> refines 30,000 hours of English, French and Spanish speech; Fish-Speech &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2024fish</span>)</cite> goes further, training on 720,000 hours while using an LLM to sidestep language-specific G2P rules. Model architectures have evolved in parallel. VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> adopts non-autoregressive flow matching, delivering cross-lingual zero-shot TTS in six languages via in-context learning. XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite>, extending Tortoise&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">betker2023better</span>)</cite>, combines a Perceiver Resampler with a speaker-consistency loss to reach 16 languages with speaker cloning. CLAM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024clam</span>)</cite> improves codec language model compression with probabilistic residual vector quantization, enabling single-step multi-token generation. However, these models often treat synthesis as a distinct task from speech editing. The challenge of <em class=\"ltx_emph ltx_font_italic\">unifying</em> high-quality, multilingual speech editing with robust multilingual speech synthesis within a single, open-source, and fully autoregressive model architecture remains largely unaddressed.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "zeroshot",
                    "tts",
                    "french",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X evolves VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> into a truly multilingual speech-editing and synthesis system, treating both tasks as a single sequence-generation problem over neural codec tokens. The core of this system, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F1\" title=\"Figure 1 &#8227; 3.1 Overview &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, is the Qwen3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">qwen3</span>)</cite> large language model. Qwen3 natively supports text input in <math alttext=\"119\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mn>119</mn><annotation encoding=\"application/x-tex\">119</annotation></semantics></math> languages and dialects, which we leverage as the cross-lingual input text tokenizer for VoiceCraft-X. This eliminates the cumbersome phoneme-conversion step that was integral to the original VoiceCraft, resulting in a simplified pipeline with a shared tokenizer across languages, without the need to curate pronunciation lexicons for each language.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X employs several token reordering steps, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, to unify speech editing and synthesis. We assume that our training examples consist of utterance waveforms accompanied by time-aligned word transcriptions (we use the Montreal Forced Aligner (MFA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mcauliffe2017montreal</span>)</cite> in our work). During training, a text transcription is randomly segmented into prefix, middle, and suffix portions. These are then rearranged into a \"prefix-suffix-middle\" sequence, where the \"middle\" segment serves as the prediction target. Finally, the corresponding speech tokens for each segment are reordered identically based on the alignment timings. This ensures a monotonic alignment between the text and speech tokens, even when performing speech edits which require infilling tokens in the middle of the speech sequence. This rearrangement serves to mirror the use case in which a user wishes to modify some, but not all of the words in an utterance - by using this rearrangement, the model can be trained to predict the speech tokens within the middle of an utterance, conditioned on the preceding (prefix) and following (suffix) speech tokens in addition to the desired text transcription.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens. The system then autoregressively generates the neural codec tokens for the target audio segment.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluating Text-to-Speech (TTS) performance, we curated an evaluation dataset from several established benchmarks. For English, we utilized the Seed-TTS test-en set&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> (1088 samples sourced from Common Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2019common</span>)</cite>). For Mandarin, we employed the Seed-TTS test-zh set (2020 samples from DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2021didispeech</span>)</cite>). Korean and Japanese evaluations were conducted using 200 randomly selected samples from KsponSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bang2020ksponspeech</span>)</cite> and KokoroSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Iida2021Kokoro</span>)</cite>, respectively. For the remaining seven languages supported by our model (Spanish, French, German, Dutch, Italian, Portuguese, and Polish), we randomly selected 100 samples for each language from their corresponding Multilingual LibriSpeech (MLS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite> test sets. To evaluate speech editing, we randomly selected 100-300 samples per language from these TTS test datasets and then utilized Gemini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to perform insertion, deletion, or substitution operations on the textual portions of these samples, with specific details available in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS2\" title=\"A.2 Speech Editing Dataset &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. We conducted subjective evaluation over a subset of languages (English, Chinese, French, Italian, Portuguese, and Spanish) using a random subset of the evaluation set: 40 English samples, 50 Chinese, and 20 for others.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "tts",
                    "portuguese",
                    "french",
                    "model",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use Qwen3-0.6B-Base as both the text tokenizer and the Transformer LM backbone (details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A2.SS2\" title=\"B.2 Qwen3 Base Model &#8227; Appendix B Implementational Details &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). The outputs from the final Transformer layer are then projected into four distinct linear layers, each producing the logits for one of the codec tokens. The model comprises 613 million total parameters (457 million excluding embeddings). The codebook weights <math alttext=\"\\boldsymbol{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\alpha}</annotation></semantics></math> are set to <math alttext=\"(1.0,0.8,0.6,0.4)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mn>1.0</mn><mo>,</mo><mn>0.8</mn><mo>,</mo><mn>0.6</mn><mo>,</mo><mn>0.4</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(1.0,0.8,0.6,0.4)</annotation></semantics></math>, influencing the contribution of each codebook during training (as further detailed in our loss formulation&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A2.SS3\" title=\"B.3 Loss Design &#8227; Appendix B Implementational Details &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>). For model training, we employ the AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">loshchilov2017decoupled</span>)</cite> with a learning rate of <math alttext=\"4\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-3}</annotation></semantics></math>, <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, an epsilon of <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m6\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>, and a weight decay of <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m7\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>. A learning rate scheduler is utilized, featuring a linear warm-up for the initial <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m8\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> steps, followed by a linear decay for the remainder of the <math alttext=\"5,000K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m9\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>,</mo><mrow><mn>000</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">5,000K</annotation></semantics></math> total training steps. Gradient accumulation is performed over <math alttext=\"8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m10\" intent=\":literal\"><semantics><mn>8</mn><annotation encoding=\"application/x-tex\">8</annotation></semantics></math> micro-batches. The training of the multilingual VoiceCraft-X model took approximately one week on 16 NVIDIA A100 40GB GPUs.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens; the model then autoregressively predicts the corresponding neural codec tokens for the target audio segment. Notably, the token reordering mechanism significantly enhances inference stability. This largely prevents repeating token loops, an issue in the original VoiceCraft which could cause artifacts (e.g., excessive silences) and required multi-sample filtering. Consequently, VoiceCraft-X reliably generates high-quality speech in a single pass without needing this filtering step. In all experiments, we employ nucleus sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">holtzman2019curious</span>)</cite> with <math alttext=\"TopK=20,TopP=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><mo>=</mo><mn>20</mn></mrow><mo>,</mo><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow><mo>=</mo><mn>1.0</mn></mrow></mrow><annotation encoding=\"application/x-tex\">TopK=20,TopP=1.0</annotation></semantics></math>, and a temperature of 1.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voicecraftx",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the English and Chinese Zero-shot TTS tasks, we compared our model with FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite>, MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>)</cite>, and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite>. For English, we also included VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> in our comparison. For the remaining languages, we benchmarked our model against the multilingual XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite> model, considering both its v1 and v2 versions. For speech editing, we compared VoiceCraft-X with the original VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> model on English.</p>\n\n",
                "matched_terms": [
                    "model",
                    "voicecraftx",
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used a combination of subjective and objective measures. Objectively, we use Word Error Rate (WER) as an automatic proxy for the intelligibility of the synthesized speech; this is calculated using Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2023funasr</span>)</cite> for Chinese and Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> for other languages. Additionally, speaker similarity (SIM-o) is objectively measured by computing the cosine similarity of speaker embeddings, which are extracted from both the generated and original target speech using a WavLM-based speaker verification model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span>)</cite>. Subjective evaluations involved human annotators (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for details) who provide Comparative Mean Opinion Scores (CMOS) and Similarity Mean Opinion Scores (SMOS) for TTS, and Naturalness Mean Opinion Scores (NMOS) and Intelligibility Mean Opinion Scores (IMOS) for speech editing. For CMOS, evaluators assess the naturalness of the synthesized speech in comparison to the ground truth, while for SMOS, they directly score the similarity between the synthesized speech and the initial speech prompt. For NMOS and IMOS, evaluators respectively assess the naturalness and intelligibility of the synthesized and original speech.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "truth",
                    "smos",
                    "ground"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated VoiceCraft-X&#8217;s zero-shot TTS performance across 11 languages, and the results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T1\" title=\"Table 1 &#8227; Baselines. &#8227; 4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. For Chinese, VoiceCraft-X was trained on a modest <math alttext=\"5K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">5K</annotation></semantics></math> hours of data, a fraction of that used by leading models (often exceeding <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> hours). Consequently, while its CER of 3.29 was higher than these specialized models, this was achieved with substantially less data, and its speaker similarity and subjective scores reflected this data disparity. In English, VoiceCraft-X, trained on <math alttext=\"14K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>14</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">14K</annotation></semantics></math> hours, showed marked improvements over its predecessor, VoiceCraft, reducing its WER from 5.28 to 4.37 and enhancing SIM-o from 0.51 to 0.54. Critically, its CMOS score of 0.63<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The generally higher English CMOS scores likely resulted from using Seed-TTS test set as prompts with atypical, exaggerated intonation (not standard read speech).</span></span></span> was the highest among compared models, indicating superior perceived naturalness. While some models trained on significantly larger datasets achieved lower WERs, VoiceCraft-X&#8217;s subjective quality in English was highly competitive.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the remaining nine languages, VoiceCraft-X, compared to XTTS (versions v1 and v2), showed strong overall performance with varying focuses. VoiceCraft-X particularly excelled in European languages like German (WER significantly better than XTTS-v2 by over 50%), Spanish (WER over 40% better than XTTS-v2 and below the ground truth), and Italian (higher data efficiency), as well as in Korean (CER reduced by over 20%). However, in languages such as Japanese and Dutch, or for those where VoiceCraft-X had considerably less training data like Portuguese and Polish, XTTS-v2 achieved lower error rates. Nevertheless, VoiceCraft-X was often favored by evaluators for its better speaker similarity, naturalness, and intelligibility. (Further results are in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>).</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "xttsv2",
                    "portuguese",
                    "voicecraftx",
                    "italian",
                    "ground",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The universal advantage of pre-training over &#8220;from Scratch&#8221; models is paramount, especially for languages with limited data. For instance, Italian (294 hours) and Polish (139 hours) saw their WERs plummet from over 140 and 160 to under 14 and 20 respectively, demonstrating pre-training&#8217;s crucial role in transferring foundational knowledge and overcoming data scarcity. Even higher-resource languages like Spanish, French and German benefited significantly. Fine-tuning from an English model initialization proved highly effective for European languages (Germanic, Romance, Slavic), leveraging linguistic similarities and robust acoustic modeling, with gains particularly vital for low-data scenarios (Italian, Portuguese, Polish). Korean showed better CER with a Japanese checkpoint (42.08) than Chinese (49.11), aligning with typological closeness. Conversely, Japanese experienced negative transfer from Chinese (CER 36.18 vs. 22.36 from scratch).</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "portuguese",
                    "french",
                    "model",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, fine-tuning from the &#8220;multilingual checkpoint&#8221; frequently yielded superior WER/CER compared to an English-only checkpoint for a range of languages including Spanish, Dutch, Italian, Portuguese, Polish, and Japanese. This advantage held across varying data volumes (e.g., Polish 139 hours, Japanese 3489 hours), suggesting that pre-training on a diverse linguistic set fosters more generalized and transferable representations than exposure to English alone, capturing a broader array of phonetic and prosodic patterns.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "italian",
                    "portuguese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For multilingual speech editing in other languages&#8212;a capability where comparative baselines are notably scarce as most models do not support multilingual editing&#8212;we conducted subjective MOS evaluations. These evaluations focused on a subset of languages (French, Italian, Portuguese, and Spanish) for which MTurk annotators were available, with results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T4\" title=\"Table 4 &#8227; 4.4 Speech Editing &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The evaluations demonstrate VoiceCraft-X&#8217;s effective performance in this challenging scenario. While naturalness (NMOS) scores for edited speech are, as anticipated, lower than the original recordings, intelligibility (IMOS) remains high across these languages. Particularly for Spanish and Italian, where edited NMOS and IMOS scores closely matched the original audio, these findings underscore VoiceCraft-X&#8217;s significant and unique capability for coherent, comprehensible multilingual speech editing.</p>\n\n",
                "matched_terms": [
                    "spanish",
                    "french",
                    "italian",
                    "portuguese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present VoiceCraft-X, an autoregressive neural codec language model that successfully unifies multilingual speech editing and Text-to-Speech (TTS) synthesis. Leveraging the Qwen3 LLM and a novel token reordering strategy, VoiceCraft-X supports eleven languages, producing high-quality, natural-sounding speech. Our model demonstrates robust performance across diverse conditions and shows that a unified framework can effectively advance both speech editing and synthesis in multilingual contexts, even with limited data for some languages. This work underscores the potential of autoregressive models for complex, real-world speech generation tasks.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, further investigation into model size scalability is also warranted. The current VoiceCraft-X utilizes the Qwen3-0.6B architecture; exploring larger model variants could unlock enhanced learning capabilities and higher fidelity in speech synthesis and editing. Systematically assessing different model sizes is crucial for optimizing the balance between performance improvements and computational demands.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The development of advanced speech models like VoiceCraft-X, which possesses strong zero-shot voice cloning and multilingual editing capabilities, carries significant ethical responsibilities. We acknowledge the potential for misuse of this technology. Malicious actors could exploit it for unauthorized voice cloning, impersonation, the creation of convincing deepfakes for fraudulent purposes, or the generation of misinformation and propaganda. These risks are particularly pronounced given the model&#8217;s ability to operate across eleven languages, broadening the potential scope for misuse on a global scale.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot nature of VoiceCraft-X lowers the barrier to entry for creating high-fidelity synthetic audio, making it accessible to a wider range of actors beyond those with specialized technical expertise. This accessibility amplifies the dual-use nature of the technology; while it empowers creativity and accessibility, it also provides a powerful tool for deception.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Qwen3-0.6B-Base model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Qwen/Qwen3-0.6B-Base\" title=\"\">https://huggingface.co/Qwen/Qwen3-0.6B-Base</a></span></span></span>, foundational to VoiceCraft-X, is a causal language model with 0.6 billion total parameters, of which 0.44 billion are non-embedding parameters. It features 28 Transformer layers, a hidden dimension of 1024, and a feed-forward network (FFN) dimension of 3072, along with 16 attention heads. The model employs Grouped-Query Attention (16 query heads and 8 key/value heads) and supports a context length of 32,768 tokens. A key factor in its suitability for VoiceCraft-X&#8217;s multilingual requirements is its pre-training on 36 trillion tokens across 119 languages. This pre-training utilized a diverse, high-quality data mix that included multilingual texts, books, and synthetic data. Furthermore, the model incorporates architectural refinements such as <span class=\"ltx_text ltx_font_italic\">qk layernorm</span> and benefits from a three-stage pre-training process designed for robust long-context handling.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X is trained as an autoregressive model to predict a sequence of neural codec tokens. Given the input context, which includes text tokens, speaker embeddings, and potentially prefix/suffix audio tokens, the model predicts the target audio tokens one by one. The overall training objective is a weighted cross-entropy loss, designed to enhance learning efficiency and focus on the crucial aspects of the speech generation task.</p>\n\n",
                "matched_terms": [
                    "voicecraftx",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let the sequence of all ground truth speech tokens (encompassing prefix, suffix, and middle segments, and structured according to the delay pattern described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.SS5\" title=\"3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>) be denoted by <math alttext=\"Z=(z_{1},z_{2},\\dots,z_{N})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=(z_{1},z_{2},\\dots,z_{N})</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the total number of tokens in the flattened sequence. Each token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> in this sequence corresponds to an original codec token <math alttext=\"Y_{t_{i},k_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>Y</mi><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msub><annotation encoding=\"application/x-tex\">Y_{t_{i},k_{i}}</annotation></semantics></math> from timestep <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math> and the <math alttext=\"k_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>k</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">k_{i}</annotation></semantics></math>-th codebook of the EnCodec output (where <math alttext=\"K=4\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">K=4</annotation></semantics></math> is the total number of codebooks). The model predicts the probability distribution for each token <math alttext=\"\\hat{z}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\hat{z}_{i}</annotation></semantics></math> conditioned on previous tokens and the input context.</p>\n\n",
                "matched_terms": [
                    "model",
                    "ground",
                    "truth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To compute our subjective evaluation metrics (SMOS and CMOS for TTS, NMOS and IMOS for Speech Editing), for all languages except Chinese, we recruited Amazon Mechanical Turk workers with a minimum approval rate of <math alttext=\"98\\%\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>98</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">98\\%</annotation></semantics></math> and at least <math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS1.p1.m2\" intent=\":literal\"><semantics><mn>1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math> successful HITs. We manually recruited university students for Chinese. We filtered workers by the following countries in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3.T7\" title=\"Table 7 &#8227; C.1 Setup &#8227; Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> for each of our languages:</p>\n\n",
                "matched_terms": [
                    "smos",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The token reordering mechanism, integral to our training methodology, introduces flexibility in how prompts are structured during zero-shot Text-to-Speech (TTS) inference. To determine the optimal placement, we evaluated several configurations for incorporating the prompt text (<math alttext=\"T_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{prompt}</annotation></semantics></math>) and prompt audio (<math alttext=\"A_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{prompt}</annotation></semantics></math>) into the input sequence. These configurations are detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T10\" title=\"Table 10 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A desirable characteristic of a multilingual Text-to-Speech (TTS) model is its ability to generate code-switched speech&#8212;that is, speech that fluidly transitions between languages. Although our model was trained exclusively on monolingual data, meaning code-switched speech is an out-of-distribution phenomenon for it, the model still demonstrated a certain capacity for code-switching without needing additional language identifiers for inputs in different languages.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess VoiceCraft-X&#8217;s adaptability and the impact of data quantity, we extended fine-tuning experiments across diverse languages. Building on cross-lingual transfer insights (Section&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.SS3\" title=\"4.3 Transfer Learning for Multilingual TTS &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>), we examined the correlation between per-language fine-tuning data volume and zero-shot Text-to-Speech (TTS) quality.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A6.F3\" title=\"Figure 3 &#8227; Appendix F Cross-lingual Finetuning Hours on Zero-Shot TTS &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates these findings, plotting per-language fine-tuning data volume (x-axis) against the relative Word Error Rate (WER) from zero-shot TTS (y-axis). This relative WER, the difference between Whisper&#8217;s WER on synthesized versus ground-truth audio, offers a normalized measure of intelligibility. The figure generally shows that more fine-tuning data improves pronunciation accuracy, especially for languages sharing similarities with VoiceCraft-X&#8217;s initial training set. However, this correlation is not universally linear. For languages like Korean and Thai, a moderate data increase (around 1000 hours) did not yield significant WER improvements. This plateauing suggests that for such languages, substantial gains may require much larger or more diverse datasets, or different fine-tuning approaches.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            }
        ]
    },
    "A4.T9": {
        "caption": "Table 9: Impact of token reordering in a low-resource scenario. Models were trained from scratch: one on English (585h LibriTTS-R), the other on Chinese (601h WenetSpeech4TTS Premium subset).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">English</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Chinese</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T9.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SIM-o<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T9.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">CER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T9.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SIM-o<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T9.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">w/o Reordering</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">104.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">262.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.29</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">w/ Reordering</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">11.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">19.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.46</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "wenetspeech4tts",
            "wer↓downarrow",
            "premium",
            "cer↓downarrow",
            "token",
            "lowresource",
            "from",
            "trained",
            "scenario",
            "english",
            "one",
            "librittsr",
            "impact",
            "601h",
            "other",
            "scratch",
            "subset",
            "reordering",
            "585h",
            "chinese",
            "simo↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For this ablation study, considering the low-resource nature of most languages, we used LibriTTS-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koizumi2023libritts</span>)</cite> and the WenetSpeech4TTS Premium&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts</span>)</cite> subset as training data. LibriTTS-R contains 585 hours of speech, while the WenetSpeech4TTS Premium subset includes 601 hours<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>YouTube clips are removed.</span></span></span>. Models were trained for 15 epochs, both with and without the reordering mechanism. The final epoch was then evaluated on the Seed-TTS test set. As can be seen from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T9\" title=\"Table 9 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the model using the reordering mechanism shows significant performance improvements across all objective evaluation metrics on both the English and Chinese datasets. Specifically, the WER for English dropped dramatically from 104.02 to 11.60, and the CER for Chinese also decreased sharply from 262.25 to 19.25. Concurrently, the SIM-o scores for both languages also showed noticeable increases, indicating an improvement in the quality and naturalness of the synthesized speech. These results strongly demonstrate that the reordering mechanism is very effective in training under low-resource scenarios.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://zhishengzheng.com/voicecraft-x/\" title=\"\">https://zhishengzheng.com/voicecraft-x/</a>.</p>\n\n",
                "matched_terms": [
                    "one",
                    "token",
                    "reordering",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past several years, the quality of TTS models has improved significantly, particularly in the zero-shot setting in which a model generates speech in a new speaker&#8217;s voice given a short (e.g. 3 second) audio prompt. Transformer-based neural networks have been central to this progress, leading to three broad paradigms: (i) autoregressive (AR), (ii) non-autoregressive (Non-AR), and (iii) hybrid models.\nAR models, such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> and its successors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025pseudo</span>)</cite>, generate frame-level speech tokens sequentially, where the tokens are typically derived from a neural audio codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeghidour2021soundstream</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speechtokenizer</span>)</cite>. These models are able to perform voice-cloning TTS via Transformer language models&#8217; in-context learning ability, demonstrating high-quality speech synthesis. Non-AR models include flow-matching models such as F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, as well as diffusion models such as NaturalSpeech 2/3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>. These models predict all tokens representing an utterance in parallel via iterative refinement. Hybrid approaches such as Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> aim to combine the strengths of both paradigms.\nWhile these models deliver impressive zero-shot quality, most of the models are either monolingual or focus on a handful of high-resource languages such as English and Chinese. This is likely due to the fact that these models are data-hungry, often requiring 10K-100K hours of training speech for SOTA performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "chinese",
                    "from",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our approach leverages the Qwen3 large language model for cross-lingual text processing, without the need for phonetic pronunciation lexicons. We also propose a novel token reordering mechanism that time-aligns text and speech, enabling a unified sequence generation approach for both editing and synthesis.</p>\n\n",
                "matched_terms": [
                    "reordering",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech editing aims to correct mispronunciations, stutters, or recording artifacts while producing speech that is indistinguishable from natural audio. Recent approaches leverage Transformer and diffusion architectures. <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">borsos2022speechpainter</span></cite> perform audio infilling with a Transformer that maintains speaker identity and prosody, generalizing to unseen speakers. &#160;<cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span></cite> use flow matching for versatile speech infilling, and <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span></cite> show that a neural-codec language model with token infilling can concurrently handle editing and synthesis. F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> extend this idea with flow-matching or diffusion, respectively. Despite these advances, most works are monolingual, motivating a unified multilingual solution.</p>\n\n",
                "matched_terms": [
                    "token",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Autoregressive (AR) models generate speech tokens sequentially. VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> pioneered neural codec language models for high-quality zero-shot TTS via in-context learning, with subsequent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kharitonov2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lajszczak2024base</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite> further refining this paradigm. Non-Autoregressive (Non-AR) models aim for faster generation by predicting tokens in parallel or using iterative refinement. Examples include flow-matching models like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> and diffusion-based models such as NaturalSpeech 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>)</cite>, NaturalSpeech 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>, and DiTTo-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lee2024ditto</span>)</cite>. Other notable non-AR approaches include Unicats&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024unicats</span>)</cite>, SimpleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech2</span>)</cite>, E2-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite> and Mega-TTS 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2025megatts</span>)</cite>. Hybrid systems combine aspects of both AR and non-AR methods. Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> uses a two-stage architecture, while CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> also represent efforts to balance quality, speed, and controllability. In this work, VoiceCraft-X follows the codec language modeling method of VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> and enables high-quality, zero-shot multilingual speech synthesis within its unified editing and generation framework.</p>\n\n",
                "matched_terms": [
                    "models",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the data side, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">saeki2024extending</span></cite> show that pairing self-supervised speech representations with unsupervised text alignment scales TTS to 100&#8201;+ languages, even when only scant transcriptions exist. Large curated corpora amplify these gains: VoxPopuliTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025voxpopulitts</span>)</cite> refines 30,000 hours of English, French and Spanish speech; Fish-Speech &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2024fish</span>)</cite> goes further, training on 720,000 hours while using an LLM to sidestep language-specific G2P rules. Model architectures have evolved in parallel. VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> adopts non-autoregressive flow matching, delivering cross-lingual zero-shot TTS in six languages via in-context learning. XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite>, extending Tortoise&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">betker2023better</span>)</cite>, combines a Perceiver Resampler with a speaker-consistency loss to reach 16 languages with speaker cloning. CLAM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024clam</span>)</cite> improves codec language model compression with probabilistic residual vector quantization, enabling single-step multi-token generation. However, these models often treat synthesis as a distinct task from speech editing. The challenge of <em class=\"ltx_emph ltx_font_italic\">unifying</em> high-quality, multilingual speech editing with robust multilingual speech synthesis within a single, open-source, and fully autoregressive model architecture remains largely unaddressed.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X employs several token reordering steps, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, to unify speech editing and synthesis. We assume that our training examples consist of utterance waveforms accompanied by time-aligned word transcriptions (we use the Montreal Forced Aligner (MFA)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">mcauliffe2017montreal</span>)</cite> in our work). During training, a text transcription is randomly segmented into prefix, middle, and suffix portions. These are then rearranged into a \"prefix-suffix-middle\" sequence, where the \"middle\" segment serves as the prediction target. Finally, the corresponding speech tokens for each segment are reordered identically based on the alignment timings. This ensures a monotonic alignment between the text and speech tokens, even when performing speech edits which require infilling tokens in the middle of the speech sequence. This rearrangement serves to mirror the use case in which a user wishes to modify some, but not all of the words in an utterance - by using this rearrangement, the model can be trained to predict the speech tokens within the middle of an utterance, conditioned on the preceding (prefix) and following (suffix) speech tokens in addition to the desired text transcription.</p>\n\n",
                "matched_terms": [
                    "reordering",
                    "token",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the token reordering, a learnable <span class=\"ltx_text ltx_font_italic\">&lt;MASK&gt;</span> token is inserted at two locations within the text-speech input sequence: one <span class=\"ltx_text ltx_font_italic\">&lt;MASK&gt;</span> token is inserted at the boundary between the prefix and suffix speech tokens, and a second <span class=\"ltx_text ltx_font_italic\">&lt;MASK&gt;</span> token is placed between the suffix audio tokens and the middle (target) audio tokens. These tokens serve to inform the model of the boundaries between the segments.</p>\n\n",
                "matched_terms": [
                    "one",
                    "token",
                    "reordering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To model the <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> parallel token sequences output by the EnCodec tokenizer autoregressively, we incorporate the &#8220;Delay Pattern&#8221; proposed by MusicGen&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">copet2023simple</span>)</cite>. Instead of predicting all <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m2\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math> codebooks for a given audio timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> simultaneously or flattening all codebooks across all timesteps into one long sequence, delay patterning inserts a cumulative time delay of one timestep per RVQ layer to the EnCodec token sequences. As a result, the prediction for the speech token at codebook level <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m4\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> at timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> can be conditioned on the model&#8217;s predictions for codebook levels 1 through <math alttext=\"k-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m6\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">k-1</annotation></semantics></math> associated with the same timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p3.m7\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "one",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We combined speech data across public datasets over 11 languages, amounting to a total of approximately 32K hours (detailed statistics provided in Appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS1\" title=\"A.1 Training Dataset Statistics &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a>). The sampling rate for all audio is 16 kHz. Audio segments longer than 25 seconds were discarded. For MLS dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite>, misalignment issues were particularly prominent, with approximately 20% of samples having extra or missing words in the transcript at the beginning or end. We found that this negatively impacted model performance for English, and subsequently removed utterances whose transcriptions differed significantly from those produced by the Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> model. While we found similar problems with the non-English European language data in MLS, we anecdotally observed better performance on those languages without performing this filtering. We speculate that this is due to the fact that the amount of available training data for those languages is already relatively low, and the performance improvements brought by the additional training data outweigh the detriments brought by transcription noise.</p>\n\n",
                "matched_terms": [
                    "from",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluating Text-to-Speech (TTS) performance, we curated an evaluation dataset from several established benchmarks. For English, we utilized the Seed-TTS test-en set&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> (1088 samples sourced from Common Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2019common</span>)</cite>). For Mandarin, we employed the Seed-TTS test-zh set (2020 samples from DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2021didispeech</span>)</cite>). Korean and Japanese evaluations were conducted using 200 randomly selected samples from KsponSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bang2020ksponspeech</span>)</cite> and KokoroSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Iida2021Kokoro</span>)</cite>, respectively. For the remaining seven languages supported by our model (Spanish, French, German, Dutch, Italian, Portuguese, and Polish), we randomly selected 100 samples for each language from their corresponding Multilingual LibriSpeech (MLS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite> test sets. To evaluate speech editing, we randomly selected 100-300 samples per language from these TTS test datasets and then utilized Gemini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to perform insertion, deletion, or substitution operations on the textual portions of these samples, with specific details available in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS2\" title=\"A.2 Speech Editing Dataset &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. We conducted subjective evaluation over a subset of languages (English, Chinese, French, Italian, Portuguese, and Spanish) using a random subset of the evaluation set: 40 English samples, 50 Chinese, and 20 for others.</p>\n\n",
                "matched_terms": [
                    "subset",
                    "chinese",
                    "from",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use Qwen3-0.6B-Base as both the text tokenizer and the Transformer LM backbone (details are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A2.SS2\" title=\"B.2 Qwen3 Base Model &#8227; Appendix B Implementational Details &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>). The outputs from the final Transformer layer are then projected into four distinct linear layers, each producing the logits for one of the codec tokens. The model comprises 613 million total parameters (457 million excluding embeddings). The codebook weights <math alttext=\"\\boldsymbol{\\alpha}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m1\" intent=\":literal\"><semantics><mi>&#120630;</mi><annotation encoding=\"application/x-tex\">\\boldsymbol{\\alpha}</annotation></semantics></math> are set to <math alttext=\"(1.0,0.8,0.6,0.4)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mn>1.0</mn><mo>,</mo><mn>0.8</mn><mo>,</mo><mn>0.6</mn><mo>,</mo><mn>0.4</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(1.0,0.8,0.6,0.4)</annotation></semantics></math>, influencing the contribution of each codebook during training (as further detailed in our loss formulation&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A2.SS3\" title=\"B.3 Loss Design &#8227; Appendix B Implementational Details &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">B.3</span></a>). For model training, we employ the AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">loshchilov2017decoupled</span>)</cite> with a learning rate of <math alttext=\"4\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4\\times 10^{-3}</annotation></semantics></math>, <math alttext=\"\\beta_{1}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{1}=0.9</annotation></semantics></math>, <math alttext=\"\\beta_{2}=0.999\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.999</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{2}=0.999</annotation></semantics></math>, an epsilon of <math alttext=\"1\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m6\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-6}</annotation></semantics></math>, and a weight decay of <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m7\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>. A learning rate scheduler is utilized, featuring a linear warm-up for the initial <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m8\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> steps, followed by a linear decay for the remainder of the <math alttext=\"5,000K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m9\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>,</mo><mrow><mn>000</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow></mrow><annotation encoding=\"application/x-tex\">5,000K</annotation></semantics></math> total training steps. Gradient accumulation is performed over <math alttext=\"8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p2.m10\" intent=\":literal\"><semantics><mn>8</mn><annotation encoding=\"application/x-tex\">8</annotation></semantics></math> micro-batches. The training of the multilingual VoiceCraft-X model took approximately one week on 16 NVIDIA A100 40GB GPUs.</p>\n\n",
                "matched_terms": [
                    "one",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens; the model then autoregressively predicts the corresponding neural codec tokens for the target audio segment. Notably, the token reordering mechanism significantly enhances inference stability. This largely prevents repeating token loops, an issue in the original VoiceCraft which could cause artifacts (e.g., excessive silences) and required multi-sample filtering. Consequently, VoiceCraft-X reliably generates high-quality speech in a single pass without needing this filtering step. In all experiments, we employ nucleus sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">holtzman2019curious</span>)</cite> with <math alttext=\"TopK=20,TopP=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><mo>=</mo><mn>20</mn></mrow><mo>,</mo><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow><mo>=</mo><mn>1.0</mn></mrow></mrow><annotation encoding=\"application/x-tex\">TopK=20,TopP=1.0</annotation></semantics></math>, and a temperature of 1.</p>\n\n",
                "matched_terms": [
                    "reordering",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the English and Chinese Zero-shot TTS tasks, we compared our model with FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite>, MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>)</cite>, and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite>. For English, we also included VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> in our comparison. For the remaining languages, we benchmarked our model against the multilingual XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite> model, considering both its v1 and v2 versions. For speech editing, we compared VoiceCraft-X with the original VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> model on English.</p>\n\n",
                "matched_terms": [
                    "chinese",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used a combination of subjective and objective measures. Objectively, we use Word Error Rate (WER) as an automatic proxy for the intelligibility of the synthesized speech; this is calculated using Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2023funasr</span>)</cite> for Chinese and Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> for other languages. Additionally, speaker similarity (SIM-o) is objectively measured by computing the cosine similarity of speaker embeddings, which are extracted from both the generated and original target speech using a WavLM-based speaker verification model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span>)</cite>. Subjective evaluations involved human annotators (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for details) who provide Comparative Mean Opinion Scores (CMOS) and Similarity Mean Opinion Scores (SMOS) for TTS, and Naturalness Mean Opinion Scores (NMOS) and Intelligibility Mean Opinion Scores (IMOS) for speech editing. For CMOS, evaluators assess the naturalness of the synthesized speech in comparison to the ground truth, while for SMOS, they directly score the similarity between the synthesized speech and the initial speech prompt. For NMOS and IMOS, evaluators respectively assess the naturalness and intelligibility of the synthesized and original speech.</p>\n\n",
                "matched_terms": [
                    "other",
                    "from",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated VoiceCraft-X&#8217;s zero-shot TTS performance across 11 languages, and the results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T1\" title=\"Table 1 &#8227; Baselines. &#8227; 4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. For Chinese, VoiceCraft-X was trained on a modest <math alttext=\"5K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">5K</annotation></semantics></math> hours of data, a fraction of that used by leading models (often exceeding <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> hours). Consequently, while its CER of 3.29 was higher than these specialized models, this was achieved with substantially less data, and its speaker similarity and subjective scores reflected this data disparity. In English, VoiceCraft-X, trained on <math alttext=\"14K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>14</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">14K</annotation></semantics></math> hours, showed marked improvements over its predecessor, VoiceCraft, reducing its WER from 5.28 to 4.37 and enhancing SIM-o from 0.51 to 0.54. Critically, its CMOS score of 0.63<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The generally higher English CMOS scores likely resulted from using Seed-TTS test set as prompts with atypical, exaggerated intonation (not standard read speech).</span></span></span> was the highest among compared models, indicating superior perceived naturalness. While some models trained on significantly larger datasets achieved lower WERs, VoiceCraft-X&#8217;s subjective quality in English was highly competitive.</p>\n\n",
                "matched_terms": [
                    "models",
                    "english",
                    "from",
                    "chinese",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To explore the benefits of multilingual training, especially for lower-resource languages, we fine-tuned <span class=\"ltx_text ltx_font_italic\">monolingual</span> models on individual languages starting from different pre-trained checkpoints, comparing these against training from scratch and the multilingual model (detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T2\" title=\"Table 2 &#8227; 4.3 Transfer Learning for Multilingual TTS &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "from",
                    "scratch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The universal advantage of pre-training over &#8220;from Scratch&#8221; models is paramount, especially for languages with limited data. For instance, Italian (294 hours) and Polish (139 hours) saw their WERs plummet from over 140 and 160 to under 14 and 20 respectively, demonstrating pre-training&#8217;s crucial role in transferring foundational knowledge and overcoming data scarcity. Even higher-resource languages like Spanish, French and German benefited significantly. Fine-tuning from an English model initialization proved highly effective for European languages (Germanic, Romance, Slavic), leveraging linguistic similarities and robust acoustic modeling, with gains particularly vital for low-data scenarios (Italian, Portuguese, Polish). Korean showed better CER with a Japanese checkpoint (42.08) than Chinese (49.11), aligning with typological closeness. Conversely, Japanese experienced negative transfer from Chinese (CER 36.18 vs. 22.36 from scratch).</p>\n\n",
                "matched_terms": [
                    "models",
                    "english",
                    "from",
                    "scratch",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, fine-tuning from the &#8220;multilingual checkpoint&#8221; frequently yielded superior WER/CER compared to an English-only checkpoint for a range of languages including Spanish, Dutch, Italian, Portuguese, Polish, and Japanese. This advantage held across varying data volumes (e.g., Polish 139 hours, Japanese 3489 hours), suggesting that pre-training on a diverse linguistic set fosters more generalized and transferable representations than exposure to English alone, capturing a broader array of phonetic and prosodic patterns.</p>\n\n",
                "matched_terms": [
                    "from",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the original multilingual model&#8217;s speaker similarity is significantly higher than models fine-tuned from other checkpoints for nearly all languages. This indicates that joint training on diverse linguistic data, leveraging collective data volume, allows the model to disentangle speaker-specific characteristics from language-specific features. This robust performance across varied languages suggests it learns a more abstract, shared representation space for speech, facilitating both high-fidelity synthesis and strong cross-lingual capabilities. While fine-tuning on single language data may impact this disentanglement ability, as evidenced by SIM-o drops in many such cases.</p>\n\n",
                "matched_terms": [
                    "impact",
                    "models",
                    "other",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For English speech editing (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T3\" title=\"Table 3 &#8227; 4.4 Speech Editing &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), VoiceCraft-X demonstrated a better Word Error Rate (WER) than VoiceCraft. Both models produced edited speech that listeners found to be highly natural (NMOS) and intelligible (IMOS), comparable to the original recordings. VoiceCraft&#8217;s slightly higher scores in these subjective tests are not surprising, given its monolingual English focus, especially considering both models have similar parameter counts and amounts of English training data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For multilingual speech editing in other languages&#8212;a capability where comparative baselines are notably scarce as most models do not support multilingual editing&#8212;we conducted subjective MOS evaluations. These evaluations focused on a subset of languages (French, Italian, Portuguese, and Spanish) for which MTurk annotators were available, with results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T4\" title=\"Table 4 &#8227; 4.4 Speech Editing &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. The evaluations demonstrate VoiceCraft-X&#8217;s effective performance in this challenging scenario. While naturalness (NMOS) scores for edited speech are, as anticipated, lower than the original recordings, intelligibility (IMOS) remains high across these languages. Particularly for Spanish and Italian, where edited NMOS and IMOS scores closely matched the original audio, these findings underscore VoiceCraft-X&#8217;s significant and unique capability for coherent, comprehensible multilingual speech editing.</p>\n\n",
                "matched_terms": [
                    "models",
                    "scenario",
                    "other",
                    "subset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present VoiceCraft-X, an autoregressive neural codec language model that successfully unifies multilingual speech editing and Text-to-Speech (TTS) synthesis. Leveraging the Qwen3 LLM and a novel token reordering strategy, VoiceCraft-X supports eleven languages, producing high-quality, natural-sounding speech. Our model demonstrates robust performance across diverse conditions and shows that a unified framework can effectively advance both speech editing and synthesis in multilingual contexts, even with limited data for some languages. This work underscores the potential of autoregressive models for complex, real-world speech generation tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "token",
                    "reordering"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One key limitation is the scale of our training data. Although VoiceCraft-X performs well with approximately 32,578 hours across eleven languages, this is notably less than some state-of-the-art models. This comparative data scarcity, particularly for lower-resource languages in our set, may limit the model&#8217;s capacity to capture the full spectrum of speech nuances as effectively as systems trained on more extensive datasets.</p>\n\n",
                "matched_terms": [
                    "models",
                    "one",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To create a comprehensive evaluation set for speech editing, we began by selecting a subset of samples from the Text-to-Speech (TTS) evaluation datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.SS1\" title=\"4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. For each language, 100-300 original text samples were chosen.</p>\n\n",
                "matched_terms": [
                    "from",
                    "subset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VoiceCraft-X is trained as an autoregressive model to predict a sequence of neural codec tokens. Given the input context, which includes text tokens, speaker embeddings, and potentially prefix/suffix audio tokens, the model predicts the target audio tokens one by one. The overall training objective is a weighted cross-entropy loss, designed to enhance learning efficiency and focus on the crucial aspects of the speech generation task.</p>\n\n",
                "matched_terms": [
                    "one",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let the sequence of all ground truth speech tokens (encompassing prefix, suffix, and middle segments, and structured according to the delay pattern described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.SS5\" title=\"3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3.5</span></a>) be denoted by <math alttext=\"Z=(z_{1},z_{2},\\dots,z_{N})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>z</mi><mi>N</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Z=(z_{1},z_{2},\\dots,z_{N})</annotation></semantics></math>, where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m2\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> is the total number of tokens in the flattened sequence. Each token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> in this sequence corresponds to an original codec token <math alttext=\"Y_{t_{i},k_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m4\" intent=\":literal\"><semantics><msub><mi>Y</mi><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msub><annotation encoding=\"application/x-tex\">Y_{t_{i},k_{i}}</annotation></semantics></math> from timestep <math alttext=\"t_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>t</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math> and the <math alttext=\"k_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>k</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">k_{i}</annotation></semantics></math>-th codebook of the EnCodec output (where <math alttext=\"K=4\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m7\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">K=4</annotation></semantics></math> is the total number of codebooks). The model predicts the probability distribution for each token <math alttext=\"\\hat{z}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p2.m8\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\hat{z}_{i}</annotation></semantics></math> conditioned on previous tokens and the input context.</p>\n\n",
                "matched_terms": [
                    "token",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Codebook Weighting</span>: As mentioned in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.SS1\" title=\"4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, each of the <math alttext=\"K=4\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>K</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">K=4</annotation></semantics></math> parallel codebooks contributes differently to the overall perceptual quality. We assign weights <math alttext=\"\\boldsymbol{\\alpha}=(\\alpha_{1},\\alpha_{2},\\alpha_{3},\\alpha_{4})=(1.0,0.8,0.6,0.4)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#120630;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#945;</mi><mn>1</mn></msub><mo>,</mo><msub><mi>&#945;</mi><mn>2</mn></msub><mo>,</mo><msub><mi>&#945;</mi><mn>3</mn></msub><mo>,</mo><msub><mi>&#945;</mi><mn>4</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mn>1.0</mn><mo>,</mo><mn>0.8</mn><mo>,</mo><mn>0.6</mn><mo>,</mo><mn>0.4</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol{\\alpha}=(\\alpha_{1},\\alpha_{2},\\alpha_{3},\\alpha_{4})=(1.0,0.8,0.6,0.4)</annotation></semantics></math> to the tokens from codebook 1 to 4, respectively. So, for a token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i1.p1.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math> corresponding to <math alttext=\"Y_{t_{i},k_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i1.p1.m4\" intent=\":literal\"><semantics><msub><mi>Y</mi><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>k</mi><mi>i</mi></msub></mrow></msub><annotation encoding=\"application/x-tex\">Y_{t_{i},k_{i}}</annotation></semantics></math>, its codebook weight is <math alttext=\"\\alpha_{k_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#945;</mi><msub><mi>k</mi><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">\\alpha_{k_{i}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "token",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment Weighting</span>: While the model is trained to predict tokens for all three segments (prefix, middle, and suffix) to improve training efficacy and contextual understanding, the primary goal is the accurate generation of the \"middle\" (target) segment. To reflect this, we introduce segment-specific weights. Tokens belonging to the \"prefix\" and \"suffix\" segments are assigned a weight <math alttext=\"w_{seg}=1\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">w_{seg}=1</annotation></semantics></math>. Tokens belonging to the \"middle\" segment, which is the primary target for generation or editing, are assigned a higher weight <math alttext=\"w_{seg}=3\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">w_{seg}=3</annotation></semantics></math>. Let <math alttext=\"w_{seg}(z_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">w_{seg}(z_{i})</annotation></semantics></math> denote the segment weight for token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.I1.i2.p1.m4\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "token",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"L_{CE}(\\hat{z}_{i},z_{i})\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p4.m2\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mover accent=\"true\"><mi>z</mi><mo>^</mo></mover><mi>i</mi></msub><mo>,</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">L_{CE}(\\hat{z}_{i},z_{i})</annotation></semantics></math> is the cross-entropy loss for predicting token <math alttext=\"z_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS3.p4.m3\" intent=\":literal\"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">z_{i}</annotation></semantics></math>. This weighted loss function guides the model to prioritize the generation of the target audio segment while still learning from the context provided by the prefix and suffix, and appropriately valuing the contribution of each codebook.</p>\n\n",
                "matched_terms": [
                    "token",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The token reordering mechanism, integral to our training methodology, introduces flexibility in how prompts are structured during zero-shot Text-to-Speech (TTS) inference. To determine the optimal placement, we evaluated several configurations for incorporating the prompt text (<math alttext=\"T_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{prompt}</annotation></semantics></math>) and prompt audio (<math alttext=\"A_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{prompt}</annotation></semantics></math>) into the input sequence. These configurations are detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T10\" title=\"Table 10 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "reordering",
                    "token"
                ]
            }
        ]
    },
    "A4.T10": {
        "caption": "Table 10: WER and SIM-o of different prompt positions in zero-shot TTS inference on Seed-TTS test-en set.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">WER</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">SIM-o</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><math alttext=\"\\varnothing,\\varnothing,T_{prompt},T_{target},\\textit{&lt;SPK&gt;},\\varnothing,\\textit{&lt;M&gt;},\\varnothing,\\textit{&lt;M&gt;},A_{prompt},A_{target}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>T</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><mtext class=\"ltx_mathvariant_italic\">&lt;SPK&gt;</mtext><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><mtext class=\"ltx_mathvariant_italic\">&lt;M&gt;</mtext><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><mtext class=\"ltx_mathvariant_italic\">&lt;M&gt;</mtext><mo>,</mo><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>A</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\varnothing,\\varnothing,T_{prompt},T_{target},\\textit{&lt;SPK&gt;},\\varnothing,\\textit{&lt;M&gt;},\\varnothing,\\textit{&lt;M&gt;},A_{prompt},A_{target}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.54</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><math alttext=\"T_{prompt},\\varnothing,T_{target},\\textit{&lt;SPK&gt;},A_{prompt},\\textit{&lt;M&gt;},\\varnothing,\\textit{&lt;M&gt;},A_{target}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m2\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><msub><mi>T</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><mtext class=\"ltx_mathvariant_italic\">&lt;SPK&gt;</mtext><mo>,</mo><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><mtext class=\"ltx_mathvariant_italic\">&lt;M&gt;</mtext><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><mtext class=\"ltx_mathvariant_italic\">&lt;M&gt;</mtext><mo>,</mo><msub><mi>A</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">T_{prompt},\\varnothing,T_{target},\\textit{&lt;SPK&gt;},A_{prompt},\\textit{&lt;M&gt;},\\varnothing,\\textit{&lt;M&gt;},A_{target}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">5.68</td>\n<td class=\"ltx_td ltx_align_center\">0.53</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><math alttext=\"\\varnothing,T_{prompt},T_{target},\\textit{&lt;SPK&gt;},\\varnothing,\\textit{&lt;M&gt;},A_{prompt},\\textit{&lt;M&gt;},A_{target}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T10.m3\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>T</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><mtext class=\"ltx_mathvariant_italic\">&lt;SPK&gt;</mtext><mo>,</mo><mi mathvariant=\"normal\">&#8709;</mi><mo>,</mo><mtext class=\"ltx_mathvariant_italic\">&lt;M&gt;</mtext><mo>,</mo><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><mtext class=\"ltx_mathvariant_italic\">&lt;M&gt;</mtext><mo>,</mo><msub><mi>A</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\varnothing,T_{prompt},T_{target},\\textit{&lt;SPK&gt;},\\varnothing,\\textit{&lt;M&gt;},A_{prompt},\\textit{&lt;M&gt;},A_{target}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">6.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.54</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "set",
            "tp​r​o​m​p​t∅tt​a​r​g​e​tspkap​r​o​m​p​tm∅mat​a​r​g​e​ttpromptvarnothingttargettextitspkaprompttextitmvarnothingtextitmatarget",
            "simo",
            "zeroshot",
            "tts",
            "wer",
            "prompt",
            "seedtts",
            "inference",
            "∅∅tp​r​o​m​p​ttt​a​r​g​e​tspk∅m∅map​r​o​m​p​tat​a​r​g​e​tvarnothingvarnothingtpromptttargettextitspkvarnothingtextitmvarnothingtextitmapromptatarget",
            "testen",
            "positions",
            "∅tp​r​o​m​p​ttt​a​r​g​e​tspk∅map​r​o​m​p​tmat​a​r​g​e​tvarnothingtpromptttargettextitspkvarnothingtextitmaprompttextitmatarget",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The token reordering mechanism, integral to our training methodology, introduces flexibility in how prompts are structured during zero-shot Text-to-Speech (TTS) inference. To determine the optimal placement, we evaluated several configurations for incorporating the prompt text (<math alttext=\"T_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{prompt}</annotation></semantics></math>) and prompt audio (<math alttext=\"A_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{prompt}</annotation></semantics></math>) into the input sequence. These configurations are detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T10\" title=\"Table 10 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://zhishengzheng.com/voicecraft-x/\" title=\"\">https://zhishengzheng.com/voicecraft-x/</a>.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past several years, the quality of TTS models has improved significantly, particularly in the zero-shot setting in which a model generates speech in a new speaker&#8217;s voice given a short (e.g. 3 second) audio prompt. Transformer-based neural networks have been central to this progress, leading to three broad paradigms: (i) autoregressive (AR), (ii) non-autoregressive (Non-AR), and (iii) hybrid models.\nAR models, such as VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> and its successors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025pseudo</span>)</cite>, generate frame-level speech tokens sequentially, where the tokens are typically derived from a neural audio codec&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">defossez2022high</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zeghidour2021soundstream</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speechtokenizer</span>)</cite>. These models are able to perform voice-cloning TTS via Transformer language models&#8217; in-context learning ability, demonstrating high-quality speech synthesis. Non-AR models include flow-matching models such as F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, as well as diffusion models such as NaturalSpeech 2/3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>. These models predict all tokens representing an utterance in parallel via iterative refinement. Hybrid approaches such as Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> aim to combine the strengths of both paradigms.\nWhile these models deliver impressive zero-shot quality, most of the models are either monolingual or focus on a handful of high-resource languages such as English and Chinese. This is likely due to the fact that these models are data-hungry, often requiring 10K-100K hours of training speech for SOTA performance.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "zeroshot",
                    "tts",
                    "seedtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper we address this gap, by introducing <span class=\"ltx_text ltx_font_bold\">VoiceCraft-X</span>, a unified autoregressive neural codec language model that performs <em class=\"ltx_emph ltx_font_italic\">both</em> speech editing and zero-shot TTS in <span class=\"ltx_text ltx_font_bold\">11 languages</span>: English (en), Mandarin (zh), Korean (ko), Japanese (ja), Spanish (es), French (fr), German (de), Dutch (nl), Italian (it), Portuguese (pt) and Polish (pl).\nOur contributions are threefold:</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce VoiceCraft-X, a single autoregressive model that unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) across 11 languages.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot Text-to-Speech (TTS) synthesis task entails generating speech in a new speaker&#8217;s voice from a short audio prompt, without assuming that the new speaker was seen during training. Recent progress is largely driven by Transformer-based neural networks, falling into autoregressive (AR), non-autoregressive (non-AR), and hybrid.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Autoregressive (AR) models generate speech tokens sequentially. VALL-E&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2023neural</span>)</cite> pioneered neural codec language models for high-quality zero-shot TTS via in-context learning, with subsequent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">han2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024vall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">xin2024rall</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">song2025ella</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kharitonov2023speak</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lajszczak2024base</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite> further refining this paradigm. Non-Autoregressive (Non-AR) models aim for faster generation by predicting tokens in parallel or using iterative refinement. Examples include flow-matching models like VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> and diffusion-based models such as NaturalSpeech 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shen2023naturalspeech</span>)</cite>, NaturalSpeech 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span>)</cite>, and DiTTo-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lee2024ditto</span>)</cite>. Other notable non-AR approaches include Unicats&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024unicats</span>)</cite>, SimpleSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2024simplespeech2</span>)</cite>, E2-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">eskimez2024e2</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite> and Mega-TTS 3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jiang2025megatts</span>)</cite>. Hybrid systems combine aspects of both AR and non-AR methods. Seed-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> uses a two-stage architecture, while CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite> and MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite> also represent efforts to balance quality, speed, and controllability. In this work, VoiceCraft-X follows the codec language modeling method of VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> and enables high-quality, zero-shot multilingual speech synthesis within its unified editing and generation framework.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts",
                    "seedtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the data side, <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">saeki2024extending</span></cite> show that pairing self-supervised speech representations with unsupervised text alignment scales TTS to 100&#8201;+ languages, even when only scant transcriptions exist. Large curated corpora amplify these gains: VoxPopuliTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2025voxpopulitts</span>)</cite> refines 30,000 hours of English, French and Spanish speech; Fish-Speech &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liao2024fish</span>)</cite> goes further, training on 720,000 hours while using an LLM to sidestep language-specific G2P rules. Model architectures have evolved in parallel. VoiceBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span>)</cite> adopts non-autoregressive flow matching, delivering cross-lingual zero-shot TTS in six languages via in-context learning. XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite>, extending Tortoise&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">betker2023better</span>)</cite>, combines a Perceiver Resampler with a speaker-consistency loss to reach 16 languages with speaker cloning. CLAM-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kim2024clam</span>)</cite> improves codec language model compression with probabilistic residual vector quantization, enabling single-step multi-token generation. However, these models often treat synthesis as a distinct task from speech editing. The challenge of <em class=\"ltx_emph ltx_font_italic\">unifying</em> high-quality, multilingual speech editing with robust multilingual speech synthesis within a single, open-source, and fully autoregressive model architecture remains largely unaddressed.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens. The system then autoregressively generates the neural codec tokens for the target audio segment.</p>\n\n",
                "matched_terms": [
                    "inference",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If a prompt text (<math alttext=\"T_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{prompt}</annotation></semantics></math>) and its corresponding prompt speech are provided, we concatenate the prompt text and the target text (<math alttext=\"T_{target}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{target}</annotation></semantics></math>) to form the middle text segment, and a speaker embedding is extracted from the prompt speech. If no such prompt is provided, we set the prompt text (<math alttext=\"T_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">T_{prompt}</annotation></semantics></math>) to empty and randomly generate a speaker embedding. The final input is as follows:</p>\n\n",
                "matched_terms": [
                    "set",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluating Text-to-Speech (TTS) performance, we curated an evaluation dataset from several established benchmarks. For English, we utilized the Seed-TTS test-en set&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">anastassiou2024seed</span>)</cite> (1088 samples sourced from Common Voice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ardila2019common</span>)</cite>). For Mandarin, we employed the Seed-TTS test-zh set (2020 samples from DiDiSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2021didispeech</span>)</cite>). Korean and Japanese evaluations were conducted using 200 randomly selected samples from KsponSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">bang2020ksponspeech</span>)</cite> and KokoroSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Iida2021Kokoro</span>)</cite>, respectively. For the remaining seven languages supported by our model (Spanish, French, German, Dutch, Italian, Portuguese, and Polish), we randomly selected 100 samples for each language from their corresponding Multilingual LibriSpeech (MLS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">pratap2020mls</span>)</cite> test sets. To evaluate speech editing, we randomly selected 100-300 samples per language from these TTS test datasets and then utilized Gemini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2023gemini</span>)</cite> to perform insertion, deletion, or substitution operations on the textual portions of these samples, with specific details available in the appendix&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A1.SS2\" title=\"A.2 Speech Editing Dataset &#8227; Appendix A Dataset &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>. We conducted subjective evaluation over a subset of languages (English, Chinese, French, Italian, Portuguese, and Spanish) using a random subset of the evaluation set: 40 English samples, 50 Chinese, and 20 for others.</p>\n\n",
                "matched_terms": [
                    "set",
                    "testen",
                    "tts",
                    "seedtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S3.F2\" title=\"Figure 2 &#8227; 3.5 Causal Masking and Delay Pattern &#8227; 3 Method &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows how, at inference time, VoiceCraft-X performs speech editing and zero-shot text-to-speech by preparing an input sequence based on the \"prefix-suffix-middle\" reordering of text and speech tokens; the model then autoregressively predicts the corresponding neural codec tokens for the target audio segment. Notably, the token reordering mechanism significantly enhances inference stability. This largely prevents repeating token loops, an issue in the original VoiceCraft which could cause artifacts (e.g., excessive silences) and required multi-sample filtering. Consequently, VoiceCraft-X reliably generates high-quality speech in a single pass without needing this filtering step. In all experiments, we employ nucleus sampling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">holtzman2019curious</span>)</cite> with <math alttext=\"TopK=20,TopP=1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><mo>=</mo><mn>20</mn></mrow><mo>,</mo><mrow><mrow><mi>T</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>P</mi></mrow><mo>=</mo><mn>1.0</mn></mrow></mrow><annotation encoding=\"application/x-tex\">TopK=20,TopP=1.0</annotation></semantics></math>, and a temperature of 1.</p>\n\n",
                "matched_terms": [
                    "inference",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the English and Chinese Zero-shot TTS tasks, we compared our model with FireRedTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024fireredtts</span>)</cite>, MaskGCT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2024maskgct</span>)</cite>, F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2024f5</span>)</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice1</span>)</cite>, and CosyVoice 2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024cosyvoice2</span>)</cite>. For English, we also included VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> in our comparison. For the remaining languages, we benchmarked our model against the multilingual XTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">casanova2024xtts</span>)</cite> model, considering both its v1 and v2 versions. For speech editing, we compared VoiceCraft-X with the original VoiceCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">peng2024voicecraft</span>)</cite> model on English.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used a combination of subjective and objective measures. Objectively, we use Word Error Rate (WER) as an automatic proxy for the intelligibility of the synthesized speech; this is calculated using Paraformer-zh&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2023funasr</span>)</cite> for Chinese and Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> for other languages. Additionally, speaker similarity (SIM-o) is objectively measured by computing the cosine similarity of speaker embeddings, which are extracted from both the generated and original target speech using a WavLM-based speaker verification model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span>)</cite>. Subjective evaluations involved human annotators (see Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3\" title=\"Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for details) who provide Comparative Mean Opinion Scores (CMOS) and Similarity Mean Opinion Scores (SMOS) for TTS, and Naturalness Mean Opinion Scores (NMOS) and Intelligibility Mean Opinion Scores (IMOS) for speech editing. For CMOS, evaluators assess the naturalness of the synthesized speech in comparison to the ground truth, while for SMOS, they directly score the similarity between the synthesized speech and the initial speech prompt. For NMOS and IMOS, evaluators respectively assess the naturalness and intelligibility of the synthesized and original speech.</p>\n\n",
                "matched_terms": [
                    "simo",
                    "prompt",
                    "tts",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated VoiceCraft-X&#8217;s zero-shot TTS performance across 11 languages, and the results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.T1\" title=\"Table 1 &#8227; Baselines. &#8227; 4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. For Chinese, VoiceCraft-X was trained on a modest <math alttext=\"5K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">5K</annotation></semantics></math> hours of data, a fraction of that used by leading models (often exceeding <math alttext=\"50K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">50K</annotation></semantics></math> hours). Consequently, while its CER of 3.29 was higher than these specialized models, this was achieved with substantially less data, and its speaker similarity and subjective scores reflected this data disparity. In English, VoiceCraft-X, trained on <math alttext=\"14K\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mn>14</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">14K</annotation></semantics></math> hours, showed marked improvements over its predecessor, VoiceCraft, reducing its WER from 5.28 to 4.37 and enhancing SIM-o from 0.51 to 0.54. Critically, its CMOS score of 0.63<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The generally higher English CMOS scores likely resulted from using Seed-TTS test set as prompts with atypical, exaggerated intonation (not standard read speech).</span></span></span> was the highest among compared models, indicating superior perceived naturalness. While some models trained on significantly larger datasets achieved lower WERs, VoiceCraft-X&#8217;s subjective quality in English was highly competitive.</p>\n\n",
                "matched_terms": [
                    "set",
                    "simo",
                    "zeroshot",
                    "tts",
                    "wer",
                    "seedtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To create a comprehensive evaluation set for speech editing, we began by selecting a subset of samples from the Text-to-Speech (TTS) evaluation datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.SS1\" title=\"4.1 Setup &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>. For each language, 100-300 original text samples were chosen.</p>\n\n",
                "matched_terms": [
                    "set",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A scarcity of Amazon Mechanical Turk workers for less common languages prevented us from collecting subjective evaluation results for all targeted languages. Consequently, the SMOS results for French, Italian, Portuguese, and Spanish on the Zero-Shot TTS task that we were able to gather are detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A3.T8\" title=\"Table 8 &#8227; C.2 Additional Results &#8227; Appendix C Subjective Evaluation &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For this ablation study, considering the low-resource nature of most languages, we used LibriTTS-R&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koizumi2023libritts</span>)</cite> and the WenetSpeech4TTS Premium&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ma2024wenetspeech4tts</span>)</cite> subset as training data. LibriTTS-R contains 585 hours of speech, while the WenetSpeech4TTS Premium subset includes 601 hours<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>YouTube clips are removed.</span></span></span>. Models were trained for 15 epochs, both with and without the reordering mechanism. The final epoch was then evaluated on the Seed-TTS test set. As can be seen from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A4.T9\" title=\"Table 9 &#8227; D.1 Reordering Mechanism &#8227; Appendix D Ablations &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, the model using the reordering mechanism shows significant performance improvements across all objective evaluation metrics on both the English and Chinese datasets. Specifically, the WER for English dropped dramatically from 104.02 to 11.60, and the CER for Chinese also decreased sharply from 262.25 to 19.25. Concurrently, the SIM-o scores for both languages also showed noticeable increases, indicating an improvement in the quality and naturalness of the synthesized speech. These results strongly demonstrate that the reordering mechanism is very effective in training under low-resource scenarios.</p>\n\n",
                "matched_terms": [
                    "set",
                    "simo",
                    "seedtts",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation, based on WER and SIM-o, revealed that placing the prompt at the beginning of the \"middle\" segment yields the most favorable overall performance. Specifically, structuring the input such that the prompt text precedes the target text within the middle text segment (i.e., <math alttext=\"T_{P}=\\varnothing,T_{S}=\\varnothing,T_{M}=(T_{prompt},T_{target})\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>T</mi><mi>P</mi></msub><mo>=</mo><mi mathvariant=\"normal\">&#8709;</mi></mrow><mo>,</mo><mrow><mrow><msub><mi>T</mi><mi>S</mi></msub><mo>=</mo><mi mathvariant=\"normal\">&#8709;</mi></mrow><mo>,</mo><mrow><msub><mi>T</mi><mi>M</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>T</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>T</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">T_{P}=\\varnothing,T_{S}=\\varnothing,T_{M}=(T_{prompt},T_{target})</annotation></semantics></math>, with <math alttext=\"A_{prompt}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{prompt}</annotation></semantics></math> appended after the mask tokens and before where <math alttext=\"A_{target}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>A</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">A_{target}</annotation></semantics></math> would be generated) resulted in a WER of 4.37, which is notably better than the alternative placements.</p>\n\n",
                "matched_terms": [
                    "simo",
                    "prompt",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A desirable characteristic of a multilingual Text-to-Speech (TTS) model is its ability to generate code-switched speech&#8212;that is, speech that fluidly transitions between languages. Although our model was trained exclusively on monolingual data, meaning code-switched speech is an out-of-distribution phenomenon for it, the model still demonstrated a certain capacity for code-switching without needing additional language identifiers for inputs in different languages.</p>\n\n",
                "matched_terms": [
                    "different",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further assess VoiceCraft-X&#8217;s adaptability and the impact of data quantity, we extended fine-tuning experiments across diverse languages. Building on cross-lingual transfer insights (Section&#160;&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#S4.SS3\" title=\"4.3 Transfer Learning for Multilingual TTS &#8227; 4 Experiments &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">4.3</span></a>), we examined the correlation between per-language fine-tuning data volume and zero-shot Text-to-Speech (TTS) quality.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.12347v1#A6.F3\" title=\"Figure 3 &#8227; Appendix F Cross-lingual Finetuning Hours on Zero-Shot TTS &#8227; VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> illustrates these findings, plotting per-language fine-tuning data volume (x-axis) against the relative Word Error Rate (WER) from zero-shot TTS (y-axis). This relative WER, the difference between Whisper&#8217;s WER on synthesized versus ground-truth audio, offers a normalized measure of intelligibility. The figure generally shows that more fine-tuning data improves pronunciation accuracy, especially for languages sharing similarities with VoiceCraft-X&#8217;s initial training set. However, this correlation is not universally linear. For languages like Korean and Thai, a moderate data increase (around 1000 hours) did not yield significant WER improvements. This plateauing suggests that for such languages, substantial gains may require much larger or more diverse datasets, or different fine-tuning approaches.</p>\n\n",
                "matched_terms": [
                    "set",
                    "zeroshot",
                    "tts",
                    "wer",
                    "different"
                ]
            }
        ]
    }
}