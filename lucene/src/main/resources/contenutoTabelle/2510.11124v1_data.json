{
    "S3.T1": {
        "source_file": "Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker",
        "caption": "Table 1: Details of the training corpora for the crosslingual model.",
        "body": "Datasets\n\n\n\n\nLanguage\n\n\n\n\nSpeaker\n\n\nEmotions\n\n\n\n\nNeutral\n\n\n\n\nHappy\n\n\n\n\nSad\n\n\n\n\nAngry\n\n\n\n\nSurprise\n\n\n\n\n\n\nESD_ch\n\n\n\n\nChinese\n\n\n\n\n10\n\n\n\n\n3,500\n\n\n\n\n3,500\n\n\n\n\n3,500\n\n\n\n\n3,500\n\n\n\n\n3,500\n\n\n\n\n\n\nESD_en\n\n\n\n\nEnglish\n\n\n\n\n10\n\n\n\n\n3,500\n\n\n\n\n3,500\n\n\n\n\n3,500\n\n\n\n\n3,500\n\n\n\n\n3,500\n\n\n\n\n\n\nBiaobei\n\n\n\n\nChinese\n\n\n\n\n1\n\n\n\n\n10,000\n\n\n\n\n0\n\n\n\n\n0\n\n\n\n\n0\n\n\n\n\n0\n\n\n\n\n\n\nLJSpeech\n\n\n\n\nEnglish\n\n\n\n\n1\n\n\n\n\n13,100\n\n\n\n\n0\n\n\n\n\n0\n\n\n\n\n0\n\n\n\n\n0\n\n\n\n\n\n\nLibirTTS\n\n\n\n\nEnglish\n\n\n\n\n1\n\n\n\n\n13,100\n\n\n\n\n0\n\n\n\n\n0\n\n\n\n\n0\n\n\n\n\n0",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" rowspan=\"2\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" rowspan=\"2\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" rowspan=\"2\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">Emotions</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Neutral</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Happy</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sad</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Angry</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Surprise</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">ESD_ch</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Chinese</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">10</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3,500</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3,500</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3,500</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3,500</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3,500</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">ESD_en</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">English</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">10</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3,500</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3,500</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3,500</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3,500</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">3,500</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Biaobei</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Chinese</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">10,000</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">LJSpeech</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">English</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">13,100</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">LibirTTS</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">English</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">13,100</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">0</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "angry",
            "datasets",
            "neutral",
            "chinese",
            "esden",
            "details",
            "crosslingual",
            "ljspeech",
            "libirtts",
            "surprise",
            "sad",
            "emotions",
            "happy",
            "training",
            "language",
            "english",
            "esdch",
            "speaker",
            "biaobei",
            "corpora",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For the cross-lingual emotional speech synthesis experiments, the ESD dataset has a limited size of 350 unique sentences per language. Therefore, training includes LJSpeech and Biaobei. To balance emotion and speaker representation, the ESD dataset is upsampled by a factor of 5 during training. The details of the training data in cross-lingual scenarios are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T1\" title=\"Table 1 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-lingual emotional text-to-speech (TTS) aims to produce speech in one language that captures the emotion of a speaker from another language while maintaining the target voice&#8217;s timbre. This process of cross-lingual emotional speech synthesis presents a complex challenge, necessitating flexible control over emotion, timbre, and language.\nHowever, emotion and timbre are highly entangled in speech signals, making fine-grained control challenging. To address this issue, we propose EMM-TTS, a novel two-stage cross-lingual emotional speech synthesis framework based on perturbed self-supervised learning (SSL) representations. In the first stage, the model explicitly and implicitly encodes prosodic cues to capture emotional expressiveness, while the second stage restores the timbre from perturbed SSL representations. We further investigate the effect of different speaker perturbation strategies&#8212;formant shifting and speaker anonymization&#8212;on the disentanglement of emotion and timbre. To strengthen speaker preservation and expressive control, we introduce Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) modules. Additionally, we find that incorporating explicit acoustic features (e.g., F0, energy, and duration) alongside pretrained latent features improves voice cloning performance. Comprehensive multi-metric evaluations, including both subjective and objective measures, demonstrate that EMM-TTS achieves superior naturalness, emotion transferability, and timbre consistency across languages.</p>\n\n",
                "matched_terms": [
                    "crosslingual",
                    "language",
                    "model",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis is a key component of the\nhuman&#8211;computer interface that is considered essential to\nresponding and plays a vital role in enabling machines to generate human-like responses.\nThe goals of speech synthesis can be hierarchically categorized, from easier to more challenging, into three levels: intelligibility, naturalness, and expressiveness.\nSpeech synthesis has made significant progress in intelligibility and naturalness, mainly due to advances in deep learning and neural networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib31\" title=\"\">2021</a>; Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nToday, we can generate speech that is often indistinguishable from human speech.\nWhile significant progress has been made in intelligibility and naturalness, achieving expressive and emotionally rich speech remains challenging.\nAnd a challenging research problem persists:\ncross-lingual emotion TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib14\" title=\"\">2024</a>)</cite> refers\nto the task of a speaker of one language to mimic the emotion of a speaker from another language while speaking a different language.</p>\n\n",
                "matched_terms": [
                    "crosslingual",
                    "language",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual synthesis poses a more complex challenge in multilingual speech synthesis, as it requires transferring a speaker&#8217;s voice characteristics across languages.\nDespite significant efforts in cross-lingual TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib5\" title=\"\">2024</a>)</cite> research, there remains a noticeable gap in the naturalness of generated speech compared to native speakers.\nThis issue primarily arises from two factors: the lack of data resources and variations in text representations across languages.\nThe most straightforward approach to cross-lingual synthesis is to train the model on bilingual speech data <cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib4\" title=\"\">2023</a>)</cite>, where the same speaker provides utterances in multiple languages.\nRegrettably, collecting such bilingual data is costly, and no large-scale bilingual speech datasets are available.\nAlong with speech data, the lack of text resources is a major obstacle in multilingual speech synthesis.\nConventional speech processing systems that are based on phonetics require pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib21\" title=\"\">2019</a>)</cite>. These dictionaries map phonetic units to their corresponding words.\nCreating such resources requires expert knowledge for each language. Despite the significant human effort involved, many languages still lack sufficient linguistic resources to develop these dictionaries.</p>\n\n",
                "matched_terms": [
                    "language",
                    "speaker",
                    "datasets",
                    "crosslingual",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past year, large-scale speech synthesis systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, leveraging codec models and language models to significantly enhance the capabilities of voice cloning, alongside models based on self-supervised representations.\nWhile these models showcase impressive performance in multilingual and emotional synthesis, their focus on voice cloning and zero-shot capabilities often comes at the expense of flexible control over emotion and timbre.\nMoreover, the entanglement of speaker timbre and emotion in speech may result in speaker timbre leakage during cross-speaker emotion transfer <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nA common strategy for decoupling involves adversarial learning and constraints on classification losses, as demonstrated in previous research <cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib19\" title=\"\">2022a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>.\nThese methods utilize classification loss or gradient reversal to learn representations that isolate emotion or speaker information.\nHowever, adversarial learning would introduce instability and degrade the quality of the synthesized speech. Furthermore, constraints on emotion classification may limit the emotional diversity of synthesized speech. Another straightforward decoupling approach involves speaker perturbation, which alters speaker-specific acoustic properties, such as formants, in speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib38\" title=\"\">2024</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib20\" title=\"\">2022b</a>)</cite>.\nThis perturbation method may degrade speech quality.\nFurthermore, the effects of recent speaker perturbation methods, such as speaker anonymization <cite class=\"ltx_cite ltx_citemacro_citep\">(Tomashenko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib35\" title=\"\">2024</a>; Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite>, on speech synthesis, especially for SSL-based synthesis models, remain an underexplored area.</p>\n\n",
                "matched_terms": [
                    "language",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To achieve effective decoupling of speaker and emotion, we propose a two-stage modeling approach: The first stage leverages explicit and implicit prosodic information to model emotions. In contrast, the second stage focuses on restoring the target timbre.</p>\n\n",
                "matched_terms": [
                    "emotions",
                    "model",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a reference speech from speaker <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> (e.g., in Chinese), our goal is to enable speaker <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> (e.g., an English native speaker) to speak Chinese with the reference speech&#8217;s emotion while retaining their own timbre, as depicted in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn contrast to the currently popular voice cloning methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, which determine all attributes of the synthesized speech&#8212;such as emotion and timbre&#8212;based on a single reference audio, our research focuses on cross-lingual emotional speech synthesis. We enable independent control over both emotion and timbre.</p>\n\n",
                "matched_terms": [
                    "crosslingual",
                    "english",
                    "speaker",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key challenge in cross-lingual TTS lies in decoupling speaker and emotion information. To address this, we propose a two-stage emotional speech synthesis system, EMM-TTS, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S2.F2\" title=\"Figure 2 &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The first stage <span class=\"ltx_text ltx_font_typewriter\">txt2vec</span> models and predicts emotions, while the second stage <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> controls speaker-specific characteristics.</p>\n\n",
                "matched_terms": [
                    "crosslingual",
                    "emotions",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For explicit information, the values (pitch, energy, and duration) are extracted from paired text-speech data in training. And we use three predictors to infer the values. The pitch and energy predictors are both based on a two-layer 1D convolutional neural network using ReLU activation, followed by layer normalization, a dropout layer, and an additional linear layer as <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib25\" title=\"\">2021</a>)</cite>.\nWe employ a learnable aligner <cite class=\"ltx_cite ltx_citemacro_citep\">(Badlani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib2\" title=\"\">2022</a>)</cite> to estimate phoneme durations.\nFor language ID, another explicit information, we use a lookup embedding.\nIt is important to note that modeling explicit information also relies on global implicit representations <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m1\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> like Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S2.F2\" title=\"Figure 2 &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One of the fundamental challenges in Cross-lingual/speaker emotional TTS is the decoupling of timbre and style.\nConsidering that the representation <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> predicted in the <span class=\"ltx_text ltx_font_typewriter\">txt2vec</span> stage contains sufficient emotional information, it also inevitably includes speaker information that is inconsistent with the target speaker&#8217;s timbre. Therefore, we propose improvements to the <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> model in ZMM-TTS to address this issue, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S2.F2\" title=\"Figure 2 &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. First, we adopt speaker ID rather than pre-trained speaker representations, as we found that pre-trained representations inevitably lead to emotional information leakage. We then introduce a global emotional representation <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> extracted from a pre-trained SSL-based emotion recognition model <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib26\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, in our approach, we perform a speaker perturbation, denoted as <math alttext=\"sp()\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">sp()</annotation></semantics></math>, on the original waveform <math alttext=\"Speech\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">Speech</annotation></semantics></math> during training, which allows us to obtain a speaker-independent\nsignal denoted as <math alttext=\"\\widetilde{\\text{Speech}}=sp(\\text{Speech})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><mo>=</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>Speech</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}=sp(\\text{Speech})</annotation></semantics></math>.\nSubsequently, we extract the multilingual discrete SSL representation and emotion representation from the perturbed <math alttext=\"\\widetilde{\\text{Speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}</annotation></semantics></math>, denoted as <math alttext=\"\\widetilde{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>R</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{R}</annotation></semantics></math> and <math alttext=\"\\widetilde{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>E</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{E}</annotation></semantics></math>. The perturbation processes for <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> and <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m8\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> are conducted independently. In this work, we explore two different speaker perturbation strategies. The first is signal-processing-based, implemented via formant shifting. The second uses speaker anonymization, generating speech with speaker characteristics that differ from those of the original audio. The process of formant shifting is illustrated in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#alg1\" title=\"Algorithm 1 &#8227; 2.3 Speech generation via speaker-perturbation representations &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m16\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m17\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math> adaptively scale and shift the normalized <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m18\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> based on the speaker and emotional representation.\nUsing SEALN, it is possible to synthesize speech with varying emotions for different speakers under the given conditions of <math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m19\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m20\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "emotions",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section describes the experimental data, preprocessing steps, and implementation details.\nThe experimental data come from two languages&#8212;Chinese and English&#8212;and consist of publicly available datasets Biaobei<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.data-baker.com/data/index/TNtts\" title=\"\">https://www.data-baker.com/data/index/TNtts</a></span></span></span>, LJSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ito &amp; Johnson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib16\" title=\"\">2017</a>)</cite>, LibriTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Zen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib36\" title=\"\">2019</a>)</cite>, and ESD <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib37\" title=\"\">2022</a>)</cite>.\nWe designed two categories of experiments: one to evaluate voice cloning performance in a monolingual setting, and the other to assess emotional speech synthesis in a cross-lingual scenario.</p>\n\n",
                "matched_terms": [
                    "crosslingual",
                    "ljspeech",
                    "datasets",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Biaobei</span> dataset contains 10,000 utterances, totaling approximately 12 hours of Mandarin speech. The recordings were conducted in a professional studio using consistent equipment and software throughout the process, with a signal-to-noise ratio (SNR) of no less than 35 dB. The audio is recorded in mono at a sampling rate of 48 kHz, 16-bit resolution, and stored in PCM WAV format. It is one of the most widely used high-quality single-speaker datasets in speech synthesis.</p>\n\n",
                "matched_terms": [
                    "biaobei",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LJSpeech</span> is a publicly available speech dataset containing 13,100 short audio clips of a single speaker reading excerpts from seven non-fiction books. The clips range from 1 to 10 seconds in length and total approximately 24 hours.</p>\n\n",
                "matched_terms": [
                    "ljspeech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ESD</span> dataset contains 350 parallel utterances spoken by 10 native Mandarin speakers, and 10 English speakers with five emotional states (neutral, happy, angry, sad, and surprise).</p>\n\n",
                "matched_terms": [
                    "english",
                    "angry",
                    "surprise",
                    "sad",
                    "neutral",
                    "happy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the voice cloning experiments in a monolingual setting, we used the LibriTTS dataset for training and the <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset of <span class=\"ltx_text ltx_font_bold\">LibriSpeech</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib30\" title=\"\">2015</a>)</cite> for evaluation. This widely used test set contains speech from 40 different speakers and totals 5.4 hours of audio. Following the method described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>, we randomly evaluated 25 utterances per speaker from the LibriSpeech test-clean dataset.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the model&#8217;s performance in zero-shot speech synthesis. Accordingly, our proposed EMM-TTS uses a pretrained speaker embedding instead of a one-hot vector to represent speaker identity. The pretrained representation is the same as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite>, extracted from a pretrained ECAPA-TDNN model. Moreover, no information perturbation was applied to the data during training or inference.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speaker",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the ability to transfer and synthesize emotions across languages. In these scenarios, our proposed EMM-TTS model adopts one-hot vectors as speaker input.\nWe experimented with two different speaker perturbation strategies. One based on signal processing, specifically formant perturbation, and the implementation of formant shifting followed the NANSY <cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib10\" title=\"\">2021</a>)</cite> model by using Praat <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.fon.hum.uva.nl/praat/\" title=\"\">https://www.fon.hum.uva.nl/praat/</a></span></span></span>.\nThe other uses an SSL-based language-independent speaker anonymization method by replacing the speaker embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib27\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite> and its official implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/nii-yamagishilab/SSL-SAS\" title=\"\">https://github.com/nii-yamagishilab/SSL-SAS</a></span></span></span>.\nThe proposed EMM-TTS model defaults to using formant shift as speaker perturbation, while an ablation experiment model, EMM-TTS-SA, is designed for speaker anonymization.</p>\n\n",
                "matched_terms": [
                    "emotions",
                    "model",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DiCLET <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>: This is a cross-lingual emotion transfer method based on a diffusion model that can transfer emotion from the source speaker to the target speaker, including both within-language and cross-lingual target speakers. Furthermore, to alleviate the entanglement among emotion, speaker, and language, multiple classification constraints, such as a speaker classifier and an emotion classifier, are employed, along with adversarial training.</p>\n\n",
                "matched_terms": [
                    "language",
                    "speaker",
                    "training",
                    "crosslingual",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SECS.</span> To assess speaker similarity, we compute SECS using the SOTA\nspeaker verification model, WavLM-Large <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/</a></span></span></span>, to evaluate the speaker similarity, enabling comparison with those studies.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">EECS.</span> Similar to speaker similarity, we compute the emotional similarity of speech, where the emotion embeddings are extracted using the model emotion2vec <span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ddlBoJack/emotion2vec\" title=\"\">https://github.com/ddlBoJack/emotion2vec</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering that objective metrics in cross-lingual scenarios may fail to capture subtle variations in emotion and speaker characteristics, we further conducted the following subjective experiments.</p>\n\n",
                "matched_terms": [
                    "crosslingual",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the subjective evaluation, each system generates 30 sentences for each language. These include six speakers, each contributing one sentence for each of five emotions. A total of 15 participants were invited to evaluate the subjective tests.</p>\n\n",
                "matched_terms": [
                    "language",
                    "emotions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we validate the effectiveness of the proposed method in both monolingual and cross-lingual scenarios. In the monolingual setting, we primarily analyze the performance of voice cloning; in the cross-lingual setting, we also evaluate emotional similarity. We further investigate the impact of speaker perturbations on the model and conduct ablation studies on SEALN and SCL.</p>\n\n",
                "matched_terms": [
                    "crosslingual",
                    "model",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results of EMM-TTS and the baseline models on the LibriSpeech test-clean set are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T2\" title=\"Table 2 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nCompared with ZMM-TTS, incorporating both explicit and implicit emotional representations enables EMM-TTS to achieve higher speaker similarity and improved speech naturalness.\nThis suggests that, in addition to timbre cloning, modeling emotional information can substantially improve speaker similarity with the reference audio.\nCompared with the current state-of-the-art multilingual synthesis model HierSpeech++, EMM-TTS achieves a notable improvement in speaker similarity under the same training data conditions.\nBy comparing the RTF and the number of parameters with HierSpeech++, our model is more lightweight and better suited for computation-constrained environments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speaker",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> present the subjective evaluation results of synthesized Chinese and English speech, respectively. The proposed EMM-TTS achieves the best naturalness. This improvement may be attributed to the XPhoneBERT-based text representation and phoneme encoder, which enable more effective modeling of pronunciations from different languages in a unified space, thereby enhancing multilingual synthesis capability. Furthermore, we find that the naturalness degrades significantly when synthesizing speech with cross-lingual speakers compared to same-lingual speakers. Specifically, when synthesizing text in language B with the voice of a speaker from language A, the generated speech often contains pronunciation errors and accent issues, particularly when English speakers synthesize Chinese speech.</p>\n\n",
                "matched_terms": [
                    "language",
                    "english",
                    "speaker",
                    "chinese",
                    "crosslingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">DMOS</span>, DiCLET-TTS and M3 achieve relatively similar results under monolingual conditions, but M3 exhibits a substantial performance drop in cross-lingual scenarios. This indicates that M3 suffers from weak disentanglement capability. When the reference audio and the target speaker&#8217;s timbre are mismatched, the synthesized speech is heavily affected by the timbre of the reference audio. In contrast, the proposed <span class=\"ltx_text ltx_font_bold\">EMM-TTS</span> consistently achieves the best speaker similarity and emotion similarity in both same-lingual and cross-lingual settings, while also showing the least performance degradation in cross-lingual scenarios. These results demonstrate the effectiveness of our proposed emotion modeling and disentanglement strategies.</p>\n\n",
                "matched_terms": [
                    "crosslingual",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the objective metrics reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T5\" title=\"Table 5 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that EMM-TTS achieves the best performance in both intelligibility and SECS across the two languages. Moreover, consistent with the subjective evaluations, when the target speaker&#8217;s language differs from the synthesized speech, both speaker similarity and intelligibility decline. In contrast, DiCLET-TTS consistently yields the poorest intelligibility (CER) in most cases, which may be attributed to its use of speaker-adversarial learning for text representations, potentially compromising the content quality of the synthesized speech.</p>\n\n",
                "matched_terms": [
                    "language",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to formant shifting, this chapter utilizes a speaker anonymization technique to alter information, aiming to investigate the effects of various interference methods on synthesized speech. First, audio samples from 10 Chinese speakers in the ESD dataset were selected for two types of speaker interference, followed by visualization and quantitative analysis of the interfered audio.\nFor each speaker and each emotion, 50 sentences were selected, resulting in a total of 2,500 sentences for analysis.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a visualization of the speaker representations extracted by a pre-trained ECAPA-TDNN speaker encoder. The representations were reduced to two dimensions using t-SNE, with different colors representing different speakers. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (a), it can be observed that, in the original audio, speaker embeddings of the same speaker cluster closely together, forming distinct clusters. Furthermore, each speaker cluster contains several sub-clusters, which, upon inspection, correspond to different emotions. This phenomenon further confirms that speaker information and emotional information are often entangled. Although ECAPA-TDNN achieves good performance in speaker classification, its learned speaker representations still contain rich emotional details. Furthermore, as shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (b) and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (c), speaker interference methods can effectively alter the speaker information in the audio. Specifically, after interference, the embeddings of audio samples from the same speaker exhibit greater distances. Among the two methods, speaker anonymization imposes the greatest interference with speaker information.</p>\n\n",
                "matched_terms": [
                    "details",
                    "emotions",
                    "speaker",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T7\" title=\"Table 7 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the objective evaluation results of the EMM-TTS model under different speaker perturbation strategies. Compared with the model that applies no speaker perturbation, introducing perturbations reduces the reference speaker&#8217;s influence on the synthesized audio, leading to improved SECS. This result indicates that perturbing speaker information facilitates disentangling emotion from speaker identity. Although the two perturbation methods yield comparable SECS scores, the anonymization-based perturbation causes a noticeable decline in speech intelligibility.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F4\" title=\"Figure 4 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the ABX test preferences for the EMM-TTS (default with formant shift) model when different speaker perturbation methods are applied. The test was conducted on samples spoken by English speakers with Chinese linguistic content. The results show that the formant-shift method outperforms the anonymization-based approach in terms of naturalness, speaker similarity, and emotional similarity. Among these aspects, the gap in emotional similarity is the most pronounced. Although the anonymization-based method effectively disrupts speaker identity, it also weakens emotional cues, leading to synthesized speech that sounds more neutral.</p>\n\n",
                "matched_terms": [
                    "english",
                    "speaker",
                    "neutral",
                    "chinese",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the proposed vec2wav model, several additional modules are incorporated, including the Speaker Consistency Loss (SCL), the Speaker-Emotion Adaptive Layer Normalization (SEALN), and the pretrained emotional representation\nE. The subjective ablation results of these modules are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. As shown in the table, both the SCL constraint and the SSALN module play a crucial role in maintaining similarity to the target speaker. Although speaker perturbation is applied during training, these components enable the model to recover accurate speaker identity from the perturbed representations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speaker",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, removing the pretrained emotional representation\nE leads to a noticeable decrease in emotional similarity. Interestingly, emotional similarity and speaker similarity tend to exhibit a negative correlation&#8212;improving one often comes at the cost of the other. The final EMM-TTS model achieves a balanced trade-off between the two, demonstrating superior overall performance. Future work will explore finer-grained control over both timbre and emotional expressiveness, aiming to achieve a more flexible balance between them.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a two-stage cross-lingual emotional speech synthesis system, EMM-TTS. The first stage focuses on modeling and predicting emotional representations, while the second stage enables fine-grained control over speaker timbre. The two stages are connected through perturbed self-supervised features, which serve as a bridge between emotion and timbre modeling. Experimental results demonstrate that EMM-TTS achieves strong zero-shot voice cloning in monolingual scenarios and effective emotion transfer across languages.</p>\n\n",
                "matched_terms": [
                    "crosslingual",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we proposed EMM-TTS, a two-stage cross-lingual emotional text-to-speech system that effectively disentangles emotion and timbre through speaker-perturbed SSL representations. By leveraging explicit prosodic modeling in the first stage and timbre restoration in the second stage, the system enables controllable emotion transfer and high-fidelity speaker imitation across languages. The proposed Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) further enhance timbre stability and expressive consistency. Moreover, experiments reveal that combining explicit acoustic features with pretrained latent representations improves timbre reproduction. Extensive subjective and objective evaluations confirm that EMM-TTS achieves superior performance in both zero-shot timbre cloning and cross-lingual emotion transfer. In future work, we will explore finer-grained control of emotion intensity and timbre style, as well as adaptive balancing strategies between emotional expressiveness and speaker identity.</p>\n\n",
                "matched_terms": [
                    "crosslingual",
                    "speaker"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker",
        "caption": "Table 2: Voice cloning performance on LibriSpeech test-clean set.",
        "body": "Method\nWER (%) \\downarrow\nUTMOS \\uparrow\nSECS \\uparrow\nRTF\\downarrow\nParams\\downarrow\n\n\nHierSpeech++ (Lee etal., 2025)\n\n2.03\n4.40\n0.591\n0.217\n204M\n\n\nZMM-TTS (Gong etal., 2024)\n\n2.37\n4.07\n0.644\n0.003\n167M\n\n\nEMM-TTS\n2.28\n4.11\n0.661\n0.027\n183M\n\n\nGround-truth\n2.14\n4.13\n-",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">WER (%) <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">UTMOS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">SECS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">RTF<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Params<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">HierSpeech++ <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib18\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">2.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">4.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.591</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.217</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">204M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">2.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">4.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.644</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.003</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">167M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">EMM-TTS</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">2.28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">4.11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.661</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.027</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">183M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Ground-truth</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">2.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">4.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">-</td>\n<td class=\"ltx_td ltx_border_bb ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_border_bb ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "wer",
            "utmos",
            "groundtruth",
            "gong",
            "uparrow",
            "cloning",
            "lee",
            "rtfdownarrow",
            "167m",
            "method",
            "voice",
            "performance",
            "downarrow",
            "secs",
            "testclean",
            "paramsdownarrow",
            "librispeech",
            "set",
            "zmmtts",
            "emmtts",
            "hierspeech",
            "183m",
            "204m"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The results of EMM-TTS and the baseline models on the LibriSpeech test-clean set are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T2\" title=\"Table 2 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nCompared with ZMM-TTS, incorporating both explicit and implicit emotional representations enables EMM-TTS to achieve higher speaker similarity and improved speech naturalness.\nThis suggests that, in addition to timbre cloning, modeling emotional information can substantially improve speaker similarity with the reference audio.\nCompared with the current state-of-the-art multilingual synthesis model HierSpeech++, EMM-TTS achieves a notable improvement in speaker similarity under the same training data conditions.\nBy comparing the RTF and the number of parameters with HierSpeech++, our model is more lightweight and better suited for computation-constrained environments.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-lingual emotional text-to-speech (TTS) aims to produce speech in one language that captures the emotion of a speaker from another language while maintaining the target voice&#8217;s timbre. This process of cross-lingual emotional speech synthesis presents a complex challenge, necessitating flexible control over emotion, timbre, and language.\nHowever, emotion and timbre are highly entangled in speech signals, making fine-grained control challenging. To address this issue, we propose EMM-TTS, a novel two-stage cross-lingual emotional speech synthesis framework based on perturbed self-supervised learning (SSL) representations. In the first stage, the model explicitly and implicitly encodes prosodic cues to capture emotional expressiveness, while the second stage restores the timbre from perturbed SSL representations. We further investigate the effect of different speaker perturbation strategies&#8212;formant shifting and speaker anonymization&#8212;on the disentanglement of emotion and timbre. To strengthen speaker preservation and expressive control, we introduce Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) modules. Additionally, we find that incorporating explicit acoustic features (e.g., F0, energy, and duration) alongside pretrained latent features improves voice cloning performance. Comprehensive multi-metric evaluations, including both subjective and objective measures, demonstrate that EMM-TTS achieves superior naturalness, emotion transferability, and timbre consistency across languages.</p>\n\n",
                "matched_terms": [
                    "cloning",
                    "emmtts",
                    "voice",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fortunately, the rise of self-supervised representations <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib3\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib15\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib7\" title=\"\">2022</a>)</cite> has reduced the model&#8217;s dependence on labeled data.\nMultilingual SSL speech or text representations <cite class=\"ltx_cite ltx_citemacro_citep\">(The Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib34\" title=\"\">2023</a>; Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib11\" title=\"\">2020</a>)</cite> can learn to extract linguistic, paralinguistic, and non-linguistic information from vast amounts of unlabeled data.\nRecently, they have been widely used in cross-lingual TTS to address the above issues and enhance the quality of cross-lingual TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>; Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib32\" title=\"\">2023</a>)</cite>.\nAmong these, ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite> integrates text-based and speech-based self-supervised learning models for multilingual speech synthesis, enabling zero-shot generation under limited data conditions.</p>\n\n",
                "matched_terms": [
                    "gong",
                    "zmmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past year, large-scale speech synthesis systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, leveraging codec models and language models to significantly enhance the capabilities of voice cloning, alongside models based on self-supervised representations.\nWhile these models showcase impressive performance in multilingual and emotional synthesis, their focus on voice cloning and zero-shot capabilities often comes at the expense of flexible control over emotion and timbre.\nMoreover, the entanglement of speaker timbre and emotion in speech may result in speaker timbre leakage during cross-speaker emotion transfer <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nA common strategy for decoupling involves adversarial learning and constraints on classification losses, as demonstrated in previous research <cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib19\" title=\"\">2022a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>.\nThese methods utilize classification loss or gradient reversal to learn representations that isolate emotion or speaker information.\nHowever, adversarial learning would introduce instability and degrade the quality of the synthesized speech. Furthermore, constraints on emotion classification may limit the emotional diversity of synthesized speech. Another straightforward decoupling approach involves speaker perturbation, which alters speaker-specific acoustic properties, such as formants, in speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib38\" title=\"\">2024</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib20\" title=\"\">2022b</a>)</cite>.\nThis perturbation method may degrade speech quality.\nFurthermore, the effects of recent speaker perturbation methods, such as speaker anonymization <cite class=\"ltx_cite ltx_citemacro_citep\">(Tomashenko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib35\" title=\"\">2024</a>; Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite>, on speech synthesis, especially for SSL-based synthesis models, remain an underexplored area.</p>\n\n",
                "matched_terms": [
                    "cloning",
                    "method",
                    "voice",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by the analysis above, this paper extends the previous SSL-based ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite> model by incorporating emotional speech synthesis capabilities and proposes an emotional multilingual multispeaker TTS system (EMM-TTS). The following are the major contributions of this work:</p>\n\n",
                "matched_terms": [
                    "gong",
                    "zmmtts",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a reference speech from speaker <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> (e.g., in Chinese), our goal is to enable speaker <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> (e.g., an English native speaker) to speak Chinese with the reference speech&#8217;s emotion while retaining their own timbre, as depicted in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn contrast to the currently popular voice cloning methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, which determine all attributes of the synthesized speech&#8212;such as emotion and timbre&#8212;based on a single reference audio, our research focuses on cross-lingual emotional speech synthesis. We enable independent control over both emotion and timbre.</p>\n\n",
                "matched_terms": [
                    "cloning",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S2.F2\" title=\"Figure 2 &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the <span class=\"ltx_text ltx_font_typewriter\">txt2vec</span> model proposed in this paper enhances the <span class=\"ltx_text ltx_font_typewriter\">txt2vec</span> model in ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite> and comprises an XPhoneBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(The Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib34\" title=\"\">2023</a>)</cite> encoder, a Mel-style encoder, a variance adaptor, and a decoder for discrete SSL representations.\nIn our study, the primary objective of <span class=\"ltx_text ltx_font_typewriter\">txt2vec</span> is to\npredict SSL representations <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> with sufficient emotional information. To achieve this, the <span class=\"ltx_text ltx_font_typewriter\">txt2vec</span> model approaches emotion modeling from both implicit and explicit perspectives.</p>\n\n",
                "matched_terms": [
                    "gong",
                    "zmmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section describes the experimental data, preprocessing steps, and implementation details.\nThe experimental data come from two languages&#8212;Chinese and English&#8212;and consist of publicly available datasets Biaobei<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.data-baker.com/data/index/TNtts\" title=\"\">https://www.data-baker.com/data/index/TNtts</a></span></span></span>, LJSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ito &amp; Johnson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib16\" title=\"\">2017</a>)</cite>, LibriTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Zen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib36\" title=\"\">2019</a>)</cite>, and ESD <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib37\" title=\"\">2022</a>)</cite>.\nWe designed two categories of experiments: one to evaluate voice cloning performance in a monolingual setting, and the other to assess emotional speech synthesis in a cross-lingual scenario.</p>\n\n",
                "matched_terms": [
                    "cloning",
                    "voice",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the voice cloning experiments in a monolingual setting, we used the LibriTTS dataset for training and the <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset of <span class=\"ltx_text ltx_font_bold\">LibriSpeech</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib30\" title=\"\">2015</a>)</cite> for evaluation. This widely used test set contains speech from 40 different speakers and totals 5.4 hours of audio. Following the method described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>, we randomly evaluated 25 utterances per speaker from the LibriSpeech test-clean dataset.</p>\n\n",
                "matched_terms": [
                    "method",
                    "voice",
                    "librispeech",
                    "set",
                    "cloning",
                    "testclean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the model&#8217;s performance in zero-shot speech synthesis. Accordingly, our proposed EMM-TTS uses a pretrained speaker embedding instead of a one-hot vector to represent speaker identity. The pretrained representation is the same as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite>, extracted from a pretrained ECAPA-TDNN model. Moreover, no information perturbation was applied to the data during training or inference.</p>\n\n",
                "matched_terms": [
                    "gong",
                    "set",
                    "emmtts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reference Model.</span>\nFor the monolingual voice cloning experiments, we compared our EMM-TTS against the following state-of-the-art (SOTA) models.</p>\n\n",
                "matched_terms": [
                    "cloning",
                    "voice",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">HierSpeech++. <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib18\" title=\"\">2025</a>)</cite></span> HierSpeech++ is a fast and efficient zero-shot speech synthesizer for text-to-speech that employs a hierarchical variational autoencoder.\nNote that, for fair comparison, we did not use the super-resolution model. We used the official code and checkpoint for the experiments<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/sh-lee-prml/HierSpeechpp\" title=\"\">https://github.com/sh-lee-prml/HierSpeechpp</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "lee",
                    "hierspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While these models can synthesize multiple languages, we trained them solely on LibriTTS-960 to ensure fairness.\nWe chose LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib30\" title=\"\">2015</a>)</cite> testclean as our benchmark dataset for the zero-shot TTS task.</p>\n\n",
                "matched_terms": [
                    "testclean",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the ability to transfer and synthesize emotions across languages. In these scenarios, our proposed EMM-TTS model adopts one-hot vectors as speaker input.\nWe experimented with two different speaker perturbation strategies. One based on signal processing, specifically formant perturbation, and the implementation of formant shifting followed the NANSY <cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib10\" title=\"\">2021</a>)</cite> model by using Praat <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.fon.hum.uva.nl/praat/\" title=\"\">https://www.fon.hum.uva.nl/praat/</a></span></span></span>.\nThe other uses an SSL-based language-independent speaker anonymization method by replacing the speaker embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib27\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite> and its official implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/nii-yamagishilab/SSL-SAS\" title=\"\">https://github.com/nii-yamagishilab/SSL-SAS</a></span></span></span>.\nThe proposed EMM-TTS model defaults to using formant shift as speaker perturbation, while an ablation experiment model, EMM-TTS-SA, is designed for speaker anonymization.</p>\n\n",
                "matched_terms": [
                    "set",
                    "method",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we validate the effectiveness of the proposed method in both monolingual and cross-lingual scenarios. In the monolingual setting, we primarily analyze the performance of voice cloning; in the cross-lingual setting, we also evaluate emotional similarity. We further investigate the impact of speaker perturbations on the model and conduct ablation studies on SEALN and SCL.</p>\n\n",
                "matched_terms": [
                    "cloning",
                    "method",
                    "voice",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> present the subjective evaluation results of synthesized Chinese and English speech, respectively. The proposed EMM-TTS achieves the best naturalness. This improvement may be attributed to the XPhoneBERT-based text representation and phoneme encoder, which enable more effective modeling of pronunciations from different languages in a unified space, thereby enhancing multilingual synthesis capability. Furthermore, we find that the naturalness degrades significantly when synthesizing speech with cross-lingual speakers compared to same-lingual speakers. Specifically, when synthesizing text in language B with the voice of a speaker from language A, the generated speech often contains pronunciation errors and accent issues, particularly when English speakers synthesize Chinese speech.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">DMOS</span>, DiCLET-TTS and M3 achieve relatively similar results under monolingual conditions, but M3 exhibits a substantial performance drop in cross-lingual scenarios. This indicates that M3 suffers from weak disentanglement capability. When the reference audio and the target speaker&#8217;s timbre are mismatched, the synthesized speech is heavily affected by the timbre of the reference audio. In contrast, the proposed <span class=\"ltx_text ltx_font_bold\">EMM-TTS</span> consistently achieves the best speaker similarity and emotion similarity in both same-lingual and cross-lingual settings, while also showing the least performance degradation in cross-lingual scenarios. These results demonstrate the effectiveness of our proposed emotion modeling and disentanglement strategies.</p>\n\n",
                "matched_terms": [
                    "emmtts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the objective metrics reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T5\" title=\"Table 5 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that EMM-TTS achieves the best performance in both intelligibility and SECS across the two languages. Moreover, consistent with the subjective evaluations, when the target speaker&#8217;s language differs from the synthesized speech, both speaker similarity and intelligibility decline. In contrast, DiCLET-TTS consistently yields the poorest intelligibility (CER) in most cases, which may be attributed to its use of speaker-adversarial learning for text representations, potentially compromising the content quality of the synthesized speech.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "emmtts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the visual analysis, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T6\" title=\"Table 6 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the objective evaluation results for audio processed with different speaker perturbation methods. The SECS results are consistent with the observations in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that both perturbation methods effectively interfere with speaker-related information in the audio. The anonymization-based method produces the most substantial perturbation to speaker identity, but it also inevitably degrades emotional expressiveness. This method, while most effective at obfuscating speaker identity, introduces the greatest emotional distortion.\nAnalysis of the UTMOS and CER values further reveals that the formant-shift method primarily affects the naturalness of speech, whereas the anonymization-based method mainly impacts the linguistic content. On one hand, directly shifting formants tends to make the speech sound less natural. On the other hand, the anonymization approach relies on recognizing and re-synthesizing the speech, and recognition errors can easily accumulate in the anonymized output.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "method",
                    "utmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T7\" title=\"Table 7 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the objective evaluation results of the EMM-TTS model under different speaker perturbation strategies. Compared with the model that applies no speaker perturbation, introducing perturbations reduces the reference speaker&#8217;s influence on the synthesized audio, leading to improved SECS. This result indicates that perturbing speaker information facilitates disentangling emotion from speaker identity. Although the two perturbation methods yield comparable SECS scores, the anonymization-based perturbation causes a noticeable decline in speech intelligibility.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F4\" title=\"Figure 4 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the ABX test preferences for the EMM-TTS (default with formant shift) model when different speaker perturbation methods are applied. The test was conducted on samples spoken by English speakers with Chinese linguistic content. The results show that the formant-shift method outperforms the anonymization-based approach in terms of naturalness, speaker similarity, and emotional similarity. Among these aspects, the gap in emotional similarity is the most pronounced. Although the anonymization-based method effectively disrupts speaker identity, it also weakens emotional cues, leading to synthesized speech that sounds more neutral.</p>\n\n",
                "matched_terms": [
                    "method",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, removing the pretrained emotional representation\nE leads to a noticeable decrease in emotional similarity. Interestingly, emotional similarity and speaker similarity tend to exhibit a negative correlation&#8212;improving one often comes at the cost of the other. The final EMM-TTS model achieves a balanced trade-off between the two, demonstrating superior overall performance. Future work will explore finer-grained control over both timbre and emotional expressiveness, aiming to achieve a more flexible balance between them.</p>\n\n",
                "matched_terms": [
                    "emmtts",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a two-stage cross-lingual emotional speech synthesis system, EMM-TTS. The first stage focuses on modeling and predicting emotional representations, while the second stage enables fine-grained control over speaker timbre. The two stages are connected through perturbed self-supervised features, which serve as a bridge between emotion and timbre modeling. Experimental results demonstrate that EMM-TTS achieves strong zero-shot voice cloning in monolingual scenarios and effective emotion transfer across languages.</p>\n\n",
                "matched_terms": [
                    "cloning",
                    "voice",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study further reveals that pretrained features&#8212;such as high-dimensional latent variables learned by speaker or emotion encoders&#8212;cannot fully replace explicit acoustic features such as pitch, energy, and duration. Experimental results show that incorporating the modeling and prediction of these explicit features enhances the model&#8217;s voice cloning capability. In the emotion transfer stage, we introduce the Speaker Consistency Loss (SCL) and the Speaker-Emotion Adaptive Layer Normalization (SEALN). The ablation results demonstrate that these components contribute positively to maintaining speaker timbre and improving the overall synthesis quality.</p>\n\n",
                "matched_terms": [
                    "cloning",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we proposed EMM-TTS, a two-stage cross-lingual emotional text-to-speech system that effectively disentangles emotion and timbre through speaker-perturbed SSL representations. By leveraging explicit prosodic modeling in the first stage and timbre restoration in the second stage, the system enables controllable emotion transfer and high-fidelity speaker imitation across languages. The proposed Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) further enhance timbre stability and expressive consistency. Moreover, experiments reveal that combining explicit acoustic features with pretrained latent representations improves timbre reproduction. Extensive subjective and objective evaluations confirm that EMM-TTS achieves superior performance in both zero-shot timbre cloning and cross-lingual emotion transfer. In future work, we will explore finer-grained control of emotion intensity and timbre style, as well as adaptive balancing strategies between emotional expressiveness and speaker identity.</p>\n\n",
                "matched_terms": [
                    "cloning",
                    "emmtts",
                    "performance"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker",
        "caption": "Table 3: Subjective evaluation results of Chinese speech (95% confidence interval under t-distribution).",
        "body": "Model/Metric\nCN Speaker\nEN Speaker\n\n\n\nMOS\nDMOS\nEMOS\nMOS\nDMOS\nEMOS\n\n\nM3 (Shang etal., 2021)\n\n3.72\\pm0.12\n3.91\\pm0.20\n3.74\\pm0.25\n3.52\\pm0.14\n3.51\\pm0.31\n3.65\\pm0.17\n\n\nDiCLET-TTS (Li etal., 2023)\n\n4.04\\pm0.28\n3.88\\pm0.16\n3.85\\pm0.18\n3.79\\pm0.31\n3.69\\pm0.30\n3.84\\pm0.25\n\n\nEMM-TTS\n4.12\\pm0.17\n3.95\\pm0.21\n3.97\\pm0.15\n3.92\\pm0.22\n3.81\\pm0.25\n3.96\\pm0.19\n\n\nGT\n4.63\\pm0.13",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model/Metric</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CN Speaker</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">EN Speaker</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">MOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">DMOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">EMOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">MOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">DMOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">EMOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">M3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib33\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.72<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.91<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.74<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.52<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.51<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.65<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.17</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">DiCLET-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">4.04<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.28</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.88<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.16</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.85<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.79<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.69<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.84<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">EMM-TTS</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">4.12<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.95<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.97<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.92<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.81<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m17\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.96<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m18\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">GT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">4.63<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m19\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.13</td>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "shang",
            "speech",
            "404pm028",
            "subjective",
            "372pm012",
            "381pm025",
            "chinese",
            "397pm015",
            "391pm020",
            "385pm018",
            "tdistribution",
            "confidence",
            "evaluation",
            "dmos",
            "379pm031",
            "384pm025",
            "results",
            "351pm031",
            "under",
            "365pm017",
            "mos",
            "412pm017",
            "374pm025",
            "emos",
            "395pm021",
            "diclettts",
            "352pm014",
            "speaker",
            "369pm030",
            "interval",
            "emmtts",
            "modelmetric",
            "396pm019",
            "388pm016",
            "463pm013",
            "392pm022"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> present the subjective evaluation results of synthesized Chinese and English speech, respectively. The proposed EMM-TTS achieves the best naturalness. This improvement may be attributed to the XPhoneBERT-based text representation and phoneme encoder, which enable more effective modeling of pronunciations from different languages in a unified space, thereby enhancing multilingual synthesis capability. Furthermore, we find that the naturalness degrades significantly when synthesizing speech with cross-lingual speakers compared to same-lingual speakers. Specifically, when synthesizing text in language B with the voice of a speaker from language A, the generated speech often contains pronunciation errors and accent issues, particularly when English speakers synthesize Chinese speech.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-lingual emotional text-to-speech (TTS) aims to produce speech in one language that captures the emotion of a speaker from another language while maintaining the target voice&#8217;s timbre. This process of cross-lingual emotional speech synthesis presents a complex challenge, necessitating flexible control over emotion, timbre, and language.\nHowever, emotion and timbre are highly entangled in speech signals, making fine-grained control challenging. To address this issue, we propose EMM-TTS, a novel two-stage cross-lingual emotional speech synthesis framework based on perturbed self-supervised learning (SSL) representations. In the first stage, the model explicitly and implicitly encodes prosodic cues to capture emotional expressiveness, while the second stage restores the timbre from perturbed SSL representations. We further investigate the effect of different speaker perturbation strategies&#8212;formant shifting and speaker anonymization&#8212;on the disentanglement of emotion and timbre. To strengthen speaker preservation and expressive control, we introduce Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) modules. Additionally, we find that incorporating explicit acoustic features (e.g., F0, energy, and duration) alongside pretrained latent features improves voice cloning performance. Comprehensive multi-metric evaluations, including both subjective and objective measures, demonstrate that EMM-TTS achieves superior naturalness, emotion transferability, and timbre consistency across languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis is a key component of the\nhuman&#8211;computer interface that is considered essential to\nresponding and plays a vital role in enabling machines to generate human-like responses.\nThe goals of speech synthesis can be hierarchically categorized, from easier to more challenging, into three levels: intelligibility, naturalness, and expressiveness.\nSpeech synthesis has made significant progress in intelligibility and naturalness, mainly due to advances in deep learning and neural networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib31\" title=\"\">2021</a>; Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nToday, we can generate speech that is often indistinguishable from human speech.\nWhile significant progress has been made in intelligibility and naturalness, achieving expressive and emotionally rich speech remains challenging.\nAnd a challenging research problem persists:\ncross-lingual emotion TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib14\" title=\"\">2024</a>)</cite> refers\nto the task of a speaker of one language to mimic the emotion of a speaker from another language while speaking a different language.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual synthesis poses a more complex challenge in multilingual speech synthesis, as it requires transferring a speaker&#8217;s voice characteristics across languages.\nDespite significant efforts in cross-lingual TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib5\" title=\"\">2024</a>)</cite> research, there remains a noticeable gap in the naturalness of generated speech compared to native speakers.\nThis issue primarily arises from two factors: the lack of data resources and variations in text representations across languages.\nThe most straightforward approach to cross-lingual synthesis is to train the model on bilingual speech data <cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib4\" title=\"\">2023</a>)</cite>, where the same speaker provides utterances in multiple languages.\nRegrettably, collecting such bilingual data is costly, and no large-scale bilingual speech datasets are available.\nAlong with speech data, the lack of text resources is a major obstacle in multilingual speech synthesis.\nConventional speech processing systems that are based on phonetics require pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib21\" title=\"\">2019</a>)</cite>. These dictionaries map phonetic units to their corresponding words.\nCreating such resources requires expert knowledge for each language. Despite the significant human effort involved, many languages still lack sufficient linguistic resources to develop these dictionaries.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fortunately, the rise of self-supervised representations <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib3\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib15\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib7\" title=\"\">2022</a>)</cite> has reduced the model&#8217;s dependence on labeled data.\nMultilingual SSL speech or text representations <cite class=\"ltx_cite ltx_citemacro_citep\">(The Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib34\" title=\"\">2023</a>; Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib11\" title=\"\">2020</a>)</cite> can learn to extract linguistic, paralinguistic, and non-linguistic information from vast amounts of unlabeled data.\nRecently, they have been widely used in cross-lingual TTS to address the above issues and enhance the quality of cross-lingual TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>; Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib32\" title=\"\">2023</a>)</cite>.\nAmong these, ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite> integrates text-based and speech-based self-supervised learning models for multilingual speech synthesis, enabling zero-shot generation under limited data conditions.</p>\n\n",
                "matched_terms": [
                    "under",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past year, large-scale speech synthesis systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, leveraging codec models and language models to significantly enhance the capabilities of voice cloning, alongside models based on self-supervised representations.\nWhile these models showcase impressive performance in multilingual and emotional synthesis, their focus on voice cloning and zero-shot capabilities often comes at the expense of flexible control over emotion and timbre.\nMoreover, the entanglement of speaker timbre and emotion in speech may result in speaker timbre leakage during cross-speaker emotion transfer <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nA common strategy for decoupling involves adversarial learning and constraints on classification losses, as demonstrated in previous research <cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib19\" title=\"\">2022a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>.\nThese methods utilize classification loss or gradient reversal to learn representations that isolate emotion or speaker information.\nHowever, adversarial learning would introduce instability and degrade the quality of the synthesized speech. Furthermore, constraints on emotion classification may limit the emotional diversity of synthesized speech. Another straightforward decoupling approach involves speaker perturbation, which alters speaker-specific acoustic properties, such as formants, in speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib38\" title=\"\">2024</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib20\" title=\"\">2022b</a>)</cite>.\nThis perturbation method may degrade speech quality.\nFurthermore, the effects of recent speaker perturbation methods, such as speaker anonymization <cite class=\"ltx_cite ltx_citemacro_citep\">(Tomashenko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib35\" title=\"\">2024</a>; Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite>, on speech synthesis, especially for SSL-based synthesis models, remain an underexplored area.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by the analysis above, this paper extends the previous SSL-based ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite> model by incorporating emotional speech synthesis capabilities and proposes an emotional multilingual multispeaker TTS system (EMM-TTS). The following are the major contributions of this work:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further improve speech similarity during the speech generation process, we propose a Speaker-Emotion Adaptive Layer Normalization (SEALN) and introduce a Speaker Consistency Loss (SCL).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section will introduce the proposed EMM-TTS framework.\nWe begin with fundamental knowledge about cross-lingual emotion speech synthesis, and then present the two-stage structure.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a reference speech from speaker <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> (e.g., in Chinese), our goal is to enable speaker <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> (e.g., an English native speaker) to speak Chinese with the reference speech&#8217;s emotion while retaining their own timbre, as depicted in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn contrast to the currently popular voice cloning methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, which determine all attributes of the synthesized speech&#8212;such as emotion and timbre&#8212;based on a single reference audio, our research focuses on cross-lingual emotional speech synthesis. We enable independent control over both emotion and timbre.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key challenge in cross-lingual TTS lies in decoupling speaker and emotion information. To address this, we propose a two-stage emotional speech synthesis system, EMM-TTS, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S2.F2\" title=\"Figure 2 &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The first stage <span class=\"ltx_text ltx_font_typewriter\">txt2vec</span> models and predicts emotions, while the second stage <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> controls speaker-specific characteristics.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, in our approach, we perform a speaker perturbation, denoted as <math alttext=\"sp()\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">sp()</annotation></semantics></math>, on the original waveform <math alttext=\"Speech\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">Speech</annotation></semantics></math> during training, which allows us to obtain a speaker-independent\nsignal denoted as <math alttext=\"\\widetilde{\\text{Speech}}=sp(\\text{Speech})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><mo>=</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>Speech</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}=sp(\\text{Speech})</annotation></semantics></math>.\nSubsequently, we extract the multilingual discrete SSL representation and emotion representation from the perturbed <math alttext=\"\\widetilde{\\text{Speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}</annotation></semantics></math>, denoted as <math alttext=\"\\widetilde{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>R</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{R}</annotation></semantics></math> and <math alttext=\"\\widetilde{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>E</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{E}</annotation></semantics></math>. The perturbation processes for <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> and <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m8\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> are conducted independently. In this work, we explore two different speaker perturbation strategies. The first is signal-processing-based, implemented via formant shifting. The second uses speaker anonymization, generating speech with speaker characteristics that differ from those of the original audio. The process of formant shifting is illustrated in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#alg1\" title=\"Algorithm 1 &#8227; 2.3 Speech generation via speaker-perturbation representations &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m16\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m17\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math> adaptively scale and shift the normalized <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m18\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> based on the speaker and emotional representation.\nUsing SEALN, it is possible to synthesize speech with varying emotions for different speakers under the given conditions of <math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m19\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m20\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "under",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> recovers the target timbre from the perturbed features and the speaker ID, we introduced a Speaker Consistency Loss (SCL), as described in paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>)</cite>.\nA pre-trained speaker encoder extracts speaker embeddings from the generated speech and the ground truth. We then maximize the cosine similarity as the speaker consistency loss. Let <math alttext=\"\\phi(.)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS3.p4.m1\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0.167em\">.</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(.)</annotation></semantics></math> be a function that outputs the embedding of a speaker. Let <math alttext=\"cos\\_sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">cos\\_sim</annotation></semantics></math> denote the cosine similarity function, and let <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> be a positive real number that controls the influence of the Speaker Contrastive Loss (SCL) in the final loss calculation. Additionally, let <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> represent the batch size. The SCL is defined as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LJSpeech</span> is a publicly available speech dataset containing 13,100 short audio clips of a single speaker reading excerpts from seven non-fiction books. The clips range from 1 to 10 seconds in length and total approximately 24 hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the voice cloning experiments in a monolingual setting, we used the LibriTTS dataset for training and the <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset of <span class=\"ltx_text ltx_font_bold\">LibriSpeech</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib30\" title=\"\">2015</a>)</cite> for evaluation. This widely used test set contains speech from 40 different speakers and totals 5.4 hours of audio. Following the method described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>, we randomly evaluated 25 utterances per speaker from the LibriSpeech test-clean dataset.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the cross-lingual emotional speech synthesis experiments, the ESD dataset has a limited size of 350 unique sentences per language. Therefore, training includes LJSpeech and Biaobei. To balance emotion and speaker representation, the ESD dataset is upsampled by a factor of 5 during training. The details of the training data in cross-lingual scenarios are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T1\" title=\"Table 1 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the model&#8217;s performance in zero-shot speech synthesis. Accordingly, our proposed EMM-TTS uses a pretrained speaker embedding instead of a one-hot vector to represent speaker identity. The pretrained representation is the same as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite>, extracted from a pretrained ECAPA-TDNN model. Moreover, no information perturbation was applied to the data during training or inference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the ability to transfer and synthesize emotions across languages. In these scenarios, our proposed EMM-TTS model adopts one-hot vectors as speaker input.\nWe experimented with two different speaker perturbation strategies. One based on signal processing, specifically formant perturbation, and the implementation of formant shifting followed the NANSY <cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib10\" title=\"\">2021</a>)</cite> model by using Praat <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.fon.hum.uva.nl/praat/\" title=\"\">https://www.fon.hum.uva.nl/praat/</a></span></span></span>.\nThe other uses an SSL-based language-independent speaker anonymization method by replacing the speaker embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib27\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite> and its official implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/nii-yamagishilab/SSL-SAS\" title=\"\">https://github.com/nii-yamagishilab/SSL-SAS</a></span></span></span>.\nThe proposed EMM-TTS model defaults to using formant shift as speaker perturbation, while an ablation experiment model, EMM-TTS-SA, is designed for speaker anonymization.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">M3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib33\" title=\"\">2021</a>)</cite>: M3 is a multi-speaker, multi-style, multilingual speech synthesis system based on FastSpeech, which incorporates a fine-grained style encoder to alleviate foreign accent issues.\nEmotion IDs and an emotion classifier are introduced into both the style predictor and style encoder to enable M3 for emotional transfer.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "shang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyzed the experimental results using both subjective and objective evaluations, with the following metrics included:</p>\n\n",
                "matched_terms": [
                    "results",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">EECS.</span> Similar to speaker similarity, we compute the emotional similarity of speech, where the emotion embeddings are extracted using the model emotion2vec <span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ddlBoJack/emotion2vec\" title=\"\">https://github.com/ddlBoJack/emotion2vec</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering that objective metrics in cross-lingual scenarios may fail to capture subtle variations in emotion and speaker characteristics, we further conducted the following subjective experiments.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DMOS.</span> The Differential Mean Opinion Score (DMOS) is employed to evaluate the speaker similarity between synthesized and reference audio, on a 1&#8211;5 scale where 1 denotes completely dissimilar and 5 denotes highly similar.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "dmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the subjective evaluation, each system generates 30 sentences for each language. These include six speakers, each contributing one sentence for each of five emotions. A total of 15 participants were invited to evaluate the subjective tests.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results of EMM-TTS and the baseline models on the LibriSpeech test-clean set are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T2\" title=\"Table 2 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nCompared with ZMM-TTS, incorporating both explicit and implicit emotional representations enables EMM-TTS to achieve higher speaker similarity and improved speech naturalness.\nThis suggests that, in addition to timbre cloning, modeling emotional information can substantially improve speaker similarity with the reference audio.\nCompared with the current state-of-the-art multilingual synthesis model HierSpeech++, EMM-TTS achieves a notable improvement in speaker similarity under the same training data conditions.\nBy comparing the RTF and the number of parameters with HierSpeech++, our model is more lightweight and better suited for computation-constrained environments.</p>\n\n",
                "matched_terms": [
                    "under",
                    "speech",
                    "speaker",
                    "emmtts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">DMOS</span>, DiCLET-TTS and M3 achieve relatively similar results under monolingual conditions, but M3 exhibits a substantial performance drop in cross-lingual scenarios. This indicates that M3 suffers from weak disentanglement capability. When the reference audio and the target speaker&#8217;s timbre are mismatched, the synthesized speech is heavily affected by the timbre of the reference audio. In contrast, the proposed <span class=\"ltx_text ltx_font_bold\">EMM-TTS</span> consistently achieves the best speaker similarity and emotion similarity in both same-lingual and cross-lingual settings, while also showing the least performance degradation in cross-lingual scenarios. These results demonstrate the effectiveness of our proposed emotion modeling and disentanglement strategies.</p>\n\n",
                "matched_terms": [
                    "diclettts",
                    "dmos",
                    "under",
                    "speech",
                    "speaker",
                    "emmtts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the objective metrics reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T5\" title=\"Table 5 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that EMM-TTS achieves the best performance in both intelligibility and SECS across the two languages. Moreover, consistent with the subjective evaluations, when the target speaker&#8217;s language differs from the synthesized speech, both speaker similarity and intelligibility decline. In contrast, DiCLET-TTS consistently yields the poorest intelligibility (CER) in most cases, which may be attributed to its use of speaker-adversarial learning for text representations, potentially compromising the content quality of the synthesized speech.</p>\n\n",
                "matched_terms": [
                    "diclettts",
                    "speech",
                    "speaker",
                    "emmtts",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to formant shifting, this chapter utilizes a speaker anonymization technique to alter information, aiming to investigate the effects of various interference methods on synthesized speech. First, audio samples from 10 Chinese speakers in the ESD dataset were selected for two types of speaker interference, followed by visualization and quantitative analysis of the interfered audio.\nFor each speaker and each emotion, 50 sentences were selected, resulting in a total of 2,500 sentences for analysis.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a visualization of the speaker representations extracted by a pre-trained ECAPA-TDNN speaker encoder. The representations were reduced to two dimensions using t-SNE, with different colors representing different speakers. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (a), it can be observed that, in the original audio, speaker embeddings of the same speaker cluster closely together, forming distinct clusters. Furthermore, each speaker cluster contains several sub-clusters, which, upon inspection, correspond to different emotions. This phenomenon further confirms that speaker information and emotional information are often entangled. Although ECAPA-TDNN achieves good performance in speaker classification, its learned speaker representations still contain rich emotional details. Furthermore, as shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (b) and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (c), speaker interference methods can effectively alter the speaker information in the audio. Specifically, after interference, the embeddings of audio samples from the same speaker exhibit greater distances. Among the two methods, speaker anonymization imposes the greatest interference with speaker information.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the visual analysis, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T6\" title=\"Table 6 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the objective evaluation results for audio processed with different speaker perturbation methods. The SECS results are consistent with the observations in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that both perturbation methods effectively interfere with speaker-related information in the audio. The anonymization-based method produces the most substantial perturbation to speaker identity, but it also inevitably degrades emotional expressiveness. This method, while most effective at obfuscating speaker identity, introduces the greatest emotional distortion.\nAnalysis of the UTMOS and CER values further reveals that the formant-shift method primarily affects the naturalness of speech, whereas the anonymization-based method mainly impacts the linguistic content. On one hand, directly shifting formants tends to make the speech sound less natural. On the other hand, the anonymization approach relies on recognizing and re-synthesizing the speech, and recognition errors can easily accumulate in the anonymized output.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T7\" title=\"Table 7 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the objective evaluation results of the EMM-TTS model under different speaker perturbation strategies. Compared with the model that applies no speaker perturbation, introducing perturbations reduces the reference speaker&#8217;s influence on the synthesized audio, leading to improved SECS. This result indicates that perturbing speaker information facilitates disentangling emotion from speaker identity. Although the two perturbation methods yield comparable SECS scores, the anonymization-based perturbation causes a noticeable decline in speech intelligibility.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "under",
                    "speaker",
                    "emmtts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F4\" title=\"Figure 4 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the ABX test preferences for the EMM-TTS (default with formant shift) model when different speaker perturbation methods are applied. The test was conducted on samples spoken by English speakers with Chinese linguistic content. The results show that the formant-shift method outperforms the anonymization-based approach in terms of naturalness, speaker similarity, and emotional similarity. Among these aspects, the gap in emotional similarity is the most pronounced. Although the anonymization-based method effectively disrupts speaker identity, it also weakens emotional cues, leading to synthesized speech that sounds more neutral.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts",
                    "results",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the proposed vec2wav model, several additional modules are incorporated, including the Speaker Consistency Loss (SCL), the Speaker-Emotion Adaptive Layer Normalization (SEALN), and the pretrained emotional representation\nE. The subjective ablation results of these modules are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. As shown in the table, both the SCL constraint and the SSALN module play a crucial role in maintaining similarity to the target speaker. Although speaker perturbation is applied during training, these components enable the model to recover accurate speaker identity from the perturbed representations.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speaker",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, removing the pretrained emotional representation\nE leads to a noticeable decrease in emotional similarity. Interestingly, emotional similarity and speaker similarity tend to exhibit a negative correlation&#8212;improving one often comes at the cost of the other. The final EMM-TTS model achieves a balanced trade-off between the two, demonstrating superior overall performance. Future work will explore finer-grained control over both timbre and emotional expressiveness, aiming to achieve a more flexible balance between them.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a two-stage cross-lingual emotional speech synthesis system, EMM-TTS. The first stage focuses on modeling and predicting emotional representations, while the second stage enables fine-grained control over speaker timbre. The two stages are connected through perturbed self-supervised features, which serve as a bridge between emotion and timbre modeling. Experimental results demonstrate that EMM-TTS achieves strong zero-shot voice cloning in monolingual scenarios and effective emotion transfer across languages.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Timbre and emotion are two highly entangled factors in speech signals, posing challenges for fine-grained control in speech synthesis. Information perturbation is a commonly adopted strategy for disentangling these factors. Previous studies have primarily focused on perturbation methods based on signal processing. In this work, we investigate the capability of recent speaker anonymization models to disentangle emotion and timbre. Our analysis combines visualization, subjective listening tests, and objective audio quality metrics. Experimental results show that signal-processing-based perturbations produce stronger distortion of speaker identity, whereas speaker anonymization models better preserve the naturalness of synthesized speech.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "speaker",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study further reveals that pretrained features&#8212;such as high-dimensional latent variables learned by speaker or emotion encoders&#8212;cannot fully replace explicit acoustic features such as pitch, energy, and duration. Experimental results show that incorporating the modeling and prediction of these explicit features enhances the model&#8217;s voice cloning capability. In the emotion transfer stage, we introduce the Speaker Consistency Loss (SCL) and the Speaker-Emotion Adaptive Layer Normalization (SEALN). The ablation results demonstrate that these components contribute positively to maintaining speaker timbre and improving the overall synthesis quality.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we proposed EMM-TTS, a two-stage cross-lingual emotional text-to-speech system that effectively disentangles emotion and timbre through speaker-perturbed SSL representations. By leveraging explicit prosodic modeling in the first stage and timbre restoration in the second stage, the system enables controllable emotion transfer and high-fidelity speaker imitation across languages. The proposed Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) further enhance timbre stability and expressive consistency. Moreover, experiments reveal that combining explicit acoustic features with pretrained latent representations improves timbre reproduction. Extensive subjective and objective evaluations confirm that EMM-TTS achieves superior performance in both zero-shot timbre cloning and cross-lingual emotion transfer. In future work, we will explore finer-grained control of emotion intensity and timbre style, as well as adaptive balancing strategies between emotional expressiveness and speaker identity.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work will explore more fine-grained control over both emotion and timbre, enabling continuous adjustment of emotional intensity and timbre style. We also plan to investigate adaptive mechanisms that can balance the trade-off between emotional expressiveness and speaker identity preservation. Extending the approach to support more languages and diverse emotional expressions will further enhance the generalization and applicability of the proposed EMM-TTS framework.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts"
                ]
            }
        ]
    },
    "S3.T4": {
        "source_file": "Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker",
        "caption": "Table 4: Subjective evaluation results of English speech (95% confidence interval under t-distribution).",
        "body": "Model/Metric\nCN Speaker\nEN Speaker\n\n\n\nMOS\nDMOS\nEMOS\nMOS\nDMOS\nEMOS\n\n\nM3 (Shang etal., 2021)\n\n3.42\\pm0.14\n2.98\\pm0.11\n3.01\\pm0.37\n3.64\\pm0.17\n3.78\\pm0.13\n3.67\\pm0.15\n\n\nDiCLET-TTS (Li etal., 2023)\n\n3.67\\pm0.18\n3.59\\pm0.12\n3.62\\pm0.22\n3.81\\pm0.31\n3.90\\pm0.26\n3.73\\pm0.20\n\n\nEMM-TTS\n3.89\\pm0.11\n3.68\\pm0.25\n3.71\\pm0.18\n4.07\\pm0.24\n4.06\\pm0.21\n3.87\\pm0.22\n\n\nGT\n\n\n\n4.37\\pm0.12",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model/Metric</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CN Speaker</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">EN Speaker</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">MOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">DMOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">EMOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">MOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">DMOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">EMOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">M3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib33\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.42<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">2.98<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.01<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.64<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.78<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.67<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">DiCLET-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.67<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.59<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.62<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.81<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.31</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.90<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.26</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.73<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">EMM-TTS</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.89<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.11</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.68<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.25</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.71<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">4.07<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">4.06<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m17\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">3.87<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m18\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.22</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">GT</td>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">4.37<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m19\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.12</td>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "shang",
            "389pm011",
            "speech",
            "subjective",
            "367pm015",
            "tdistribution",
            "confidence",
            "evaluation",
            "dmos",
            "results",
            "368pm025",
            "387pm022",
            "390pm026",
            "362pm022",
            "359pm012",
            "406pm021",
            "under",
            "364pm017",
            "301pm037",
            "mos",
            "298pm011",
            "373pm020",
            "emos",
            "371pm018",
            "437pm012",
            "english",
            "diclettts",
            "407pm024",
            "speaker",
            "378pm013",
            "interval",
            "381pm031",
            "modelmetric",
            "emmtts",
            "367pm018",
            "342pm014"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> present the subjective evaluation results of synthesized Chinese and English speech, respectively. The proposed EMM-TTS achieves the best naturalness. This improvement may be attributed to the XPhoneBERT-based text representation and phoneme encoder, which enable more effective modeling of pronunciations from different languages in a unified space, thereby enhancing multilingual synthesis capability. Furthermore, we find that the naturalness degrades significantly when synthesizing speech with cross-lingual speakers compared to same-lingual speakers. Specifically, when synthesizing text in language B with the voice of a speaker from language A, the generated speech often contains pronunciation errors and accent issues, particularly when English speakers synthesize Chinese speech.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-lingual emotional text-to-speech (TTS) aims to produce speech in one language that captures the emotion of a speaker from another language while maintaining the target voice&#8217;s timbre. This process of cross-lingual emotional speech synthesis presents a complex challenge, necessitating flexible control over emotion, timbre, and language.\nHowever, emotion and timbre are highly entangled in speech signals, making fine-grained control challenging. To address this issue, we propose EMM-TTS, a novel two-stage cross-lingual emotional speech synthesis framework based on perturbed self-supervised learning (SSL) representations. In the first stage, the model explicitly and implicitly encodes prosodic cues to capture emotional expressiveness, while the second stage restores the timbre from perturbed SSL representations. We further investigate the effect of different speaker perturbation strategies&#8212;formant shifting and speaker anonymization&#8212;on the disentanglement of emotion and timbre. To strengthen speaker preservation and expressive control, we introduce Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) modules. Additionally, we find that incorporating explicit acoustic features (e.g., F0, energy, and duration) alongside pretrained latent features improves voice cloning performance. Comprehensive multi-metric evaluations, including both subjective and objective measures, demonstrate that EMM-TTS achieves superior naturalness, emotion transferability, and timbre consistency across languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis is a key component of the\nhuman&#8211;computer interface that is considered essential to\nresponding and plays a vital role in enabling machines to generate human-like responses.\nThe goals of speech synthesis can be hierarchically categorized, from easier to more challenging, into three levels: intelligibility, naturalness, and expressiveness.\nSpeech synthesis has made significant progress in intelligibility and naturalness, mainly due to advances in deep learning and neural networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib31\" title=\"\">2021</a>; Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nToday, we can generate speech that is often indistinguishable from human speech.\nWhile significant progress has been made in intelligibility and naturalness, achieving expressive and emotionally rich speech remains challenging.\nAnd a challenging research problem persists:\ncross-lingual emotion TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib14\" title=\"\">2024</a>)</cite> refers\nto the task of a speaker of one language to mimic the emotion of a speaker from another language while speaking a different language.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual synthesis poses a more complex challenge in multilingual speech synthesis, as it requires transferring a speaker&#8217;s voice characteristics across languages.\nDespite significant efforts in cross-lingual TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib5\" title=\"\">2024</a>)</cite> research, there remains a noticeable gap in the naturalness of generated speech compared to native speakers.\nThis issue primarily arises from two factors: the lack of data resources and variations in text representations across languages.\nThe most straightforward approach to cross-lingual synthesis is to train the model on bilingual speech data <cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib4\" title=\"\">2023</a>)</cite>, where the same speaker provides utterances in multiple languages.\nRegrettably, collecting such bilingual data is costly, and no large-scale bilingual speech datasets are available.\nAlong with speech data, the lack of text resources is a major obstacle in multilingual speech synthesis.\nConventional speech processing systems that are based on phonetics require pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib21\" title=\"\">2019</a>)</cite>. These dictionaries map phonetic units to their corresponding words.\nCreating such resources requires expert knowledge for each language. Despite the significant human effort involved, many languages still lack sufficient linguistic resources to develop these dictionaries.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fortunately, the rise of self-supervised representations <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib3\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib15\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib7\" title=\"\">2022</a>)</cite> has reduced the model&#8217;s dependence on labeled data.\nMultilingual SSL speech or text representations <cite class=\"ltx_cite ltx_citemacro_citep\">(The Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib34\" title=\"\">2023</a>; Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib11\" title=\"\">2020</a>)</cite> can learn to extract linguistic, paralinguistic, and non-linguistic information from vast amounts of unlabeled data.\nRecently, they have been widely used in cross-lingual TTS to address the above issues and enhance the quality of cross-lingual TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>; Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib32\" title=\"\">2023</a>)</cite>.\nAmong these, ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite> integrates text-based and speech-based self-supervised learning models for multilingual speech synthesis, enabling zero-shot generation under limited data conditions.</p>\n\n",
                "matched_terms": [
                    "under",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past year, large-scale speech synthesis systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, leveraging codec models and language models to significantly enhance the capabilities of voice cloning, alongside models based on self-supervised representations.\nWhile these models showcase impressive performance in multilingual and emotional synthesis, their focus on voice cloning and zero-shot capabilities often comes at the expense of flexible control over emotion and timbre.\nMoreover, the entanglement of speaker timbre and emotion in speech may result in speaker timbre leakage during cross-speaker emotion transfer <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nA common strategy for decoupling involves adversarial learning and constraints on classification losses, as demonstrated in previous research <cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib19\" title=\"\">2022a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>.\nThese methods utilize classification loss or gradient reversal to learn representations that isolate emotion or speaker information.\nHowever, adversarial learning would introduce instability and degrade the quality of the synthesized speech. Furthermore, constraints on emotion classification may limit the emotional diversity of synthesized speech. Another straightforward decoupling approach involves speaker perturbation, which alters speaker-specific acoustic properties, such as formants, in speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib38\" title=\"\">2024</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib20\" title=\"\">2022b</a>)</cite>.\nThis perturbation method may degrade speech quality.\nFurthermore, the effects of recent speaker perturbation methods, such as speaker anonymization <cite class=\"ltx_cite ltx_citemacro_citep\">(Tomashenko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib35\" title=\"\">2024</a>; Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite>, on speech synthesis, especially for SSL-based synthesis models, remain an underexplored area.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by the analysis above, this paper extends the previous SSL-based ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite> model by incorporating emotional speech synthesis capabilities and proposes an emotional multilingual multispeaker TTS system (EMM-TTS). The following are the major contributions of this work:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further improve speech similarity during the speech generation process, we propose a Speaker-Emotion Adaptive Layer Normalization (SEALN) and introduce a Speaker Consistency Loss (SCL).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section will introduce the proposed EMM-TTS framework.\nWe begin with fundamental knowledge about cross-lingual emotion speech synthesis, and then present the two-stage structure.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a reference speech from speaker <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> (e.g., in Chinese), our goal is to enable speaker <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> (e.g., an English native speaker) to speak Chinese with the reference speech&#8217;s emotion while retaining their own timbre, as depicted in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn contrast to the currently popular voice cloning methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, which determine all attributes of the synthesized speech&#8212;such as emotion and timbre&#8212;based on a single reference audio, our research focuses on cross-lingual emotional speech synthesis. We enable independent control over both emotion and timbre.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key challenge in cross-lingual TTS lies in decoupling speaker and emotion information. To address this, we propose a two-stage emotional speech synthesis system, EMM-TTS, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S2.F2\" title=\"Figure 2 &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The first stage <span class=\"ltx_text ltx_font_typewriter\">txt2vec</span> models and predicts emotions, while the second stage <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> controls speaker-specific characteristics.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, in our approach, we perform a speaker perturbation, denoted as <math alttext=\"sp()\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">sp()</annotation></semantics></math>, on the original waveform <math alttext=\"Speech\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">Speech</annotation></semantics></math> during training, which allows us to obtain a speaker-independent\nsignal denoted as <math alttext=\"\\widetilde{\\text{Speech}}=sp(\\text{Speech})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><mo>=</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>Speech</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}=sp(\\text{Speech})</annotation></semantics></math>.\nSubsequently, we extract the multilingual discrete SSL representation and emotion representation from the perturbed <math alttext=\"\\widetilde{\\text{Speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}</annotation></semantics></math>, denoted as <math alttext=\"\\widetilde{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>R</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{R}</annotation></semantics></math> and <math alttext=\"\\widetilde{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>E</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{E}</annotation></semantics></math>. The perturbation processes for <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> and <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m8\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> are conducted independently. In this work, we explore two different speaker perturbation strategies. The first is signal-processing-based, implemented via formant shifting. The second uses speaker anonymization, generating speech with speaker characteristics that differ from those of the original audio. The process of formant shifting is illustrated in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#alg1\" title=\"Algorithm 1 &#8227; 2.3 Speech generation via speaker-perturbation representations &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m16\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m17\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math> adaptively scale and shift the normalized <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m18\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> based on the speaker and emotional representation.\nUsing SEALN, it is possible to synthesize speech with varying emotions for different speakers under the given conditions of <math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m19\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m20\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "under",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> recovers the target timbre from the perturbed features and the speaker ID, we introduced a Speaker Consistency Loss (SCL), as described in paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>)</cite>.\nA pre-trained speaker encoder extracts speaker embeddings from the generated speech and the ground truth. We then maximize the cosine similarity as the speaker consistency loss. Let <math alttext=\"\\phi(.)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS3.p4.m1\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0.167em\">.</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(.)</annotation></semantics></math> be a function that outputs the embedding of a speaker. Let <math alttext=\"cos\\_sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">cos\\_sim</annotation></semantics></math> denote the cosine similarity function, and let <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> be a positive real number that controls the influence of the Speaker Contrastive Loss (SCL) in the final loss calculation. Additionally, let <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> represent the batch size. The SCL is defined as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LJSpeech</span> is a publicly available speech dataset containing 13,100 short audio clips of a single speaker reading excerpts from seven non-fiction books. The clips range from 1 to 10 seconds in length and total approximately 24 hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the voice cloning experiments in a monolingual setting, we used the LibriTTS dataset for training and the <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset of <span class=\"ltx_text ltx_font_bold\">LibriSpeech</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib30\" title=\"\">2015</a>)</cite> for evaluation. This widely used test set contains speech from 40 different speakers and totals 5.4 hours of audio. Following the method described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>, we randomly evaluated 25 utterances per speaker from the LibriSpeech test-clean dataset.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the cross-lingual emotional speech synthesis experiments, the ESD dataset has a limited size of 350 unique sentences per language. Therefore, training includes LJSpeech and Biaobei. To balance emotion and speaker representation, the ESD dataset is upsampled by a factor of 5 during training. The details of the training data in cross-lingual scenarios are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T1\" title=\"Table 1 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the model&#8217;s performance in zero-shot speech synthesis. Accordingly, our proposed EMM-TTS uses a pretrained speaker embedding instead of a one-hot vector to represent speaker identity. The pretrained representation is the same as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite>, extracted from a pretrained ECAPA-TDNN model. Moreover, no information perturbation was applied to the data during training or inference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the ability to transfer and synthesize emotions across languages. In these scenarios, our proposed EMM-TTS model adopts one-hot vectors as speaker input.\nWe experimented with two different speaker perturbation strategies. One based on signal processing, specifically formant perturbation, and the implementation of formant shifting followed the NANSY <cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib10\" title=\"\">2021</a>)</cite> model by using Praat <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.fon.hum.uva.nl/praat/\" title=\"\">https://www.fon.hum.uva.nl/praat/</a></span></span></span>.\nThe other uses an SSL-based language-independent speaker anonymization method by replacing the speaker embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib27\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite> and its official implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/nii-yamagishilab/SSL-SAS\" title=\"\">https://github.com/nii-yamagishilab/SSL-SAS</a></span></span></span>.\nThe proposed EMM-TTS model defaults to using formant shift as speaker perturbation, while an ablation experiment model, EMM-TTS-SA, is designed for speaker anonymization.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">M3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib33\" title=\"\">2021</a>)</cite>: M3 is a multi-speaker, multi-style, multilingual speech synthesis system based on FastSpeech, which incorporates a fine-grained style encoder to alleviate foreign accent issues.\nEmotion IDs and an emotion classifier are introduced into both the style predictor and style encoder to enable M3 for emotional transfer.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "shang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyzed the experimental results using both subjective and objective evaluations, with the following metrics included:</p>\n\n",
                "matched_terms": [
                    "results",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">EECS.</span> Similar to speaker similarity, we compute the emotional similarity of speech, where the emotion embeddings are extracted using the model emotion2vec <span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ddlBoJack/emotion2vec\" title=\"\">https://github.com/ddlBoJack/emotion2vec</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering that objective metrics in cross-lingual scenarios may fail to capture subtle variations in emotion and speaker characteristics, we further conducted the following subjective experiments.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DMOS.</span> The Differential Mean Opinion Score (DMOS) is employed to evaluate the speaker similarity between synthesized and reference audio, on a 1&#8211;5 scale where 1 denotes completely dissimilar and 5 denotes highly similar.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "dmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the subjective evaluation, each system generates 30 sentences for each language. These include six speakers, each contributing one sentence for each of five emotions. A total of 15 participants were invited to evaluate the subjective tests.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results of EMM-TTS and the baseline models on the LibriSpeech test-clean set are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T2\" title=\"Table 2 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nCompared with ZMM-TTS, incorporating both explicit and implicit emotional representations enables EMM-TTS to achieve higher speaker similarity and improved speech naturalness.\nThis suggests that, in addition to timbre cloning, modeling emotional information can substantially improve speaker similarity with the reference audio.\nCompared with the current state-of-the-art multilingual synthesis model HierSpeech++, EMM-TTS achieves a notable improvement in speaker similarity under the same training data conditions.\nBy comparing the RTF and the number of parameters with HierSpeech++, our model is more lightweight and better suited for computation-constrained environments.</p>\n\n",
                "matched_terms": [
                    "under",
                    "speech",
                    "speaker",
                    "emmtts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">DMOS</span>, DiCLET-TTS and M3 achieve relatively similar results under monolingual conditions, but M3 exhibits a substantial performance drop in cross-lingual scenarios. This indicates that M3 suffers from weak disentanglement capability. When the reference audio and the target speaker&#8217;s timbre are mismatched, the synthesized speech is heavily affected by the timbre of the reference audio. In contrast, the proposed <span class=\"ltx_text ltx_font_bold\">EMM-TTS</span> consistently achieves the best speaker similarity and emotion similarity in both same-lingual and cross-lingual settings, while also showing the least performance degradation in cross-lingual scenarios. These results demonstrate the effectiveness of our proposed emotion modeling and disentanglement strategies.</p>\n\n",
                "matched_terms": [
                    "diclettts",
                    "dmos",
                    "under",
                    "speech",
                    "speaker",
                    "emmtts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the objective metrics reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T5\" title=\"Table 5 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that EMM-TTS achieves the best performance in both intelligibility and SECS across the two languages. Moreover, consistent with the subjective evaluations, when the target speaker&#8217;s language differs from the synthesized speech, both speaker similarity and intelligibility decline. In contrast, DiCLET-TTS consistently yields the poorest intelligibility (CER) in most cases, which may be attributed to its use of speaker-adversarial learning for text representations, potentially compromising the content quality of the synthesized speech.</p>\n\n",
                "matched_terms": [
                    "diclettts",
                    "speech",
                    "speaker",
                    "emmtts",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to formant shifting, this chapter utilizes a speaker anonymization technique to alter information, aiming to investigate the effects of various interference methods on synthesized speech. First, audio samples from 10 Chinese speakers in the ESD dataset were selected for two types of speaker interference, followed by visualization and quantitative analysis of the interfered audio.\nFor each speaker and each emotion, 50 sentences were selected, resulting in a total of 2,500 sentences for analysis.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a visualization of the speaker representations extracted by a pre-trained ECAPA-TDNN speaker encoder. The representations were reduced to two dimensions using t-SNE, with different colors representing different speakers. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (a), it can be observed that, in the original audio, speaker embeddings of the same speaker cluster closely together, forming distinct clusters. Furthermore, each speaker cluster contains several sub-clusters, which, upon inspection, correspond to different emotions. This phenomenon further confirms that speaker information and emotional information are often entangled. Although ECAPA-TDNN achieves good performance in speaker classification, its learned speaker representations still contain rich emotional details. Furthermore, as shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (b) and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (c), speaker interference methods can effectively alter the speaker information in the audio. Specifically, after interference, the embeddings of audio samples from the same speaker exhibit greater distances. Among the two methods, speaker anonymization imposes the greatest interference with speaker information.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the visual analysis, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T6\" title=\"Table 6 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the objective evaluation results for audio processed with different speaker perturbation methods. The SECS results are consistent with the observations in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that both perturbation methods effectively interfere with speaker-related information in the audio. The anonymization-based method produces the most substantial perturbation to speaker identity, but it also inevitably degrades emotional expressiveness. This method, while most effective at obfuscating speaker identity, introduces the greatest emotional distortion.\nAnalysis of the UTMOS and CER values further reveals that the formant-shift method primarily affects the naturalness of speech, whereas the anonymization-based method mainly impacts the linguistic content. On one hand, directly shifting formants tends to make the speech sound less natural. On the other hand, the anonymization approach relies on recognizing and re-synthesizing the speech, and recognition errors can easily accumulate in the anonymized output.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T7\" title=\"Table 7 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the objective evaluation results of the EMM-TTS model under different speaker perturbation strategies. Compared with the model that applies no speaker perturbation, introducing perturbations reduces the reference speaker&#8217;s influence on the synthesized audio, leading to improved SECS. This result indicates that perturbing speaker information facilitates disentangling emotion from speaker identity. Although the two perturbation methods yield comparable SECS scores, the anonymization-based perturbation causes a noticeable decline in speech intelligibility.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "under",
                    "speaker",
                    "emmtts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F4\" title=\"Figure 4 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the ABX test preferences for the EMM-TTS (default with formant shift) model when different speaker perturbation methods are applied. The test was conducted on samples spoken by English speakers with Chinese linguistic content. The results show that the formant-shift method outperforms the anonymization-based approach in terms of naturalness, speaker similarity, and emotional similarity. Among these aspects, the gap in emotional similarity is the most pronounced. Although the anonymization-based method effectively disrupts speaker identity, it also weakens emotional cues, leading to synthesized speech that sounds more neutral.</p>\n\n",
                "matched_terms": [
                    "english",
                    "speech",
                    "speaker",
                    "emmtts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the proposed vec2wav model, several additional modules are incorporated, including the Speaker Consistency Loss (SCL), the Speaker-Emotion Adaptive Layer Normalization (SEALN), and the pretrained emotional representation\nE. The subjective ablation results of these modules are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. As shown in the table, both the SCL constraint and the SSALN module play a crucial role in maintaining similarity to the target speaker. Although speaker perturbation is applied during training, these components enable the model to recover accurate speaker identity from the perturbed representations.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speaker",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, removing the pretrained emotional representation\nE leads to a noticeable decrease in emotional similarity. Interestingly, emotional similarity and speaker similarity tend to exhibit a negative correlation&#8212;improving one often comes at the cost of the other. The final EMM-TTS model achieves a balanced trade-off between the two, demonstrating superior overall performance. Future work will explore finer-grained control over both timbre and emotional expressiveness, aiming to achieve a more flexible balance between them.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a two-stage cross-lingual emotional speech synthesis system, EMM-TTS. The first stage focuses on modeling and predicting emotional representations, while the second stage enables fine-grained control over speaker timbre. The two stages are connected through perturbed self-supervised features, which serve as a bridge between emotion and timbre modeling. Experimental results demonstrate that EMM-TTS achieves strong zero-shot voice cloning in monolingual scenarios and effective emotion transfer across languages.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Timbre and emotion are two highly entangled factors in speech signals, posing challenges for fine-grained control in speech synthesis. Information perturbation is a commonly adopted strategy for disentangling these factors. Previous studies have primarily focused on perturbation methods based on signal processing. In this work, we investigate the capability of recent speaker anonymization models to disentangle emotion and timbre. Our analysis combines visualization, subjective listening tests, and objective audio quality metrics. Experimental results show that signal-processing-based perturbations produce stronger distortion of speaker identity, whereas speaker anonymization models better preserve the naturalness of synthesized speech.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "speaker",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study further reveals that pretrained features&#8212;such as high-dimensional latent variables learned by speaker or emotion encoders&#8212;cannot fully replace explicit acoustic features such as pitch, energy, and duration. Experimental results show that incorporating the modeling and prediction of these explicit features enhances the model&#8217;s voice cloning capability. In the emotion transfer stage, we introduce the Speaker Consistency Loss (SCL) and the Speaker-Emotion Adaptive Layer Normalization (SEALN). The ablation results demonstrate that these components contribute positively to maintaining speaker timbre and improving the overall synthesis quality.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we proposed EMM-TTS, a two-stage cross-lingual emotional text-to-speech system that effectively disentangles emotion and timbre through speaker-perturbed SSL representations. By leveraging explicit prosodic modeling in the first stage and timbre restoration in the second stage, the system enables controllable emotion transfer and high-fidelity speaker imitation across languages. The proposed Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) further enhance timbre stability and expressive consistency. Moreover, experiments reveal that combining explicit acoustic features with pretrained latent representations improves timbre reproduction. Extensive subjective and objective evaluations confirm that EMM-TTS achieves superior performance in both zero-shot timbre cloning and cross-lingual emotion transfer. In future work, we will explore finer-grained control of emotion intensity and timbre style, as well as adaptive balancing strategies between emotional expressiveness and speaker identity.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work will explore more fine-grained control over both emotion and timbre, enabling continuous adjustment of emotional intensity and timbre style. We also plan to investigate adaptive mechanisms that can balance the trade-off between emotional expressiveness and speaker identity preservation. Extending the approach to support more languages and diverse emotional expressions will further enhance the generalization and applicability of the proposed EMM-TTS framework.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts"
                ]
            }
        ]
    },
    "S3.T5": {
        "source_file": "Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker",
        "caption": "Table 5: Objective evaluation results of Chinese and English speech synthesized by different systems.",
        "body": "CN Speech\nEN speech\n\n\nModel/Metric\nCN Speaker\nEN Speaker\nCN Speaker\nEN Speaker\n\n\n\nSECS\nCER\nSECS\nCER\nSECS\nCER\nSECS\nCER\n\n\nM3 (Shang etal., 2021)\n\n0.563\n8.13\n0.521\n10.03\n0.607\n10.15\n0.538\n9.74\n\n\nDiCLET-TTS (Li etal., 2023)\n\n0.621\n9.92\n0.557\n10.91\n0.524\n11.26\n0.552\n10.25\n\n\nEMM-TTS\n0.662\n7.13\n0.643\n7.47\n0.597\n8.90\n0.614\n8.21",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CN Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">EN speech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model/Metric</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CN Speaker</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">EN Speaker</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CN Speaker</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">EN Speaker</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">SECS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">SECS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">SECS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">SECS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CER</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">M3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib33\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.563</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">8.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.521</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">10.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.607</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">10.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.538</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">9.74</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">DiCLET-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.621</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">9.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.557</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">10.91</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.524</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">11.26</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.552</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">10.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">EMM-TTS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.662</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">7.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.643</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">7.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.597</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">8.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.614</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">8.21</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "english",
            "synthesized",
            "systems",
            "diclettts",
            "shang",
            "evaluation",
            "speech",
            "speaker",
            "emmtts",
            "objective",
            "results",
            "different",
            "modelmetric",
            "secs",
            "chinese",
            "cer"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">From the objective metrics reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T5\" title=\"Table 5 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that EMM-TTS achieves the best performance in both intelligibility and SECS across the two languages. Moreover, consistent with the subjective evaluations, when the target speaker&#8217;s language differs from the synthesized speech, both speaker similarity and intelligibility decline. In contrast, DiCLET-TTS consistently yields the poorest intelligibility (CER) in most cases, which may be attributed to its use of speaker-adversarial learning for text representations, potentially compromising the content quality of the synthesized speech.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-lingual emotional text-to-speech (TTS) aims to produce speech in one language that captures the emotion of a speaker from another language while maintaining the target voice&#8217;s timbre. This process of cross-lingual emotional speech synthesis presents a complex challenge, necessitating flexible control over emotion, timbre, and language.\nHowever, emotion and timbre are highly entangled in speech signals, making fine-grained control challenging. To address this issue, we propose EMM-TTS, a novel two-stage cross-lingual emotional speech synthesis framework based on perturbed self-supervised learning (SSL) representations. In the first stage, the model explicitly and implicitly encodes prosodic cues to capture emotional expressiveness, while the second stage restores the timbre from perturbed SSL representations. We further investigate the effect of different speaker perturbation strategies&#8212;formant shifting and speaker anonymization&#8212;on the disentanglement of emotion and timbre. To strengthen speaker preservation and expressive control, we introduce Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) modules. Additionally, we find that incorporating explicit acoustic features (e.g., F0, energy, and duration) alongside pretrained latent features improves voice cloning performance. Comprehensive multi-metric evaluations, including both subjective and objective measures, demonstrate that EMM-TTS achieves superior naturalness, emotion transferability, and timbre consistency across languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts",
                    "different",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis is a key component of the\nhuman&#8211;computer interface that is considered essential to\nresponding and plays a vital role in enabling machines to generate human-like responses.\nThe goals of speech synthesis can be hierarchically categorized, from easier to more challenging, into three levels: intelligibility, naturalness, and expressiveness.\nSpeech synthesis has made significant progress in intelligibility and naturalness, mainly due to advances in deep learning and neural networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib31\" title=\"\">2021</a>; Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nToday, we can generate speech that is often indistinguishable from human speech.\nWhile significant progress has been made in intelligibility and naturalness, achieving expressive and emotionally rich speech remains challenging.\nAnd a challenging research problem persists:\ncross-lingual emotion TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib14\" title=\"\">2024</a>)</cite> refers\nto the task of a speaker of one language to mimic the emotion of a speaker from another language while speaking a different language.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual synthesis poses a more complex challenge in multilingual speech synthesis, as it requires transferring a speaker&#8217;s voice characteristics across languages.\nDespite significant efforts in cross-lingual TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib5\" title=\"\">2024</a>)</cite> research, there remains a noticeable gap in the naturalness of generated speech compared to native speakers.\nThis issue primarily arises from two factors: the lack of data resources and variations in text representations across languages.\nThe most straightforward approach to cross-lingual synthesis is to train the model on bilingual speech data <cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib4\" title=\"\">2023</a>)</cite>, where the same speaker provides utterances in multiple languages.\nRegrettably, collecting such bilingual data is costly, and no large-scale bilingual speech datasets are available.\nAlong with speech data, the lack of text resources is a major obstacle in multilingual speech synthesis.\nConventional speech processing systems that are based on phonetics require pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib21\" title=\"\">2019</a>)</cite>. These dictionaries map phonetic units to their corresponding words.\nCreating such resources requires expert knowledge for each language. Despite the significant human effort involved, many languages still lack sufficient linguistic resources to develop these dictionaries.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past year, large-scale speech synthesis systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, leveraging codec models and language models to significantly enhance the capabilities of voice cloning, alongside models based on self-supervised representations.\nWhile these models showcase impressive performance in multilingual and emotional synthesis, their focus on voice cloning and zero-shot capabilities often comes at the expense of flexible control over emotion and timbre.\nMoreover, the entanglement of speaker timbre and emotion in speech may result in speaker timbre leakage during cross-speaker emotion transfer <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nA common strategy for decoupling involves adversarial learning and constraints on classification losses, as demonstrated in previous research <cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib19\" title=\"\">2022a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>.\nThese methods utilize classification loss or gradient reversal to learn representations that isolate emotion or speaker information.\nHowever, adversarial learning would introduce instability and degrade the quality of the synthesized speech. Furthermore, constraints on emotion classification may limit the emotional diversity of synthesized speech. Another straightforward decoupling approach involves speaker perturbation, which alters speaker-specific acoustic properties, such as formants, in speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib38\" title=\"\">2024</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib20\" title=\"\">2022b</a>)</cite>.\nThis perturbation method may degrade speech quality.\nFurthermore, the effects of recent speaker perturbation methods, such as speaker anonymization <cite class=\"ltx_cite ltx_citemacro_citep\">(Tomashenko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib35\" title=\"\">2024</a>; Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite>, on speech synthesis, especially for SSL-based synthesis models, remain an underexplored area.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "speech",
                    "synthesized",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by the analysis above, this paper extends the previous SSL-based ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite> model by incorporating emotional speech synthesis capabilities and proposes an emotional multilingual multispeaker TTS system (EMM-TTS). The following are the major contributions of this work:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we explore the effects of two different speaker perturbation methods&#8212;formant shift and speaker anonymization&#8212;on the quality of synthesized audio.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "synthesized",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further improve speech similarity during the speech generation process, we propose a Speaker-Emotion Adaptive Layer Normalization (SEALN) and introduce a Speaker Consistency Loss (SCL).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section will introduce the proposed EMM-TTS framework.\nWe begin with fundamental knowledge about cross-lingual emotion speech synthesis, and then present the two-stage structure.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a reference speech from speaker <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> (e.g., in Chinese), our goal is to enable speaker <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> (e.g., an English native speaker) to speak Chinese with the reference speech&#8217;s emotion while retaining their own timbre, as depicted in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn contrast to the currently popular voice cloning methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, which determine all attributes of the synthesized speech&#8212;such as emotion and timbre&#8212;based on a single reference audio, our research focuses on cross-lingual emotional speech synthesis. We enable independent control over both emotion and timbre.</p>\n\n",
                "matched_terms": [
                    "english",
                    "synthesized",
                    "speech",
                    "speaker",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key challenge in cross-lingual TTS lies in decoupling speaker and emotion information. To address this, we propose a two-stage emotional speech synthesis system, EMM-TTS, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S2.F2\" title=\"Figure 2 &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The first stage <span class=\"ltx_text ltx_font_typewriter\">txt2vec</span> models and predicts emotions, while the second stage <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> controls speaker-specific characteristics.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, in our approach, we perform a speaker perturbation, denoted as <math alttext=\"sp()\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">sp()</annotation></semantics></math>, on the original waveform <math alttext=\"Speech\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">Speech</annotation></semantics></math> during training, which allows us to obtain a speaker-independent\nsignal denoted as <math alttext=\"\\widetilde{\\text{Speech}}=sp(\\text{Speech})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><mo>=</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>Speech</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}=sp(\\text{Speech})</annotation></semantics></math>.\nSubsequently, we extract the multilingual discrete SSL representation and emotion representation from the perturbed <math alttext=\"\\widetilde{\\text{Speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}</annotation></semantics></math>, denoted as <math alttext=\"\\widetilde{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>R</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{R}</annotation></semantics></math> and <math alttext=\"\\widetilde{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>E</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{E}</annotation></semantics></math>. The perturbation processes for <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> and <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m8\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> are conducted independently. In this work, we explore two different speaker perturbation strategies. The first is signal-processing-based, implemented via formant shifting. The second uses speaker anonymization, generating speech with speaker characteristics that differ from those of the original audio. The process of formant shifting is illustrated in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#alg1\" title=\"Algorithm 1 &#8227; 2.3 Speech generation via speaker-perturbation representations &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m16\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m17\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math> adaptively scale and shift the normalized <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m18\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> based on the speaker and emotional representation.\nUsing SEALN, it is possible to synthesize speech with varying emotions for different speakers under the given conditions of <math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m19\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m20\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> recovers the target timbre from the perturbed features and the speaker ID, we introduced a Speaker Consistency Loss (SCL), as described in paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>)</cite>.\nA pre-trained speaker encoder extracts speaker embeddings from the generated speech and the ground truth. We then maximize the cosine similarity as the speaker consistency loss. Let <math alttext=\"\\phi(.)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS3.p4.m1\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0.167em\">.</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(.)</annotation></semantics></math> be a function that outputs the embedding of a speaker. Let <math alttext=\"cos\\_sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">cos\\_sim</annotation></semantics></math> denote the cosine similarity function, and let <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> be a positive real number that controls the influence of the Speaker Contrastive Loss (SCL) in the final loss calculation. Additionally, let <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> represent the batch size. The SCL is defined as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"g\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m5\" intent=\":literal\"><semantics><mi>g</mi><annotation encoding=\"application/x-tex\">g</annotation></semantics></math> and <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m6\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> represent, respectively, the ground truth and the generated speaker audio.\nFinally, the optimization objective of the entire <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> process consists of two components: reconstruction loss used in the original <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> of ZMM-TTS and speaker consistency loss.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LJSpeech</span> is a publicly available speech dataset containing 13,100 short audio clips of a single speaker reading excerpts from seven non-fiction books. The clips range from 1 to 10 seconds in length and total approximately 24 hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the voice cloning experiments in a monolingual setting, we used the LibriTTS dataset for training and the <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset of <span class=\"ltx_text ltx_font_bold\">LibriSpeech</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib30\" title=\"\">2015</a>)</cite> for evaluation. This widely used test set contains speech from 40 different speakers and totals 5.4 hours of audio. Following the method described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>, we randomly evaluated 25 utterances per speaker from the LibriSpeech test-clean dataset.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the cross-lingual emotional speech synthesis experiments, the ESD dataset has a limited size of 350 unique sentences per language. Therefore, training includes LJSpeech and Biaobei. To balance emotion and speaker representation, the ESD dataset is upsampled by a factor of 5 during training. The details of the training data in cross-lingual scenarios are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T1\" title=\"Table 1 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection presents the details of two different experimental setups, including baseline models, evaluation metrics, and other relevant configurations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the model&#8217;s performance in zero-shot speech synthesis. Accordingly, our proposed EMM-TTS uses a pretrained speaker embedding instead of a one-hot vector to represent speaker identity. The pretrained representation is the same as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite>, extracted from a pretrained ECAPA-TDNN model. Moreover, no information perturbation was applied to the data during training or inference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the ability to transfer and synthesize emotions across languages. In these scenarios, our proposed EMM-TTS model adopts one-hot vectors as speaker input.\nWe experimented with two different speaker perturbation strategies. One based on signal processing, specifically formant perturbation, and the implementation of formant shifting followed the NANSY <cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib10\" title=\"\">2021</a>)</cite> model by using Praat <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.fon.hum.uva.nl/praat/\" title=\"\">https://www.fon.hum.uva.nl/praat/</a></span></span></span>.\nThe other uses an SSL-based language-independent speaker anonymization method by replacing the speaker embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib27\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite> and its official implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/nii-yamagishilab/SSL-SAS\" title=\"\">https://github.com/nii-yamagishilab/SSL-SAS</a></span></span></span>.\nThe proposed EMM-TTS model defaults to using formant shift as speaker perturbation, while an ablation experiment model, EMM-TTS-SA, is designed for speaker anonymization.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">M3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Shang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib33\" title=\"\">2021</a>)</cite>: M3 is a multi-speaker, multi-style, multilingual speech synthesis system based on FastSpeech, which incorporates a fine-grained style encoder to alleviate foreign accent issues.\nEmotion IDs and an emotion classifier are introduced into both the style predictor and style encoder to enable M3 for emotional transfer.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "shang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyzed the experimental results using both subjective and objective evaluations, with the following metrics included:</p>\n\n",
                "matched_terms": [
                    "results",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective metrics mainly evaluate the naturalness and similarity of the synthesized audio in both monolingual and cross-lingual experiments.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SECS.</span> To assess speaker similarity, we compute SECS using the SOTA\nspeaker verification model, WavLM-Large <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/</a></span></span></span>, to evaluate the speaker similarity, enabling comparison with those studies.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CER.</span> We employ whisper-large-v3 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span> to transcribe the synthesized speech into text, which is then compared with the ground-truth transcripts to compute the character error rate (CER).</p>\n\n",
                "matched_terms": [
                    "cer",
                    "speech",
                    "synthesized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">EECS.</span> Similar to speaker similarity, we compute the emotional similarity of speech, where the emotion embeddings are extracted using the model emotion2vec <span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ddlBoJack/emotion2vec\" title=\"\">https://github.com/ddlBoJack/emotion2vec</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering that objective metrics in cross-lingual scenarios may fail to capture subtle variations in emotion and speaker characteristics, we further conducted the following subjective experiments.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DMOS.</span> The Differential Mean Opinion Score (DMOS) is employed to evaluate the speaker similarity between synthesized and reference audio, on a 1&#8211;5 scale where 1 denotes completely dissimilar and 5 denotes highly similar.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "synthesized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results of EMM-TTS and the baseline models on the LibriSpeech test-clean set are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T2\" title=\"Table 2 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nCompared with ZMM-TTS, incorporating both explicit and implicit emotional representations enables EMM-TTS to achieve higher speaker similarity and improved speech naturalness.\nThis suggests that, in addition to timbre cloning, modeling emotional information can substantially improve speaker similarity with the reference audio.\nCompared with the current state-of-the-art multilingual synthesis model HierSpeech++, EMM-TTS achieves a notable improvement in speaker similarity under the same training data conditions.\nBy comparing the RTF and the number of parameters with HierSpeech++, our model is more lightweight and better suited for computation-constrained environments.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> present the subjective evaluation results of synthesized Chinese and English speech, respectively. The proposed EMM-TTS achieves the best naturalness. This improvement may be attributed to the XPhoneBERT-based text representation and phoneme encoder, which enable more effective modeling of pronunciations from different languages in a unified space, thereby enhancing multilingual synthesis capability. Furthermore, we find that the naturalness degrades significantly when synthesizing speech with cross-lingual speakers compared to same-lingual speakers. Specifically, when synthesizing text in language B with the voice of a speaker from language A, the generated speech often contains pronunciation errors and accent issues, particularly when English speakers synthesize Chinese speech.</p>\n\n",
                "matched_terms": [
                    "english",
                    "synthesized",
                    "evaluation",
                    "speech",
                    "speaker",
                    "emmtts",
                    "different",
                    "results",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">DMOS</span>, DiCLET-TTS and M3 achieve relatively similar results under monolingual conditions, but M3 exhibits a substantial performance drop in cross-lingual scenarios. This indicates that M3 suffers from weak disentanglement capability. When the reference audio and the target speaker&#8217;s timbre are mismatched, the synthesized speech is heavily affected by the timbre of the reference audio. In contrast, the proposed <span class=\"ltx_text ltx_font_bold\">EMM-TTS</span> consistently achieves the best speaker similarity and emotion similarity in both same-lingual and cross-lingual settings, while also showing the least performance degradation in cross-lingual scenarios. These results demonstrate the effectiveness of our proposed emotion modeling and disentanglement strategies.</p>\n\n",
                "matched_terms": [
                    "diclettts",
                    "synthesized",
                    "speech",
                    "speaker",
                    "emmtts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to formant shifting, this chapter utilizes a speaker anonymization technique to alter information, aiming to investigate the effects of various interference methods on synthesized speech. First, audio samples from 10 Chinese speakers in the ESD dataset were selected for two types of speaker interference, followed by visualization and quantitative analysis of the interfered audio.\nFor each speaker and each emotion, 50 sentences were selected, resulting in a total of 2,500 sentences for analysis.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a visualization of the speaker representations extracted by a pre-trained ECAPA-TDNN speaker encoder. The representations were reduced to two dimensions using t-SNE, with different colors representing different speakers. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (a), it can be observed that, in the original audio, speaker embeddings of the same speaker cluster closely together, forming distinct clusters. Furthermore, each speaker cluster contains several sub-clusters, which, upon inspection, correspond to different emotions. This phenomenon further confirms that speaker information and emotional information are often entangled. Although ECAPA-TDNN achieves good performance in speaker classification, its learned speaker representations still contain rich emotional details. Furthermore, as shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (b) and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (c), speaker interference methods can effectively alter the speaker information in the audio. Specifically, after interference, the embeddings of audio samples from the same speaker exhibit greater distances. Among the two methods, speaker anonymization imposes the greatest interference with speaker information.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "speech",
                    "speaker",
                    "different",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the visual analysis, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T6\" title=\"Table 6 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the objective evaluation results for audio processed with different speaker perturbation methods. The SECS results are consistent with the observations in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that both perturbation methods effectively interfere with speaker-related information in the audio. The anonymization-based method produces the most substantial perturbation to speaker identity, but it also inevitably degrades emotional expressiveness. This method, while most effective at obfuscating speaker identity, introduces the greatest emotional distortion.\nAnalysis of the UTMOS and CER values further reveals that the formant-shift method primarily affects the naturalness of speech, whereas the anonymization-based method mainly impacts the linguistic content. On one hand, directly shifting formants tends to make the speech sound less natural. On the other hand, the anonymization approach relies on recognizing and re-synthesizing the speech, and recognition errors can easily accumulate in the anonymized output.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "speaker",
                    "objective",
                    "results",
                    "different",
                    "secs",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T7\" title=\"Table 7 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the objective evaluation results of the EMM-TTS model under different speaker perturbation strategies. Compared with the model that applies no speaker perturbation, introducing perturbations reduces the reference speaker&#8217;s influence on the synthesized audio, leading to improved SECS. This result indicates that perturbing speaker information facilitates disentangling emotion from speaker identity. Although the two perturbation methods yield comparable SECS scores, the anonymization-based perturbation causes a noticeable decline in speech intelligibility.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "evaluation",
                    "speech",
                    "speaker",
                    "emmtts",
                    "objective",
                    "results",
                    "different",
                    "secs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F4\" title=\"Figure 4 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the ABX test preferences for the EMM-TTS (default with formant shift) model when different speaker perturbation methods are applied. The test was conducted on samples spoken by English speakers with Chinese linguistic content. The results show that the formant-shift method outperforms the anonymization-based approach in terms of naturalness, speaker similarity, and emotional similarity. Among these aspects, the gap in emotional similarity is the most pronounced. Although the anonymization-based method effectively disrupts speaker identity, it also weakens emotional cues, leading to synthesized speech that sounds more neutral.</p>\n\n",
                "matched_terms": [
                    "english",
                    "synthesized",
                    "speech",
                    "speaker",
                    "emmtts",
                    "different",
                    "results",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the proposed vec2wav model, several additional modules are incorporated, including the Speaker Consistency Loss (SCL), the Speaker-Emotion Adaptive Layer Normalization (SEALN), and the pretrained emotional representation\nE. The subjective ablation results of these modules are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. As shown in the table, both the SCL constraint and the SSALN module play a crucial role in maintaining similarity to the target speaker. Although speaker perturbation is applied during training, these components enable the model to recover accurate speaker identity from the perturbed representations.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, removing the pretrained emotional representation\nE leads to a noticeable decrease in emotional similarity. Interestingly, emotional similarity and speaker similarity tend to exhibit a negative correlation&#8212;improving one often comes at the cost of the other. The final EMM-TTS model achieves a balanced trade-off between the two, demonstrating superior overall performance. Future work will explore finer-grained control over both timbre and emotional expressiveness, aiming to achieve a more flexible balance between them.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a two-stage cross-lingual emotional speech synthesis system, EMM-TTS. The first stage focuses on modeling and predicting emotional representations, while the second stage enables fine-grained control over speaker timbre. The two stages are connected through perturbed self-supervised features, which serve as a bridge between emotion and timbre modeling. Experimental results demonstrate that EMM-TTS achieves strong zero-shot voice cloning in monolingual scenarios and effective emotion transfer across languages.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Timbre and emotion are two highly entangled factors in speech signals, posing challenges for fine-grained control in speech synthesis. Information perturbation is a commonly adopted strategy for disentangling these factors. Previous studies have primarily focused on perturbation methods based on signal processing. In this work, we investigate the capability of recent speaker anonymization models to disentangle emotion and timbre. Our analysis combines visualization, subjective listening tests, and objective audio quality metrics. Experimental results show that signal-processing-based perturbations produce stronger distortion of speaker identity, whereas speaker anonymization models better preserve the naturalness of synthesized speech.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "speech",
                    "speaker",
                    "objective",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study further reveals that pretrained features&#8212;such as high-dimensional latent variables learned by speaker or emotion encoders&#8212;cannot fully replace explicit acoustic features such as pitch, energy, and duration. Experimental results show that incorporating the modeling and prediction of these explicit features enhances the model&#8217;s voice cloning capability. In the emotion transfer stage, we introduce the Speaker Consistency Loss (SCL) and the Speaker-Emotion Adaptive Layer Normalization (SEALN). The ablation results demonstrate that these components contribute positively to maintaining speaker timbre and improving the overall synthesis quality.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we proposed EMM-TTS, a two-stage cross-lingual emotional text-to-speech system that effectively disentangles emotion and timbre through speaker-perturbed SSL representations. By leveraging explicit prosodic modeling in the first stage and timbre restoration in the second stage, the system enables controllable emotion transfer and high-fidelity speaker imitation across languages. The proposed Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) further enhance timbre stability and expressive consistency. Moreover, experiments reveal that combining explicit acoustic features with pretrained latent representations improves timbre reproduction. Extensive subjective and objective evaluations confirm that EMM-TTS achieves superior performance in both zero-shot timbre cloning and cross-lingual emotion transfer. In future work, we will explore finer-grained control of emotion intensity and timbre style, as well as adaptive balancing strategies between emotional expressiveness and speaker identity.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work will explore more fine-grained control over both emotion and timbre, enabling continuous adjustment of emotional intensity and timbre style. We also plan to investigate adaptive mechanisms that can balance the trade-off between emotional expressiveness and speaker identity preservation. Extending the approach to support more languages and diverse emotional expressions will further enhance the generalization and applicability of the proposed EMM-TTS framework.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts"
                ]
            }
        ]
    },
    "S4.T6": {
        "source_file": "Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker",
        "caption": "Table 6: Objective evaluation of speech after applying two different speaker perturbations.",
        "body": "Method\nSECS\nEECS\nCER (%)\nUTMOS\n\n\nFormant shift\n0.514\n0.848\n9.07\n2.163\n\n\nSpeaker anonymization\n0.354\n0.799\n20.57\n3.055\n\n\nOriginal audio\n1.000\n1.000\n4.88\n2.907",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">SECS</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">EECS</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">CER (%)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">UTMOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Formant shift</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.514</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.848</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">9.07</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.163</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Speaker anonymization</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.354</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">0.799</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">20.57</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.055</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Original audio</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.000</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">1.000</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">4.88</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">2.907</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "original",
            "speech",
            "anonymization",
            "utmos",
            "evaluation",
            "objective",
            "different",
            "audio",
            "applying",
            "method",
            "shift",
            "secs",
            "perturbations",
            "two",
            "speaker",
            "after",
            "eecs",
            "cer",
            "formant"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In addition to the visual analysis, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T6\" title=\"Table 6 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the objective evaluation results for audio processed with different speaker perturbation methods. The SECS results are consistent with the observations in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that both perturbation methods effectively interfere with speaker-related information in the audio. The anonymization-based method produces the most substantial perturbation to speaker identity, but it also inevitably degrades emotional expressiveness. This method, while most effective at obfuscating speaker identity, introduces the greatest emotional distortion.\nAnalysis of the UTMOS and CER values further reveals that the formant-shift method primarily affects the naturalness of speech, whereas the anonymization-based method mainly impacts the linguistic content. On one hand, directly shifting formants tends to make the speech sound less natural. On the other hand, the anonymization approach relies on recognizing and re-synthesizing the speech, and recognition errors can easily accumulate in the anonymized output.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-lingual emotional text-to-speech (TTS) aims to produce speech in one language that captures the emotion of a speaker from another language while maintaining the target voice&#8217;s timbre. This process of cross-lingual emotional speech synthesis presents a complex challenge, necessitating flexible control over emotion, timbre, and language.\nHowever, emotion and timbre are highly entangled in speech signals, making fine-grained control challenging. To address this issue, we propose EMM-TTS, a novel two-stage cross-lingual emotional speech synthesis framework based on perturbed self-supervised learning (SSL) representations. In the first stage, the model explicitly and implicitly encodes prosodic cues to capture emotional expressiveness, while the second stage restores the timbre from perturbed SSL representations. We further investigate the effect of different speaker perturbation strategies&#8212;formant shifting and speaker anonymization&#8212;on the disentanglement of emotion and timbre. To strengthen speaker preservation and expressive control, we introduce Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) modules. Additionally, we find that incorporating explicit acoustic features (e.g., F0, energy, and duration) alongside pretrained latent features improves voice cloning performance. Comprehensive multi-metric evaluations, including both subjective and objective measures, demonstrate that EMM-TTS achieves superior naturalness, emotion transferability, and timbre consistency across languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "different",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis is a key component of the\nhuman&#8211;computer interface that is considered essential to\nresponding and plays a vital role in enabling machines to generate human-like responses.\nThe goals of speech synthesis can be hierarchically categorized, from easier to more challenging, into three levels: intelligibility, naturalness, and expressiveness.\nSpeech synthesis has made significant progress in intelligibility and naturalness, mainly due to advances in deep learning and neural networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib31\" title=\"\">2021</a>; Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nToday, we can generate speech that is often indistinguishable from human speech.\nWhile significant progress has been made in intelligibility and naturalness, achieving expressive and emotionally rich speech remains challenging.\nAnd a challenging research problem persists:\ncross-lingual emotion TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib14\" title=\"\">2024</a>)</cite> refers\nto the task of a speaker of one language to mimic the emotion of a speaker from another language while speaking a different language.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual synthesis poses a more complex challenge in multilingual speech synthesis, as it requires transferring a speaker&#8217;s voice characteristics across languages.\nDespite significant efforts in cross-lingual TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib5\" title=\"\">2024</a>)</cite> research, there remains a noticeable gap in the naturalness of generated speech compared to native speakers.\nThis issue primarily arises from two factors: the lack of data resources and variations in text representations across languages.\nThe most straightforward approach to cross-lingual synthesis is to train the model on bilingual speech data <cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib4\" title=\"\">2023</a>)</cite>, where the same speaker provides utterances in multiple languages.\nRegrettably, collecting such bilingual data is costly, and no large-scale bilingual speech datasets are available.\nAlong with speech data, the lack of text resources is a major obstacle in multilingual speech synthesis.\nConventional speech processing systems that are based on phonetics require pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib21\" title=\"\">2019</a>)</cite>. These dictionaries map phonetic units to their corresponding words.\nCreating such resources requires expert knowledge for each language. Despite the significant human effort involved, many languages still lack sufficient linguistic resources to develop these dictionaries.</p>\n\n",
                "matched_terms": [
                    "two",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past year, large-scale speech synthesis systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, leveraging codec models and language models to significantly enhance the capabilities of voice cloning, alongside models based on self-supervised representations.\nWhile these models showcase impressive performance in multilingual and emotional synthesis, their focus on voice cloning and zero-shot capabilities often comes at the expense of flexible control over emotion and timbre.\nMoreover, the entanglement of speaker timbre and emotion in speech may result in speaker timbre leakage during cross-speaker emotion transfer <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nA common strategy for decoupling involves adversarial learning and constraints on classification losses, as demonstrated in previous research <cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib19\" title=\"\">2022a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>.\nThese methods utilize classification loss or gradient reversal to learn representations that isolate emotion or speaker information.\nHowever, adversarial learning would introduce instability and degrade the quality of the synthesized speech. Furthermore, constraints on emotion classification may limit the emotional diversity of synthesized speech. Another straightforward decoupling approach involves speaker perturbation, which alters speaker-specific acoustic properties, such as formants, in speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib38\" title=\"\">2024</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib20\" title=\"\">2022b</a>)</cite>.\nThis perturbation method may degrade speech quality.\nFurthermore, the effects of recent speaker perturbation methods, such as speaker anonymization <cite class=\"ltx_cite ltx_citemacro_citep\">(Tomashenko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib35\" title=\"\">2024</a>; Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite>, on speech synthesis, especially for SSL-based synthesis models, remain an underexplored area.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "speech",
                    "method",
                    "anonymization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we explore the effects of two different speaker perturbation methods&#8212;formant shift and speaker anonymization&#8212;on the quality of synthesized audio.</p>\n\n",
                "matched_terms": [
                    "shift",
                    "speaker",
                    "different",
                    "two",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further improve speech similarity during the speech generation process, we propose a Speaker-Emotion Adaptive Layer Normalization (SEALN) and introduce a Speaker Consistency Loss (SCL).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a reference speech from speaker <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> (e.g., in Chinese), our goal is to enable speaker <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> (e.g., an English native speaker) to speak Chinese with the reference speech&#8217;s emotion while retaining their own timbre, as depicted in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn contrast to the currently popular voice cloning methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, which determine all attributes of the synthesized speech&#8212;such as emotion and timbre&#8212;based on a single reference audio, our research focuses on cross-lingual emotional speech synthesis. We enable independent control over both emotion and timbre.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key challenge in cross-lingual TTS lies in decoupling speaker and emotion information. To address this, we propose a two-stage emotional speech synthesis system, EMM-TTS, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S2.F2\" title=\"Figure 2 &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The first stage <span class=\"ltx_text ltx_font_typewriter\">txt2vec</span> models and predicts emotions, while the second stage <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> controls speaker-specific characteristics.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, in our approach, we perform a speaker perturbation, denoted as <math alttext=\"sp()\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">sp()</annotation></semantics></math>, on the original waveform <math alttext=\"Speech\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">Speech</annotation></semantics></math> during training, which allows us to obtain a speaker-independent\nsignal denoted as <math alttext=\"\\widetilde{\\text{Speech}}=sp(\\text{Speech})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><mo>=</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>Speech</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}=sp(\\text{Speech})</annotation></semantics></math>.\nSubsequently, we extract the multilingual discrete SSL representation and emotion representation from the perturbed <math alttext=\"\\widetilde{\\text{Speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}</annotation></semantics></math>, denoted as <math alttext=\"\\widetilde{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>R</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{R}</annotation></semantics></math> and <math alttext=\"\\widetilde{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>E</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{E}</annotation></semantics></math>. The perturbation processes for <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> and <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m8\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> are conducted independently. In this work, we explore two different speaker perturbation strategies. The first is signal-processing-based, implemented via formant shifting. The second uses speaker anonymization, generating speech with speaker characteristics that differ from those of the original audio. The process of formant shifting is illustrated in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#alg1\" title=\"Algorithm 1 &#8227; 2.3 Speech generation via speaker-perturbation representations &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "original",
                    "speech",
                    "speaker",
                    "different",
                    "anonymization",
                    "two",
                    "audio",
                    "formant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m16\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m17\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math> adaptively scale and shift the normalized <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m18\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> based on the speaker and emotional representation.\nUsing SEALN, it is possible to synthesize speech with varying emotions for different speakers under the given conditions of <math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m19\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m20\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "shift",
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> recovers the target timbre from the perturbed features and the speaker ID, we introduced a Speaker Consistency Loss (SCL), as described in paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>)</cite>.\nA pre-trained speaker encoder extracts speaker embeddings from the generated speech and the ground truth. We then maximize the cosine similarity as the speaker consistency loss. Let <math alttext=\"\\phi(.)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS3.p4.m1\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0.167em\">.</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(.)</annotation></semantics></math> be a function that outputs the embedding of a speaker. Let <math alttext=\"cos\\_sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">cos\\_sim</annotation></semantics></math> denote the cosine similarity function, and let <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> be a positive real number that controls the influence of the Speaker Contrastive Loss (SCL) in the final loss calculation. Additionally, let <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> represent the batch size. The SCL is defined as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"g\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m5\" intent=\":literal\"><semantics><mi>g</mi><annotation encoding=\"application/x-tex\">g</annotation></semantics></math> and <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m6\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> represent, respectively, the ground truth and the generated speaker audio.\nFinally, the optimization objective of the entire <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> process consists of two components: reconstruction loss used in the original <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> of ZMM-TTS and speaker consistency loss.</p>\n\n",
                "matched_terms": [
                    "original",
                    "speaker",
                    "objective",
                    "two",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section describes the experimental data, preprocessing steps, and implementation details.\nThe experimental data come from two languages&#8212;Chinese and English&#8212;and consist of publicly available datasets Biaobei<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.data-baker.com/data/index/TNtts\" title=\"\">https://www.data-baker.com/data/index/TNtts</a></span></span></span>, LJSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Ito &amp; Johnson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib16\" title=\"\">2017</a>)</cite>, LibriTTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Zen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib36\" title=\"\">2019</a>)</cite>, and ESD <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib37\" title=\"\">2022</a>)</cite>.\nWe designed two categories of experiments: one to evaluate voice cloning performance in a monolingual setting, and the other to assess emotional speech synthesis in a cross-lingual scenario.</p>\n\n",
                "matched_terms": [
                    "two",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Biaobei</span> dataset contains 10,000 utterances, totaling approximately 12 hours of Mandarin speech. The recordings were conducted in a professional studio using consistent equipment and software throughout the process, with a signal-to-noise ratio (SNR) of no less than 35 dB. The audio is recorded in mono at a sampling rate of 48 kHz, 16-bit resolution, and stored in PCM WAV format. It is one of the most widely used high-quality single-speaker datasets in speech synthesis.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LJSpeech</span> is a publicly available speech dataset containing 13,100 short audio clips of a single speaker reading excerpts from seven non-fiction books. The clips range from 1 to 10 seconds in length and total approximately 24 hours.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the voice cloning experiments in a monolingual setting, we used the LibriTTS dataset for training and the <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset of <span class=\"ltx_text ltx_font_bold\">LibriSpeech</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib30\" title=\"\">2015</a>)</cite> for evaluation. This widely used test set contains speech from 40 different speakers and totals 5.4 hours of audio. Following the method described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>, we randomly evaluated 25 utterances per speaker from the LibriSpeech test-clean dataset.</p>\n\n",
                "matched_terms": [
                    "method",
                    "evaluation",
                    "speech",
                    "speaker",
                    "different",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the cross-lingual emotional speech synthesis experiments, the ESD dataset has a limited size of 350 unique sentences per language. Therefore, training includes LJSpeech and Biaobei. To balance emotion and speaker representation, the ESD dataset is upsampled by a factor of 5 during training. The details of the training data in cross-lingual scenarios are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T1\" title=\"Table 1 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection presents the details of two different experimental setups, including baseline models, evaluation metrics, and other relevant configurations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "two",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the model&#8217;s performance in zero-shot speech synthesis. Accordingly, our proposed EMM-TTS uses a pretrained speaker embedding instead of a one-hot vector to represent speaker identity. The pretrained representation is the same as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite>, extracted from a pretrained ECAPA-TDNN model. Moreover, no information perturbation was applied to the data during training or inference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the ability to transfer and synthesize emotions across languages. In these scenarios, our proposed EMM-TTS model adopts one-hot vectors as speaker input.\nWe experimented with two different speaker perturbation strategies. One based on signal processing, specifically formant perturbation, and the implementation of formant shifting followed the NANSY <cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib10\" title=\"\">2021</a>)</cite> model by using Praat <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.fon.hum.uva.nl/praat/\" title=\"\">https://www.fon.hum.uva.nl/praat/</a></span></span></span>.\nThe other uses an SSL-based language-independent speaker anonymization method by replacing the speaker embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib27\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite> and its official implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/nii-yamagishilab/SSL-SAS\" title=\"\">https://github.com/nii-yamagishilab/SSL-SAS</a></span></span></span>.\nThe proposed EMM-TTS model defaults to using formant shift as speaker perturbation, while an ablation experiment model, EMM-TTS-SA, is designed for speaker anonymization.</p>\n\n",
                "matched_terms": [
                    "method",
                    "shift",
                    "speaker",
                    "different",
                    "anonymization",
                    "two",
                    "formant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">DiCLET <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>: This is a cross-lingual emotion transfer method based on a diffusion model that can transfer emotion from the source speaker to the target speaker, including both within-language and cross-lingual target speakers. Furthermore, to alleviate the entanglement among emotion, speaker, and language, multiple classification constraints, such as a speaker classifier and an emotion classifier, are employed, along with adversarial training.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective metrics mainly evaluate the naturalness and similarity of the synthesized audio in both monolingual and cross-lingual experiments.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SECS.</span> To assess speaker similarity, we compute SECS using the SOTA\nspeaker verification model, WavLM-Large <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/</a></span></span></span>, to evaluate the speaker similarity, enabling comparison with those studies.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CER.</span> We employ whisper-large-v3 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span> to transcribe the synthesized speech into text, which is then compared with the ground-truth transcripts to compute the character error rate (CER).</p>\n\n",
                "matched_terms": [
                    "cer",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">UTMOS.</span> We adopt a state-of-the-art MOS prediction model, UTMOS <span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/sarulab-speech/UTMOS22\" title=\"\">https://github.com/sarulab-speech/UTMOS22</a></span></span></span>, to objectively evaluate the naturalness of the generated audio.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "utmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">EECS.</span> Similar to speaker similarity, we compute the emotional similarity of speech, where the emotion embeddings are extracted using the model emotion2vec <span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ddlBoJack/emotion2vec\" title=\"\">https://github.com/ddlBoJack/emotion2vec</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "eecs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to evaluating speech quality, the proposed model&#8217;s complexity is assessed based on the real-time factor <span class=\"ltx_text ltx_font_bold\">(RTF)</span> and the number of parameters <span class=\"ltx_text ltx_font_bold\">(Params)</span>. RTF measures the time required to generate one second of audio on a GPU. In this experiment, RTF is tested on a single NVIDIA RTX 4090 GPU with 24 GB of memory.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering that objective metrics in cross-lingual scenarios may fail to capture subtle variations in emotion and speaker characteristics, we further conducted the following subjective experiments.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DMOS.</span> The Differential Mean Opinion Score (DMOS) is employed to evaluate the speaker similarity between synthesized and reference audio, on a 1&#8211;5 scale where 1 denotes completely dissimilar and 5 denotes highly similar.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ABX test.</span> The ABX test is employed to evaluate perceptual preference by asking listeners to judge which of two audio samples exhibits higher naturalness or greater similarity. Listeners may also indicate that the two samples are indistinguishable.</p>\n\n",
                "matched_terms": [
                    "two",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we validate the effectiveness of the proposed method in both monolingual and cross-lingual scenarios. In the monolingual setting, we primarily analyze the performance of voice cloning; in the cross-lingual setting, we also evaluate emotional similarity. We further investigate the impact of speaker perturbations on the model and conduct ablation studies on SEALN and SCL.</p>\n\n",
                "matched_terms": [
                    "perturbations",
                    "method",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results of EMM-TTS and the baseline models on the LibriSpeech test-clean set are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T2\" title=\"Table 2 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nCompared with ZMM-TTS, incorporating both explicit and implicit emotional representations enables EMM-TTS to achieve higher speaker similarity and improved speech naturalness.\nThis suggests that, in addition to timbre cloning, modeling emotional information can substantially improve speaker similarity with the reference audio.\nCompared with the current state-of-the-art multilingual synthesis model HierSpeech++, EMM-TTS achieves a notable improvement in speaker similarity under the same training data conditions.\nBy comparing the RTF and the number of parameters with HierSpeech++, our model is more lightweight and better suited for computation-constrained environments.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> present the subjective evaluation results of synthesized Chinese and English speech, respectively. The proposed EMM-TTS achieves the best naturalness. This improvement may be attributed to the XPhoneBERT-based text representation and phoneme encoder, which enable more effective modeling of pronunciations from different languages in a unified space, thereby enhancing multilingual synthesis capability. Furthermore, we find that the naturalness degrades significantly when synthesizing speech with cross-lingual speakers compared to same-lingual speakers. Specifically, when synthesizing text in language B with the voice of a speaker from language A, the generated speech often contains pronunciation errors and accent issues, particularly when English speakers synthesize Chinese speech.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">DMOS</span>, DiCLET-TTS and M3 achieve relatively similar results under monolingual conditions, but M3 exhibits a substantial performance drop in cross-lingual scenarios. This indicates that M3 suffers from weak disentanglement capability. When the reference audio and the target speaker&#8217;s timbre are mismatched, the synthesized speech is heavily affected by the timbre of the reference audio. In contrast, the proposed <span class=\"ltx_text ltx_font_bold\">EMM-TTS</span> consistently achieves the best speaker similarity and emotion similarity in both same-lingual and cross-lingual settings, while also showing the least performance degradation in cross-lingual scenarios. These results demonstrate the effectiveness of our proposed emotion modeling and disentanglement strategies.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the objective metrics reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T5\" title=\"Table 5 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that EMM-TTS achieves the best performance in both intelligibility and SECS across the two languages. Moreover, consistent with the subjective evaluations, when the target speaker&#8217;s language differs from the synthesized speech, both speaker similarity and intelligibility decline. In contrast, DiCLET-TTS consistently yields the poorest intelligibility (CER) in most cases, which may be attributed to its use of speaker-adversarial learning for text representations, potentially compromising the content quality of the synthesized speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "objective",
                    "secs",
                    "two",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to formant shifting, this chapter utilizes a speaker anonymization technique to alter information, aiming to investigate the effects of various interference methods on synthesized speech. First, audio samples from 10 Chinese speakers in the ESD dataset were selected for two types of speaker interference, followed by visualization and quantitative analysis of the interfered audio.\nFor each speaker and each emotion, 50 sentences were selected, resulting in a total of 2,500 sentences for analysis.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a visualization of the speaker representations extracted by a pre-trained ECAPA-TDNN speaker encoder. The representations were reduced to two dimensions using t-SNE, with different colors representing different speakers. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (a), it can be observed that, in the original audio, speaker embeddings of the same speaker cluster closely together, forming distinct clusters. Furthermore, each speaker cluster contains several sub-clusters, which, upon inspection, correspond to different emotions. This phenomenon further confirms that speaker information and emotional information are often entangled. Although ECAPA-TDNN achieves good performance in speaker classification, its learned speaker representations still contain rich emotional details. Furthermore, as shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (b) and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (c), speaker interference methods can effectively alter the speaker information in the audio. Specifically, after interference, the embeddings of audio samples from the same speaker exhibit greater distances. Among the two methods, speaker anonymization imposes the greatest interference with speaker information.</p>\n\n",
                "matched_terms": [
                    "original",
                    "speech",
                    "speaker",
                    "different",
                    "anonymization",
                    "after",
                    "two",
                    "audio",
                    "formant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T7\" title=\"Table 7 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the objective evaluation results of the EMM-TTS model under different speaker perturbation strategies. Compared with the model that applies no speaker perturbation, introducing perturbations reduces the reference speaker&#8217;s influence on the synthesized audio, leading to improved SECS. This result indicates that perturbing speaker information facilitates disentangling emotion from speaker identity. Although the two perturbation methods yield comparable SECS scores, the anonymization-based perturbation causes a noticeable decline in speech intelligibility.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "speaker",
                    "different",
                    "objective",
                    "secs",
                    "perturbations",
                    "two",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F4\" title=\"Figure 4 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the ABX test preferences for the EMM-TTS (default with formant shift) model when different speaker perturbation methods are applied. The test was conducted on samples spoken by English speakers with Chinese linguistic content. The results show that the formant-shift method outperforms the anonymization-based approach in terms of naturalness, speaker similarity, and emotional similarity. Among these aspects, the gap in emotional similarity is the most pronounced. Although the anonymization-based method effectively disrupts speaker identity, it also weakens emotional cues, leading to synthesized speech that sounds more neutral.</p>\n\n",
                "matched_terms": [
                    "method",
                    "shift",
                    "speech",
                    "speaker",
                    "different",
                    "formant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, removing the pretrained emotional representation\nE leads to a noticeable decrease in emotional similarity. Interestingly, emotional similarity and speaker similarity tend to exhibit a negative correlation&#8212;improving one often comes at the cost of the other. The final EMM-TTS model achieves a balanced trade-off between the two, demonstrating superior overall performance. Future work will explore finer-grained control over both timbre and emotional expressiveness, aiming to achieve a more flexible balance between them.</p>\n\n",
                "matched_terms": [
                    "two",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a two-stage cross-lingual emotional speech synthesis system, EMM-TTS. The first stage focuses on modeling and predicting emotional representations, while the second stage enables fine-grained control over speaker timbre. The two stages are connected through perturbed self-supervised features, which serve as a bridge between emotion and timbre modeling. Experimental results demonstrate that EMM-TTS achieves strong zero-shot voice cloning in monolingual scenarios and effective emotion transfer across languages.</p>\n\n",
                "matched_terms": [
                    "two",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Timbre and emotion are two highly entangled factors in speech signals, posing challenges for fine-grained control in speech synthesis. Information perturbation is a commonly adopted strategy for disentangling these factors. Previous studies have primarily focused on perturbation methods based on signal processing. In this work, we investigate the capability of recent speaker anonymization models to disentangle emotion and timbre. Our analysis combines visualization, subjective listening tests, and objective audio quality metrics. Experimental results show that signal-processing-based perturbations produce stronger distortion of speaker identity, whereas speaker anonymization models better preserve the naturalness of synthesized speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "objective",
                    "anonymization",
                    "perturbations",
                    "two",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we proposed EMM-TTS, a two-stage cross-lingual emotional text-to-speech system that effectively disentangles emotion and timbre through speaker-perturbed SSL representations. By leveraging explicit prosodic modeling in the first stage and timbre restoration in the second stage, the system enables controllable emotion transfer and high-fidelity speaker imitation across languages. The proposed Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) further enhance timbre stability and expressive consistency. Moreover, experiments reveal that combining explicit acoustic features with pretrained latent representations improves timbre reproduction. Extensive subjective and objective evaluations confirm that EMM-TTS achieves superior performance in both zero-shot timbre cloning and cross-lingual emotion transfer. In future work, we will explore finer-grained control of emotion intensity and timbre style, as well as adaptive balancing strategies between emotional expressiveness and speaker identity.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "objective"
                ]
            }
        ]
    },
    "S4.T7": {
        "source_file": "Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker",
        "caption": "Table 7: Objective metrics of synthesized speech under different speaker perturbation methods.",
        "body": "CN Speech\nEN Speech\n\n\nModel/Metric\nCN Speaker\nEN Speaker\nCN Speaker\nEN Speaker\n\n\n\nSECS\nCER\nSECS\nCER\nSECS\nCER\nSECS\nCER\n\n\nEMM-TTS (w/ formant shift)\n0.662\n7.13\n0.643\n7.47\n0.597\n8.90\n0.614\n8.21\n\n\nEMM-TTS (w/ speaker anonymization)\n0.657\n12.24\n0.541\n11.13\n0.550\n11.30\n0.617\n10.37\n\n\nEMM-TTS (w/o speaker erturbation)\n0.627\n7.08\n0.503\n7.44\n0.532\n9.12\n0.603\n8.07",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CN Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">EN Speech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model/Metric</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CN Speaker</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">EN Speaker</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CN Speaker</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">EN Speaker</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">SECS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">SECS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">SECS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CER</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">SECS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">CER</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">EMM-TTS (w/ formant shift)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.662</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">7.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.643</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">7.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.597</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">8.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.614</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">8.21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">EMM-TTS (w/ speaker anonymization)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.657</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">12.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.541</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">11.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.550</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">11.30</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.617</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">10.37</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">EMM-TTS (w/o speaker erturbation)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.627</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">7.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.503</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">7.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.532</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">9.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">0.603</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">8.07</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "synthesized",
            "formant",
            "shift",
            "under",
            "speech",
            "speaker",
            "methods",
            "objective",
            "different",
            "modelmetric",
            "secs",
            "emmtts",
            "erturbation",
            "anonymization",
            "cer",
            "metrics",
            "perturbation"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T7\" title=\"Table 7 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the objective evaluation results of the EMM-TTS model under different speaker perturbation strategies. Compared with the model that applies no speaker perturbation, introducing perturbations reduces the reference speaker&#8217;s influence on the synthesized audio, leading to improved SECS. This result indicates that perturbing speaker information facilitates disentangling emotion from speaker identity. Although the two perturbation methods yield comparable SECS scores, the anonymization-based perturbation causes a noticeable decline in speech intelligibility.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-lingual emotional text-to-speech (TTS) aims to produce speech in one language that captures the emotion of a speaker from another language while maintaining the target voice&#8217;s timbre. This process of cross-lingual emotional speech synthesis presents a complex challenge, necessitating flexible control over emotion, timbre, and language.\nHowever, emotion and timbre are highly entangled in speech signals, making fine-grained control challenging. To address this issue, we propose EMM-TTS, a novel two-stage cross-lingual emotional speech synthesis framework based on perturbed self-supervised learning (SSL) representations. In the first stage, the model explicitly and implicitly encodes prosodic cues to capture emotional expressiveness, while the second stage restores the timbre from perturbed SSL representations. We further investigate the effect of different speaker perturbation strategies&#8212;formant shifting and speaker anonymization&#8212;on the disentanglement of emotion and timbre. To strengthen speaker preservation and expressive control, we introduce Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) modules. Additionally, we find that incorporating explicit acoustic features (e.g., F0, energy, and duration) alongside pretrained latent features improves voice cloning performance. Comprehensive multi-metric evaluations, including both subjective and objective measures, demonstrate that EMM-TTS achieves superior naturalness, emotion transferability, and timbre consistency across languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts",
                    "different",
                    "objective",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis is a key component of the\nhuman&#8211;computer interface that is considered essential to\nresponding and plays a vital role in enabling machines to generate human-like responses.\nThe goals of speech synthesis can be hierarchically categorized, from easier to more challenging, into three levels: intelligibility, naturalness, and expressiveness.\nSpeech synthesis has made significant progress in intelligibility and naturalness, mainly due to advances in deep learning and neural networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib31\" title=\"\">2021</a>; Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nToday, we can generate speech that is often indistinguishable from human speech.\nWhile significant progress has been made in intelligibility and naturalness, achieving expressive and emotionally rich speech remains challenging.\nAnd a challenging research problem persists:\ncross-lingual emotion TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib14\" title=\"\">2024</a>)</cite> refers\nto the task of a speaker of one language to mimic the emotion of a speaker from another language while speaking a different language.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual synthesis poses a more complex challenge in multilingual speech synthesis, as it requires transferring a speaker&#8217;s voice characteristics across languages.\nDespite significant efforts in cross-lingual TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib5\" title=\"\">2024</a>)</cite> research, there remains a noticeable gap in the naturalness of generated speech compared to native speakers.\nThis issue primarily arises from two factors: the lack of data resources and variations in text representations across languages.\nThe most straightforward approach to cross-lingual synthesis is to train the model on bilingual speech data <cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib4\" title=\"\">2023</a>)</cite>, where the same speaker provides utterances in multiple languages.\nRegrettably, collecting such bilingual data is costly, and no large-scale bilingual speech datasets are available.\nAlong with speech data, the lack of text resources is a major obstacle in multilingual speech synthesis.\nConventional speech processing systems that are based on phonetics require pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib21\" title=\"\">2019</a>)</cite>. These dictionaries map phonetic units to their corresponding words.\nCreating such resources requires expert knowledge for each language. Despite the significant human effort involved, many languages still lack sufficient linguistic resources to develop these dictionaries.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fortunately, the rise of self-supervised representations <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib3\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib15\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib7\" title=\"\">2022</a>)</cite> has reduced the model&#8217;s dependence on labeled data.\nMultilingual SSL speech or text representations <cite class=\"ltx_cite ltx_citemacro_citep\">(The Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib34\" title=\"\">2023</a>; Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib11\" title=\"\">2020</a>)</cite> can learn to extract linguistic, paralinguistic, and non-linguistic information from vast amounts of unlabeled data.\nRecently, they have been widely used in cross-lingual TTS to address the above issues and enhance the quality of cross-lingual TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>; Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib32\" title=\"\">2023</a>)</cite>.\nAmong these, ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite> integrates text-based and speech-based self-supervised learning models for multilingual speech synthesis, enabling zero-shot generation under limited data conditions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past year, large-scale speech synthesis systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, leveraging codec models and language models to significantly enhance the capabilities of voice cloning, alongside models based on self-supervised representations.\nWhile these models showcase impressive performance in multilingual and emotional synthesis, their focus on voice cloning and zero-shot capabilities often comes at the expense of flexible control over emotion and timbre.\nMoreover, the entanglement of speaker timbre and emotion in speech may result in speaker timbre leakage during cross-speaker emotion transfer <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nA common strategy for decoupling involves adversarial learning and constraints on classification losses, as demonstrated in previous research <cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib19\" title=\"\">2022a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>.\nThese methods utilize classification loss or gradient reversal to learn representations that isolate emotion or speaker information.\nHowever, adversarial learning would introduce instability and degrade the quality of the synthesized speech. Furthermore, constraints on emotion classification may limit the emotional diversity of synthesized speech. Another straightforward decoupling approach involves speaker perturbation, which alters speaker-specific acoustic properties, such as formants, in speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib38\" title=\"\">2024</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib20\" title=\"\">2022b</a>)</cite>.\nThis perturbation method may degrade speech quality.\nFurthermore, the effects of recent speaker perturbation methods, such as speaker anonymization <cite class=\"ltx_cite ltx_citemacro_citep\">(Tomashenko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib35\" title=\"\">2024</a>; Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite>, on speech synthesis, especially for SSL-based synthesis models, remain an underexplored area.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "speech",
                    "speaker",
                    "methods",
                    "anonymization",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by the analysis above, this paper extends the previous SSL-based ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite> model by incorporating emotional speech synthesis capabilities and proposes an emotional multilingual multispeaker TTS system (EMM-TTS). The following are the major contributions of this work:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we explore the effects of two different speaker perturbation methods&#8212;formant shift and speaker anonymization&#8212;on the quality of synthesized audio.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "shift",
                    "speaker",
                    "different",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further improve speech similarity during the speech generation process, we propose a Speaker-Emotion Adaptive Layer Normalization (SEALN) and introduce a Speaker Consistency Loss (SCL).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section will introduce the proposed EMM-TTS framework.\nWe begin with fundamental knowledge about cross-lingual emotion speech synthesis, and then present the two-stage structure.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a reference speech from speaker <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> (e.g., in Chinese), our goal is to enable speaker <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> (e.g., an English native speaker) to speak Chinese with the reference speech&#8217;s emotion while retaining their own timbre, as depicted in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn contrast to the currently popular voice cloning methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, which determine all attributes of the synthesized speech&#8212;such as emotion and timbre&#8212;based on a single reference audio, our research focuses on cross-lingual emotional speech synthesis. We enable independent control over both emotion and timbre.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "speech",
                    "synthesized",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key challenge in cross-lingual TTS lies in decoupling speaker and emotion information. To address this, we propose a two-stage emotional speech synthesis system, EMM-TTS, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S2.F2\" title=\"Figure 2 &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The first stage <span class=\"ltx_text ltx_font_typewriter\">txt2vec</span> models and predicts emotions, while the second stage <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> controls speaker-specific characteristics.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, in our approach, we perform a speaker perturbation, denoted as <math alttext=\"sp()\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">sp()</annotation></semantics></math>, on the original waveform <math alttext=\"Speech\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">Speech</annotation></semantics></math> during training, which allows us to obtain a speaker-independent\nsignal denoted as <math alttext=\"\\widetilde{\\text{Speech}}=sp(\\text{Speech})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><mo>=</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>Speech</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}=sp(\\text{Speech})</annotation></semantics></math>.\nSubsequently, we extract the multilingual discrete SSL representation and emotion representation from the perturbed <math alttext=\"\\widetilde{\\text{Speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}</annotation></semantics></math>, denoted as <math alttext=\"\\widetilde{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>R</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{R}</annotation></semantics></math> and <math alttext=\"\\widetilde{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>E</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{E}</annotation></semantics></math>. The perturbation processes for <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> and <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m8\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> are conducted independently. In this work, we explore two different speaker perturbation strategies. The first is signal-processing-based, implemented via formant shifting. The second uses speaker anonymization, generating speech with speaker characteristics that differ from those of the original audio. The process of formant shifting is illustrated in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#alg1\" title=\"Algorithm 1 &#8227; 2.3 Speech generation via speaker-perturbation representations &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "different",
                    "anonymization",
                    "perturbation",
                    "formant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m16\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m17\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math> adaptively scale and shift the normalized <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m18\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> based on the speaker and emotional representation.\nUsing SEALN, it is possible to synthesize speech with varying emotions for different speakers under the given conditions of <math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m19\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m20\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "shift",
                    "speech",
                    "under",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> recovers the target timbre from the perturbed features and the speaker ID, we introduced a Speaker Consistency Loss (SCL), as described in paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>)</cite>.\nA pre-trained speaker encoder extracts speaker embeddings from the generated speech and the ground truth. We then maximize the cosine similarity as the speaker consistency loss. Let <math alttext=\"\\phi(.)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS3.p4.m1\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0.167em\">.</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(.)</annotation></semantics></math> be a function that outputs the embedding of a speaker. Let <math alttext=\"cos\\_sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">cos\\_sim</annotation></semantics></math> denote the cosine similarity function, and let <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> be a positive real number that controls the influence of the Speaker Contrastive Loss (SCL) in the final loss calculation. Additionally, let <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> represent the batch size. The SCL is defined as follows:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"g\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m5\" intent=\":literal\"><semantics><mi>g</mi><annotation encoding=\"application/x-tex\">g</annotation></semantics></math> and <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m6\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> represent, respectively, the ground truth and the generated speaker audio.\nFinally, the optimization objective of the entire <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> process consists of two components: reconstruction loss used in the original <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> of ZMM-TTS and speaker consistency loss.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LJSpeech</span> is a publicly available speech dataset containing 13,100 short audio clips of a single speaker reading excerpts from seven non-fiction books. The clips range from 1 to 10 seconds in length and total approximately 24 hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the voice cloning experiments in a monolingual setting, we used the LibriTTS dataset for training and the <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset of <span class=\"ltx_text ltx_font_bold\">LibriSpeech</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib30\" title=\"\">2015</a>)</cite> for evaluation. This widely used test set contains speech from 40 different speakers and totals 5.4 hours of audio. Following the method described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>, we randomly evaluated 25 utterances per speaker from the LibriSpeech test-clean dataset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the cross-lingual emotional speech synthesis experiments, the ESD dataset has a limited size of 350 unique sentences per language. Therefore, training includes LJSpeech and Biaobei. To balance emotion and speaker representation, the ESD dataset is upsampled by a factor of 5 during training. The details of the training data in cross-lingual scenarios are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T1\" title=\"Table 1 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection presents the details of two different experimental setups, including baseline models, evaluation metrics, and other relevant configurations.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the model&#8217;s performance in zero-shot speech synthesis. Accordingly, our proposed EMM-TTS uses a pretrained speaker embedding instead of a one-hot vector to represent speaker identity. The pretrained representation is the same as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite>, extracted from a pretrained ECAPA-TDNN model. Moreover, no information perturbation was applied to the data during training or inference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the ability to transfer and synthesize emotions across languages. In these scenarios, our proposed EMM-TTS model adopts one-hot vectors as speaker input.\nWe experimented with two different speaker perturbation strategies. One based on signal processing, specifically formant perturbation, and the implementation of formant shifting followed the NANSY <cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib10\" title=\"\">2021</a>)</cite> model by using Praat <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.fon.hum.uva.nl/praat/\" title=\"\">https://www.fon.hum.uva.nl/praat/</a></span></span></span>.\nThe other uses an SSL-based language-independent speaker anonymization method by replacing the speaker embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib27\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite> and its official implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/nii-yamagishilab/SSL-SAS\" title=\"\">https://github.com/nii-yamagishilab/SSL-SAS</a></span></span></span>.\nThe proposed EMM-TTS model defaults to using formant shift as speaker perturbation, while an ablation experiment model, EMM-TTS-SA, is designed for speaker anonymization.</p>\n\n",
                "matched_terms": [
                    "shift",
                    "speaker",
                    "emmtts",
                    "different",
                    "anonymization",
                    "perturbation",
                    "formant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyzed the experimental results using both subjective and objective evaluations, with the following metrics included:</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective metrics mainly evaluate the naturalness and similarity of the synthesized audio in both monolingual and cross-lingual experiments.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SECS.</span> To assess speaker similarity, we compute SECS using the SOTA\nspeaker verification model, WavLM-Large <span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/microsoft/UniSpeech/tree/main/\" title=\"\">https://github.com/microsoft/UniSpeech/tree/main/</a></span></span></span>, to evaluate the speaker similarity, enabling comparison with those studies.</p>\n\n",
                "matched_terms": [
                    "secs",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CER.</span> We employ whisper-large-v3 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span> to transcribe the synthesized speech into text, which is then compared with the ground-truth transcripts to compute the character error rate (CER).</p>\n\n",
                "matched_terms": [
                    "cer",
                    "speech",
                    "synthesized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">EECS.</span> Similar to speaker similarity, we compute the emotional similarity of speech, where the emotion embeddings are extracted using the model emotion2vec <span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ddlBoJack/emotion2vec\" title=\"\">https://github.com/ddlBoJack/emotion2vec</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering that objective metrics in cross-lingual scenarios may fail to capture subtle variations in emotion and speaker characteristics, we further conducted the following subjective experiments.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DMOS.</span> The Differential Mean Opinion Score (DMOS) is employed to evaluate the speaker similarity between synthesized and reference audio, on a 1&#8211;5 scale where 1 denotes completely dissimilar and 5 denotes highly similar.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "synthesized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results of EMM-TTS and the baseline models on the LibriSpeech test-clean set are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T2\" title=\"Table 2 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nCompared with ZMM-TTS, incorporating both explicit and implicit emotional representations enables EMM-TTS to achieve higher speaker similarity and improved speech naturalness.\nThis suggests that, in addition to timbre cloning, modeling emotional information can substantially improve speaker similarity with the reference audio.\nCompared with the current state-of-the-art multilingual synthesis model HierSpeech++, EMM-TTS achieves a notable improvement in speaker similarity under the same training data conditions.\nBy comparing the RTF and the number of parameters with HierSpeech++, our model is more lightweight and better suited for computation-constrained environments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "under",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> present the subjective evaluation results of synthesized Chinese and English speech, respectively. The proposed EMM-TTS achieves the best naturalness. This improvement may be attributed to the XPhoneBERT-based text representation and phoneme encoder, which enable more effective modeling of pronunciations from different languages in a unified space, thereby enhancing multilingual synthesis capability. Furthermore, we find that the naturalness degrades significantly when synthesizing speech with cross-lingual speakers compared to same-lingual speakers. Specifically, when synthesizing text in language B with the voice of a speaker from language A, the generated speech often contains pronunciation errors and accent issues, particularly when English speakers synthesize Chinese speech.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "speech",
                    "speaker",
                    "emmtts",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">DMOS</span>, DiCLET-TTS and M3 achieve relatively similar results under monolingual conditions, but M3 exhibits a substantial performance drop in cross-lingual scenarios. This indicates that M3 suffers from weak disentanglement capability. When the reference audio and the target speaker&#8217;s timbre are mismatched, the synthesized speech is heavily affected by the timbre of the reference audio. In contrast, the proposed <span class=\"ltx_text ltx_font_bold\">EMM-TTS</span> consistently achieves the best speaker similarity and emotion similarity in both same-lingual and cross-lingual settings, while also showing the least performance degradation in cross-lingual scenarios. These results demonstrate the effectiveness of our proposed emotion modeling and disentanglement strategies.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "speech",
                    "under",
                    "emmtts",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the objective metrics reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T5\" title=\"Table 5 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that EMM-TTS achieves the best performance in both intelligibility and SECS across the two languages. Moreover, consistent with the subjective evaluations, when the target speaker&#8217;s language differs from the synthesized speech, both speaker similarity and intelligibility decline. In contrast, DiCLET-TTS consistently yields the poorest intelligibility (CER) in most cases, which may be attributed to its use of speaker-adversarial learning for text representations, potentially compromising the content quality of the synthesized speech.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "speech",
                    "speaker",
                    "emmtts",
                    "objective",
                    "secs",
                    "cer",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to formant shifting, this chapter utilizes a speaker anonymization technique to alter information, aiming to investigate the effects of various interference methods on synthesized speech. First, audio samples from 10 Chinese speakers in the ESD dataset were selected for two types of speaker interference, followed by visualization and quantitative analysis of the interfered audio.\nFor each speaker and each emotion, 50 sentences were selected, resulting in a total of 2,500 sentences for analysis.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a visualization of the speaker representations extracted by a pre-trained ECAPA-TDNN speaker encoder. The representations were reduced to two dimensions using t-SNE, with different colors representing different speakers. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (a), it can be observed that, in the original audio, speaker embeddings of the same speaker cluster closely together, forming distinct clusters. Furthermore, each speaker cluster contains several sub-clusters, which, upon inspection, correspond to different emotions. This phenomenon further confirms that speaker information and emotional information are often entangled. Although ECAPA-TDNN achieves good performance in speaker classification, its learned speaker representations still contain rich emotional details. Furthermore, as shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (b) and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (c), speaker interference methods can effectively alter the speaker information in the audio. Specifically, after interference, the embeddings of audio samples from the same speaker exhibit greater distances. Among the two methods, speaker anonymization imposes the greatest interference with speaker information.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "speech",
                    "speaker",
                    "methods",
                    "different",
                    "anonymization",
                    "formant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the visual analysis, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T6\" title=\"Table 6 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the objective evaluation results for audio processed with different speaker perturbation methods. The SECS results are consistent with the observations in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that both perturbation methods effectively interfere with speaker-related information in the audio. The anonymization-based method produces the most substantial perturbation to speaker identity, but it also inevitably degrades emotional expressiveness. This method, while most effective at obfuscating speaker identity, introduces the greatest emotional distortion.\nAnalysis of the UTMOS and CER values further reveals that the formant-shift method primarily affects the naturalness of speech, whereas the anonymization-based method mainly impacts the linguistic content. On one hand, directly shifting formants tends to make the speech sound less natural. On the other hand, the anonymization approach relies on recognizing and re-synthesizing the speech, and recognition errors can easily accumulate in the anonymized output.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "methods",
                    "different",
                    "objective",
                    "secs",
                    "anonymization",
                    "cer",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F4\" title=\"Figure 4 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the ABX test preferences for the EMM-TTS (default with formant shift) model when different speaker perturbation methods are applied. The test was conducted on samples spoken by English speakers with Chinese linguistic content. The results show that the formant-shift method outperforms the anonymization-based approach in terms of naturalness, speaker similarity, and emotional similarity. Among these aspects, the gap in emotional similarity is the most pronounced. Although the anonymization-based method effectively disrupts speaker identity, it also weakens emotional cues, leading to synthesized speech that sounds more neutral.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "shift",
                    "speech",
                    "speaker",
                    "methods",
                    "emmtts",
                    "different",
                    "perturbation",
                    "formant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the proposed vec2wav model, several additional modules are incorporated, including the Speaker Consistency Loss (SCL), the Speaker-Emotion Adaptive Layer Normalization (SEALN), and the pretrained emotional representation\nE. The subjective ablation results of these modules are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. As shown in the table, both the SCL constraint and the SSALN module play a crucial role in maintaining similarity to the target speaker. Although speaker perturbation is applied during training, these components enable the model to recover accurate speaker identity from the perturbed representations.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, removing the pretrained emotional representation\nE leads to a noticeable decrease in emotional similarity. Interestingly, emotional similarity and speaker similarity tend to exhibit a negative correlation&#8212;improving one often comes at the cost of the other. The final EMM-TTS model achieves a balanced trade-off between the two, demonstrating superior overall performance. Future work will explore finer-grained control over both timbre and emotional expressiveness, aiming to achieve a more flexible balance between them.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a two-stage cross-lingual emotional speech synthesis system, EMM-TTS. The first stage focuses on modeling and predicting emotional representations, while the second stage enables fine-grained control over speaker timbre. The two stages are connected through perturbed self-supervised features, which serve as a bridge between emotion and timbre modeling. Experimental results demonstrate that EMM-TTS achieves strong zero-shot voice cloning in monolingual scenarios and effective emotion transfer across languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Timbre and emotion are two highly entangled factors in speech signals, posing challenges for fine-grained control in speech synthesis. Information perturbation is a commonly adopted strategy for disentangling these factors. Previous studies have primarily focused on perturbation methods based on signal processing. In this work, we investigate the capability of recent speaker anonymization models to disentangle emotion and timbre. Our analysis combines visualization, subjective listening tests, and objective audio quality metrics. Experimental results show that signal-processing-based perturbations produce stronger distortion of speaker identity, whereas speaker anonymization models better preserve the naturalness of synthesized speech.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "speech",
                    "speaker",
                    "methods",
                    "objective",
                    "anonymization",
                    "metrics",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we proposed EMM-TTS, a two-stage cross-lingual emotional text-to-speech system that effectively disentangles emotion and timbre through speaker-perturbed SSL representations. By leveraging explicit prosodic modeling in the first stage and timbre restoration in the second stage, the system enables controllable emotion transfer and high-fidelity speaker imitation across languages. The proposed Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) further enhance timbre stability and expressive consistency. Moreover, experiments reveal that combining explicit acoustic features with pretrained latent representations improves timbre reproduction. Extensive subjective and objective evaluations confirm that EMM-TTS achieves superior performance in both zero-shot timbre cloning and cross-lingual emotion transfer. In future work, we will explore finer-grained control of emotion intensity and timbre style, as well as adaptive balancing strategies between emotional expressiveness and speaker identity.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work will explore more fine-grained control over both emotion and timbre, enabling continuous adjustment of emotional intensity and timbre style. We also plan to investigate adaptive mechanisms that can balance the trade-off between emotional expressiveness and speaker identity preservation. Extending the approach to support more languages and diverse emotional expressions will further enhance the generalization and applicability of the proposed EMM-TTS framework.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts"
                ]
            }
        ]
    },
    "S4.T8": {
        "source_file": "Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker",
        "caption": "Table 8: Subjective evaluation results of synthesized Chinese speech across different models.",
        "body": "Model/Metric\n\nCN Speaker\n\nEN Speaker\n\n\n\nMOS\nDMOS\nEMOS\nMOS\nDMOS\nEMOS\n\n\nEMM-TTS\n4.12\\pm0.17\n3.95\\pm0.21\n3.97\\pm0.15\n3.92\\pm0.22\n3.81\\pm0.25\n3.96\\pm0.19\n\n\nw/o SCL\n4.09\\pm0.22\n3.87\\pm0.21\n4.02\\pm0.23\n3.89\\pm0.19\n3.58\\pm0.15\n4.10\\pm0.27\n\n\nw/o emo\n4.13\\pm0.21\n4.09\\pm0.13\n3.86\\pm0.20\n3.90\\pm0.14\n3.88\\pm0.22\n3.87\\pm0.13\n\n\nw/o SSALN\n4.10\\pm0.23\n3.82\\pm0.30\n4.02\\pm0.13\n3.99\\pm0.18\n3.61\\pm0.11\n3.97\\pm0.24",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:12.7pt;padding-right:12.7pt;\"><span class=\"ltx_text ltx_font_bold\">Model/Metric</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">\n<span class=\"ltx_text ltx_font_bold\">CN Speaker</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">\n<span class=\"ltx_text ltx_font_bold\">EN Speaker</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-left:12.7pt;padding-right:12.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\"><span class=\"ltx_text ltx_font_bold\">MOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\"><span class=\"ltx_text ltx_font_bold\">DMOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\"><span class=\"ltx_text ltx_font_bold\">EMOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\"><span class=\"ltx_text ltx_font_bold\">MOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\"><span class=\"ltx_text ltx_font_bold\">DMOS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\"><span class=\"ltx_text ltx_font_bold\">EMOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">EMM-TTS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">4.12<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.95<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.97<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.92<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.81<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.96<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">&#8195;&#8197;&#8195;w/o SCL</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">4.09<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.87<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">4.02<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.23</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.89<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.19</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.58<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">4.10<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">&#8195;&#8197;&#8195;w/o emo</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">4.13<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">4.09<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.13</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.86<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.90<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.88<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m17\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.22</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.87<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m18\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.13</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">&#8195;&#8197;&#8195;w/o SSALN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">4.10<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m19\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.82<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m20\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">4.02<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m21\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.99<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m22\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.61<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m23\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:12.7pt;padding-right:12.7pt;\">3.97<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m24\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.24</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "scl",
            "409pm022",
            "speech",
            "402pm023",
            "409pm013",
            "subjective",
            "361pm011",
            "381pm025",
            "397pm015",
            "chinese",
            "358pm015",
            "389pm019",
            "399pm018",
            "dmos",
            "evaluation",
            "different",
            "results",
            "410pm027",
            "387pm013",
            "386pm020",
            "382pm030",
            "410pm023",
            "synthesized",
            "emo",
            "mos",
            "412pm017",
            "ssaln",
            "emos",
            "395pm021",
            "413pm021",
            "390pm014",
            "across",
            "397pm024",
            "models",
            "388pm022",
            "speaker",
            "emmtts",
            "modelmetric",
            "396pm019",
            "387pm021",
            "392pm022",
            "402pm013"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In the proposed vec2wav model, several additional modules are incorporated, including the Speaker Consistency Loss (SCL), the Speaker-Emotion Adaptive Layer Normalization (SEALN), and the pretrained emotional representation\nE. The subjective ablation results of these modules are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T8\" title=\"Table 8 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. As shown in the table, both the SCL constraint and the SSALN module play a crucial role in maintaining similarity to the target speaker. Although speaker perturbation is applied during training, these components enable the model to recover accurate speaker identity from the perturbed representations.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-lingual emotional text-to-speech (TTS) aims to produce speech in one language that captures the emotion of a speaker from another language while maintaining the target voice&#8217;s timbre. This process of cross-lingual emotional speech synthesis presents a complex challenge, necessitating flexible control over emotion, timbre, and language.\nHowever, emotion and timbre are highly entangled in speech signals, making fine-grained control challenging. To address this issue, we propose EMM-TTS, a novel two-stage cross-lingual emotional speech synthesis framework based on perturbed self-supervised learning (SSL) representations. In the first stage, the model explicitly and implicitly encodes prosodic cues to capture emotional expressiveness, while the second stage restores the timbre from perturbed SSL representations. We further investigate the effect of different speaker perturbation strategies&#8212;formant shifting and speaker anonymization&#8212;on the disentanglement of emotion and timbre. To strengthen speaker preservation and expressive control, we introduce Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) modules. Additionally, we find that incorporating explicit acoustic features (e.g., F0, energy, and duration) alongside pretrained latent features improves voice cloning performance. Comprehensive multi-metric evaluations, including both subjective and objective measures, demonstrate that EMM-TTS achieves superior naturalness, emotion transferability, and timbre consistency across languages.</p>\n\n",
                "matched_terms": [
                    "scl",
                    "across",
                    "speech",
                    "speaker",
                    "emmtts",
                    "subjective",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech synthesis is a key component of the\nhuman&#8211;computer interface that is considered essential to\nresponding and plays a vital role in enabling machines to generate human-like responses.\nThe goals of speech synthesis can be hierarchically categorized, from easier to more challenging, into three levels: intelligibility, naturalness, and expressiveness.\nSpeech synthesis has made significant progress in intelligibility and naturalness, mainly due to advances in deep learning and neural networks <cite class=\"ltx_cite ltx_citemacro_citep\">(Ren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib31\" title=\"\">2021</a>; Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nToday, we can generate speech that is often indistinguishable from human speech.\nWhile significant progress has been made in intelligibility and naturalness, achieving expressive and emotionally rich speech remains challenging.\nAnd a challenging research problem persists:\ncross-lingual emotion TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib14\" title=\"\">2024</a>)</cite> refers\nto the task of a speaker of one language to mimic the emotion of a speaker from another language while speaking a different language.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-lingual synthesis poses a more complex challenge in multilingual speech synthesis, as it requires transferring a speaker&#8217;s voice characteristics across languages.\nDespite significant efforts in cross-lingual TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib5\" title=\"\">2024</a>)</cite> research, there remains a noticeable gap in the naturalness of generated speech compared to native speakers.\nThis issue primarily arises from two factors: the lack of data resources and variations in text representations across languages.\nThe most straightforward approach to cross-lingual synthesis is to train the model on bilingual speech data <cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib4\" title=\"\">2023</a>)</cite>, where the same speaker provides utterances in multiple languages.\nRegrettably, collecting such bilingual data is costly, and no large-scale bilingual speech datasets are available.\nAlong with speech data, the lack of text resources is a major obstacle in multilingual speech synthesis.\nConventional speech processing systems that are based on phonetics require pronunciation dictionaries <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib21\" title=\"\">2019</a>)</cite>. These dictionaries map phonetic units to their corresponding words.\nCreating such resources requires expert knowledge for each language. Despite the significant human effort involved, many languages still lack sufficient linguistic resources to develop these dictionaries.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "speech",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fortunately, the rise of self-supervised representations <cite class=\"ltx_cite ltx_citemacro_citep\">(Baevski et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib3\" title=\"\">2020</a>; Hsu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib15\" title=\"\">2021</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib7\" title=\"\">2022</a>)</cite> has reduced the model&#8217;s dependence on labeled data.\nMultilingual SSL speech or text representations <cite class=\"ltx_cite ltx_citemacro_citep\">(The Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib34\" title=\"\">2023</a>; Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib11\" title=\"\">2020</a>)</cite> can learn to extract linguistic, paralinguistic, and non-linguistic information from vast amounts of unlabeled data.\nRecently, they have been widely used in cross-lingual TTS to address the above issues and enhance the quality of cross-lingual TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>; Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib32\" title=\"\">2023</a>)</cite>.\nAmong these, ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite> integrates text-based and speech-based self-supervised learning models for multilingual speech synthesis, enabling zero-shot generation under limited data conditions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Over the past year, large-scale speech synthesis systems have emerged <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, leveraging codec models and language models to significantly enhance the capabilities of voice cloning, alongside models based on self-supervised representations.\nWhile these models showcase impressive performance in multilingual and emotional synthesis, their focus on voice cloning and zero-shot capabilities often comes at the expense of flexible control over emotion and timbre.\nMoreover, the entanglement of speaker timbre and emotion in speech may result in speaker timbre leakage during cross-speaker emotion transfer <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib23\" title=\"\">2022</a>)</cite>.\nA common strategy for decoupling involves adversarial learning and constraints on classification losses, as demonstrated in previous research <cite class=\"ltx_cite ltx_citemacro_citep\">(Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib19\" title=\"\">2022a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib22\" title=\"\">2023</a>)</cite>.\nThese methods utilize classification loss or gradient reversal to learn representations that isolate emotion or speaker information.\nHowever, adversarial learning would introduce instability and degrade the quality of the synthesized speech. Furthermore, constraints on emotion classification may limit the emotional diversity of synthesized speech. Another straightforward decoupling approach involves speaker perturbation, which alters speaker-specific acoustic properties, such as formants, in speech <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib38\" title=\"\">2024</a>; Lei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib20\" title=\"\">2022b</a>)</cite>.\nThis perturbation method may degrade speech quality.\nFurthermore, the effects of recent speaker perturbation methods, such as speaker anonymization <cite class=\"ltx_cite ltx_citemacro_citep\">(Tomashenko et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib35\" title=\"\">2024</a>; Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite>, on speech synthesis, especially for SSL-based synthesis models, remain an underexplored area.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "models",
                    "speech",
                    "synthesized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by the analysis above, this paper extends the previous SSL-based ZMM-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite> model by incorporating emotional speech synthesis capabilities and proposes an emotional multilingual multispeaker TTS system (EMM-TTS). The following are the major contributions of this work:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, we explore the effects of two different speaker perturbation methods&#8212;formant shift and speaker anonymization&#8212;on the quality of synthesized audio.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "synthesized",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further improve speech similarity during the speech generation process, we propose a Speaker-Emotion Adaptive Layer Normalization (SEALN) and introduce a Speaker Consistency Loss (SCL).</p>\n\n",
                "matched_terms": [
                    "scl",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section will introduce the proposed EMM-TTS framework.\nWe begin with fundamental knowledge about cross-lingual emotion speech synthesis, and then present the two-stage structure.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a reference speech from speaker <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> (e.g., in Chinese), our goal is to enable speaker <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> (e.g., an English native speaker) to speak Chinese with the reference speech&#8217;s emotion while retaining their own timbre, as depicted in the Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn contrast to the currently popular voice cloning methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib8\" title=\"\">2025</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib9\" title=\"\">2024</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib12\" title=\"\">2024</a>; Anastassiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib1\" title=\"\">2024</a>)</cite>, which determine all attributes of the synthesized speech&#8212;such as emotion and timbre&#8212;based on a single reference audio, our research focuses on cross-lingual emotional speech synthesis. We enable independent control over both emotion and timbre.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "speech",
                    "synthesized",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key challenge in cross-lingual TTS lies in decoupling speaker and emotion information. To address this, we propose a two-stage emotional speech synthesis system, EMM-TTS, shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S2.F2\" title=\"Figure 2 &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. The first stage <span class=\"ltx_text ltx_font_typewriter\">txt2vec</span> models and predicts emotions, while the second stage <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> controls speaker-specific characteristics.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Specifically, in our approach, we perform a speaker perturbation, denoted as <math alttext=\"sp()\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">sp()</annotation></semantics></math>, on the original waveform <math alttext=\"Speech\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow><annotation encoding=\"application/x-tex\">Speech</annotation></semantics></math> during training, which allows us to obtain a speaker-independent\nsignal denoted as <math alttext=\"\\widetilde{\\text{Speech}}=sp(\\text{Speech})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><mo>=</mo><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mtext>Speech</mtext><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}=sp(\\text{Speech})</annotation></semantics></math>.\nSubsequently, we extract the multilingual discrete SSL representation and emotion representation from the perturbed <math alttext=\"\\widetilde{\\text{Speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><mtext>Speech</mtext><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{\\text{Speech}}</annotation></semantics></math>, denoted as <math alttext=\"\\widetilde{R}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>R</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{R}</annotation></semantics></math> and <math alttext=\"\\widetilde{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>E</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\widetilde{E}</annotation></semantics></math>. The perturbation processes for <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m7\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> and <math alttext=\"E\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p2.m8\" intent=\":literal\"><semantics><mi>E</mi><annotation encoding=\"application/x-tex\">E</annotation></semantics></math> are conducted independently. In this work, we explore two different speaker perturbation strategies. The first is signal-processing-based, implemented via formant shifting. The second uses speaker anonymization, generating speech with speaker characteristics that differ from those of the original audio. The process of formant shifting is illustrated in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#alg1\" title=\"Algorithm 1 &#8227; 2.3 Speech generation via speaker-perturbation representations &#8227; 2 Propose method &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m16\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m17\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math> adaptively scale and shift the normalized <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m18\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> based on the speaker and emotional representation.\nUsing SEALN, it is possible to synthesize speech with varying emotions for different speakers under the given conditions of <math alttext=\"g(S)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m19\" intent=\":literal\"><semantics><mrow><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">g(S)</annotation></semantics></math> and <math alttext=\"b(E)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p3.m20\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>E</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">b(E)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the <span class=\"ltx_text ltx_font_typewriter\">vec2wav</span> recovers the target timbre from the perturbed features and the speaker ID, we introduced a Speaker Consistency Loss (SCL), as described in paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Casanova et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib6\" title=\"\">2022</a>)</cite>.\nA pre-trained speaker encoder extracts speaker embeddings from the generated speech and the ground truth. We then maximize the cosine similarity as the speaker consistency loss. Let <math alttext=\"\\phi(.)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS3.p4.m1\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0.167em\">.</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(.)</annotation></semantics></math> be a function that outputs the embedding of a speaker. Let <math alttext=\"cos\\_sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m2\" intent=\":literal\"><semantics><mrow><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">cos\\_sim</annotation></semantics></math> denote the cosine similarity function, and let <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m3\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> be a positive real number that controls the influence of the Speaker Contrastive Loss (SCL) in the final loss calculation. Additionally, let <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p4.m4\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> represent the batch size. The SCL is defined as follows:</p>\n\n",
                "matched_terms": [
                    "scl",
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LJSpeech</span> is a publicly available speech dataset containing 13,100 short audio clips of a single speaker reading excerpts from seven non-fiction books. The clips range from 1 to 10 seconds in length and total approximately 24 hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the voice cloning experiments in a monolingual setting, we used the LibriTTS dataset for training and the <span class=\"ltx_text ltx_font_italic\">test-clean</span> subset of <span class=\"ltx_text ltx_font_bold\">LibriSpeech</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib30\" title=\"\">2015</a>)</cite> for evaluation. This widely used test set contains speech from 40 different speakers and totals 5.4 hours of audio. Following the method described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib17\" title=\"\">2024</a>)</cite>, we randomly evaluated 25 utterances per speaker from the LibriSpeech test-clean dataset.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "speaker",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the cross-lingual emotional speech synthesis experiments, the ESD dataset has a limited size of 350 unique sentences per language. Therefore, training includes LJSpeech and Biaobei. To balance emotion and speaker representation, the ESD dataset is upsampled by a factor of 5 during training. The details of the training data in cross-lingual scenarios are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T1\" title=\"Table 1 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection presents the details of two different experimental setups, including baseline models, evaluation metrics, and other relevant configurations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "models",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the model&#8217;s performance in zero-shot speech synthesis. Accordingly, our proposed EMM-TTS uses a pretrained speaker embedding instead of a one-hot vector to represent speaker identity. The pretrained representation is the same as in <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib13\" title=\"\">2024</a>)</cite>, extracted from a pretrained ECAPA-TDNN model. Moreover, no information perturbation was applied to the data during training or inference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reference Model.</span>\nFor the monolingual voice cloning experiments, we compared our EMM-TTS against the following state-of-the-art (SOTA) models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This set of experiments primarily evaluates the ability to transfer and synthesize emotions across languages. In these scenarios, our proposed EMM-TTS model adopts one-hot vectors as speaker input.\nWe experimented with two different speaker perturbation strategies. One based on signal processing, specifically formant perturbation, and the implementation of formant shifting followed the NANSY <cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib10\" title=\"\">2021</a>)</cite> model by using Praat <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.fon.hum.uva.nl/praat/\" title=\"\">https://www.fon.hum.uva.nl/praat/</a></span></span></span>.\nThe other uses an SSL-based language-independent speaker anonymization method by replacing the speaker embedding <cite class=\"ltx_cite ltx_citemacro_citep\">(Miao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib27\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#bib.bib28\" title=\"\">2023</a>)</cite> and its official implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/nii-yamagishilab/SSL-SAS\" title=\"\">https://github.com/nii-yamagishilab/SSL-SAS</a></span></span></span>.\nThe proposed EMM-TTS model defaults to using formant shift as speaker perturbation, while an ablation experiment model, EMM-TTS-SA, is designed for speaker anonymization.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "across",
                    "emmtts",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reference Model.</span> We refer to our proposed model as EMM-TTS and the two baseline models as DiCLET and M3:</p>\n\n",
                "matched_terms": [
                    "models",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyzed the experimental results using both subjective and objective evaluations, with the following metrics included:</p>\n\n",
                "matched_terms": [
                    "results",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CER.</span> We employ whisper-large-v3 <span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span> to transcribe the synthesized speech into text, which is then compared with the ground-truth transcripts to compute the character error rate (CER).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "synthesized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">EECS.</span> Similar to speaker similarity, we compute the emotional similarity of speech, where the emotion embeddings are extracted using the model emotion2vec <span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ddlBoJack/emotion2vec\" title=\"\">https://github.com/ddlBoJack/emotion2vec</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering that objective metrics in cross-lingual scenarios may fail to capture subtle variations in emotion and speaker characteristics, we further conducted the following subjective experiments.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DMOS.</span> The Differential Mean Opinion Score (DMOS) is employed to evaluate the speaker similarity between synthesized and reference audio, on a 1&#8211;5 scale where 1 denotes completely dissimilar and 5 denotes highly similar.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "synthesized",
                    "dmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">EMOS.</span> The Emotion Mean Opinion Score (EMOS) is employed to evaluate the emotional similarity between synthesized and reference audio, on a 1&#8211;5 scale where 1 denotes completely dissimilar and 5 denotes highly similar.</p>\n\n",
                "matched_terms": [
                    "emos",
                    "synthesized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the subjective evaluation, each system generates 30 sentences for each language. These include six speakers, each contributing one sentence for each of five emotions. A total of 15 participants were invited to evaluate the subjective tests.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we validate the effectiveness of the proposed method in both monolingual and cross-lingual scenarios. In the monolingual setting, we primarily analyze the performance of voice cloning; in the cross-lingual setting, we also evaluate emotional similarity. We further investigate the impact of speaker perturbations on the model and conduct ablation studies on SEALN and SCL.</p>\n\n",
                "matched_terms": [
                    "scl",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results of EMM-TTS and the baseline models on the LibriSpeech test-clean set are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T2\" title=\"Table 2 &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nCompared with ZMM-TTS, incorporating both explicit and implicit emotional representations enables EMM-TTS to achieve higher speaker similarity and improved speech naturalness.\nThis suggests that, in addition to timbre cloning, modeling emotional information can substantially improve speaker similarity with the reference audio.\nCompared with the current state-of-the-art multilingual synthesis model HierSpeech++, EMM-TTS achieves a notable improvement in speaker similarity under the same training data conditions.\nBy comparing the RTF and the number of parameters with HierSpeech++, our model is more lightweight and better suited for computation-constrained environments.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "speaker",
                    "emmtts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T3\" title=\"Table 3 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> present the subjective evaluation results of synthesized Chinese and English speech, respectively. The proposed EMM-TTS achieves the best naturalness. This improvement may be attributed to the XPhoneBERT-based text representation and phoneme encoder, which enable more effective modeling of pronunciations from different languages in a unified space, thereby enhancing multilingual synthesis capability. Furthermore, we find that the naturalness degrades significantly when synthesizing speech with cross-lingual speakers compared to same-lingual speakers. Specifically, when synthesizing text in language B with the voice of a speaker from language A, the generated speech often contains pronunciation errors and accent issues, particularly when English speakers synthesize Chinese speech.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "evaluation",
                    "speech",
                    "speaker",
                    "emmtts",
                    "subjective",
                    "different",
                    "results",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">DMOS</span>, DiCLET-TTS and M3 achieve relatively similar results under monolingual conditions, but M3 exhibits a substantial performance drop in cross-lingual scenarios. This indicates that M3 suffers from weak disentanglement capability. When the reference audio and the target speaker&#8217;s timbre are mismatched, the synthesized speech is heavily affected by the timbre of the reference audio. In contrast, the proposed <span class=\"ltx_text ltx_font_bold\">EMM-TTS</span> consistently achieves the best speaker similarity and emotion similarity in both same-lingual and cross-lingual settings, while also showing the least performance degradation in cross-lingual scenarios. These results demonstrate the effectiveness of our proposed emotion modeling and disentanglement strategies.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "dmos",
                    "speech",
                    "speaker",
                    "emmtts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the objective metrics reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S3.T5\" title=\"Table 5 &#8227; 3.3 Evaluation Metrics &#8227; 3 EXPERIMENTS &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that EMM-TTS achieves the best performance in both intelligibility and SECS across the two languages. Moreover, consistent with the subjective evaluations, when the target speaker&#8217;s language differs from the synthesized speech, both speaker similarity and intelligibility decline. In contrast, DiCLET-TTS consistently yields the poorest intelligibility (CER) in most cases, which may be attributed to its use of speaker-adversarial learning for text representations, potentially compromising the content quality of the synthesized speech.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "across",
                    "speech",
                    "speaker",
                    "emmtts",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to formant shifting, this chapter utilizes a speaker anonymization technique to alter information, aiming to investigate the effects of various interference methods on synthesized speech. First, audio samples from 10 Chinese speakers in the ESD dataset were selected for two types of speaker interference, followed by visualization and quantitative analysis of the interfered audio.\nFor each speaker and each emotion, 50 sentences were selected, resulting in a total of 2,500 sentences for analysis.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents a visualization of the speaker representations extracted by a pre-trained ECAPA-TDNN speaker encoder. The representations were reduced to two dimensions using t-SNE, with different colors representing different speakers. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (a), it can be observed that, in the original audio, speaker embeddings of the same speaker cluster closely together, forming distinct clusters. Furthermore, each speaker cluster contains several sub-clusters, which, upon inspection, correspond to different emotions. This phenomenon further confirms that speaker information and emotional information are often entangled. Although ECAPA-TDNN achieves good performance in speaker classification, its learned speaker representations still contain rich emotional details. Furthermore, as shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (b) and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (c), speaker interference methods can effectively alter the speaker information in the audio. Specifically, after interference, the embeddings of audio samples from the same speaker exhibit greater distances. Among the two methods, speaker anonymization imposes the greatest interference with speaker information.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "speech",
                    "speaker",
                    "different",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the visual analysis, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T6\" title=\"Table 6 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> presents the objective evaluation results for audio processed with different speaker perturbation methods. The SECS results are consistent with the observations in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F3\" title=\"Figure 3 &#8227; Objective results. &#8227; 4.2.1 Compare with baseline methods &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, showing that both perturbation methods effectively interfere with speaker-related information in the audio. The anonymization-based method produces the most substantial perturbation to speaker identity, but it also inevitably degrades emotional expressiveness. This method, while most effective at obfuscating speaker identity, introduces the greatest emotional distortion.\nAnalysis of the UTMOS and CER values further reveals that the formant-shift method primarily affects the naturalness of speech, whereas the anonymization-based method mainly impacts the linguistic content. On one hand, directly shifting formants tends to make the speech sound less natural. On the other hand, the anonymization approach relies on recognizing and re-synthesizing the speech, and recognition errors can easily accumulate in the anonymized output.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "speech",
                    "speaker",
                    "different",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.T7\" title=\"Table 7 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the objective evaluation results of the EMM-TTS model under different speaker perturbation strategies. Compared with the model that applies no speaker perturbation, introducing perturbations reduces the reference speaker&#8217;s influence on the synthesized audio, leading to improved SECS. This result indicates that perturbing speaker information facilitates disentangling emotion from speaker identity. Although the two perturbation methods yield comparable SECS scores, the anonymization-based perturbation causes a noticeable decline in speech intelligibility.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "evaluation",
                    "speech",
                    "speaker",
                    "emmtts",
                    "different",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11124v1#S4.F4\" title=\"Figure 4 &#8227; 4.2.2 Analysis of the effectiveness of speaker perturbation &#8227; 4.2 Performance on cross-lingual emotion speech synthesis &#8227; 4 Experiment Results &#8227; Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> illustrates the ABX test preferences for the EMM-TTS (default with formant shift) model when different speaker perturbation methods are applied. The test was conducted on samples spoken by English speakers with Chinese linguistic content. The results show that the formant-shift method outperforms the anonymization-based approach in terms of naturalness, speaker similarity, and emotional similarity. Among these aspects, the gap in emotional similarity is the most pronounced. Although the anonymization-based method effectively disrupts speaker identity, it also weakens emotional cues, leading to synthesized speech that sounds more neutral.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "speech",
                    "speaker",
                    "emmtts",
                    "different",
                    "results",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, removing the pretrained emotional representation\nE leads to a noticeable decrease in emotional similarity. Interestingly, emotional similarity and speaker similarity tend to exhibit a negative correlation&#8212;improving one often comes at the cost of the other. The final EMM-TTS model achieves a balanced trade-off between the two, demonstrating superior overall performance. Future work will explore finer-grained control over both timbre and emotional expressiveness, aiming to achieve a more flexible balance between them.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose a two-stage cross-lingual emotional speech synthesis system, EMM-TTS. The first stage focuses on modeling and predicting emotional representations, while the second stage enables fine-grained control over speaker timbre. The two stages are connected through perturbed self-supervised features, which serve as a bridge between emotion and timbre modeling. Experimental results demonstrate that EMM-TTS achieves strong zero-shot voice cloning in monolingual scenarios and effective emotion transfer across languages.</p>\n\n",
                "matched_terms": [
                    "across",
                    "speech",
                    "speaker",
                    "emmtts",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Timbre and emotion are two highly entangled factors in speech signals, posing challenges for fine-grained control in speech synthesis. Information perturbation is a commonly adopted strategy for disentangling these factors. Previous studies have primarily focused on perturbation methods based on signal processing. In this work, we investigate the capability of recent speaker anonymization models to disentangle emotion and timbre. Our analysis combines visualization, subjective listening tests, and objective audio quality metrics. Experimental results show that signal-processing-based perturbations produce stronger distortion of speaker identity, whereas speaker anonymization models better preserve the naturalness of synthesized speech.</p>\n\n",
                "matched_terms": [
                    "synthesized",
                    "models",
                    "speech",
                    "speaker",
                    "subjective",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study further reveals that pretrained features&#8212;such as high-dimensional latent variables learned by speaker or emotion encoders&#8212;cannot fully replace explicit acoustic features such as pitch, energy, and duration. Experimental results show that incorporating the modeling and prediction of these explicit features enhances the model&#8217;s voice cloning capability. In the emotion transfer stage, we introduce the Speaker Consistency Loss (SCL) and the Speaker-Emotion Adaptive Layer Normalization (SEALN). The ablation results demonstrate that these components contribute positively to maintaining speaker timbre and improving the overall synthesis quality.</p>\n\n",
                "matched_terms": [
                    "scl",
                    "results",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we proposed EMM-TTS, a two-stage cross-lingual emotional text-to-speech system that effectively disentangles emotion and timbre through speaker-perturbed SSL representations. By leveraging explicit prosodic modeling in the first stage and timbre restoration in the second stage, the system enables controllable emotion transfer and high-fidelity speaker imitation across languages. The proposed Speaker Consistency Loss (SCL) and Speaker&#8211;Emotion Adaptive Layer Normalization (SEALN) further enhance timbre stability and expressive consistency. Moreover, experiments reveal that combining explicit acoustic features with pretrained latent representations improves timbre reproduction. Extensive subjective and objective evaluations confirm that EMM-TTS achieves superior performance in both zero-shot timbre cloning and cross-lingual emotion transfer. In future work, we will explore finer-grained control of emotion intensity and timbre style, as well as adaptive balancing strategies between emotional expressiveness and speaker identity.</p>\n\n",
                "matched_terms": [
                    "scl",
                    "across",
                    "speaker",
                    "emmtts",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Future work will explore more fine-grained control over both emotion and timbre, enabling continuous adjustment of emotional intensity and timbre style. We also plan to investigate adaptive mechanisms that can balance the trade-off between emotional expressiveness and speaker identity preservation. Extending the approach to support more languages and diverse emotional expressions will further enhance the generalization and applicability of the proposed EMM-TTS framework.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "emmtts"
                ]
            }
        ]
    }
}