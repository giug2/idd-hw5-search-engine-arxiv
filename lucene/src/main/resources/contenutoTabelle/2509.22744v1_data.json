{
    "S3.T1": {
        "caption": "Table 1: ASR performance of different modality models on the in-house subtitle dataset.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" style=\"padding-top:1pt;padding-bottom:1pt;\">WER</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">ID</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Size</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Modality</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Substitution</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Deletion</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Insertion</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Overall</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">a1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Conformer-ASR-S</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">126M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">5650</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">664</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">506</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">10.36 %</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">m1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Conformer-ASR+OCR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">126M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Speech+Visual</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">4364</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">553</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">836</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">8.73 %</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">m2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Index-MSR-S</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">140M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Speech+Visual</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">2772</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">556</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">835</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">6.32 %</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">a2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Conformer-ASR-L</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">432M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3740</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">842</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_ERROR undefined\">\\ul</span>382</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">7.54 %</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">m3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Index-MSR-L</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">464M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Speech+Visual</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">1850</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">406</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">619</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span class=\"ltx_text ltx_font_bold\">4.37 %</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">a3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Paraformer-Large</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">220M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3746</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">1315</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">585</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">8.58 %</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">a4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">FireRedASR-AED-L</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">1.1 B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Speech</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">2230</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_ERROR undefined\">\\ul</span>339</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">625</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">4.85 %</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">m4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Gemini-2.5-Pro</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">175B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-top:1pt;padding-bottom:1pt;\">Speech+Visual</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_ERROR undefined\">\\ul</span>1762</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">1135</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">12518</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1pt;padding-bottom:1pt;\">23.42%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "wer",
            "size",
            "overall",
            "140m",
            "subtitle",
            "conformerasrocr",
            "indexmsrl",
            "175b",
            "modality",
            "ul339",
            "ul1762",
            "gemini25pro",
            "paraformerlarge",
            "inhouse",
            "substitution",
            "speechvisual",
            "performance",
            "220m",
            "ul382",
            "conformerasrs",
            "insertion",
            "asr",
            "conformerasrl",
            "speech",
            "indexmsrs",
            "fireredasraedl",
            "464m",
            "model",
            "deletion",
            "126m",
            "432m",
            "dataset",
            "different"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Driven by large-scale datasets and LLM-based architectures, automatic speech recognition (ASR) systems have achieved remarkable improvements in accuracy. However, challenges persist for domain-specific terminology, and short utterances lacking semantic coherence, where recognition performance often degrades significantly.\nIn this work, we present Index-MSR, an efficient multimodal speech recognition framework. At its core is a novel Multimodal Fusion Decoder (MFD), which effectively incorporates text-related information from videos (e.g., subtitles and presentation slides) into the speech recognition. This cross-modal integration not only enhances overall ASR accuracy but also yields substantial reductions in substitution errors.\nExtensive evaluations on both an in-house subtitle dataset and a public AVSR dataset demonstrate that Index-MSR achieves state-of-the-art accuracy, with substitution errors reduced by 20&#8211;50%.\nThese results demonstrate that our approach efficiently exploits text-related cues from video to improve speech recognition accuracy, showing strong potential in applications requiring strict audio-text synchronization, such as audio translation.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "inhouse",
                    "substitution",
                    "asr",
                    "speech",
                    "dataset",
                    "subtitle",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, the availability of large-scale speech datasets and the introduction of LLM-based recognition frameworks into ASR models have led to significant improvements in accuracy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib2\" title=\"\">2</a>]</cite>. Large-scale multilingual pre-trained models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib3\" title=\"\">3</a>]</cite> have helped narrow the performance gap between high-resource and low-resource languages. However, for accented speech, unclear pronunciation, named entities, and short utterances with limited semantic coherence, the accuracy of speech-only recognition still degrades considerably.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In ASR applications, an important scenario involves the word-level transcription of speech embedded in videos. Videos frequently contain abundant text-related cues (e.g., subtitles and presentation slides) related to the spoken content, which can substantially complement the speech signal and are available at scale across multiple languages. Leveraging visual information in these video scenarios thus holds significant potential for enhancing the robustness and generalization of ASR models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, directly aligning text information from visual content with ASR transcripts for correction remains challenging. Even the most relevant subtitles often undergo rephrasings (e.g., shortening, paraphrasing), stylistic edits, and temporal misalignments, posing challenges for achieving precise word-level recognition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib4\" title=\"\">4</a>]</cite>. Moreover, videos typically include substantial non-subtitle textual content, making subtitle extraction inherently challenging. Errors arising from this process can significantly compromise the alignment quality in ASR. Importantly, visual cues are not limited to subtitles; for example, domain-specific terminology appearing in slides or presentations can also provide valuable references to improve ASR accuracy. Consequently, a generalizable multimodal speech recognition approach holds significant importance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "subtitle",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some multimodal large language models (MLLMs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib6\" title=\"\">6</a>]</cite> can incorporate both audio and video information to provide ASR results. However, when subtitles and audio are misaligned, such methods tend to favor subtitle content over accurate word-level transcription, limiting their applicability in scenarios requiring precise temporal alignment. Moreover, they typically demand substantial computational resources, as they rely on large-scale language models (LLMs) to interpret multimodal inputs.\nTherefore, efficiently leveraging video information to enhance ASR accuracy while maintaining its intrinsic temporal alignment is highly valuable.</p>\n\n",
                "matched_terms": [
                    "models",
                    "subtitle",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose Index-MSR, an innovative and efficient multimodal alignment model that preserves the strict temporal alignment capability of ASR while effectively leveraging visual information to significantly enhance recognition performance.\nThe main contributions of this work are:</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1. We investigate efficient, low-resource, end-to-end multimodal ASR training methods that integrate text-related features from video into verbatim speech recognition.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">2.We propose a novel Multimodal Fusion Decoder (MFD) that substantially enhances ASR accuracy and achieves a significant reduction in substitution errors in the multimodal recognition scenario.</p>\n\n",
                "matched_terms": [
                    "substitution",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">3. We conduct extensive evaluations on both in-house and open-source datasets, comparing against state-of-the-art models and MLLM methods.</p>\n\n",
                "matched_terms": [
                    "models",
                    "inhouse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work is built upon an end-to-end (E2E) ASR system <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib7\" title=\"\">7</a>]</cite>, where the textual information is derived from weakly supervised OCR features. The feature extraction encoder is kept frozen during training and does not perform any explicit discrimination between subtitle text and background text. To preserve the capability for verbatim transcription, we adopt a Conformer-based E2E ASR architecture. The key innovation lies in our Multimodal Fusion decoder (MFD) design, which enables efficient fusion of multimodal information and leads to substantial improvements in recognition accuracy.</p>\n\n",
                "matched_terms": [
                    "subtitle",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extract visual textual features from video frames using a SOTA OCR pipeline based on PP-OCRv5 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib9\" title=\"\">9</a>]</cite>. First, an image preprocessing module enhances input image quality and corrects distortions or misorientations. Next, the text detection model using an advanced PP-HGNetV2 backbone. Detected text lines are then passed through a text line orientation classification model. Finally, the text recognition model encodes each text line into feature representations. A lightweight CTC-based branch for efficient decoding of text features. The image features extracted prior to CTC alignment are used for feature representation, offering a compact and semantically enriched embedding well-suited for multimodal fusion in ASR.\nImportantly, we did not perform any dataset-specific optimization on the OCR pipeline, nor did we distinguish whether the extracted textual information originated from subtitles or background content; instead, all visual features were uniformly packed and fed into the MFD module.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although both audio and visual encoders adopt CTC alignment, their features come from different domains, temporal for speech and spatial for images. Our goal is strict time-aligned transcription; thus, we design a spatiotemporal interleaved cross-attention decoder that fuses features across domains while preserving word-level alignment.\nIn our proposed Multimodal Fusion Decoder (MFD), we extend the cross-attention mechanism within a transformer-based multimodal decoder architecture. Specifically, audio embeddings from the speech encoder <math alttext=\"(T)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(T)</annotation></semantics></math> and visual embeddings <math alttext=\"(I)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(I)</annotation></semantics></math> from the OCR-based text encoder are jointly injected into the decoder&#8217;s attention module. By integrating these heterogeneous modalities together with historical target information, MFD produces context representations through the cross-modal fusion mechanism, followed by the feed-forward layer and softmax prediction. These representations jointly encode information from both modalities and are then used to construct the output decoding sequence. This design not only preserves the strict temporal alignment inherent in ASR but also incorporates domain-specific textual information from visual content, thereby significantly improving recognition accuracy&#8212;particularly for named entities and rare words. (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 OCR-Integrated Text Feature Encoder &#8227; 2 Methods &#8227; INDEX-MSR: A HIGH-EFFICIENCY MULTIMODAL FUSION FRAMEWORK FOR SPEECH RECOGNITION\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>)</p>\n\n",
                "matched_terms": [
                    "speech",
                    "different",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct extensive experiments on both an in-house dataset and the publicly available Chinese-Lips dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib10\" title=\"\">10</a>]</cite> to evaluate the effectiveness of our proposed approach.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "inhouse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The in-house dataset consists of videos with embedded subtitles and word-level aligned speech annotations. In total, it contains approximately 190 hours of data, of which 171 hours are used for training and 19 hours for validation. The test set, comprising 8.6 hours of subtitled video with word-level aligned speech annotations, is isolated from the training data and used to evaluate ASR accuracy.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "inhouse",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Chinese-Lips dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib10\" title=\"\">10</a>]</cite>. This is a large-scale multimodal AVSR dataset comprising around 100 hours of speech, video, and corresponding manual transcriptions. The visual modality in this dataset covers both lip-reading information and presentation slides used by the speakers, as well as background scenes and other visual contexts.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our speech recognition backbone is an attention-based encoder&#8211;decoder (AED) architecture implemented with the WeNet toolkit <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib11\" title=\"\">11</a>]</cite>. The encoder adopts a Conformer structure with 12 blocks (a1: 8 attention heads, 2048 linear units; a2: 16 attention heads, 4096 linear units), while the decoder is a Transformer-based module with 3 blocks (a1: 8 attention heads, 2048 linear units; a2: 16 attention heads, 4096 linear units). The model is initialized from a pre-trained checkpoint trained on the Mandarin&#8211;English bilingual speech data, providing strong acoustic representations prior to multimodal fine-tuning. During multimodal alignment, the speech encoder parameters are frozen to ensure that performance improvements stem from the proposed multimodal fusion rather than additional training of the acoustic backbone.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed MFD structure is trained on the in-house audio-labeled subtitle dataset. To ensure that performance gains are not simply due to additional training data, we also trained a Conformer-based ASR baseline using only the speech modality from the same dataset (a1, a2 in Table 1). These serve as direct unimodal comparisons for validating the benefit of multimodal fusion.</p>\n\n",
                "matched_terms": [
                    "modality",
                    "inhouse",
                    "asr",
                    "speech",
                    "dataset",
                    "subtitle",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Index-MSR, we used the same training data as a1/a2 but additionally incorporated textual features from video, with results reported as m2 and m3. To further probe alignment effectiveness, we conducted a control experiment where raw OCR outputs from PP-OCRv5 were directly embedded as textual features (m1). While m1 improves WER over a1, its substitution error reduction is far smaller than m2, underscoring the importance of high-quality textual feature extraction for multimodal alignment.</p>\n\n",
                "matched_terms": [
                    "substitution",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To examine scalability, we tested larger ASR encoders initialized from a 30,000 hour bilingual corpus. While a stronger encoder improves unimodal accuracy (a2), MFD still delivers &#160;50% substitution error reduction (m3), demonstrating robustness across encoder capacities. Moreover, with only &#160;200 hours of multimodal data, our approach matches or surpasses state-of-the-art speech-only models trained on tens of thousands of hours (e.g., a 1.1B-parameter AED system, a4 in Table 1) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib12\" title=\"\">12</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "substitution",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we compared against large multimodal models. In m4, Gemini 2.5 Pro jointly processes video and audio, achieving the lowest substitution errors by leveraging subtitle text. However, in cases of video&#8211;audio mismatch, it often inserts subtitle words absent from speech, leading to high insertion errors.</p>\n\n",
                "matched_terms": [
                    "models",
                    "insertion",
                    "substitution",
                    "speech",
                    "subtitle"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to validating the generalization and scalability of our model on the in-house dataset, we further evaluated it on the Chinese-Lip dataset, which consists of multimodal video with weakly supervised textual signals derived from presentation slides.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "inhouse",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Chinese-LiPS dataset provides benchmark baselines <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib10\" title=\"\">10</a>]</cite> for both speech only and multimodal speech recognition. For the speech only setting, the Whisper large-v2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib13\" title=\"\">13</a>]</cite> model is adopted, achieving a CER of 3.99% when trained and evaluated solely on audio signals. For multimodal baselines, Whisper is extended with additional inputs derived from the video stream: text extracted from presentation slides using PP-OCR, and semantic keywords obtained with InternVL2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib17\" title=\"\">17</a>]</cite>. These multimodal variants serve as the official baselines for evaluating ASR performance on Chinese-LiPS.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr",
                    "speech",
                    "dataset",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compared models of the Index-MSR-L against both unimodal and multimodal baselines. Consistent with our in-house results, the proposed MFD significantly improves alignment under weak supervision. Specifically, we observed a 20% relative reduction in substitution errors, which are especially prevalent in named entities and domain-specific terminology. Compared to baseline multimodal approaches that simply concatenate speech and image features, our MFD achieves both the lowest substitution error rate and the lowest overall WER. (Table 2)</p>\n\n",
                "matched_terms": [
                    "models",
                    "indexmsrl",
                    "wer",
                    "overall",
                    "inhouse",
                    "substitution",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose Index-MSR, an efficient multimodal fusion framework that leverages limited alignment data to calibrate ASR outputs with visual cues, reducing substitution errors by 50% and effectively handling named entities and rare words. We further validate the MFD by scaling ASR models and training on weakly supervised multimodal datasets, where it consistently improves accuracy, confirming its robustness and scalability.\nCompared with SOTA unimodal ASR and multimodal large models, Index-MSR uniquely combines low-resource efficiency, high accuracy, and fine-grained word-level alignment with timestamps, making it well-suited for verbatim ASR in video-rich environments. It is particularly promising for source-audio translation, where precise timing and accurate entity recognition are critical.</p>\n\n",
                "matched_terms": [
                    "models",
                    "substitution",
                    "asr"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: ASR performance of different modality models on the Chinese-LiPS dataset [10].",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">ID</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Modality</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">S</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">D</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">I</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">WER</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\">b1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\">Speech only</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\">3851</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\">1697</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\">437</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">3.99 %</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">b2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">Speech + Slides</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">3531</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">447</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">510</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">2.99 %</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">b3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">Speech + Lip</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">4499</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">509</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">522</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">3.69 %</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">b4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">Speech + Lip + Slides</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">3047</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">335</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1pt 2.5pt;\">484</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">2.57 %</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#656565;\">a2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#656565;\">Conformer-ASR-L</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#656565;\">3088</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#656565;\">514</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#656565;\">599</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">2.80 %</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">m3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">Index-MSR-L</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">2483 &#8595;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">233 &#8595;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">413 &#8595;</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">2.08 % &#8595;</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "chineselips",
            "lip",
            "wer",
            "indexmsrl",
            "modality",
            "slides",
            "asr",
            "conformerasrl",
            "speech",
            "dataset",
            "only",
            "different",
            "performance"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Driven by large-scale datasets and LLM-based architectures, automatic speech recognition (ASR) systems have achieved remarkable improvements in accuracy. However, challenges persist for domain-specific terminology, and short utterances lacking semantic coherence, where recognition performance often degrades significantly.\nIn this work, we present Index-MSR, an efficient multimodal speech recognition framework. At its core is a novel Multimodal Fusion Decoder (MFD), which effectively incorporates text-related information from videos (e.g., subtitles and presentation slides) into the speech recognition. This cross-modal integration not only enhances overall ASR accuracy but also yields substantial reductions in substitution errors.\nExtensive evaluations on both an in-house subtitle dataset and a public AVSR dataset demonstrate that Index-MSR achieves state-of-the-art accuracy, with substitution errors reduced by 20&#8211;50%.\nThese results demonstrate that our approach efficiently exploits text-related cues from video to improve speech recognition accuracy, showing strong potential in applications requiring strict audio-text synchronization, such as audio translation.</p>\n\n",
                "matched_terms": [
                    "slides",
                    "asr",
                    "speech",
                    "dataset",
                    "only",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In recent years, the availability of large-scale speech datasets and the introduction of LLM-based recognition frameworks into ASR models have led to significant improvements in accuracy <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib2\" title=\"\">2</a>]</cite>. Large-scale multilingual pre-trained models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib3\" title=\"\">3</a>]</cite> have helped narrow the performance gap between high-resource and low-resource languages. However, for accented speech, unclear pronunciation, named entities, and short utterances with limited semantic coherence, the accuracy of speech-only recognition still degrades considerably.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In ASR applications, an important scenario involves the word-level transcription of speech embedded in videos. Videos frequently contain abundant text-related cues (e.g., subtitles and presentation slides) related to the spoken content, which can substantially complement the speech signal and are available at scale across multiple languages. Leveraging visual information in these video scenarios thus holds significant potential for enhancing the robustness and generalization of ASR models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "asr",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, directly aligning text information from visual content with ASR transcripts for correction remains challenging. Even the most relevant subtitles often undergo rephrasings (e.g., shortening, paraphrasing), stylistic edits, and temporal misalignments, posing challenges for achieving precise word-level recognition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib4\" title=\"\">4</a>]</cite>. Moreover, videos typically include substantial non-subtitle textual content, making subtitle extraction inherently challenging. Errors arising from this process can significantly compromise the alignment quality in ASR. Importantly, visual cues are not limited to subtitles; for example, domain-specific terminology appearing in slides or presentations can also provide valuable references to improve ASR accuracy. Consequently, a generalizable multimodal speech recognition approach holds significant importance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Some multimodal large language models (MLLMs) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib6\" title=\"\">6</a>]</cite> can incorporate both audio and video information to provide ASR results. However, when subtitles and audio are misaligned, such methods tend to favor subtitle content over accurate word-level transcription, limiting their applicability in scenarios requiring precise temporal alignment. Moreover, they typically demand substantial computational resources, as they rely on large-scale language models (LLMs) to interpret multimodal inputs.\nTherefore, efficiently leveraging video information to enhance ASR accuracy while maintaining its intrinsic temporal alignment is highly valuable.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we propose Index-MSR, an innovative and efficient multimodal alignment model that preserves the strict temporal alignment capability of ASR while effectively leveraging visual information to significantly enhance recognition performance.\nThe main contributions of this work are:</p>\n\n",
                "matched_terms": [
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1. We investigate efficient, low-resource, end-to-end multimodal ASR training methods that integrate text-related features from video into verbatim speech recognition.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although both audio and visual encoders adopt CTC alignment, their features come from different domains, temporal for speech and spatial for images. Our goal is strict time-aligned transcription; thus, we design a spatiotemporal interleaved cross-attention decoder that fuses features across domains while preserving word-level alignment.\nIn our proposed Multimodal Fusion Decoder (MFD), we extend the cross-attention mechanism within a transformer-based multimodal decoder architecture. Specifically, audio embeddings from the speech encoder <math alttext=\"(T)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(T)</annotation></semantics></math> and visual embeddings <math alttext=\"(I)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(I)</annotation></semantics></math> from the OCR-based text encoder are jointly injected into the decoder&#8217;s attention module. By integrating these heterogeneous modalities together with historical target information, MFD produces context representations through the cross-modal fusion mechanism, followed by the feed-forward layer and softmax prediction. These representations jointly encode information from both modalities and are then used to construct the output decoding sequence. This design not only preserves the strict temporal alignment inherent in ASR but also incorporates domain-specific textual information from visual content, thereby significantly improving recognition accuracy&#8212;particularly for named entities and rare words. (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#S2.F1\" title=\"Figure 1 &#8227; 2.2 OCR-Integrated Text Feature Encoder &#8227; 2 Methods &#8227; INDEX-MSR: A HIGH-EFFICIENCY MULTIMODAL FUSION FRAMEWORK FOR SPEECH RECOGNITION\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>)</p>\n\n",
                "matched_terms": [
                    "speech",
                    "only",
                    "different",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct extensive experiments on both an in-house dataset and the publicly available Chinese-Lips dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib10\" title=\"\">10</a>]</cite> to evaluate the effectiveness of our proposed approach.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "chineselips"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The in-house dataset consists of videos with embedded subtitles and word-level aligned speech annotations. In total, it contains approximately 190 hours of data, of which 171 hours are used for training and 19 hours for validation. The test set, comprising 8.6 hours of subtitled video with word-level aligned speech annotations, is isolated from the training data and used to evaluate ASR accuracy.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Chinese-Lips dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib10\" title=\"\">10</a>]</cite>. This is a large-scale multimodal AVSR dataset comprising around 100 hours of speech, video, and corresponding manual transcriptions. The visual modality in this dataset covers both lip-reading information and presentation slides used by the speakers, as well as background scenes and other visual contexts.</p>\n\n",
                "matched_terms": [
                    "chineselips",
                    "modality",
                    "slides",
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our speech recognition backbone is an attention-based encoder&#8211;decoder (AED) architecture implemented with the WeNet toolkit <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib11\" title=\"\">11</a>]</cite>. The encoder adopts a Conformer structure with 12 blocks (a1: 8 attention heads, 2048 linear units; a2: 16 attention heads, 4096 linear units), while the decoder is a Transformer-based module with 3 blocks (a1: 8 attention heads, 2048 linear units; a2: 16 attention heads, 4096 linear units). The model is initialized from a pre-trained checkpoint trained on the Mandarin&#8211;English bilingual speech data, providing strong acoustic representations prior to multimodal fine-tuning. During multimodal alignment, the speech encoder parameters are frozen to ensure that performance improvements stem from the proposed multimodal fusion rather than additional training of the acoustic backbone.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our proposed MFD structure is trained on the in-house audio-labeled subtitle dataset. To ensure that performance gains are not simply due to additional training data, we also trained a Conformer-based ASR baseline using only the speech modality from the same dataset (a1, a2 in Table 1). These serve as direct unimodal comparisons for validating the benefit of multimodal fusion.</p>\n\n",
                "matched_terms": [
                    "modality",
                    "asr",
                    "speech",
                    "dataset",
                    "only",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To examine scalability, we tested larger ASR encoders initialized from a 30,000 hour bilingual corpus. While a stronger encoder improves unimodal accuracy (a2), MFD still delivers &#160;50% substitution error reduction (m3), demonstrating robustness across encoder capacities. Moreover, with only &#160;200 hours of multimodal data, our approach matches or surpasses state-of-the-art speech-only models trained on tens of thousands of hours (e.g., a 1.1B-parameter AED system, a4 in Table 1) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib12\" title=\"\">12</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "only",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we compared against large multimodal models. In m4, Gemini 2.5 Pro jointly processes video and audio, achieving the lowest substitution errors by leveraging subtitle text. However, in cases of video&#8211;audio mismatch, it often inserts subtitle words absent from speech, leading to high insertion errors.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to validating the generalization and scalability of our model on the in-house dataset, we further evaluated it on the Chinese-Lip dataset, which consists of multimodal video with weakly supervised textual signals derived from presentation slides.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "slides"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Chinese-LiPS dataset provides benchmark baselines <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib10\" title=\"\">10</a>]</cite> for both speech only and multimodal speech recognition. For the speech only setting, the Whisper large-v2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib13\" title=\"\">13</a>]</cite> model is adopted, achieving a CER of 3.99% when trained and evaluated solely on audio signals. For multimodal baselines, Whisper is extended with additional inputs derived from the video stream: text extracted from presentation slides using PP-OCR, and semantic keywords obtained with InternVL2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22744v1#bib.bib17\" title=\"\">17</a>]</cite>. These multimodal variants serve as the official baselines for evaluating ASR performance on Chinese-LiPS.</p>\n\n",
                "matched_terms": [
                    "chineselips",
                    "slides",
                    "asr",
                    "speech",
                    "dataset",
                    "only",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compared models of the Index-MSR-L against both unimodal and multimodal baselines. Consistent with our in-house results, the proposed MFD significantly improves alignment under weak supervision. Specifically, we observed a 20% relative reduction in substitution errors, which are especially prevalent in named entities and domain-specific terminology. Compared to baseline multimodal approaches that simply concatenate speech and image features, our MFD achieves both the lowest substitution error rate and the lowest overall WER. (Table 2)</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "indexmsrl",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose Index-MSR, an efficient multimodal fusion framework that leverages limited alignment data to calibrate ASR outputs with visual cues, reducing substitution errors by 50% and effectively handling named entities and rare words. We further validate the MFD by scaling ASR models and training on weakly supervised multimodal datasets, where it consistently improves accuracy, confirming its robustness and scalability.\nCompared with SOTA unimodal ASR and multimodal large models, Index-MSR uniquely combines low-resource efficiency, high accuracy, and fine-grained word-level alignment with timestamps, making it well-suited for verbatim ASR in video-rich environments. It is particularly promising for source-audio translation, where precise timing and accurate entity recognition are critical.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            }
        ]
    }
}