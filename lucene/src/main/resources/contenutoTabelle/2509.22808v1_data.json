{
    "S4.T1": {
        "source_file": "ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection",
        "caption": "Table 1: Equal Error Rate (EER) and Accuracy comparison across FishSpeech, XTTS-v2, ArTST, and VITS models.",
        "body": "Model\nFishSpeech\nXTTS-v2\nArTST\nVITS\n\n\nEER (%)↓\\downarrow\nACC (%)↑\\uparrow\nEER (%)↓\\downarrow\nACC (%)↑\\uparrow\nEER (%)↓\\downarrow\nACC (%)↑\\uparrow\nEER (%)↓\\downarrow\nACC (%)↑\\uparrow\n\n\nBaseline(ASVSPOOF benchmark system)\n\n\nRawNet2\n14.92\n87.84\n7.18\n93.48\n0.70\n99.52\n2.80\n98.50\n\n\nEmbedding-Based Models\n\n\nWav2vec2.0\n43.51\n63.35\n2.44\n98.20\n0.85\n99.56\n0.77\n99.27\n\n\nHuBERT-base\n13.75\n86.61\n0.07\n99.96\n0.08\n99.93\n0.84\n99.52\n\n\nWhisper-small\n10.64\n93.32\n2.74\n97.29\n0.55\n99.71\n0.69\n99.67\n\n\nWhisper-large\n6.92\n94.35\n1.73\n98.50\n0.28\n99.85\n1.14\n99.38\n\n\nTraditional ML Models (MFCC-Based)\n\n\nLogistic Regression\n33.29\n66.73\n9.44\n90.29\n0.16\n99.74\n0.23\n99.78\n\n\nSVM\n14.58\n85.55\n3.82\n96.19\n0.00\n100\n0.00\n100\n\n\nKNN\n37.86\n57.21\n8.17\n88.06\n0.28\n99.45\n0.63\n99.12\n\n\nDecision Tree\n33.67\n66.84\n18.35\n81.25\n0.47\n99.63\n0.91\n99.23\n\n\nRandom Forest\n19.56\n81.07\n9.18\n90.73\n0.07\n99.96\n0.10\n99.93\n\n\nGradient Boosting\n25.13\n74.58\n9.91\n90.07\n0.08\n99.96\n0.15\n99.71\n\n\nAdaBoost\n33.51\n66.24\n12.93\n86.89\n0.08\n99.96\n0.31\n99.71\n\n\nExtra Trees\n20.20\n80.50\n7.90\n91.72\n0.00\n100\n0.08\n99.96\n\n\nNaive Bayes\n37.82\n60.32\n19.89\n79.63\n0.70\n99.34\n1.05\n98.83",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">FishSpeech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">XTTS-v2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ArTST</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">VITS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">EER (%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ACC (%)<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">EER (%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ACC (%)<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">EER (%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ACC (%)<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">EER (%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ACC (%)<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"9\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:70%;\">Baseline(ASVSPOOF benchmark system)</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">RawNet2</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">14.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">87.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">7.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">93.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.80</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">98.50</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"9\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:70%;\">Embedding-Based Models</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Wav2vec2.0</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">43.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">63.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">98.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.77</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.27</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">HuBERT-base</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">13.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">86.61</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">99.96</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.93</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.84</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.52</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Whisper-small</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">10.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">93.32</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">97.29</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.69</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.67</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Whisper-large</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">6.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">94.35</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">1.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">98.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.85</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">1.14</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.38</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"9\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:70%;\">Traditional ML Models (MFCC-Based)</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Logistic Regression</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">33.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">66.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">9.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">90.29</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.23</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.78</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">SVM</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">14.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">85.55</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">3.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">96.19</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.00</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">100</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">KNN</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">37.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">57.21</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">8.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">88.06</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.45</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.63</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.12</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Decision Tree</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">33.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">66.84</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">18.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">81.25</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.63</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.91</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Random Forest</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">19.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">81.07</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">9.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">90.73</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.96</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.10</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.93</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Gradient Boosting</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">25.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">74.58</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">9.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">90.07</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.96</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.15</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.71</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">AdaBoost</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">33.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">66.24</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">12.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">86.89</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.96</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.31</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.71</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Extra Trees</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">20.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">80.50</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">7.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">91.72</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">100</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.08</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.96</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Naive Bayes</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">37.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">60.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">19.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">79.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">99.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">1.05</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">98.83</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "baselineasvspoof",
            "extra",
            "knn",
            "benchmark",
            "↑uparrow",
            "wav2vec20",
            "forest",
            "random",
            "equal",
            "error",
            "xttsv2",
            "rate",
            "accuracy",
            "fishspeech",
            "traditional",
            "whispersmall",
            "logistic",
            "vits",
            "artst",
            "system",
            "hubertbase",
            "embeddingbased",
            "decision",
            "↓downarrow",
            "mfccbased",
            "tree",
            "gradient",
            "adaboost",
            "comparison",
            "acc",
            "boosting",
            "eer",
            "svm",
            "across",
            "naive",
            "models",
            "whisperlarge",
            "bayes",
            "trees",
            "model",
            "regression",
            "rawnet2"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this, we designed a broader dataset by combining spoofed audio from Fish-Speech, XTTS-v2, and ArTST while excluding VITS. The training set included 70% of bonafide samples together with spoofed samples at ratios of 70%, 50%, and 40% for Fish-Speech, XTTS-v2, and ArTST, respectively. Validation used 10% of the training set, and the test set consisted of 30%, 30%, 50%, and 60% from bonafide, Fish-Speech, XTTS-v2, and ArTST, respectively.\nThis produced dataset with a size of 54413 utterances that is splitted as 31302 utterances in the training set and 23111 in the test set, as shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 3.1.5 Toward a More Generalizable Dataset &#8227; 3.1 Stage 1: Building the ArFake Corpus &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We then retrained the best-performing models from Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on this dataset and evaluated them against our test set and VITS-generated audios. As summarized in Table &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4.3 Constructing the Final Dataset &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, even without exposure to VITS during training, all models demonstrated strong performance on its synthesized samples.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes the Equal Error Rate (EER) and accuracy results for both classical classifiers and encoder-based models, evaluated on data generated by four different TTS models. The final TTS models&#8217; percentage used for generating the synthetic dataset was selected based on a combination of classifier performance, Mean Opinion Score (MOS), and ASR results. Initially, datasets were generated using each TTS model, and the one that proved most challenging for classifiers to distinguish between bonafide and spoofed samples will be chosen either to generate the dataset or to have a high percentage while generating the final dataset. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the dataset generated by FishSpeech was the most difficult for classifiers, followed by XTTS-v2, ArTST, and VITS. As expected, embedding-based models outperformed traditional machine learning models, particularly on datasets generated by FishSpeech and XTTS-v2. Notably, Whisper-large achieved a strong performance with an EER of 6.92%, outperforming other models and even surpassing the ASVSpoof benchmark by nearly 7%. HuBERT-base also demonstrated strong results on XTTS-v2, achieving an EER of just 0.07%.\nFor the dataset generated by ArTST and VITS, SVM achieved perfect performance with 100% accuracy, with Extra Trees performing similarly and achieving 99.96% accuracy on the VITS-generated dataset. It is worth noting that the embedding-based models on ArTST and VITS produced fair results, considering they were trained for only one epoch due to the relative ease of the spoofed datasets. Classifiers were not the only criterion used to determine the most effective TTS model for generating our dataset. We also calculated the Mean Opinion Score (MOS), as shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The results further support our findings, with FishSpeech achieving the highest overall MOS of 3.72 out of 5, followed by XTTS-v2. FishSpeech outperformed the other models across all dialects except for AE, where it ranked second to XTTS-v2 by a margin of just 0.08.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The results from Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS4.SSS1\" style=\"font-size:90%;\" title=\"4.4.1 Classifiers &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS4.SSS2\" style=\"font-size:90%;\" title=\"4.4.2 ASR evaluation &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicate that the spoofed dataset generated by FishSpeech is more realistic than those from other models. This raises an important question: should our benchmark dataset be generated using only one voice cloning (VC) system?\nFrom Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we evaluated Whisper-Large, trained on the FishSpeech dataset, on an unseen spoofed dataset generated by VITS and achieved an accuracy of 70.91%. This prompted us to consider making our dataset more generalizable, as described in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.SS1.SSS5\" style=\"font-size:90%;\" title=\"3.1.5 Toward a More Generalizable Dataset &#8227; 3.1 Stage 1: Building the ArFake Corpus &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1.5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. After combining datasets from multiple TTS models, we trained the best classifiers identified in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and tested them on both our test set and the spoofed audios generated by the VITS system.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4.3 Constructing the Final Dataset &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, although none of the models were trained on VITS-generated audios, they still performed well on this synthesized data. Note that EER is not defined for the VITS dataset here, as it contains only spoofed samples (one class). According to Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4.3 Constructing the Final Dataset &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Whisper-large outperforms other models by achieving a 4.88% EER on the ArFake test set. While Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the best EER on the FishSpeech dataset is 6.92%, this does not necessarily mean that FishSpeech-generated data is more difficult. The ArFake test set contains nearly 23,000 samples compared to only 2,637 in the FishSpeech dataset. Additionally, models in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4.3 Constructing the Final Dataset &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> were trained on a combination of spoofed datasets, whereas those in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> were trained on data from a single system. Finally, Whisper-small demonstrates strong performance on VITS-generated spoofed data with an accuracy of 98.30%, closely followed by Whisper-large at 97.94%. As a result combining the dataset from different systems made the the classifiers more generalizable</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the rise of generative text-to-speech models, distinguishing between real and synthetic speech has become challenging, especially for Arabic that have received limited research attention. Most spoof detection efforts have focused on English, leaving a significant gap for Arabic and its many dialects. In this work, we introduce the first multi-dialect Arabic spoofed speech dataset. To evaluate the difficulty of the synthesized audio from each model and determine which produces the most challenging samples, we aimed to guide the construction of our final dataset either by merging audios from multiple models or by selecting the best-performing model, we conducted an evaluation pipeline that included training classifiers using two approaches: modern embedding-based methods combined with classifier heads; classical machine learning algorithms applied to MFCC features; and the RawNet2 architecture. The pipeline further incorporated the calculation of Mean Opinion Score based on human ratings, as well as processing both original and synthesized datasets through an Automatic Speech Recognition model to measure the Word Error Rate. Our results demonstrate that FishSpeech outperforms other TTS models in Arabic voice cloning on the Casablanca corpus, producing more realistic and challenging synthetic speech samples. However, relying on a single TTS for dataset creation may limit generalizability.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text\" style=\"font-size:111%;\">1</span></span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/Mohammed01/ArFake\" style=\"font-size:111%;\" title=\"\">ArFake dataset on Hugging Face</a></span></span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "embeddingbased",
                    "error",
                    "rate",
                    "models",
                    "fishspeech",
                    "model",
                    "rawnet2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Neural text-to-speech (TTS) and voice&#8211;cloning systems, starting with WaveNet </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib1\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">1</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Tacotron </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib2\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">2</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and advancing to zero-shot models such as VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib3\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">3</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, can now generate speech that is almost indistinguishable from real human voices. While these innovations have spurred impressive progress in accessibility and conversational AI, they also make it easy to fabricate convincing audio deepfakes. Deepfakes across all media erode information integrity and public trust </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib4\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">4</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In the audio domain, they further enable voice phishing and large-scale misinformation, prompting a surge of research on spoof-speech detection and counter-measures. The anti-spoofing literature, however, remains overwhelmingly English-centric. Successive ASVspoof challenges have driven rapid gains in English detection pipelines and datasets </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yet Arabic, spanning many dialects and phonological patterns, has received far less attention. Existing Arabic resources are restricted to single speakers or narrow domains </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib8\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">8</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and recent surveys highlight persistent gaps in Arabic TTS quality and evaluation tools </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib9\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">9</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As generative models improve, this shortage leaves Arabic-speaking users particularly vulnerable to audio manipulation. To help close this gap, we introduce the first </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">multi-dialect</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Arabic spoof-speech corpus. Starting from the Casablanca speech dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib10\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">10</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we synthesize utterances with four Arabic-trained TTS models, XTTS-v2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib11\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">11</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, FishSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib12\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">12</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ArTST </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib13\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">13</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and VITS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib14\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">14</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, whose disparate training scales yield a realistic range of audio quality and spoof difficulty. We then benchmark two detection paradigms: (i) conventional machine-learning classifiers trained on MFCC features, and (ii) embedding-based approaches that exploit frozen HuBERT </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib15\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">15</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Whisper </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib16\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">16</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> encoders, and wav2vec </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib17\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">17</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> , then we compared our results with RawNet2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib18\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">18</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> which is an ASVSpoof benchmark system.\nAdditionally, ASR-based evaluation using Whisper-Large </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib16\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">16</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nprovides an extra measure of perceptual and linguistic similarity by quantifying recognition errors.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "across",
                    "embeddingbased",
                    "xttsv2",
                    "models",
                    "fishspeech",
                    "whisperlarge",
                    "extra",
                    "benchmark",
                    "rawnet2",
                    "vits",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in text-to-speech (TTS) have dramatically improved synthetic voice quality, making voice cloning more accessible and harder to detect. Early models like WaveNet </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib1\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">1</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Tacotron </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib2\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">2</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> generated natural speech but required speaker-specific training. This changed with zero-shot systems like VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib3\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">3</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which treat TTS as language modeling over codec tokens, cloning voices from seconds of audio. VALL-E R improves efficiency via monotonic alignment. Voicebox </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib19\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">19</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> advances further with non-autoregressive, real-time, cross-lingual voice cloning. YourTTS shows that high-quality cloning is possible with limited data. Most recently, FishSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib12\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">12</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained on 720K hours of multilingual speech (20K in Arabic), uses an LLM front-end and dual decoder to produce highly natural speech, especially in low-resource settings like Arabic. As synthetic speech realism grows, detecting deepfakes has become urgent. ASVspoof </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib5\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">5</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> offers benchmarks using Equal Error Rate (EER), but generalization is weak: performance drops sharply on more natural examples like those in In-the-Wild </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib20\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">20</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Partially spoofed speech poses further challenges. The PartialSpoof dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib21\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">21</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> addresses this, but most work still targets fully synthetic audio. Arabic deepfake research remains limited. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib22\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">22</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> used a small MSA corpus with a single speaker; </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib23\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">23</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focused on Egyptian Arabic using one-second clips. More recently, </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib24\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">24</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced the Tell me Habibi dataset, a large-scale Arabic-English audio-visual deepfake resource featuring intra-utterance code-switching, dialectal variation, and monolingual Arabic speech. MLAAD </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib25\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">25</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> includes Arabic but lacks dialect diversity and long-form speech. In parallel, unified spoofing detection approaches such as </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib26\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">26</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> propose spectra-temporal frame-to-utterance convergence methods that improve robustness across diverse spoofing attacks. Our work addresses these gaps with the first multi-dialect Arabic spoof benchmark using the Casablanca corpus </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib10\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">10</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, covering eight dialects (&#160;6 hours each) and spoofed versions generated by FishSpeech, XTTS-v2,and ArTST. This creates a graded-difficulty, realistic benchmark combining consistent transcriptions, high-fidelity synthesis, and broad dialectal and speaker variability, offering a stronger foundation for robust Arabic deepfake detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "equal",
                    "error",
                    "rate",
                    "models",
                    "fishspeech",
                    "benchmark",
                    "eer",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To construct our dataset, we start by utilizing the Casablanca corpus, a publicly available resource containing multi-dialect Arabic speech. Casablanca includes utterances from eight dialectal regions, each with roughly six hours of recordings. The diversity in Arabic dialect coverage makes it a strong foundation for training spoof detection models that need to generalize across a wide range of regional speech patterns.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For each utterance, we generate spoofed versions using four advanced voice cloning and TTS systems: XTTS-v2, VITS, ArTST, and FishSpeech. XTTS-v2 is a multilingual extension of YourTTS that is capable of zero-shot cloning using a multilingual tokenizer. VITS is a foundational architecture that merges a VAE with a GAN to produce highly expressive speech directly from text. It is recognized for its ability to model the one-to-many relationship between text and audio, enabling diverse prosody and intonation. ArTST is an Arabic-specific TTS model built upon SpeechT5 and pre-trained on Arabic dataset then fine-tuned to address two tasks (ASR and TTS). It serves as a strong baseline for Arabic-specific performance. FishSpeech, on the other hand, is a significantly larger and more sophisticated model trained on 720,000 hours of multilingual speech, including about 20,000 hours in Arabic. It features a two-stage architecture that combines a text-to-semantic transformer frontend with a GAN-based VQ decoder, resulting in highly expressive and natural-sounding audio that closely preserves speaker identity and prosody.</span>\n</p>\n\n",
                "matched_terms": [
                    "xttsv2",
                    "fishspeech",
                    "model",
                    "vits",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To synthesize speech using FishSpeech,\nwe processes the original utterance, generates codec tokens using the text-to-semantic model, and then synthesizes the final audio with the VQ decoder. XTTS generates audio through three main components: a GPT-2 encoder predicts a sequence of audio codes from the input text and a speaker embedding, which is derived from a short audio clip to enable voice cloning. A conditioning encoder processes the speaker audio to capture its unique characteristics, and a HiFi-GAN vocoder converts the predicted audio codes into a high-quality waveform.\nArTST TTS was trained on a dialectal Arabic dataset. For both TTS systems, ArTST and VITS, we provided the transcripts and generated the corresponding audio.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "fishspeech",
                    "vits",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the first stage, we constructed balanced datasets by pairing bonafide and spoofed samples. Each dataset contained approximately 13,615 bonafide samples and about 13,536 spoofed samples generated by a single TTS system. For each TTS model, a separate classifier was trained using the same bonafide data combined with spoofed data from one system. The combined dataset was split into 80% for training, 10% for validation, and 10% for testing. As discussed in Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS4.SSS1\" style=\"font-size:90%;\" title=\"4.4.1 Classifiers &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS4.SSS2\" style=\"font-size:90%;\" title=\"4.4.2 ASR evaluation &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, results showed that audio generated by Fish-Speech was harder to separate from bonafide data, posing greater challenges compared to the other TTS systems. This raised the question: Does constructing a benchmark from a single TTS model limit generalizability?</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "model",
                    "fishspeech",
                    "benchmark"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the constructed dataset, we frame the detection of spoofed audio as a binary classification task. All models are trained to distinguish between genuine (bonafide) and cloned (spoofed) utterances. We experimented with two families of models: traditional machine learning classifiers based on MFCC features, and modern embedding-based classifiers using self-supervised audio representations. For the traditional machine learning approach, we extracted 40-dimensional Mel-frequency cepstral coefficients (MFCCs) from each 16 kHz waveform. These features were mean-pooled across frames to produce fixed-length vectors, which were then </span>\n  <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">z</mi>\n      <annotation encoding=\"application/x-tex\">z</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-normalised before classification. We trained standard machine-learning classifiers (linear, tree-based, ensemble, distance-based and probabilistic), tuning all hyperparameters to minimize the equal-error rate (EER) and specifically optimized the SVM&#8217;s regularization constant and kernel bandwidth for robust generalization. In contrast to MFCC-based models, we also explored classifiers built on top of fixed embeddings from pretrained self-supervised models as presented in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS2\" style=\"font-size:90%;\" title=\"4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "embeddingbased",
                    "mfccbased",
                    "rate",
                    "models",
                    "traditional",
                    "eer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated the models\non the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ArFake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset constructed in Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.SS1.SSS4\" style=\"font-size:90%;\" title=\"3.1.4 Initial Benchmark Setup &#8227; 3.1 Stage 1: Building the ArFake Corpus &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1.4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.SS1.SSS5\" style=\"font-size:90%;\" title=\"3.1.5 Toward a More Generalizable Dataset &#8227; 3.1 Stage 1: Building the ArFake Corpus &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1.5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As detailed in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Stage 2: Fine-Tuning Spoof Detectors &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our experiments included two model families: traditional machine learning classifiers trained on MFCC features, and neural classifiers built on top of frozen embeddings from HuBERT, Whisper encoders, and wav2vec2.\nTraining the classifiers and inference of the TTS models were done on a single Nvidia RTX 4090 GPU with 24 GB of memory.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "traditional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For our baseline models, we used Whisper-small, Whisper-large, wav2vec2, and HuBERT-base speech encoders due to their vast popularity in the speech domain. We applied two identical feed-forward sublayers, each comprising a fully connected layer followed by a ReLU activation function and a dropout layer. This feed-forward block is repeated twice. After the feed-forward modules, the output is passed to a final classification layer that maps the learned features to the desired output classes.\nWe trained the models using Adam optimizer with learning rate </span>\n  <math alttext=\"1\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">3</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and dropout 0.5.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "models",
                    "whisperlarge",
                    "whispersmall",
                    "hubertbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated model performance using Accuracy and Equal Error Rate (EER) evaluation metrics. The lower EER values indicate more reliable detectors and thus receive greater focus in our analysis. To assess the difficulty of the dataset across different TTS models, we used synthesized audio from each model and applied Whisper Large as an ASR system, reporting the Word Error Rate (WER). Additionally, we asked 12 native Arabic speakers to rate the audio samples on a scale from 1 to 5 based on how realistic they sounded, and we calculated the Mean Opinion Score (MOS). For this evaluation, we randomly selected 8 audio samples from each TTS model, choosing one sample per dialect.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "across",
                    "equal",
                    "error",
                    "rate",
                    "accuracy",
                    "models",
                    "eer",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further assess the realism of the generated audio, we evaluated spoofed samples using the Whisper-Large model in an ASR setting. For this, the original bonafide data was compared with its ground-truth transcription to establish a baseline weighted word error rate (WER). The spoofed data generated by each TTS model was then aligned against the same original transcription, ensuring a consistent reference for comparison. We report WER, average utterance duration, and perceptual quality. As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.4.2 ASR evaluation &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Fish-Speech achieved an average WER of 97.26%, which is very close to the original data at 94.40%, while XTTS-v2 and ArTST reached much lower scores of 62.61% and 65.58%, respectively, and VITS obtained 110.90%, indicating greater ASR difficulty. When examining the average utterance duration, Fish-Speech (4.24 s) closely matched the original data (4.13 s), similar to VITS (4.19 s), whereas XTTS-v2 (6.65 s) and ArTST (5.59 s) produced noticeably longer utterances. However, despite its temporal similarity, VITS was rated poorly in terms of perceptual quality, with a mean opinion score (MOS) of only 1.7 As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, making it easier to detect as spoofed speech. Taken together, these results demonstrate that Fish-Speech not only produces ASR outputs that are harder to distinguish from genuine data but also maintains temporal and perceptual consistency, making its spoofed samples substantially more challenging to detect compared to other TTS systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "error",
                    "xttsv2",
                    "rate",
                    "fishspeech",
                    "comparison",
                    "whisperlarge",
                    "model",
                    "vits",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduced the first multi-dialect Arabic spoof-speech corpus generated by three VC and TTS systems. Through a comprehensive evaluation pipeline combining classifiers, Mean Opinion Score (MOS), and ASR performance, we observed that Fish-Speech samples were the most challenging to detect, outperforming the other TTS models in realism. To improve generalizability, we further constructed a combined benchmark dataset guided by the evaluation findings. Moreover, we proposed baseline classifiers that not only performed strongly on the in-distribution test sets but also generalized effectively to unseen audio. These findings highlight the importance of resources that encompass multiple dialects and diverse TTS systems, as well as adaptive detectors capable of keeping pace with synthesis advances. Our corpus provides a solid foundation for developing more robust Arabic anti-spoofing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "fishspeech",
                    "benchmark"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection",
        "caption": "Table 2: Mean Opinion Scores (MOS) on a 1-5 scale for four TTS models evaluated across eight Arabic dialects, identified by their ISO 3166-1 alpha-2 country codes.",
        "body": "TTS\nDZ\n\n\nEG\n\n\n\n\nJO\n\n\n\n\nMA\n\n\n\n\nMR\n\n\n\n\nPS\n\n\n\n\nAE\n\n\n\n\nYE\n\n\n\n\nAVG\n\n\n\n\n\n\nFish-Speech\n3.08\n\n\n4.00\n\n\n\n\n3.83\n\n\n\n\n3.58\n\n\n\n\n4.33\n\n\n\n\n3.92\n\n\n\n\n2.92\n\n\n\n\n4.08\n\n\n\n\n3.72\n\n\n\n\nXTTS-v2\n3.00\n\n\n2.42\n\n\n\n\n3.08\n\n\n\n\n2.75\n\n\n\n\n2.58\n\n\n\n\n3.33\n\n\n\n\n3.00\n\n\n\n\n3.75\n\n\n\n\n2.99\n\n\n\n\nArTST\n1.75\n\n\n1.42\n\n\n\n\n1.83\n\n\n\n\n1.83\n\n\n\n\n2.25\n\n\n\n\n2.25\n\n\n\n\n2.08\n\n\n\n\n2.00\n\n\n\n\n1.93\n\n\n\n\nVITS\n2.17\n\n\n1.08\n\n\n\n\n1.33\n\n\n\n\n2.42\n\n\n\n\n1.92\n\n\n\n\n1.33\n\n\n\n\n1.50\n\n\n\n\n1.83\n\n\n\n\n1.70",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">TTS</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DZ</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EG</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">JO</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MA</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MR</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">PS</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AE</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">YE</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AVG</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Fish-Speech</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.08</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.58</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.33</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.92</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.92</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.08</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.72</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">XTTS-v2</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.00</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.42</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.08</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.75</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.58</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.33</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.75</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.99</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">ArTST</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.75</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.42</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.25</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.25</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.08</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.00</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.93</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">VITS</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.17</span></th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.08</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.33</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.42</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.92</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.33</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.50</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.83</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:14.2pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.70</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "avg",
            "dialects",
            "opinion",
            "codes",
            "xttsv2",
            "country",
            "fishspeech",
            "arabic",
            "eight",
            "scores",
            "vits",
            "artst",
            "iso",
            "alpha2",
            "four",
            "evaluated",
            "their",
            "identified",
            "mos",
            "scale",
            "across",
            "models",
            "tts",
            "mean"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes the Equal Error Rate (EER) and accuracy results for both classical classifiers and encoder-based models, evaluated on data generated by four different TTS models. The final TTS models&#8217; percentage used for generating the synthetic dataset was selected based on a combination of classifier performance, Mean Opinion Score (MOS), and ASR results. Initially, datasets were generated using each TTS model, and the one that proved most challenging for classifiers to distinguish between bonafide and spoofed samples will be chosen either to generate the dataset or to have a high percentage while generating the final dataset. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the dataset generated by FishSpeech was the most difficult for classifiers, followed by XTTS-v2, ArTST, and VITS. As expected, embedding-based models outperformed traditional machine learning models, particularly on datasets generated by FishSpeech and XTTS-v2. Notably, Whisper-large achieved a strong performance with an EER of 6.92%, outperforming other models and even surpassing the ASVSpoof benchmark by nearly 7%. HuBERT-base also demonstrated strong results on XTTS-v2, achieving an EER of just 0.07%.\nFor the dataset generated by ArTST and VITS, SVM achieved perfect performance with 100% accuracy, with Extra Trees performing similarly and achieving 99.96% accuracy on the VITS-generated dataset. It is worth noting that the embedding-based models on ArTST and VITS produced fair results, considering they were trained for only one epoch due to the relative ease of the spoofed datasets. Classifiers were not the only criterion used to determine the most effective TTS model for generating our dataset. We also calculated the Mean Opinion Score (MOS), as shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The results further support our findings, with FishSpeech achieving the highest overall MOS of 3.72 out of 5, followed by XTTS-v2. FishSpeech outperformed the other models across all dialects except for AE, where it ranked second to XTTS-v2 by a margin of just 0.08.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further assess the realism of the generated audio, we evaluated spoofed samples using the Whisper-Large model in an ASR setting. For this, the original bonafide data was compared with its ground-truth transcription to establish a baseline weighted word error rate (WER). The spoofed data generated by each TTS model was then aligned against the same original transcription, ensuring a consistent reference for comparison. We report WER, average utterance duration, and perceptual quality. As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.4.2 ASR evaluation &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Fish-Speech achieved an average WER of 97.26%, which is very close to the original data at 94.40%, while XTTS-v2 and ArTST reached much lower scores of 62.61% and 65.58%, respectively, and VITS obtained 110.90%, indicating greater ASR difficulty. When examining the average utterance duration, Fish-Speech (4.24 s) closely matched the original data (4.13 s), similar to VITS (4.19 s), whereas XTTS-v2 (6.65 s) and ArTST (5.59 s) produced noticeably longer utterances. However, despite its temporal similarity, VITS was rated poorly in terms of perceptual quality, with a mean opinion score (MOS) of only 1.7 As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, making it easier to detect as spoofed speech. Taken together, these results demonstrate that Fish-Speech not only produces ASR outputs that are harder to distinguish from genuine data but also maintains temporal and perceptual consistency, making its spoofed samples substantially more challenging to detect compared to other TTS systems.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the rise of generative text-to-speech models, distinguishing between real and synthetic speech has become challenging, especially for Arabic that have received limited research attention. Most spoof detection efforts have focused on English, leaving a significant gap for Arabic and its many dialects. In this work, we introduce the first multi-dialect Arabic spoofed speech dataset. To evaluate the difficulty of the synthesized audio from each model and determine which produces the most challenging samples, we aimed to guide the construction of our final dataset either by merging audios from multiple models or by selecting the best-performing model, we conducted an evaluation pipeline that included training classifiers using two approaches: modern embedding-based methods combined with classifier heads; classical machine learning algorithms applied to MFCC features; and the RawNet2 architecture. The pipeline further incorporated the calculation of Mean Opinion Score based on human ratings, as well as processing both original and synthesized datasets through an Automatic Speech Recognition model to measure the Word Error Rate. Our results demonstrate that FishSpeech outperforms other TTS models in Arabic voice cloning on the Casablanca corpus, producing more realistic and challenging synthetic speech samples. However, relying on a single TTS for dataset creation may limit generalizability.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text\" style=\"font-size:111%;\">1</span></span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/Mohammed01/ArFake\" style=\"font-size:111%;\" title=\"\">ArFake dataset on Hugging Face</a></span></span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "opinion",
                    "models",
                    "fishspeech",
                    "tts",
                    "arabic",
                    "mean",
                    "dialects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Neural text-to-speech (TTS) and voice&#8211;cloning systems, starting with WaveNet </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib1\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">1</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Tacotron </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib2\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">2</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and advancing to zero-shot models such as VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib3\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">3</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, can now generate speech that is almost indistinguishable from real human voices. While these innovations have spurred impressive progress in accessibility and conversational AI, they also make it easy to fabricate convincing audio deepfakes. Deepfakes across all media erode information integrity and public trust </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib4\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">4</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In the audio domain, they further enable voice phishing and large-scale misinformation, prompting a surge of research on spoof-speech detection and counter-measures. The anti-spoofing literature, however, remains overwhelmingly English-centric. Successive ASVspoof challenges have driven rapid gains in English detection pipelines and datasets </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yet Arabic, spanning many dialects and phonological patterns, has received far less attention. Existing Arabic resources are restricted to single speakers or narrow domains </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib8\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">8</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and recent surveys highlight persistent gaps in Arabic TTS quality and evaluation tools </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib9\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">9</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As generative models improve, this shortage leaves Arabic-speaking users particularly vulnerable to audio manipulation. To help close this gap, we introduce the first </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">multi-dialect</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Arabic spoof-speech corpus. Starting from the Casablanca speech dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib10\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">10</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we synthesize utterances with four Arabic-trained TTS models, XTTS-v2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib11\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">11</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, FishSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib12\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">12</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ArTST </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib13\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">13</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and VITS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib14\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">14</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, whose disparate training scales yield a realistic range of audio quality and spoof difficulty. We then benchmark two detection paradigms: (i) conventional machine-learning classifiers trained on MFCC features, and (ii) embedding-based approaches that exploit frozen HuBERT </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib15\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">15</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Whisper </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib16\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">16</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> encoders, and wav2vec </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib17\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">17</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> , then we compared our results with RawNet2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib18\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">18</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> which is an ASVSpoof benchmark system.\nAdditionally, ASR-based evaluation using Whisper-Large </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib16\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">16</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nprovides an extra measure of perceptual and linguistic similarity by quantifying recognition errors.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "xttsv2",
                    "four",
                    "models",
                    "fishspeech",
                    "tts",
                    "vits",
                    "arabic",
                    "dialects",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in text-to-speech (TTS) have dramatically improved synthetic voice quality, making voice cloning more accessible and harder to detect. Early models like WaveNet </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib1\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">1</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Tacotron </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib2\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">2</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> generated natural speech but required speaker-specific training. This changed with zero-shot systems like VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib3\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">3</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which treat TTS as language modeling over codec tokens, cloning voices from seconds of audio. VALL-E R improves efficiency via monotonic alignment. Voicebox </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib19\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">19</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> advances further with non-autoregressive, real-time, cross-lingual voice cloning. YourTTS shows that high-quality cloning is possible with limited data. Most recently, FishSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib12\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">12</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained on 720K hours of multilingual speech (20K in Arabic), uses an LLM front-end and dual decoder to produce highly natural speech, especially in low-resource settings like Arabic. As synthetic speech realism grows, detecting deepfakes has become urgent. ASVspoof </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib5\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">5</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> offers benchmarks using Equal Error Rate (EER), but generalization is weak: performance drops sharply on more natural examples like those in In-the-Wild </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib20\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">20</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Partially spoofed speech poses further challenges. The PartialSpoof dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib21\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">21</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> addresses this, but most work still targets fully synthetic audio. Arabic deepfake research remains limited. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib22\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">22</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> used a small MSA corpus with a single speaker; </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib23\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">23</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focused on Egyptian Arabic using one-second clips. More recently, </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib24\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">24</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced the Tell me Habibi dataset, a large-scale Arabic-English audio-visual deepfake resource featuring intra-utterance code-switching, dialectal variation, and monolingual Arabic speech. MLAAD </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib25\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">25</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> includes Arabic but lacks dialect diversity and long-form speech. In parallel, unified spoofing detection approaches such as </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib26\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">26</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> propose spectra-temporal frame-to-utterance convergence methods that improve robustness across diverse spoofing attacks. Our work addresses these gaps with the first multi-dialect Arabic spoof benchmark using the Casablanca corpus </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib10\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">10</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, covering eight dialects (&#160;6 hours each) and spoofed versions generated by FishSpeech, XTTS-v2,and ArTST. This creates a graded-difficulty, realistic benchmark combining consistent transcriptions, high-fidelity synthesis, and broad dialectal and speaker variability, offering a stronger foundation for robust Arabic deepfake detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "models",
                    "fishspeech",
                    "tts",
                    "arabic",
                    "eight",
                    "dialects",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our pipeline comprises three main stages: (1) constructing a dialect-balanced corpus containing spoofed and bonafide speech. (2) training binary classification models , and assessing spoofed outputs under an ASR evaluation to measure perceptual consistency. (3) Given the results received from stage 2, we decided that merging the dataset from different VC and TTS will be better for generalizability.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To construct our dataset, we start by utilizing the Casablanca corpus, a publicly available resource containing multi-dialect Arabic speech. Casablanca includes utterances from eight dialectal regions, each with roughly six hours of recordings. The diversity in Arabic dialect coverage makes it a strong foundation for training spoof detection models that need to generalize across a wide range of regional speech patterns.</span>\n</p>\n\n",
                "matched_terms": [
                    "arabic",
                    "models",
                    "eight",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For each utterance, we generate spoofed versions using four advanced voice cloning and TTS systems: XTTS-v2, VITS, ArTST, and FishSpeech. XTTS-v2 is a multilingual extension of YourTTS that is capable of zero-shot cloning using a multilingual tokenizer. VITS is a foundational architecture that merges a VAE with a GAN to produce highly expressive speech directly from text. It is recognized for its ability to model the one-to-many relationship between text and audio, enabling diverse prosody and intonation. ArTST is an Arabic-specific TTS model built upon SpeechT5 and pre-trained on Arabic dataset then fine-tuned to address two tasks (ASR and TTS). It serves as a strong baseline for Arabic-specific performance. FishSpeech, on the other hand, is a significantly larger and more sophisticated model trained on 720,000 hours of multilingual speech, including about 20,000 hours in Arabic. It features a two-stage architecture that combines a text-to-semantic transformer frontend with a GAN-based VQ decoder, resulting in highly expressive and natural-sounding audio that closely preserves speaker identity and prosody.</span>\n</p>\n\n",
                "matched_terms": [
                    "xttsv2",
                    "four",
                    "fishspeech",
                    "tts",
                    "arabic",
                    "vits",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To synthesize speech using FishSpeech,\nwe processes the original utterance, generates codec tokens using the text-to-semantic model, and then synthesizes the final audio with the VQ decoder. XTTS generates audio through three main components: a GPT-2 encoder predicts a sequence of audio codes from the input text and a speaker embedding, which is derived from a short audio clip to enable voice cloning. A conditioning encoder processes the speaker audio to capture its unique characteristics, and a HiFi-GAN vocoder converts the predicted audio codes into a high-quality waveform.\nArTST TTS was trained on a dialectal Arabic dataset. For both TTS systems, ArTST and VITS, we provided the transcripts and generated the corresponding audio.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "codes",
                    "fishspeech",
                    "tts",
                    "arabic",
                    "vits",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the first stage, we constructed balanced datasets by pairing bonafide and spoofed samples. Each dataset contained approximately 13,615 bonafide samples and about 13,536 spoofed samples generated by a single TTS system. For each TTS model, a separate classifier was trained using the same bonafide data combined with spoofed data from one system. The combined dataset was split into 80% for training, 10% for validation, and 10% for testing. As discussed in Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS4.SSS1\" style=\"font-size:90%;\" title=\"4.4.1 Classifiers &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS4.SSS2\" style=\"font-size:90%;\" title=\"4.4.2 ASR evaluation &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, results showed that audio generated by Fish-Speech was harder to separate from bonafide data, posing greater challenges compared to the other TTS systems. This raised the question: Does constructing a benchmark from a single TTS model limit generalizability?</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "fishspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this, we designed a broader dataset by combining spoofed audio from Fish-Speech, XTTS-v2, and ArTST while excluding VITS. The training set included 70% of bonafide samples together with spoofed samples at ratios of 70%, 50%, and 40% for Fish-Speech, XTTS-v2, and ArTST, respectively. Validation used 10% of the training set, and the test set consisted of 30%, 30%, 50%, and 60% from bonafide, Fish-Speech, XTTS-v2, and ArTST, respectively.\nThis produced dataset with a size of 54413 utterances that is splitted as 31302 utterances in the training set and 23111 in the test set, as shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 3.1.5 Toward a More Generalizable Dataset &#8227; 3.1 Stage 1: Building the ArFake Corpus &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We then retrained the best-performing models from Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on this dataset and evaluated them against our test set and VITS-generated audios. As summarized in Table &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4.3 Constructing the Final Dataset &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, even without exposure to VITS during training, all models demonstrated strong performance on its synthesized samples.</span>\n</p>\n\n",
                "matched_terms": [
                    "xttsv2",
                    "models",
                    "evaluated",
                    "fishspeech",
                    "vits",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the constructed dataset, we frame the detection of spoofed audio as a binary classification task. All models are trained to distinguish between genuine (bonafide) and cloned (spoofed) utterances. We experimented with two families of models: traditional machine learning classifiers based on MFCC features, and modern embedding-based classifiers using self-supervised audio representations. For the traditional machine learning approach, we extracted 40-dimensional Mel-frequency cepstral coefficients (MFCCs) from each 16 kHz waveform. These features were mean-pooled across frames to produce fixed-length vectors, which were then </span>\n  <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">z</mi>\n      <annotation encoding=\"application/x-tex\">z</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-normalised before classification. We trained standard machine-learning classifiers (linear, tree-based, ensemble, distance-based and probabilistic), tuning all hyperparameters to minimize the equal-error rate (EER) and specifically optimized the SVM&#8217;s regularization constant and kernel bandwidth for robust generalization. In contrast to MFCC-based models, we also explored classifiers built on top of fixed embeddings from pretrained self-supervised models as presented in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS2\" style=\"font-size:90%;\" title=\"4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated the models\non the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ArFake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset constructed in Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.SS1.SSS4\" style=\"font-size:90%;\" title=\"3.1.4 Initial Benchmark Setup &#8227; 3.1 Stage 1: Building the ArFake Corpus &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1.4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.SS1.SSS5\" style=\"font-size:90%;\" title=\"3.1.5 Toward a More Generalizable Dataset &#8227; 3.1 Stage 1: Building the ArFake Corpus &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1.5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As detailed in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Stage 2: Fine-Tuning Spoof Detectors &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our experiments included two model families: traditional machine learning classifiers trained on MFCC features, and neural classifiers built on top of frozen embeddings from HuBERT, Whisper encoders, and wav2vec2.\nTraining the classifiers and inference of the TTS models were done on a single Nvidia RTX 4090 GPU with 24 GB of memory.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluated",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For our baseline models, we used Whisper-small, Whisper-large, wav2vec2, and HuBERT-base speech encoders due to their vast popularity in the speech domain. We applied two identical feed-forward sublayers, each comprising a fully connected layer followed by a ReLU activation function and a dropout layer. This feed-forward block is repeated twice. After the feed-forward modules, the output is passed to a final classification layer that maps the learned features to the desired output classes.\nWe trained the models using Adam optimizer with learning rate </span>\n  <math alttext=\"1\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">3</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and dropout 0.5.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated model performance using Accuracy and Equal Error Rate (EER) evaluation metrics. The lower EER values indicate more reliable detectors and thus receive greater focus in our analysis. To assess the difficulty of the dataset across different TTS models, we used synthesized audio from each model and applied Whisper Large as an ASR system, reporting the Word Error Rate (WER). Additionally, we asked 12 native Arabic speakers to rate the audio samples on a scale from 1 to 5 based on how realistic they sounded, and we calculated the Mean Opinion Score (MOS). For this evaluation, we randomly selected 8 audio samples from each TTS model, choosing one sample per dialect.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "opinion",
                    "models",
                    "evaluated",
                    "tts",
                    "arabic",
                    "mos",
                    "scale",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The results from Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS4.SSS1\" style=\"font-size:90%;\" title=\"4.4.1 Classifiers &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS4.SSS2\" style=\"font-size:90%;\" title=\"4.4.2 ASR evaluation &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicate that the spoofed dataset generated by FishSpeech is more realistic than those from other models. This raises an important question: should our benchmark dataset be generated using only one voice cloning (VC) system?\nFrom Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we evaluated Whisper-Large, trained on the FishSpeech dataset, on an unseen spoofed dataset generated by VITS and achieved an accuracy of 70.91%. This prompted us to consider making our dataset more generalizable, as described in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.SS1.SSS5\" style=\"font-size:90%;\" title=\"3.1.5 Toward a More Generalizable Dataset &#8227; 3.1 Stage 1: Building the ArFake Corpus &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1.5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. After combining datasets from multiple TTS models, we trained the best classifiers identified in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and tested them on both our test set and the spoofed audios generated by the VITS system.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4.3 Constructing the Final Dataset &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, although none of the models were trained on VITS-generated audios, they still performed well on this synthesized data. Note that EER is not defined for the VITS dataset here, as it contains only spoofed samples (one class). According to Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4.3 Constructing the Final Dataset &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Whisper-large outperforms other models by achieving a 4.88% EER on the ArFake test set. While Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the best EER on the FishSpeech dataset is 6.92%, this does not necessarily mean that FishSpeech-generated data is more difficult. The ArFake test set contains nearly 23,000 samples compared to only 2,637 in the FishSpeech dataset. Additionally, models in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4.3 Constructing the Final Dataset &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> were trained on a combination of spoofed datasets, whereas those in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> were trained on data from a single system. Finally, Whisper-small demonstrates strong performance on VITS-generated spoofed data with an accuracy of 98.30%, closely followed by Whisper-large at 97.94%. As a result combining the dataset from different systems made the the classifiers more generalizable</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluated",
                    "fishspeech",
                    "tts",
                    "identified",
                    "mean",
                    "vits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduced the first multi-dialect Arabic spoof-speech corpus generated by three VC and TTS systems. Through a comprehensive evaluation pipeline combining classifiers, Mean Opinion Score (MOS), and ASR performance, we observed that Fish-Speech samples were the most challenging to detect, outperforming the other TTS models in realism. To improve generalizability, we further constructed a combined benchmark dataset guided by the evaluation findings. Moreover, we proposed baseline classifiers that not only performed strongly on the in-distribution test sets but also generalized effectively to unseen audio. These findings highlight the importance of resources that encompass multiple dialects and diverse TTS systems, as well as adaptive detectors capable of keeping pace with synthesis advances. Our corpus provides a solid foundation for developing more robust Arabic anti-spoofing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "opinion",
                    "models",
                    "fishspeech",
                    "tts",
                    "arabic",
                    "mos",
                    "mean",
                    "dialects"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection",
        "caption": "Table 3: Comparison of Equal Error Rate (EER) and Accuracy on the ArFake test set—constructed from FishSpeech, XTTS-v2, and ArTST generated data—against samples generated by VITS (not seen in the training). All models were trained on the ArFake training set, which combines data from FishSpeech, XTTS-v2, and ArTST",
        "body": "Model\nArFake\nVITS\n\n\nEER (%)↓\\downarrow\nACC (%)↑\\uparrow\nEER (%)↓\\downarrow\nACC (%)↑\\uparrow\n\n\nEmbedding-Based Models\n\n\nHuBERT-base\n7.96\n96.11\n-\n94.02\n\n\nWhisper-small\n5.42\n96.56\n-\n98.30\n\n\nWhisper-large\n4.88\n96.86\n-\n97.94\n\n\nTraditional ML Models (MFCC-Based)\n\n\nSVM\n10.76\n92.08\n-\n73.85",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ArFake</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">VITS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">EER (%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ACC (%)<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">EER (%)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ACC (%)<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:70%;\">Embedding-Based Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">HuBERT-base</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">7.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">96.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">94.02</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Whisper-small</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">5.42</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">96.56</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">98.30</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Whisper-large</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">96.86</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">97.94</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"5\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:70%;\">Traditional ML Models (MFCC-Based)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">SVM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">10.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">92.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">-</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:-2.4pt;padding-bottom:-2.4pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">73.85</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "data",
            "set—constructed",
            "seen",
            "↑uparrow",
            "samples",
            "equal",
            "error",
            "xttsv2",
            "rate",
            "accuracy",
            "fishspeech",
            "which",
            "traditional",
            "whispersmall",
            "vits",
            "artst",
            "hubertbase",
            "data—against",
            "embeddingbased",
            "↓downarrow",
            "mfccbased",
            "comparison",
            "trained",
            "acc",
            "training",
            "from",
            "eer",
            "svm",
            "test",
            "all",
            "generated",
            "models",
            "set",
            "whisperlarge",
            "arfake",
            "combines",
            "model",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address this, we designed a broader dataset by combining spoofed audio from Fish-Speech, XTTS-v2, and ArTST while excluding VITS. The training set included 70% of bonafide samples together with spoofed samples at ratios of 70%, 50%, and 40% for Fish-Speech, XTTS-v2, and ArTST, respectively. Validation used 10% of the training set, and the test set consisted of 30%, 30%, 50%, and 60% from bonafide, Fish-Speech, XTTS-v2, and ArTST, respectively.\nThis produced dataset with a size of 54413 utterances that is splitted as 31302 utterances in the training set and 23111 in the test set, as shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 3.1.5 Toward a More Generalizable Dataset &#8227; 3.1 Stage 1: Building the ArFake Corpus &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We then retrained the best-performing models from Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on this dataset and evaluated them against our test set and VITS-generated audios. As summarized in Table &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4.3 Constructing the Final Dataset &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, even without exposure to VITS during training, all models demonstrated strong performance on its synthesized samples.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The results from Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS4.SSS1\" style=\"font-size:90%;\" title=\"4.4.1 Classifiers &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS4.SSS2\" style=\"font-size:90%;\" title=\"4.4.2 ASR evaluation &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicate that the spoofed dataset generated by FishSpeech is more realistic than those from other models. This raises an important question: should our benchmark dataset be generated using only one voice cloning (VC) system?\nFrom Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we evaluated Whisper-Large, trained on the FishSpeech dataset, on an unseen spoofed dataset generated by VITS and achieved an accuracy of 70.91%. This prompted us to consider making our dataset more generalizable, as described in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.SS1.SSS5\" style=\"font-size:90%;\" title=\"3.1.5 Toward a More Generalizable Dataset &#8227; 3.1 Stage 1: Building the ArFake Corpus &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1.5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. After combining datasets from multiple TTS models, we trained the best classifiers identified in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and tested them on both our test set and the spoofed audios generated by the VITS system.\nAs shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4.3 Constructing the Final Dataset &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, although none of the models were trained on VITS-generated audios, they still performed well on this synthesized data. Note that EER is not defined for the VITS dataset here, as it contains only spoofed samples (one class). According to Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4.3 Constructing the Final Dataset &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Whisper-large outperforms other models by achieving a 4.88% EER on the ArFake test set. While Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows the best EER on the FishSpeech dataset is 6.92%, this does not necessarily mean that FishSpeech-generated data is more difficult. The ArFake test set contains nearly 23,000 samples compared to only 2,637 in the FishSpeech dataset. Additionally, models in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 4.4.3 Constructing the Final Dataset &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> were trained on a combination of spoofed datasets, whereas those in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> were trained on data from a single system. Finally, Whisper-small demonstrates strong performance on VITS-generated spoofed data with an accuracy of 98.30%, closely followed by Whisper-large at 97.94%. As a result combining the dataset from different systems made the the classifiers more generalizable</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the rise of generative text-to-speech models, distinguishing between real and synthetic speech has become challenging, especially for Arabic that have received limited research attention. Most spoof detection efforts have focused on English, leaving a significant gap for Arabic and its many dialects. In this work, we introduce the first multi-dialect Arabic spoofed speech dataset. To evaluate the difficulty of the synthesized audio from each model and determine which produces the most challenging samples, we aimed to guide the construction of our final dataset either by merging audios from multiple models or by selecting the best-performing model, we conducted an evaluation pipeline that included training classifiers using two approaches: modern embedding-based methods combined with classifier heads; classical machine learning algorithms applied to MFCC features; and the RawNet2 architecture. The pipeline further incorporated the calculation of Mean Opinion Score based on human ratings, as well as processing both original and synthesized datasets through an Automatic Speech Recognition model to measure the Word Error Rate. Our results demonstrate that FishSpeech outperforms other TTS models in Arabic voice cloning on the Casablanca corpus, producing more realistic and challenging synthetic speech samples. However, relying on a single TTS for dataset creation may limit generalizability.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text\" style=\"font-size:111%;\">1</span></span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/Mohammed01/ArFake\" style=\"font-size:111%;\" title=\"\">ArFake dataset on Hugging Face</a></span></span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "samples",
                    "embeddingbased",
                    "error",
                    "rate",
                    "models",
                    "which",
                    "fishspeech",
                    "training",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Neural text-to-speech (TTS) and voice&#8211;cloning systems, starting with WaveNet </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib1\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">1</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Tacotron </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib2\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">2</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and advancing to zero-shot models such as VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib3\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">3</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, can now generate speech that is almost indistinguishable from real human voices. While these innovations have spurred impressive progress in accessibility and conversational AI, they also make it easy to fabricate convincing audio deepfakes. Deepfakes across all media erode information integrity and public trust </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib4\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">4</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In the audio domain, they further enable voice phishing and large-scale misinformation, prompting a surge of research on spoof-speech detection and counter-measures. The anti-spoofing literature, however, remains overwhelmingly English-centric. Successive ASVspoof challenges have driven rapid gains in English detection pipelines and datasets </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yet Arabic, spanning many dialects and phonological patterns, has received far less attention. Existing Arabic resources are restricted to single speakers or narrow domains </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib8\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">8</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and recent surveys highlight persistent gaps in Arabic TTS quality and evaluation tools </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib9\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">9</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As generative models improve, this shortage leaves Arabic-speaking users particularly vulnerable to audio manipulation. To help close this gap, we introduce the first </span>\n  <em class=\"ltx_emph ltx_font_italic\" style=\"font-size:90%;\">multi-dialect</em>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> Arabic spoof-speech corpus. Starting from the Casablanca speech dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib10\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">10</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we synthesize utterances with four Arabic-trained TTS models, XTTS-v2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib11\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">11</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, FishSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib12\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">12</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ArTST </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib13\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">13</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and VITS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib14\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">14</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, whose disparate training scales yield a realistic range of audio quality and spoof difficulty. We then benchmark two detection paradigms: (i) conventional machine-learning classifiers trained on MFCC features, and (ii) embedding-based approaches that exploit frozen HuBERT </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib15\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">15</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Whisper </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib16\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">16</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> encoders, and wav2vec </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib17\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">17</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> , then we compared our results with RawNet2 </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib18\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">18</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> which is an ASVSpoof benchmark system.\nAdditionally, ASR-based evaluation using Whisper-Large </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib16\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">16</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nprovides an extra measure of perceptual and linguistic similarity by quantifying recognition errors.</span>\n</p>\n\n",
                "matched_terms": [
                    "embeddingbased",
                    "all",
                    "xttsv2",
                    "models",
                    "fishspeech",
                    "which",
                    "whisperlarge",
                    "trained",
                    "training",
                    "from",
                    "vits",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Recent advances in text-to-speech (TTS) have dramatically improved synthetic voice quality, making voice cloning more accessible and harder to detect. Early models like WaveNet </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib1\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">1</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Tacotron </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib2\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">2</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> generated natural speech but required speaker-specific training. This changed with zero-shot systems like VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib3\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">3</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which treat TTS as language modeling over codec tokens, cloning voices from seconds of audio. VALL-E R improves efficiency via monotonic alignment. Voicebox </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib19\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">19</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> advances further with non-autoregressive, real-time, cross-lingual voice cloning. YourTTS shows that high-quality cloning is possible with limited data. Most recently, FishSpeech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib12\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">12</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, trained on 720K hours of multilingual speech (20K in Arabic), uses an LLM front-end and dual decoder to produce highly natural speech, especially in low-resource settings like Arabic. As synthetic speech realism grows, detecting deepfakes has become urgent. ASVspoof </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib5\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">5</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> offers benchmarks using Equal Error Rate (EER), but generalization is weak: performance drops sharply on more natural examples like those in In-the-Wild </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib20\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">20</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Partially spoofed speech poses further challenges. The PartialSpoof dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib21\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">21</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> addresses this, but most work still targets fully synthetic audio. Arabic deepfake research remains limited. </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib22\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">22</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> used a small MSA corpus with a single speaker; </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib23\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">23</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> focused on Egyptian Arabic using one-second clips. More recently, </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib24\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">24</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced the Tell me Habibi dataset, a large-scale Arabic-English audio-visual deepfake resource featuring intra-utterance code-switching, dialectal variation, and monolingual Arabic speech. MLAAD </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib25\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">25</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> includes Arabic but lacks dialect diversity and long-form speech. In parallel, unified spoofing detection approaches such as </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib26\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">26</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> propose spectra-temporal frame-to-utterance convergence methods that improve robustness across diverse spoofing attacks. Our work addresses these gaps with the first multi-dialect Arabic spoof benchmark using the Casablanca corpus </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#bib.bib10\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">10</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, covering eight dialects (&#160;6 hours each) and spoofed versions generated by FishSpeech, XTTS-v2,and ArTST. This creates a graded-difficulty, realistic benchmark combining consistent transcriptions, high-fidelity synthesis, and broad dialectal and speaker variability, offering a stronger foundation for robust Arabic deepfake detection.</span>\n</p>\n\n",
                "matched_terms": [
                    "equal",
                    "error",
                    "eer",
                    "rate",
                    "generated",
                    "models",
                    "fishspeech",
                    "data",
                    "which",
                    "trained",
                    "training",
                    "from",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our pipeline comprises three main stages: (1) constructing a dialect-balanced corpus containing spoofed and bonafide speech. (2) training binary classification models , and assessing spoofed outputs under an ASR evaluation to measure perceptual consistency. (3) Given the results received from stage 2, we decided that merging the dataset from different VC and TTS will be better for generalizability.</span>\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "models",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To construct our dataset, we start by utilizing the Casablanca corpus, a publicly available resource containing multi-dialect Arabic speech. Casablanca includes utterances from eight dialectal regions, each with roughly six hours of recordings. The diversity in Arabic dialect coverage makes it a strong foundation for training spoof detection models that need to generalize across a wide range of regional speech patterns.</span>\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "models",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For each utterance, we generate spoofed versions using four advanced voice cloning and TTS systems: XTTS-v2, VITS, ArTST, and FishSpeech. XTTS-v2 is a multilingual extension of YourTTS that is capable of zero-shot cloning using a multilingual tokenizer. VITS is a foundational architecture that merges a VAE with a GAN to produce highly expressive speech directly from text. It is recognized for its ability to model the one-to-many relationship between text and audio, enabling diverse prosody and intonation. ArTST is an Arabic-specific TTS model built upon SpeechT5 and pre-trained on Arabic dataset then fine-tuned to address two tasks (ASR and TTS). It serves as a strong baseline for Arabic-specific performance. FishSpeech, on the other hand, is a significantly larger and more sophisticated model trained on 720,000 hours of multilingual speech, including about 20,000 hours in Arabic. It features a two-stage architecture that combines a text-to-semantic transformer frontend with a GAN-based VQ decoder, resulting in highly expressive and natural-sounding audio that closely preserves speaker identity and prosody.</span>\n</p>\n\n",
                "matched_terms": [
                    "combines",
                    "xttsv2",
                    "fishspeech",
                    "trained",
                    "from",
                    "model",
                    "vits",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To synthesize speech using FishSpeech,\nwe processes the original utterance, generates codec tokens using the text-to-semantic model, and then synthesizes the final audio with the VQ decoder. XTTS generates audio through three main components: a GPT-2 encoder predicts a sequence of audio codes from the input text and a speaker embedding, which is derived from a short audio clip to enable voice cloning. A conditioning encoder processes the speaker audio to capture its unique characteristics, and a HiFi-GAN vocoder converts the predicted audio codes into a high-quality waveform.\nArTST TTS was trained on a dialectal Arabic dataset. For both TTS systems, ArTST and VITS, we provided the transcripts and generated the corresponding audio.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "generated",
                    "fishspeech",
                    "which",
                    "trained",
                    "from",
                    "model",
                    "vits",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the first stage, we constructed balanced datasets by pairing bonafide and spoofed samples. Each dataset contained approximately 13,615 bonafide samples and about 13,536 spoofed samples generated by a single TTS system. For each TTS model, a separate classifier was trained using the same bonafide data combined with spoofed data from one system. The combined dataset was split into 80% for training, 10% for validation, and 10% for testing. As discussed in Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS4.SSS1\" style=\"font-size:90%;\" title=\"4.4.1 Classifiers &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and &#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS4.SSS2\" style=\"font-size:90%;\" title=\"4.4.2 ASR evaluation &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, results showed that audio generated by Fish-Speech was harder to separate from bonafide data, posing greater challenges compared to the other TTS systems. This raised the question: Does constructing a benchmark from a single TTS model limit generalizability?</span>\n</p>\n\n",
                "matched_terms": [
                    "samples",
                    "generated",
                    "fishspeech",
                    "data",
                    "trained",
                    "training",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">With the constructed dataset, we frame the detection of spoofed audio as a binary classification task. All models are trained to distinguish between genuine (bonafide) and cloned (spoofed) utterances. We experimented with two families of models: traditional machine learning classifiers based on MFCC features, and modern embedding-based classifiers using self-supervised audio representations. For the traditional machine learning approach, we extracted 40-dimensional Mel-frequency cepstral coefficients (MFCCs) from each 16 kHz waveform. These features were mean-pooled across frames to produce fixed-length vectors, which were then </span>\n  <math alttext=\"z\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">z</mi>\n      <annotation encoding=\"application/x-tex\">z</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-normalised before classification. We trained standard machine-learning classifiers (linear, tree-based, ensemble, distance-based and probabilistic), tuning all hyperparameters to minimize the equal-error rate (EER) and specifically optimized the SVM&#8217;s regularization constant and kernel bandwidth for robust generalization. In contrast to MFCC-based models, we also explored classifiers built on top of fixed embeddings from pretrained self-supervised models as presented in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.SS2\" style=\"font-size:90%;\" title=\"4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "embeddingbased",
                    "all",
                    "mfccbased",
                    "rate",
                    "eer",
                    "models",
                    "which",
                    "traditional",
                    "trained",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated the models\non the </span>\n  <span class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ArFake</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dataset constructed in Sections&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.SS1.SSS4\" style=\"font-size:90%;\" title=\"3.1.4 Initial Benchmark Setup &#8227; 3.1 Stage 1: Building the ArFake Corpus &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1.4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.SS1.SSS5\" style=\"font-size:90%;\" title=\"3.1.5 Toward a More Generalizable Dataset &#8227; 3.1 Stage 1: Building the ArFake Corpus &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1.5</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As detailed in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Stage 2: Fine-Tuning Spoof Detectors &#8227; 3 Methodology &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, our experiments included two model families: traditional machine learning classifiers trained on MFCC features, and neural classifiers built on top of frozen embeddings from HuBERT, Whisper encoders, and wav2vec2.\nTraining the classifiers and inference of the TTS models were done on a single Nvidia RTX 4090 GPU with 24 GB of memory.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "traditional",
                    "trained",
                    "arfake",
                    "training",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For our baseline models, we used Whisper-small, Whisper-large, wav2vec2, and HuBERT-base speech encoders due to their vast popularity in the speech domain. We applied two identical feed-forward sublayers, each comprising a fully connected layer followed by a ReLU activation function and a dropout layer. This feed-forward block is repeated twice. After the feed-forward modules, the output is passed to a final classification layer that maps the learned features to the desired output classes.\nWe trained the models using Adam optimizer with learning rate </span>\n  <math alttext=\"1\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">3</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\times 10^{-3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and dropout 0.5.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "models",
                    "whisperlarge",
                    "whispersmall",
                    "trained",
                    "hubertbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluated model performance using Accuracy and Equal Error Rate (EER) evaluation metrics. The lower EER values indicate more reliable detectors and thus receive greater focus in our analysis. To assess the difficulty of the dataset across different TTS models, we used synthesized audio from each model and applied Whisper Large as an ASR system, reporting the Word Error Rate (WER). Additionally, we asked 12 native Arabic speakers to rate the audio samples on a scale from 1 to 5 based on how realistic they sounded, and we calculated the Mean Opinion Score (MOS). For this evaluation, we randomly selected 8 audio samples from each TTS model, choosing one sample per dialect.</span>\n</p>\n\n",
                "matched_terms": [
                    "samples",
                    "equal",
                    "error",
                    "eer",
                    "rate",
                    "accuracy",
                    "models",
                    "from",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> summarizes the Equal Error Rate (EER) and accuracy results for both classical classifiers and encoder-based models, evaluated on data generated by four different TTS models. The final TTS models&#8217; percentage used for generating the synthetic dataset was selected based on a combination of classifier performance, Mean Opinion Score (MOS), and ASR results. Initially, datasets were generated using each TTS model, and the one that proved most challenging for classifiers to distinguish between bonafide and spoofed samples will be chosen either to generate the dataset or to have a high percentage while generating the final dataset. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 4.2 Baselines &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the dataset generated by FishSpeech was the most difficult for classifiers, followed by XTTS-v2, ArTST, and VITS. As expected, embedding-based models outperformed traditional machine learning models, particularly on datasets generated by FishSpeech and XTTS-v2. Notably, Whisper-large achieved a strong performance with an EER of 6.92%, outperforming other models and even surpassing the ASVSpoof benchmark by nearly 7%. HuBERT-base also demonstrated strong results on XTTS-v2, achieving an EER of just 0.07%.\nFor the dataset generated by ArTST and VITS, SVM achieved perfect performance with 100% accuracy, with Extra Trees performing similarly and achieving 99.96% accuracy on the VITS-generated dataset. It is worth noting that the embedding-based models on ArTST and VITS produced fair results, considering they were trained for only one epoch due to the relative ease of the spoofed datasets. Classifiers were not the only criterion used to determine the most effective TTS model for generating our dataset. We also calculated the Mean Opinion Score (MOS), as shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The results further support our findings, with FishSpeech achieving the highest overall MOS of 3.72 out of 5, followed by XTTS-v2. FishSpeech outperformed the other models across all dialects except for AE, where it ranked second to XTTS-v2 by a margin of just 0.08.</span>\n</p>\n\n",
                "matched_terms": [
                    "data",
                    "samples",
                    "equal",
                    "error",
                    "xttsv2",
                    "rate",
                    "accuracy",
                    "fishspeech",
                    "traditional",
                    "vits",
                    "hubertbase",
                    "artst",
                    "embeddingbased",
                    "trained",
                    "eer",
                    "svm",
                    "all",
                    "generated",
                    "models",
                    "whisperlarge",
                    "model",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further assess the realism of the generated audio, we evaluated spoofed samples using the Whisper-Large model in an ASR setting. For this, the original bonafide data was compared with its ground-truth transcription to establish a baseline weighted word error rate (WER). The spoofed data generated by each TTS model was then aligned against the same original transcription, ensuring a consistent reference for comparison. We report WER, average utterance duration, and perceptual quality. As shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 4.4.2 ASR evaluation &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Fish-Speech achieved an average WER of 97.26%, which is very close to the original data at 94.40%, while XTTS-v2 and ArTST reached much lower scores of 62.61% and 65.58%, respectively, and VITS obtained 110.90%, indicating greater ASR difficulty. When examining the average utterance duration, Fish-Speech (4.24 s) closely matched the original data (4.13 s), similar to VITS (4.19 s), whereas XTTS-v2 (6.65 s) and ArTST (5.59 s) produced noticeably longer utterances. However, despite its temporal similarity, VITS was rated poorly in terms of perceptual quality, with a mean opinion score (MOS) of only 1.7 As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22808v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.4 Results and Analysis &#8227; 4 Experiments &#8227; ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, making it easier to detect as spoofed speech. Taken together, these results demonstrate that Fish-Speech not only produces ASR outputs that are harder to distinguish from genuine data but also maintains temporal and perceptual consistency, making its spoofed samples substantially more challenging to detect compared to other TTS systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "samples",
                    "error",
                    "xttsv2",
                    "rate",
                    "generated",
                    "not",
                    "fishspeech",
                    "data",
                    "which",
                    "comparison",
                    "whisperlarge",
                    "from",
                    "model",
                    "vits",
                    "artst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We introduced the first multi-dialect Arabic spoof-speech corpus generated by three VC and TTS systems. Through a comprehensive evaluation pipeline combining classifiers, Mean Opinion Score (MOS), and ASR performance, we observed that Fish-Speech samples were the most challenging to detect, outperforming the other TTS models in realism. To improve generalizability, we further constructed a combined benchmark dataset guided by the evaluation findings. Moreover, we proposed baseline classifiers that not only performed strongly on the in-distribution test sets but also generalized effectively to unseen audio. These findings highlight the importance of resources that encompass multiple dialects and diverse TTS systems, as well as adaptive detectors capable of keeping pace with synthesis advances. Our corpus provides a solid foundation for developing more robust Arabic anti-spoofing systems.</span>\n</p>\n\n",
                "matched_terms": [
                    "samples",
                    "generated",
                    "models",
                    "fishspeech",
                    "test",
                    "not"
                ]
            }
        ]
    }
}