{
    "S5.T1": {
        "source_file": "Large Language Model Prompt Datasets: An In-depth Analysis and Insights",
        "caption": "Table 1: Key characteristics of the seven datasets selected for analysis, where size represents the number of prompts after preprocessing removes incorrectly extracted or malformed entries.",
        "body": "Dataset\nSize\nPublisher Type\nGeneration Method\nDisplay Form\nDomain\n\n\n\n\n1.1k-business\n1235\nEnd User\nUnknown\nUser Prompt\nBusiness\n\n\nBoredHumans\n956\nEnd User\nDataset Derivation\nUser Prompt\nGeneral\n\n\ndolly-15k\n14779\nLLM Researcher\nHuman Generated\nSingle-turn Conversation\nGeneral\n\n\nmedical-o1\n19679\nDomain Scientist\nModel Generated\nSingle-turn Conversation\nMedical\n\n\nOASST1\n22079\nLLM Researcher\nHuman Generated\nTree-structured Conversation\nGeneral\n\n\nSelf-Instruct\n81673\nLLM Researcher\nModel Generated\nSingle-turn Conversation\nGeneral\n\n\nShareGPT\n181570\nEnd User\nHuman Generated\nMulti-turn Conversation\nGeneral",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Size</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Publisher Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Generation Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Display Form</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Domain</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.1k-business</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">1235</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">End User</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Unknown</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">User Prompt</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Business</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">BoredHumans</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">956</th>\n<td class=\"ltx_td ltx_align_center\">End User</td>\n<td class=\"ltx_td ltx_align_center\">Dataset Derivation</td>\n<td class=\"ltx_td ltx_align_center\">User Prompt</td>\n<td class=\"ltx_td ltx_align_center\">General</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">dolly-15k</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">14779</th>\n<td class=\"ltx_td ltx_align_center\">LLM Researcher</td>\n<td class=\"ltx_td ltx_align_center\">Human Generated</td>\n<td class=\"ltx_td ltx_align_center\">Single-turn Conversation</td>\n<td class=\"ltx_td ltx_align_center\">General</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">medical-o1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">19679</th>\n<td class=\"ltx_td ltx_align_center\">Domain Scientist</td>\n<td class=\"ltx_td ltx_align_center\">Model Generated</td>\n<td class=\"ltx_td ltx_align_center\">Single-turn Conversation</td>\n<td class=\"ltx_td ltx_align_center\">Medical</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">OASST1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">22079</th>\n<td class=\"ltx_td ltx_align_center\">LLM Researcher</td>\n<td class=\"ltx_td ltx_align_center\">Human Generated</td>\n<td class=\"ltx_td ltx_align_center\">Tree-structured Conversation</td>\n<td class=\"ltx_td ltx_align_center\">General</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">Self-Instruct</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">81673</th>\n<td class=\"ltx_td ltx_align_center\">LLM Researcher</td>\n<td class=\"ltx_td ltx_align_center\">Model Generated</td>\n<td class=\"ltx_td ltx_align_center\">Single-turn Conversation</td>\n<td class=\"ltx_td ltx_align_center\">General</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">ShareGPT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">181570</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">End User</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Human Generated</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Multi-turn Conversation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">General</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "11kbusiness",
            "medical",
            "business",
            "key",
            "type",
            "characteristics",
            "prompt",
            "medicalo1",
            "treestructured",
            "datasets",
            "selfinstruct",
            "end",
            "preprocessing",
            "oasst1",
            "where",
            "researcher",
            "conversation",
            "malformed",
            "domain",
            "seven",
            "form",
            "removes",
            "general",
            "llm",
            "sharegpt",
            "scientist",
            "extracted",
            "unknown",
            "incorrectly",
            "method",
            "number",
            "generation",
            "singleturn",
            "analysis",
            "display",
            "prompts",
            "publisher",
            "selected",
            "dataset",
            "derivation",
            "generated",
            "size",
            "multiturn",
            "human",
            "boredhumans",
            "user",
            "after",
            "dolly15k",
            "entries",
            "model",
            "represents"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We present the key characteristics of these seven datasets in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Dataset Selection &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">A prompt is a natural language instruction that defines a specific task for a large language model (LLM) and serves as the primary interface for human-LLM interaction. With the growing deployment of LLMs, diverse prompt datasets are emerging from platforms such as GitHub and social media. These datasets span a wide array of applications and content types, facilitating both broader LLM utilization and improved prompt engineering.\nIn this work, we&#8211;for the first time&#8211;have compiled an extensive list of prompt datasets sourced from various channels, representing a spectrum of downstream tasks, languages, engineering techniques, attributes, and modalities. We select key representative datasets for systematic analysis, revealing commonalities and differences in prompt construction across categories, distinguishing them from other text corpora like literature and web.\nWe further propose a prompt optimization approach that leverages syntactic embeddings of part-of-speech and dependency structures. By identifying a centroid representation of prompts and guiding LLMs to rewrite prompts toward this centroid, our method improves the meaningfulness of model outputs.\nWe have made our datasets and code available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/LLM-Prompt-Datasets-7416\" title=\"\">https://anonymous.4open.science/r/LLM-Prompt-Datasets-7416</a>.</p>\n\n",
                "matched_terms": [
                    "method",
                    "key",
                    "prompt",
                    "datasets",
                    "analysis",
                    "llm",
                    "prompts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent large language model (LLM) advancements have spurred the proliferation of custom prompts optimized for specific tasks. This trend spans technology communities&#8211;from GitHub repositories (e.g., f/awesome-chatgpt-prompts <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib17\" title=\"\">Ak&#305;n, </a>)</cite>) and Reddit forums (e.g., ChatGPTPromptGenius <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib2\" title=\"\">Cha, </a>)</cite>) to platforms like PromptBase and PromptGenius <cite class=\"ltx_cite ltx_citemacro_citep\">(Pro, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib6\" title=\"\">a</a>)</cite>. AI researchers and domain experts also share prompts to promote transparency, reproducibility, and collaborative innovation <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>. Collectively, these datasets enable detailed analysis of usage patterns and high-performing prompt designs.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "domain",
                    "datasets",
                    "analysis",
                    "llm",
                    "prompts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, prior research has largely neglected comprehensive examinations of available prompt datasets. To address this gap, we apply stringent criteria to select, refine, and evaluate datasets that enable analysis of diverse prompts across multiple sources, content types, and target applications. Our survey encompasses over 1.22 TB of data, comprising more than 673M prompt instances from 129 heterogeneous sources. Our first contribution is a hierarchical taxonomy of LLM prompt datasets that serves as a detailed reference for researchers and informs future studies.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "datasets",
                    "analysis",
                    "llm",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we perform multi-level linguistic analysis&#8212;lexical, syntactic, and semantic&#8212;across seven meticulously selected, large-scale, diverse, and representative prompt datasets. By integrating statistical and machine learning methods, our study reveals key insights into compositional patterns, domain-specific variations, and unique linguistic properties that distinguish these prompts from other text corpora, such as literature and web content.</p>\n\n",
                "matched_terms": [
                    "key",
                    "prompt",
                    "datasets",
                    "seven",
                    "prompts",
                    "selected"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we propose a prompt optimization method leveraging part-of-speech and dependency embeddings. By aligning target prompts with a centroid of high-performing syntactic patterns, our approach improves the meaningfulness and quality of LLM responses. This data-driven method provides a foundation for more effective prompt selection and refinement in LLMs.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "method",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets for LLMs.</span>\nLiu et al. <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib39\" title=\"\">2024</a>)</cite> discuss broadly the topic of datasets for LLMs, but emphasize more on corpus datasets for training and fine-tuning LLMs rather than providing a detailed analysis of prompt datasets.\nA few works consider LLM prompt datasets but with a narrower objective compared to ours. For instance, <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib70\" title=\"\">2023</a>)</cite> and OpenCodeInstruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib15\" title=\"\">2025a</a>)</cite> focus only on datasets for instruction tuning&#8211;a technique for fine-tuning LLMs using carefully constructed instruction-response pairs. LLMSecEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tony et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib62\" title=\"\">2023</a>)</cite> introduces a prompt dataset specifically designed for evaluating the safety of codes generated by LLMs, whereas\n<cite class=\"ltx_cite ltx_citemacro_citet\">Lu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib41\" title=\"\">2024</a>)</cite> survey datasets for LLMs&#8217; evaluation.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "generated",
                    "datasets",
                    "analysis",
                    "llm",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tools and frameworks for prompt engineering.</span>\nThe prompt report <cite class=\"ltx_cite ltx_citemacro_citep\">(Schulhoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib57\" title=\"\">2024</a>)</cite> offers a thorough survey of prompt engineering techniques, providing detailed scheme definitions and corresponding examples.\nSeveral works focus on developing tools that streamline prompt construction. Both\nPromptAid&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mishra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib43\" title=\"\">2025</a>)</cite> and PromptLandscape&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib63\" title=\"\">2024a</a>)</cite> present visual support systems to simplify the creation and engineering of prompts.\nPEPR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Feffer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib28\" title=\"\">2024</a>)</cite> assesses various prompt combinations to determine the most optimal one for a given scenario.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Saletta &amp; Ferretti (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib53\" title=\"\">2024</a>)</cite> introduce a grammar-based evolutionary method to systematically optimize prompts for specific use cases.\nPromptaware&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib23\" title=\"\">2025</a>)</cite> integrates software engineering principles into the prompt engineering process. There are also works proposing solutions for generating prompts for specific scenarios.\nPromptAgent&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib64\" title=\"\">2024b</a>)</cite> introduces a model that automatically crafts and optimizes prompts with quality on par with those handcrafted by experts.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "model",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to previous studies, our work is the first to compile a comprehensive list of prompt datasets and we also extract valuable insights through their analysis.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "analysis",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data discovery guideline.</span> We employ a systematic dataset discovery process across multiple sources to compile a diverse repository of prompt datasets. Our objective is to capture real-world, user-generated prompts, instruction-following interactions, and domain-specific scenarios. In particular, our primary objectives for datasets discovery are three-fold: <span class=\"ltx_text ltx_font_bold\">(1)</span> collecting datasets that are composed of prompts, i.e., natural language instructions that describe a certain task the LLM should perform and guide the LLM towards generating a desired output; <span class=\"ltx_text ltx_font_bold\">(2)</span> ensuring that the extracted data cover various domains, including day-to-day scenarios such as travel planning, professional scenarios such as academic writing, and specialized scenarios such as healthcare and finance; and <span class=\"ltx_text ltx_font_bold\">(3)</span> allowing different forms of prompts, e.g., single instruction, conversations, etc.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "datasets",
                    "llm",
                    "extracted",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data discovery process.</span> We collect publicly available datasets from the following four types of sources. <span class=\"ltx_text ltx_font_bold\">First</span>, we consult <span class=\"ltx_text ltx_font_italic\">dataset collection platforms</span>, including Hugging Face Datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib12\" title=\"\">hug, </a>)</cite>, Kaggle <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib13\" title=\"\">kag, </a>)</cite>, Google Dataset Search <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib11\" title=\"\">goo, </a>)</cite>, and Papers with Code <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib14\" title=\"\">pap, </a>)</cite>. Targeted searches using keywords, e.g., \"prompt dataset\", \"instruction-following dataset\", and \"conversation dataset\" yield 60 prompt datasets. <span class=\"ltx_text ltx_font_bold\">Second</span>, we review the latest <span class=\"ltx_text ltx_font_italic\">academic publications</span>, specifically papers on prompt engineering, natural language understanding, and dialogue systems, published at NeurIPS, ICLR, and ICML between 2023-2024 and identify 73 datasets shared across them. <span class=\"ltx_text ltx_font_bold\">Third</span>, we also examine <span class=\"ltx_text ltx_font_italic\">public repositories</span> by systematically surveying open-source GitHub projects using keywords, e.g., &#8220;prompt collection&#8221;, &#8220;LLM prompts&#8221;, and &#8220;instruction dataset&#8221;. We identify 21 prompt repositories that typically contain curated prompt lists derived from user interactions or synthesized from public APIs. Some of these repositories are &#8220;awesome-lists&#8221;, which are curated collections of high-quality prompts or links to prompt datasets.\nNotable examples include <span class=\"ltx_text ltx_font_sansserif\">Awesome Instruction Datasets</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib46\" title=\"\">Nie, </a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering Guide</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Saravia, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib56\" title=\"\">2022</a>)</cite>, and <span class=\"ltx_text ltx_font_sansserif\">LLMDataHub</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib71\" title=\"\">Zhao, </a>)</cite>. <span class=\"ltx_text ltx_font_bold\">Finally</span>, we extract 14 datasets from <span class=\"ltx_text ltx_font_italic\">popular websites dedicated to prompt-sharing</span>, including <span class=\"ltx_text ltx_font_sansserif\">Prompt Genius</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Pro, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib7\" title=\"\">b</a>)</cite> and <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib10\" title=\"\">bor, </a>)</cite>. These platforms feature user-written prompts for practical purposes.</p>\n\n",
                "matched_terms": [
                    "conversation",
                    "prompt",
                    "datasets",
                    "boredhumans",
                    "user",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data filtering.</span>\nWe remove duplicate entries (e.g., CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib51\" title=\"\">2024</a>)</cite> appears in both Hugging Face and NeurIPS 2024) and then filter the remaining candidates using four quality criteria for inclusion in this paper.\n<span class=\"ltx_text ltx_font_bold\">First</span>, <span class=\"ltx_text ltx_font_italic\">Dataset size.</span> We prioritize datasets containing at least 1K prompts to ensure robustness in diversity and statistical power. In contrast, due to their generally limited scope, user-shared datasets are filtered with a minimum threshold of 50 prompts.\n<span class=\"ltx_text ltx_font_bold\">Second</span>, <span class=\"ltx_text ltx_font_italic\">Data quality.</span> We evaluate the quality of prompts based on their cleanliness. Most datasets (e.g., <span class=\"ltx_text ltx_font_sansserif\">OpenCodeReasoning</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib16\" title=\"\">2025b</a>)</cite>) on data hosting platforms (e.g., Hugging Face and Kaggle) are well-formatted and clean. For the remaining data, we exclude samples with inconsistent formatting or unclear structure. For instance, the <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering Guide</span>&#8211;a resource that offers both curated prompt datasets and instructional examples&#8211;contains many illustrative prompts scattered throughout the material and are thus omitted from our datasets.\n<span class=\"ltx_text ltx_font_bold\">Third</span>, <span class=\"ltx_text ltx_font_italic\">Data relevance.</span> We assess whether the prompts are aligned with our data discovery guidelines, specifically emphasizing on those that represent common usage scenarios for broad audiences (e.g., <span class=\"ltx_text ltx_font_sansserif\">Chinese-DeepSeek-R1-Distill-data-110k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib38\" title=\"\">2025a</a>)</cite>), and tasks from various domains (e.g., <span class=\"ltx_text ltx_font_sansserif\">Medical Verifiable Problems</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">OpenMathReasoning</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Moshkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib44\" title=\"\">2025</a>)</cite>). Datasets that violate our discovery guidelines are omitted. For instance, the <span class=\"ltx_text ltx_font_sansserif\">PersonaHub</span> dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib5\" title=\"\">Per, </a>)</cite> is excluded because it does not meet <span class=\"ltx_text ltx_font_bold\">data discovery guideline (1)</span>, which mandates that prompts be linked to specific, well-defined tasks. Although <span class=\"ltx_text ltx_font_sansserif\">PersonaHub</span> demonstrates the potential of synthetic personas in generating diverse content (e.g., reasoning problems, dialogues, or non-player character behaviors), it predominantly comprises persona descriptions without clear task formulation.\n<span class=\"ltx_text ltx_font_bold\">Fourth</span>, <span class=\"ltx_text ltx_font_italic\">Accessibility.</span> Datasets must be publicly accessible or retrievable via automated crawling, and their licensing terms must permit research use. After filtering, we identify 129 distinct prompt datasets for taxonomic analysis (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S4\" title=\"4 Dataset Taxonomy &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "medical",
                    "prompt",
                    "size",
                    "datasets",
                    "entries",
                    "analysis",
                    "after",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We categorize our collected prompt datasets across multiple dimensions and hierarchies, creating a detailed taxonomy illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S3.F1\" title=\"Figure 1 &#8227; 3 Prompt Datasets Discovery and Refinement &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. We discuss certain key aspects in this taxonomy below.</p>\n\n",
                "matched_terms": [
                    "key",
                    "prompt",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source.</span>\nWe classify prompt sources by publisher, release channel, and generation process.</p>\n\n",
                "matched_terms": [
                    "publisher",
                    "prompt",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Publisher</span> denotes the source&#8217;s identity and intent. We distinguish among <span class=\"ltx_text ltx_font_italic\">end users</span> who share prompts for practical tasks like writing/ coding (e.g., <span class=\"ltx_text ltx_font_sansserif\">Prompt Genius</span>), <span class=\"ltx_text ltx_font_italic\">LLM researchers</span> who publish prompts for fine-tuning and benchmarking (e.g., <span class=\"ltx_text ltx_font_sansserif\">OpenMathReasoning</span> by NVIDIA), and <span class=\"ltx_text ltx_font_italic\">domain scientists</span> who use LLMs in their specific fields (e.g., <span class=\"ltx_text ltx_font_sansserif\">ChatGPT Data Science Prompts</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib60\" title=\"\">Tang, </a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "domain",
                    "end",
                    "llm",
                    "publisher",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Release channel</span> refers to the platform where a dataset is published. Common platforms include <span class=\"ltx_text ltx_font_italic\">data hosting sites</span> such as GitHub, Hugging Face, and Kaggle, where structured prompt formats (e.g., CSV, JSON) dominate. <span class=\"ltx_text ltx_font_italic\">Personal sites</span> or <span class=\"ltx_text ltx_font_italic\">notes</span> (e.g., Notion workspaces) often host informal, user-oriented prompts. Dedicated <span class=\"ltx_text ltx_font_italic\">prompt sharing websites</span> vary from open-access (e.g., <span class=\"ltx_text ltx_font_sansserif\">QuickRef.ME</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib8\" title=\"\">Qui, </a>)</cite>) to commercial marketplaces (e.g., <span class=\"ltx_text ltx_font_sansserif\">PromptBase</span>). <span class=\"ltx_text ltx_font_italic\">Social media</span>, like Reddit&#8217;s <span class=\"ltx_text ltx_font_sansserif\">r/ChatGPTPromptGenius</span>, also plays a key role in community-driven prompt exchange.</p>\n\n",
                "matched_terms": [
                    "key",
                    "prompt",
                    "prompts",
                    "where",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generation process</span> describes how the prompts are created. <span class=\"ltx_text ltx_font_italic\">Human-generated prompts</span> are either manually authored (e.g., <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>)</cite>) or collected from user queries (e.g., <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib36\" title=\"\">Li, </a>)</cite>), <span class=\"ltx_text ltx_font_italic\">Model-generated prompts</span> include those created via self-instruct techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib65\" title=\"\">2023</a>)</cite> (e.g., <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>), multi-agent simulations (e.g., <span class=\"ltx_text ltx_font_sansserif\">AI Society</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib34\" title=\"\">2023a</a>)</cite>), or reverse instruction generation (e.g., <span class=\"ltx_text ltx_font_sansserif\">LongForm</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;ksal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib32\" title=\"\">2023</a>)</cite>). Finally, <span class=\"ltx_text ltx_font_italic\">derivative datasets</span> build on existing resources through task expansion or reformatted aggregation (e.g., <span class=\"ltx_text ltx_font_sansserif\">Flan 2022</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib4\" title=\"\">Fla, </a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">xP3</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib45\" title=\"\">2022</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "generation",
                    "datasets",
                    "user",
                    "sharegpt",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Content.</span>\nPrompt datasets are characterized by distinct linguistic and structural attributes.\n<span class=\"ltx_text ltx_font_bold\">Linguistically</span>, they may be <span class=\"ltx_text ltx_font_italic\">monolingual</span> or <span class=\"ltx_text ltx_font_italic\">multilingual</span>; in the latter case, datasets are deemed <span class=\"ltx_text ltx_font_italic\">semantically aligned</span> if each entry includes multilingual counterparts with identical semantics, thereby enhancing LLM performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib35\" title=\"\">2023b</a>)</cite>.\nIn terms of <span class=\"ltx_text ltx_font_bold\">display form</span>, prompts appear either as <span class=\"ltx_text ltx_font_italic\">conversation</span> (e.g., single-round, multi-round, or tree-structured) or <span class=\"ltx_text ltx_font_italic\">instruction</span> (e.g., user prompt, system prompt). The prompt <span class=\"ltx_text ltx_font_bold\">format</span>&#8211;ranging from <span class=\"ltx_text ltx_font_italic\">free-form</span> to <span class=\"ltx_text ltx_font_italic\">structured</span> (e.g., JSON, Markdown, HTML), or a combination thereof&#8211;substantially influences LLM response quality <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib40\" title=\"\">2025b</a>)</cite>.\nFinally, datasets differ in their use of <span class=\"ltx_text ltx_font_bold\">placeholders</span>, which allow for text substitution and enable diversified prompt transformations <cite class=\"ltx_cite ltx_citemacro_citep\">(Shin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib58\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "treestructured",
                    "conversation",
                    "prompt",
                    "datasets",
                    "form",
                    "user",
                    "llm",
                    "display",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt engineering</span> methods are critical for enhancing prompt performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Sahoo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib52\" title=\"\">2025</a>)</cite>. Common techniques include <span class=\"ltx_text ltx_font_italic\">few-shot</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib20\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">role playing</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib69\" title=\"\">2018</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">chain-of-thought</span> (CoT) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib67\" title=\"\">2022</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">rephrase-and-respond</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib27\" title=\"\">2023</a>)</cite>. Some datasets adopt a single method (e.g., <span class=\"ltx_text ltx_font_sansserif\">awesome-chatgpt-prompts</span> uses role playing), while others combine multiple techniques (e.g., <span class=\"ltx_text ltx_font_sansserif\">PromptBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib72\" title=\"\">2024</a>)</cite> integrates six methods), or leave the strategy unspecified.\nMoreover, datasets may include <span class=\"ltx_text ltx_font_bold\">extra attributes</span>, including <span class=\"ltx_text ltx_font_italic\">labeled data</span> (e.g., response supervision, safety labels), <span class=\"ltx_text ltx_font_italic\">analytical data</span> (e.g., token count, user behavior), <span class=\"ltx_text ltx_font_italic\">structural information</span> (e.g., timestamp, multi-turn context), and <span class=\"ltx_text ltx_font_italic\">ground truth</span> (e.g., gold answers, attribution references)&#8211;as seen in datasets including <span class=\"ltx_text ltx_font_sansserif\">hh-rlhf</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib18\" title=\"\">2022</a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">UltraFeedback</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Cui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib26\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span>. <span class=\"ltx_text ltx_font_italic\">Additional tags</span> may indicate <span class=\"ltx_text ltx_font_italic\">usage context</span>, such as associated models, cultural regions, or domain specificity.\nFinally, datasets vary in <span class=\"ltx_text ltx_font_bold\">dynamicity</span> (i.e., static vs. dynamically updated) and <span class=\"ltx_text ltx_font_bold\">modality</span> (i.e., single-modal vs. cross-/multi-modal). For example, <span class=\"ltx_text ltx_font_sansserif\">awesome-chatgpt-prompts</span> is a dynamically updated prompts collection, while <span class=\"ltx_text ltx_font_sansserif\">PLM-Video-Human</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib24\" title=\"\">2025</a>)</cite> supports multi-modal learning for video understanding.</p>\n\n",
                "matched_terms": [
                    "method",
                    "prompt",
                    "domain",
                    "multiturn",
                    "datasets",
                    "user",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Target.</span>\nTarget defines the purpose and applications of prompt datasets. From <span class=\"ltx_text ltx_font_bold\">cognitive intents and applications</span> perspective, prompts may aim for <span class=\"ltx_text ltx_font_italic\">information retrieval</span> (e.g., <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> includes information extraction category), <span class=\"ltx_text ltx_font_italic\">text processing</span> (e.g., <span class=\"ltx_text ltx_font_sansserif\">StrategyQA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib9\" title=\"\">Str, </a>)</cite> requires implicit reasoning steps in the question), <span class=\"ltx_text ltx_font_italic\">analytical decision-making</span> (e.g., <span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span>\n<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>\nfor consultation decision), <span class=\"ltx_text ltx_font_italic\">logical reasoning</span> (e.g., <span class=\"ltx_text ltx_font_sansserif\">DeepSeek-Prover-V1</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Xin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib68\" title=\"\">2024</a>)</cite>), <span class=\"ltx_text ltx_font_italic\">creative generation</span> (e.g., <span class=\"ltx_text ltx_font_sansserif\">No Robots</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib49\" title=\"\">2023</a>)</cite> includes generation category), or <span class=\"ltx_text ltx_font_italic\">emotion-oriented</span> (e.g., <span class=\"ltx_text ltx_font_sansserif\">empathetic-dialogues-facebook-ai</span>\n<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib3\" title=\"\">Emp, </a>)</cite>) tasks. In terms of <span class=\"ltx_text ltx_font_bold\">downstream tasks</span>, we identify three major categories: <span class=\"ltx_text ltx_font_italic\">data-oriented</span> (e.g., synthetic data generation, data augmentation), <span class=\"ltx_text ltx_font_italic\">model-oriented</span> (e.g., model training, finetuning, benchmarking, RLHF), and <span class=\"ltx_text ltx_font_italic\">prompt-oriented</span> (e.g., prompt composition studies, prompt engineering effectiveness analysis, instruction pattern analysis).</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "generation",
                    "datasets",
                    "analysis",
                    "prompts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among these, instruction fine-tuning datasets represent a prominent and widely-used subset of prompt datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib39\" title=\"\">2024</a>)</cite>. These datasets comprise instruction-response pairs, where the \"instruction\" serves as a prompt and the \"response\" represents the target model output. They are primarily employed for supervised fine-tuning to enhance model capability and controllability. As a result, models trained on these datasets exhibit superior alignment with human intent, improved instruction-following, and increased safety characteristics <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib70\" title=\"\">2023</a>)</cite>. Furthermore, the instructions in these datasets often reflect real-world user queries, making them both practical for deployment and valuable for prompt-related research. Notable examples are <span class=\"ltx_text ltx_font_sansserif\">Alpaca</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Taori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib61\" title=\"\">2023</a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;pf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib31\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_sansserif\">FLAN 2022</span>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "characteristics",
                    "human",
                    "datasets",
                    "user",
                    "oasst1",
                    "where",
                    "model",
                    "represents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We summarize the key characetristics and metadata attributes&#8211; including sources&#8211;of all our collected and filtered 129 prompt datasets in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A5\" title=\"Appendix E Summary of Prompt Datasets for Taxonomic Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "key",
                    "prompt",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We next conduct an in-depth analysis across three linguistic levels&#8212;lexical, syntactic, and semantic&#8212;of prompts derived from seven distinct sources. Our approach integrates statistical techniques with machine learning methods to identify compositional patterns and inter-source variations.</p>\n\n",
                "matched_terms": [
                    "seven",
                    "analysis",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to ensure reliable analysis of prompt characteristics, we curate multiple prompt-centric datasets with the following selection principles.\n<span class=\"ltx_text ltx_font_bold\">(1)</span> <span class=\"ltx_text ltx_font_italic\">Language consistency.</span> Only English-language data have been included to ensure uniform linguistic features and avoid cross-linguistic biases.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> <span class=\"ltx_text ltx_font_italic\">Exclusion of benchmark-style prompts.</span> Prompts designed for LLM performance evaluations (e.g., <span class=\"ltx_text ltx_font_sansserif\">PHYBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Qiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib48\" title=\"\">2025</a>)</cite>) are excluded to focus on natural usage scenarios.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> <span class=\"ltx_text ltx_font_italic\">Source and content diversity.</span> To achieve sufficient coverage and reduce sampling bias, we have selected datasets that differ in <span class=\"ltx_text ltx_font_italic\">publisher type</span> (i.e., end user vs. LLM researcher and domain scientist), <span class=\"ltx_text ltx_font_italic\">instruction generation method</span> (i.e., human vs. model generated), and <span class=\"ltx_text ltx_font_italic\">domain scope</span> (i.e., general vs. domain-specific tasks).</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "characteristics",
                    "datasets",
                    "end",
                    "researcher",
                    "domain",
                    "general",
                    "llm",
                    "scientist",
                    "method",
                    "generation",
                    "analysis",
                    "publisher",
                    "selected",
                    "generated",
                    "human",
                    "user",
                    "prompts",
                    "model",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following our selection principles, we curated seven representative datasets spanning different user types, instruction methods, and domains. For <span class=\"ltx_text ltx_font_bold\">end users</span>, general-domain prompts include single-turn prompts (<span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span>) and multi-turn conversations (<span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>), while business-domain single-turn prompts are represented by <span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span>. For <span class=\"ltx_text ltx_font_bold\">LLM researchers</span>, human-generated datasets include <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> and <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and model-generated prompts are captured by <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>. For <span class=\"ltx_text ltx_font_bold\">domain scientists</span>, we include model-generated medical prompts from <span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span>. This collection ensures diversity in publisher type, prompt structure, and application domain.</p>\n\n",
                "matched_terms": [
                    "medical",
                    "business",
                    "selfinstruct",
                    "prompt",
                    "domain",
                    "singleturn",
                    "datasets",
                    "multiturn",
                    "end",
                    "seven",
                    "boredhumans",
                    "oasst1",
                    "sharegpt",
                    "llm",
                    "user",
                    "publisher",
                    "prompts",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span> (<span class=\"ltx_text ltx_font_bold\">1.1k-business</span>). A curated dataset of 1&#8201;235 prompts oriented toward professional and business-related use cases, such as marketing, productivity, and decision-making. It represents structured, domain-specific prompting behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(1., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib1\" title=\"\">1</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "business",
                    "prompts",
                    "represents",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">BoredHumans Prompts</span> (<span class=\"ltx_text ltx_font_bold\">BoredHumans</span>).\nA smaller collection of 964 prompts compiled from publicly shared prompts on the boredhumans.com website. Some of the prompts on this site come from other community shared sources (e.g., <span class=\"ltx_text ltx_font_sansserif\">awesome-chatgpt-prompts</span>). It reflects community-created content and captures user creativity and experimentation <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib10\" title=\"\">bor, </a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "boredhumans",
                    "user",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> (<span class=\"ltx_text ltx_font_bold\">dolly-15k</span>).\nThis dataset includes 15&#8201;000 human-authored instruction&#8211;response pairs covering a range of everyday tasks. It is single-turn and domain-general, curated to support instruction-following models <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dolly15k",
                    "singleturn",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span> (<span class=\"ltx_text ltx_font_bold\">medical-o1</span>).\nSynthetic data of 90&#8201;120 open-ended questions and GPT-4o generated CoTs and responses. Open-ended questions are reformatted by GPT-4o based on close-set medical examination questions. The dataset is used to fine-tune HuatuoGPT-o1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "medicalo1",
                    "medical",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">OASST1.</span>\nThe Open Assistant dataset (OASST1) contains over 30&#8201;000 human-written messages arranged in dialogue trees. It emphasizes cooperative, open-domain assistant behavior and includes branching conversations rather than linear interactions <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;pf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib33\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "oasst1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">Self-Instruct.</span>\nA synthetic dataset with 82&#8201;646 prompts generated by large language models based on a small seed pool of human-written instructions. For every generation step, it samples 6 human-written tasks and 2 model-generated tasks in previous steps to promote diversity <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib65\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "generated",
                    "generation",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">ShareGPT.</span>\nA large-scale collection of approximately 90&#8201;000 ChatGPT conversation logs shared by users. It represents multi-turn, organically generated interactions and captures diverse user intentions in real-world usage scenarios <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib36\" title=\"\">Li, </a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "conversation",
                    "generated",
                    "multiturn",
                    "user",
                    "sharegpt",
                    "represents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform token-level analysis using <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram models to capture local textual patterns <cite class=\"ltx_cite ltx_citemacro_citep\">(Jurafsky &amp; Martin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib30\" title=\"\">2000</a>; Cavnar &amp; Trenkle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib21\" title=\"\">1994</a>; Manning &amp; Schutze, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib42\" title=\"\">2001</a>)</cite>. Initially, all tokens are lemmatized to mitigate inflectional variability, after which we extract 3-gram, 4-gram, and 5-gram sequences to compute their frequency distributions. By analyzing high-frequency <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams, we identify prevalent instruction templates, keyword combinations, and syntactic patterns, laying the groundwork for subsequent syntactic and semantic investigations.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span> The <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram frequency distributions reveal several notable patterns that highlight the distinct functional and stylistic characteristics across datasets.</p>\n\n",
                "matched_terms": [
                    "characteristics",
                    "analysis",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> High-frequency <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams reveal domain and prompt-engineering differences, such as role-playing cues in <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> (&#8220;you to act as&#8221;) versus medical reasoning in <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> (&#8220;what be the,&#8221; &#8220;the most likely diagnosis&#8221;). <span class=\"ltx_text ltx_font_bold\">(2)</span> While 3-grams capture general-purpose queries or commands (e.g., &#8220;what be the,&#8221; &#8220;I want to&#8221;), longer <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams (4&#8211;5) reflect task-specific patterns, as in <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> where frequent 5-grams (&#8220;please write in English language,&#8221; &#8220;write a comprehensive reply to&#8221;) highlight its instruction-following orientation. <span class=\"ltx_text ltx_font_bold\">(3)</span> Compared to Google Books 5-grams (e.g., &#8220;at the end of the,&#8221; &#8220;in whole or in part&#8221;) that serve narrative or descriptive purposes, prompt datasets exhibit inquiry- or command-focused <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams, underscoring a clear divergence in linguistic patterns across corpora.</p>\n\n",
                "matched_terms": [
                    "medical",
                    "prompt",
                    "medicalo1",
                    "domain",
                    "datasets",
                    "end",
                    "oasst1",
                    "sharegpt",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To gain deeper insights into the linguistic structure of prompts, we perform syntactic analysis from three perspectives: dependency parsing <cite class=\"ltx_cite ltx_citemacro_citep\">(Nivre, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib47\" title=\"\">2003</a>)</cite>, part-of-speech (POS) tagging <cite class=\"ltx_cite ltx_citemacro_citep\">(Brill, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib19\" title=\"\">1992</a>)</cite>, and term frequency-inverse document frequency (TF-IDF) scoring <cite class=\"ltx_cite ltx_citemacro_citep\">(Salton &amp; Buckley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib54\" title=\"\">1988</a>)</cite>. These features are both descriptive and can be aggregated into vector representations for tasks like prompt classification.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "analysis",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparative analysis with non-prompt text datasets, we have used Universal Dependencies\ncorpora for English: EWT <cite class=\"ltx_cite ltx_citemacro_citep\">(Silveira et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib59\" title=\"\">2014</a>)</cite> and ParTUT <cite class=\"ltx_cite ltx_citemacro_citep\">(Sanguinetti &amp; Bosco, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib55\" title=\"\">2014</a>)</cite>, where EWT contains informal contents&#8211;blog, social, reviews, email, and web, and ParTUT contains more formal contents&#8211;legal, news, and wiki.</p>\n\n",
                "matched_terms": [
                    "where",
                    "analysis",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply the spaCy <span class=\"ltx_text ltx_font_typewriter\">en_core_web_sm</span> parser <cite class=\"ltx_cite ltx_citemacro_citep\">(Honnibal &amp; Montani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib29\" title=\"\">2017</a>)</cite> to extract syntactic dependencies and determine the frequency of key grammatical relations in each dataset. For the EWT and ParTUT corpora, we rely on officially published dependency type annotations. This analysis reveals systematic variations in linguistic style across prompt sources. Additionally, we track verb&#8211;object (dobj) pairs to capture the task-oriented diversity of the prompts (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "key",
                    "prompt",
                    "analysis",
                    "prompts",
                    "type",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of Results.</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T2\" title=\"Table 2 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the distribution of eight common dependency types across seven prompt datasets and two reference corpora (EWT and ParTUT), revealing three key findings.</p>\n\n",
                "matched_terms": [
                    "key",
                    "prompt",
                    "datasets",
                    "seven",
                    "analysis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> The <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> dataset is characterized by its high use of adjectival modifiers (amod, 0.11) and low direct object frequency (dobj, 0.03), reflecting a preference for precise, state-oriented descriptions over action-driven narratives, often framed through linking verbs&#8212;typical of medical contexts detailing conditions, symptoms, and diagnoses. <span class=\"ltx_text ltx_font_bold\">(2)</span> In contrast, the <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> dataset favors concise, goal-driven imperatives with bare noun phrases as direct objects (dobj, 0.09) and minimal use of determiners (det, 0.05), aligning with its project-planning focus. <span class=\"ltx_text ltx_font_bold\">(3)</span> Verb&#8211;noun dependency analysis further distinguishes domains: medical instructions cluster around technical, domain-specific pairs like &#8220;have history&#8221; and &#8220;experience pain,&#8221; while datasets such as <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> use broader, generic pairs like &#8220;write answer&#8221; and &#8220;use code&#8221;. These syntactic patterns highlight each corpus&#8217; thematic priorities and inform strategies for domain-aware model training.</p>\n\n",
                "matched_terms": [
                    "medical",
                    "11kbusiness",
                    "medicalo1",
                    "datasets",
                    "analysis",
                    "sharegpt",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight the stylistic diversity among prompt datasets, where domain and intent directly influence grammatical structure. Medical prompts stress detailed specificity and descriptive richness, while business prompts favor concise, directive clarity, illustrating the functional interplay between form and purpose.</p>\n\n",
                "matched_terms": [
                    "medical",
                    "business",
                    "prompt",
                    "domain",
                    "datasets",
                    "form",
                    "prompts",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We annotate the datasets with POS tags and calculate the distribution of nouns, verbs, adjectives, and adverbs. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T3\" title=\"Table 3 &#8227; 5.3.1 Dependency Parsing &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the functional composition of prompts, contrasting content and function words. For example, a high verb frequency indicates action-oriented prompts, while a predominance of nouns suggests more objective narratives. These distributional differences reveal stylistic and structural variations across sources.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1)</span> Domain-specific datasets such as <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> and <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> exhibit a noun proportion of <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 0.26, surpassing that found in formal corpora like ParTUT. This reflects a concept-driven focus on domain entities and technical terms.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> Additionally, <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> also registers an unusually high adjective ratio (0.11), indicating a repeated emphasis on specifying medical attributes and conditions, consistent with the descriptive nature of clinical reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "medical",
                    "11kbusiness",
                    "medicalo1",
                    "domain",
                    "datasets",
                    "analysis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze lexical patterns across prompt datasets using TF-IDF. Each dataset&#8217;s prompts are concatenated into a single document (yielding seven corpus-level documents), and a TF-IDF vectorizer (with a 5000-word limit and English stopwords removed) computes sparse term importance representations. We then assess <span class=\"ltx_text ltx_font_bold\">inter-dataset lexical similarity</span> via pairwise cosine similarity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F3.sf3\" title=\"In Figure 3 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>) and extract the top three highest-weight tokens per dataset for <span class=\"ltx_text ltx_font_bold\">intra-dataset characterization</span> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T4\" title=\"Table 4 &#8227; 5.3.2 Part-of-Speech Tagging &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "datasets",
                    "seven",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1) Intra-dataset analysis</span>\ndelineates each dataset&#8217;s lexical focus and stylistic characteristics. For instance, <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> emphasizes business-specific terms like &#8220;content&#8221; and &#8220;email&#8221;, while <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span> features imperatives such as &#8220;act&#8221;, indicative of role-playing instructions. Similarly, <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> shows a dominant TF-IDF score for &#8220;output&#8221; (0.772), highlighting a structural prompt style based on explicit instruction&#8211;response formats.\n<span class=\"ltx_text ltx_font_bold\">(2) Inter-dataset comparison.</span>\nTF-IDF vectors show varying overlaps across datasets. The highest cosine similarity between <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> suggests a similar vocabulary&#8212;likely due to shared human-generation processes. In contrast, <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> is lexically distant from the others, especially <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> and <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span>, reflecting stylistic and domain-specific differences.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "prompt",
                    "characteristics",
                    "selfinstruct",
                    "medicalo1",
                    "datasets",
                    "analysis",
                    "boredhumans",
                    "oasst1",
                    "sharegpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze prompt semantics by encoding each prompt into a 384-dimensional dense vector using Sentence-BERT&#8217;s pretrained model <span class=\"ltx_text ltx_font_typewriter\">all-MiniLM-L6-v2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Reimers &amp; Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib50\" title=\"\">2019</a>)</cite>. Each prompt is encoded into a 384-dimensional dense vector that captures its semantic content. These embeddings serve as the foundation for classification, clustering, and visualization analysis. We perform Principal Component Analysis (PCA) to reduce sentence embeddings to two dimensions. For fair comparison, we uniformly at random sample 500 prompts per dataset and visualize their distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Semantic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "analysis",
                    "prompts",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1) Wide coverage in Self-Instruct:</span> The <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> dataset exhibits the most dispersed and evenly distributed semantic space, suggesting a broad topical coverage. This aligns with the self-instruction paradigm&#8217;s goal of generating diverse instruction types.\n<span class=\"ltx_text ltx_font_bold\">(2) Semantic cohesion in specific domains:</span> Prompts from <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> form more concentrated clusters, indicating domain-specific semantic cohesion.\n<span class=\"ltx_text ltx_font_bold\">(3) Overlap among human-generated sets:</span> The embeddings of <span class=\"ltx_text ltx_font_sansserif\">dolly-15k</span>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> overlap substantially across both PCA dimensions. This suggests that these datasets share stylistic and semantic characteristics, possibly due to their common reliance on human-LLM interactions for data generation.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "selfinstruct",
                    "characteristics",
                    "medicalo1",
                    "generation",
                    "datasets",
                    "analysis",
                    "form",
                    "oasst1",
                    "sharegpt",
                    "dolly15k",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the above analysis, we propose a new prompt engineering method that leverages structural linguistic features. Specifically, we take the average of the high-dimensional embeddings of POS tags and dependency relations from the analyzed dataset to define a centroid representation. This centroid captures the &#8220;central&#8221; syntactic patterns that are associated with higher-performing prompts.</p>\n\n",
                "matched_terms": [
                    "method",
                    "prompt",
                    "analysis",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each target prompt, we first analyze its POS and dependency embeddings to identify deviations from the centroid. Based on this analysis, a modification plan is generated, specifying how the prompt&#8217;s syntactic structure should be adjusted. The LLM is then guided to rewrite the prompt according to this plan, producing an optimized prompt whose embeddings are closer to the centroid. This process allows peripheral prompts that initially deviate from effective syntactic patterns to be systematically aligned with the central region of the embedding space.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "generated",
                    "analysis",
                    "llm",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To illustrate the practical impact of this approach, we present one representative case study in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S6.F5\" title=\"Figure 5 &#8227; 6 Application &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, where our prompt optimization successfully corrected the model&#8217;s initial incorrect responses, while another case study and the complete text are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.SS4\" title=\"F.4 Application &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">F.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By aligning prompts with this centroid, our method aims to improve the likelihood that the LLM generates correct or more meaningful responses.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "method",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We addressed the underexplored challenge of collecting and categorizing LLM prompt datasets into a structured taxonomy. Our lexical, syntactic, and semantic explorations uncover key linguistic patterns, inter-dataset similarities, differences, and distinctions from other corpora such as literature and web content. Our novel application enhances domain-specific prompt filtering pipelines by automatically flagging irrelevant or malformed prompts in an unsupervised, data-driven manner before inference. Future work should leverage these extensive datasets to advance LLM architectures, prompt engineering, and human-AI interactions&#8212;including adaptive quality assessments and pricing models in AI prompt marketplaces.</p>\n\n",
                "matched_terms": [
                    "key",
                    "prompt",
                    "malformed",
                    "datasets",
                    "llm",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate a focused analysis, we selected seven datasets representing various publisher types, generation methods, display formats, and domain categories. However, the number of datasets in each category is relatively limited, which may affect the representativeness of the results. Additionally, the scope of categories included in our analysis could be further expanded in future work to improve the comprehensiveness of the evaluation.</p>\n\n",
                "matched_terms": [
                    "number",
                    "generation",
                    "domain",
                    "datasets",
                    "seven",
                    "analysis",
                    "display",
                    "publisher",
                    "selected"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to the diversity of tasks among our selected datasets, this study does not include an evaluation of prompt effects. Our research primarily focuses on Natural Language Processing and Machine Learning methods, and thus does not leverage Large Language Models to analyze the impact of prompts. We recognize this as a limitation and suggest that future research incorporate prompt-based approaches for a more thorough assessment.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "selected",
                    "prompts",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The amount of prompt data is growing rapidly. Based on our proposed taxonomy, we encourage further studies to continuously explore and update analyses in accordance with emerging trends. Additionally, we hope that future research will conduct more in-depth experiments on prompt datasets, which could be vital for advancing prompt design.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work conforms to the ICLR Code of Ethics by responsibly compiling and analyzing existing prompt datasets rather than collecting new sensitive data. We ensure proper documentation of all datasets analyzed, respect original licenses and sources, and have made our code and datasets publicly available for transparency and reproducibility. The research poses minimal risk for misuse as it focuses on analytical insights rather than creating potentially harmful technologies, and we have documented our methodology thoroughly to enable external scrutiny.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We are committed to ensuring the reproducibility of our results. All code used in this research is publicly available through links in our abstract. The repository includes detailed instructions for dataset preprocessing, and running experiments. We also specify the exact versions of dependencies and libraries used in our experiments. All datasets employed in this study are either publicly accessible or their sources are clearly documented. Random seeds are set for all experiments where applicable to minimize variability. Together, these resources enable researchers to reproduce our analyses and results with minimal effort.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "where",
                    "preprocessing",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We briefly discuss all 129 prompt datasets collected for taxonomic analysis (&#167;3 and &#167;4).</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "analysis",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span>\n</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "business"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: \"1100+ ChatGPT Prompts for Business\" is a Notion-based dataset containing 1,235 curated prompts tailored for diverse business scenarios. It spans key domains such as buyer persona development, content strategy, digital marketing, narrative marketing, email campaigns, market research, product innovation, and finance. The collection includes specialized roles like Simulation Specialist, offering practical guidance for professionals, marketers, and entrepreneurs aiming to optimize operations, boost engagement, and enhance strategic decision-making.</p>\n\n",
                "matched_terms": [
                    "key",
                    "prompts",
                    "dataset",
                    "business"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset comprises over 1,000 curated ChatGPT prompt templates in Notion Workspace format, spanning diverse domains such as AI, marketing, education, healthcare, and code generation. Each entry typically includes a prompt, an automatic prompt (system prompt like), and a concise description.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This repository offers a well-curated collection of conversation prompts tailored for OpenAI&#8217;s GPT-3 model.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "model",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: AI Short is a public prompt-sharing platform with 5,867 categorized prompts. Each prompt is available in multiple languages, enabling cross-linguistic studies of prompt effectiveness and translation consistency.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_sansserif\">AI-Generated Prompts Dataset</span>\n</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset features thousands of prompts generated by the teknium/OpenHermes-2p5-Mistral-7B model, each designed to elicit diverse and contextually rich responses. Stored as JSON objects, it enables research in synthetic prompt generation, model creativity evaluation, and downstream fine-tuning.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "generated",
                    "generation",
                    "prompts",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: AIPRM is a community-curated prompt library and management platform featuring 5,325 publicly accessible prompts categorized by topic and activity. Its user-driven structure offers valuable insights into real-world prompt usage, preferences, and task design patterns.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Stanford Alpaca dataset comprises 52K high-quality, instruction-following examples generated via a modified Self-Instruct pipeline using text-davinci-003. Designed for fine-tuning LLaMA models, it enables research in alignment, instruction tuning, and synthetic data generation.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "generated",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: AM-DeepSeek-Distilled-40M is a multilingual (zh/en) reasoning dataset comprising 3.34 million prompts paired with 40 million model-generated responses across code, math, science, instruction-following and general reasoning. Each query includes four samples from three models (1.5B, 7B, and R1), with pass rates computed per model to assign unbiased difficulty scores. Released under CC-BY-NC 4.0, its unified JSONL format supports supervised fine-tuning, preference learning and reinforcement learning applications, enabling selection of subsets by category or difficulty level. It fosters robust LLM development research.</p>\n\n",
                "matched_terms": [
                    "general",
                    "llm",
                    "prompts",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: AM-DeepSeek-R1-Distilled-1.4M is a bilingual (Chinese and English) reasoning dataset of 1.4 million challenging problem-solution pairs. Collected from diverse open-source sources, it features semantically deduplicated instructions spanning text, code, and math domains. It provides high-quality, comprehensive, and diverse reasoning challenges. Solutions are distilled mainly from DeepSeek-R1-671B and rigorously validated via test-case execution, answer checking, and reward-model scoring. Structured as user-assistant exchanges with reasoning traces and metadata, this cc-by-nc-4.0 dataset also offers 0.5M, 0.9M, and 1K-sample zstd-compressed configs to support scalable LLM research.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: AM-Math-Difficulty-RL is an English math dataset comprising three difficulty tiers designed for RL of LLMs. It contains 100k+ problems from repositories and categorized by pass rates of Qwen models. Tier 1 includes tasks with partial success by Qwen-1.5B; Tier 2 covers problems where smaller models fail but larger ones succeed; Tier 3 features examples that even Qwen-32B struggles with. Problems span algebra, calculus, and combinatorics. Licensed under CC-BY-NC-4.0, it supports text-generation tasks and research on difficulty-aware staged RL strategies.</p>\n\n",
                "matched_terms": [
                    "where",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The APIGen-MT-5k dataset comprises 5000 realistic, high-quality, multi-turn function-calling dialogues generated by APIGen-MT, a scalable automated agentic pipeline simulating agent-human interactions. Covering retail and airline domains, each trajectory is verified through format checks, function executions, and semantic validations, achieving a 99% success rate in human evaluation. Provided in ShareGPT-style JSON and licensed under CC-BY-NC-4.0, it supports question-answering, text generation, and reinforcement learning benchmarks.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "multiturn",
                    "generation",
                    "human",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Awesome ChatGPT Prompts dataset is a collaboratively curated collection of diverse prompts optimized for interactive AI models, including ChatGPT, Claude, and LLaMA. Featuring both human- and LLM-generated entries with clear attribution, it supports research in prompt engineering, prompt effectiveness, and cross-model generalization.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "human",
                    "prompts",
                    "entries",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Aya Collection is a massive multilingual instruction tuning dataset comprising over 513 million prompt-completion pairs across 115 languages. It integrates three sources: human-crafted instruction templates created by fluent speakers for diverse tasks, machine translations of 19 top-tier datasets into 101 languages via NLLB, and the human-annotated Aya Dataset subset of 204K examples. Split by dataset, each record includes id, inputs, targets, language, script, and task type. Licensed under Apache-2.0, it supports academic and commercial classification, summarization, translation, and QA research.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "type",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Bactrian-X is a multilingual instruction-following dataset containing 3.4 million instruction-input-response triplets across 52 languages. It builds upon 67K unique English prompts drawn from Alpaca and Dolly, automatically translated via Google Translate into 51 languages. For each translated prompt (and optional input), GPT-3.5-Turbo generates a corresponding response, yielding 3.4 million examples. Each record includes an id, instruction, optional input, and model-generated output. Released under CC-BY-NC 4.0, Bactrian-X supports text-generation research, fine-tuning, and evaluation in low-resource and high-resource language settings, covering diverse tasks and domains.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Baize Chat Data is an instruction-finetuning corpus combining four sources: Alpaca, Medical, Quora, and StackOverflow. It contains about 210,000 conversational examples, each formatted with [|Human|] prompts and [|AI|] responses. Designed to enhance the Baize family of language models, this unified dataset supports interactive text generation and dialogue training. Sourced from the Baize GitHub repository, it provides diverse conversational scenarios ranging from general queries to specialized medical and technical discussions. It is optimized for instruction-following tasks. It enables realistic user interactions.</p>\n\n",
                "matched_terms": [
                    "medical",
                    "generation",
                    "human",
                    "general",
                    "user",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BELLE_Generated_Chat contains approx. 400k personalized Chinese character dialogues generated by the BELLE project. Each record includes an instruction, an (empty) input, and a generated output. Created by ChatGPT and not strictly verified, the dataset may contain factual inaccuracies. Licensed under GPL-3.0 for research use only. With around 0.4 million entries, it supports text-to-text generation and conversational modeling.</p>\n\n",
                "matched_terms": [
                    "entries",
                    "generated",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BELLE_Multiturn_Chat is a Chinese multi-turn conversational dataset comprising approximately 0.8 million human-assistant dialogues generated by the BELLE project using ChatGPT. Each record pairs an instruction containing prior context labeled with &#8220;Human:&#8221; and &#8220;Assistant:&#8221; with the assistant&#8217;s subsequent reply. Intended for text-to-text generation tasks, the GPL-3.0-licensed collection covers only Chinese interactions. As this data is automatically generated and unverified, factual errors and inconsistencies may arise. It is provided strictly for non-commercial research under the project&#8217;s usage restrictions; developers should validate outputs and adhere to licensing terms.</p>\n\n",
                "matched_terms": [
                    "multiturn",
                    "generated",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The BELLE_train_3.5M_CN dataset comprises approximately 3.5 million monolingual Chinese instruction-response pairs generated by the BELLE project, formatted as multi-turn and single-turn dialogues with unique IDs. It includes human-assistant exchanges across 13 instruction categories. Licensed under GPL-3.0, it supports text-to-text generation research exclusively; commercial or harmful use is prohibited. The JSON records each conversation&#8217;s ID and bilingual content.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "multiturn",
                    "generation",
                    "singleturn",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Best Chinese Prompt dataset is a comprehensive, well-structured collection of Chinese-language prompts spanning diverse categories such as casual chat, knowledge Q&amp;A, creative planning, copywriting, and code generation. It provides real multi-model response comparisons (e.g., GPT-4, ChatGPT, NewBing, Wenxin) and continuous updates via collaborative platforms.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BigDocs-Bench is a CC-BY-4.0 benchmark suite for training and evaluating multimodal models on document and code tasks. It comprises seven configurations: GUI-VQA, GUI2BBox, GUI2Summary, GUI2UserIntent, Image2Flow (GraphViz/JSON), and Table2LaTex, each containing thousands of samples across train, validation, and test splits. Spanning over 7.6 TB with 200K+ annotated examples, it includes screenshots or generated images paired with queries, annotations, metadata, and optional filter flags. Auxiliary fields trace provenance and dependencies on arXiv, SeeClick, AFTdb, InternVL-8B, LLaMA 3.1, and Graphviz.</p>\n\n",
                "matched_terms": [
                    "seven",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BoredHumans is a diverse and extensive prompt dataset compiled from multiple sources, including Awesome ChatGPT Prompts, Data Science Prompts, and Tree-of-Thought Prompting, among others. Its rich variety covers numerous domains and prompt styles, enabling comprehensive research on prompt engineering, AI model behavior, and in-context learning strategies.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "boredhumans",
                    "prompts",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: CAMEL AI Society is a synthetic dialogue corpus comprising 25,000 simulated conversations between GPT-3.5-turbo agents role-playing across 50 distinct user roles and 50 assistant roles on ten tasks per pairing. Available in both chat and instruction formats, each example includes metadata such as role identifiers, original and specified task descriptions, input context, generated responses, and conversation termination reasons. Designed for instruction-tuning and text-generation research, CAMEL is licensed under CC-BY-NC-4.0 and intended solely for non-commercial academic use, acknowledging potential synthetic inaccuracies.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "user",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The ChatGPT &amp; Bing AI Prompts dataset offers a diverse collection of prompts designed to optimize interaction with advanced conversational AI models, including ChatGPT and Bing AI. It enables research on prompt engineering techniques, model behavior across different AI platforms, and strategies for enhancing response quality.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The ChatGPT Prompts for Data Science dataset offers a curated collection of specialized prompts designed to enhance AI applications in data science tasks. It facilitates research on natural language interfaces for data analysis, model explanation, and automation of complex workflows.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "model",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The ChatGPT-Prompts dataset compiles diverse prompt templates focused on educational and productivity applications, including tutoring in web development, algorithm explanation, Excel formulas, social media strategies, and mental health support.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The ChatGPT Prompts dataset offers a broad collection of prompts covering diverse topics, designed for use with GPT 3.5. Its value lies in providing versatile, real-world prompt examples that support research on prompt engineering and AI interaction across various domains.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The ChatGPT Prompts dataset originates from a web application offering a diverse set of prompts generated by OpenAI&#8217;s GPT-3 model. These prompts serve multiple research purposes, including natural language generation, prompt engineering, and AI-driven creativity.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "generated",
                    "generation",
                    "prompts",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Chinese-DeepSeek-R1-Distill-data-110k is a 110K-entry Chinese dataset distilled from DeepSeek-R1, supporting text generation, text2text generation, and question answering under Apache-2.0. It covers four domains: Math (36 568 samples), Exam (2 432), STEM (12 648) and General (58 352). Each record includes input, reasoning content, output, source repo name and model-assigned score. Data originate from diverse math and instruction corpora, distilled via R1 with temperature 0.6, step-by-step math prompts, and validation using Math-Verify and Qwen2.5-72B.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "general",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Licensed under Apache-2.0, Chinese-DeepSeek-R1-Distill-data-110k-SFT is an open-source, Chinese-language instruction-tuning dataset distilled from DeepSeek-R1 outputs, formatted for direct supervised fine-tuning. It comprises 110K examples spanning math (36.6K), exam questions (2.4K), STEM (12.6K), and diverse general prompts (58.4K). Prompts are sourced from multiple Chinese math and STEM repositories, with distillation performed at temperature 0.6 and special step-by-step cues for calculations. Each sample includes integrated reasoning, answers, and model-based scores, facilitating reproducibility of high-performance SFT training. It supports text-generation, text-to-text generation, and question-answering tasks.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "general",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: CoCoNot is a novel English dataset for benchmarking and improving contextual noncompliance in chat-based language models. It offers three configurations: &#8220;original&#8221; contains 11K training and 1K test examples of user prompts that models should refuse; &#8220;contrast&#8221; comprises 379 test examples requiring compliant responses; and &#8220;pref&#8221; holds 927 preference-labeled training pairs contrasting optimal with noncompliant replies. Examples include metadata (id, category, subcategory, prompt, response) across five noncompliance categories. Developed by AI2, CoCoNot supports text-generation tasks aimed at refining models&#8217; refusal behavior.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "user",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: COIG-CQIA (Chinese Open Instruction Generalist - Quality is All You Need) is a high-quality, open-source Chinese instruction tuning dataset designed to align language models with human interactive behavior. It aggregates over 45,000 manually cleansed, restructured, and reviewed examples spanning social media dialogs, encyclopedic articles, exam questions, finance, medical, legal, traditional culture, and NLP tasks. Each entry includes instruction, optional input, output, task type, domain, and human verification metadata. COIG-CQIA aims to facilitate instruction fine-tuning for Chinese NLP research and applications.</p>\n\n",
                "matched_terms": [
                    "medical",
                    "domain",
                    "human",
                    "type",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Databricks-dolly-15K is an open-source corpus of over 15,000 human-generated instruction-response pairs created by Databricks employees across eight behavioral categories defined by InstructGPT, including brainstorming, classification, closed and open QA, generation, information extraction, and summarization. Provided under a CC-BY-SA 3.0 license, this English-language dataset supports academic or commercial use. With context passages drawn from Wikipedia when required, it enables training and fine-tuning of large language models, as well as synthetic data generation and data augmentation for robust, scalable instruction-following capabilities.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: DeepSeek-Prover-V1 is a large-scale synthetic proof dataset for Lean 4 theorem proving. It comprises 8 million formal statements and corresponding proofs generated from high-school and undergraduate-level mathematical contest problems. Natural language problems are translated into formal Lean 4 statements, filtered for quality, and paired with automatically generated proofs. Released under the deepseek-license, this dataset enables fine-tuning of large language models, improving whole-proof generation accuracy on benchmarks like miniF2F and FIMO. It supports research in formalized mathematical reasoning, automated theorem proving.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Size</span>: 87 datasets</p>\n\n",
                "matched_terms": [
                    "size",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: DialogStudio is a large-scale, unified collection of dialogue datasets curated to advance conversational AI. It integrates a wide range of domains&#8212;such as task-oriented dialogue, open-domain conversation, knowledge-grounded dialogue, and more&#8212;while preserving original metadata and structure. The dataset supports instruction-tuned training and evaluation across over 30 datasets with consistency. It includes model checkpoints (e.g., dialogstudio-t5-base-v1.0) and evaluation scripts using GPT-3.5 for quality metrics like coherence, completeness, and correctness. DialogStudio serves as a robust benchmark for multi-task generalization, instruction-following, and multi-domain dialogue modeling.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model",
                    "datasets",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: DMind_Benchmark is a comprehensive dataset for evaluating large language models on blockchain, cryptocurrency, and Web3 knowledge. It provides objective (multiple choice) and subjective (open ended) questions across nine domains: Fundamentals, Infrastructure, Smart Contracts, DeFi, DAOs, NFTs, Security, Tokenomics, and MEME coins&#8212;organized into CSV and JSONL splits. The benchmark supports diverse question types&#8212;calculations, code audits, risk and scenario analyses&#8212;with automated scoring and evaluation. It features standardized data configurations, leaderboards, and extensible evaluation pipelines for comparative analysis of LLM performance in specialized Web3 tasks.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "llm",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Dynosaur introduces a dynamic and low-cost paradigm for curating instruction-tuning datasets. It automatically generates diverse instructions by leveraging metadata from HuggingFace datasets, combined with LLM-based instruction synthesis (e.g., via ChatGPT). The result is Dynosaur-full, a large-scale dataset (800K+ samples, generated at &#160; $11.5) that supports dynamic growth and general-purpose instruction-tuning. Empirically, models fine-tuned on Dynosaur outperform Alpaca and GPT-4-Instruct baselines on Super-NI. The project includes: metadata crawling tools, instruction generation pipelines, and fine-tuned T5-3B and LLaMA-7B models. All generated instructions are under Apache 2.0, with task data adhering to original dataset licenses.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "generated",
                    "generation",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: \"Exploring the Possibilities of AI Prompts Over 200 Ideas\" is a comprehensive dataset featuring over 200 prompts spanning diverse marketing and content creation domains such as blog writing, email marketing, social media ads, influencer campaigns, and copywriting.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Firefly is a Chinese instruction-tuning dataset comprising 1.15 million high-quality examples drawn from 23 common Chinese natural language processing datasets. Each example includes a task type, an input prompt, and a target output, ensuring diverse coverage. Data templates were manually designed for each task to ensure quality and richness. Token length analysis shows that most examples are under 600 tokens. Firefly was used to train the Firefly-1b4 Chinese dialogue LLM, available on GitHub and Hugging Face, fostering reproducibility, community collaboration.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "datasets",
                    "analysis",
                    "llm",
                    "type",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Size</span>: 62 datasets</p>\n\n",
                "matched_terms": [
                    "size",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The FLAN Instruction Tuning Repository provides datasets and code to generate instruction tuning collections that improve language model generalization and zero-shot performance. Originating with FLAN 2021 and expanded in the FLAN Collection, this resource supports research on fine-tuning methods that enable large models to better follow human instructions. It underpins influential models like FLAN-T5 and FLAN-PaLM, facilitating advances in instruction-based learning and enabling systematic exploration of tuning strategies for enhanced natural language understanding.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Size</span>: 1836 datasets</p>\n\n",
                "matched_terms": [
                    "size",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset aggregates tasks from Flan, T0, Super-Natural Instructions, Chain-of-Thought, and Dialog into a training split. Each task is provided in zero-/few-shot and option/no-option formats as JSONL entries including inputs, targets, and task identifiers. Released under Apache-2.0, it includes scripts for building dependencies, fixing version mismatches, and exporting per-task JSONL data. Mixing ratios can be tuned for optimal downstream performance via guidelines in the associated paper and public GitHub repository.</p>\n\n",
                "matched_terms": [
                    "entries",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Flan-mini is a curated 1.34 M-example subset of the FLAN instruction-tuning collection augmented with code and conversational tasks. It pools 388K Flan2021 instructions, 320K public prompt templates, 200K Natural Instructions v2 instances, 100K chain-of-thought examples, plus code datasets (100K Code Search, 50K Code Contests, 50K APPS). It further integrates 132K ChatGPT-generated examples from GPT-4-Alpaca, Code-Alpaca, and ShareGPT. Each example is randomly paired with handcrafted prompt templates for zero- or few-shot fine-tuning, ensuring diverse task coverage. Released under a permissive CC license.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "sharegpt",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The GPT4All dataset comprises 437,604 English prompt-response pairs drawn from diverse sources to facilitate training and fine-tuning of open-source text generation models. It pairs user prompts with AI-generated replies and source metadata, covering various topics and styles. Released under Apache-2.0, the training split occupies approximately 782 MB on disk and requires 398 MB download. Curated by Nomic AI, GPT4All supports reproducible research in conversational AI. Hosted on GitHub with an accompanying technical report. It includes benchmarks along with extensive tests.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "user",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: GraphWalks is an open-source benchmark dataset designed to evaluate multi-hop reasoning over long graph contexts. Released under the MIT license, it provides directed graphs as edge lists alongside user-specified operations&#8212;such as breadth-first searches or parent retrieval&#8212;for models to execute. Each prompt comprises three demonstration examples, a target graph, and a query, with expected outputs formatted as node ID lists. Accompanying metadata includes prompt character counts and problem types. Standardized extraction and F1-based grading scripts ensure consistent answer parsing and evaluation.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: GSM8K (Grade School Math 8K) is an English monolingual dataset of 8.8K crowd-sourced grade school math word problems paired with multi-step solutions. It contains a main configuration and a Socratic variant, each offering questions and answers with calculator annotations and step-by-step reasoning expressed in natural language. Problems require two to eight elementary arithmetic steps. Split into training (7,473 examples) and test (1,319 examples), GSM8K supports text-to-text generation benchmarks under MIT license. All annotations were crowdsourced via Upwork and Surge AI.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Human ChatGPT Comparison Corpus (HC3) is the first large-scale bilingual dataset enabling direct comparison of human and ChatGPT-generated text. Spanning English and Chinese samples, it encompasses between 10,000 and 100,000 prompt-response pairs covering tasks such as text classification, question-answering, sentence similarity, and zero-shot classification. Released under a CC-BY-SA license, HC3 supports research in performance evaluation, detection, and analysis of AI-generated content. Accompanying code, models, and benchmarks are available on GitHub, facilitating open science, reproducible experimentation, and collaborative, community-driven global efforts.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "human",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: hh-rlhf provides valuable human preference data focused on helpfulness and harmlessness for training safer AI assistants using Reinforcement Learning from Human Feedback. It includes paired comparison data from base and iterated models, as well as red teaming transcripts designed to expose model vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Size</span>: 59 datasets</p>\n\n",
                "matched_terms": [
                    "size",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: InstructDial is a comprehensive instruction tuning framework designed to improve zero-shot and few-shot generalization in dialogue systems. It unifies 48 diverse dialogue tasks from 59 datasets into a text-to-text format, enabling models to learn across multiple dialogue-related functions such as understanding, generation, and intent detection.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: InstructWild is a large-scale, user-sourced instruction dataset comprising over 110K high-quality, diverse instructions collected from real ChatGPT usage shared on social media. Unlike previous synthetic datasets, InstructWild emphasizes authentic, varied user intents without relying on self-generated instructions. It supports both English and Chinese and enhances model capabilities in generation, open-domain QA, and creative thinking. This dataset provides a valuable resource for instruction tuning, advancing large language model generalization with naturally occurring user prompts.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "datasets",
                    "user",
                    "prompts",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Intellect-2-RL-Dataset is a large-scale collection of 284,741 training examples, designed for reinforcement learning in mathematical and coding problem solving. Each entry includes a unique problem_id, a task_type label, the problem prompt, verification_info detailing solution validity, and a baseline solve_rate from the Qwen-R1-Distill-7B model. Released under Apache-2.0 license, this dataset supports fine-tuning and evaluation of reasoning-oriented language models, facilitating research on algorithmic proficiency and reward-driven optimization within distributed asynchronous RL frameworks.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LaMini-Instruction is an English text-to-text generation dataset comprising 2.58M instruction-response pairs distilled from GPT-3.5-Turbo. Each sample includes an instruction, a corresponding model-generated response, and the instruction&#8217;s provenance&#8212;drawn from sources such as Alpaca, FLAN, P3, and Self-Instruct. Released under CC-BY-NC 4.0, it spans a single training split of over 1.16 GB and supports fine-tuning of compact language models. LaMini-Instruction enables research in instruction-based learning but inherits biases and errors from its GPT-3.5 teacher.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LCCC (Large-scale Cleaned Chinese Conversation Corpus) is a monolingual Chinese dialogue dataset with over 12 million conversations collected from social media. A strict and rigorous cleaning pipeline&#8212;including manual rules and classifier-based filters&#8212;removes noisy utterances such as offensive language, emojis, special symbols, ungrammatical or incoherent exchanges. The base configuration offers 6.8 M training samples with 20 K validation and 10 K test dialogues, while a larger variant provides 12 M training instances. Licensed under MIT, LCCC supports two key tasks: response generation and retrieval.</p>\n\n",
                "matched_terms": [
                    "key",
                    "generation",
                    "dataset",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The LIMA dataset contains 1,000 high-quality prompt-response pairs designed to align language models with the style of a helpful AI assistant. Prompts are diverse, sourced from Stack Exchange, wikiHow, WritingPrompts, Natural Instructions, and manually authored examples. Despite limited size (&#160;750K tokens), all responses are stylistically consistent. The dataset includes a 50-example development set and a 300-prompt test set. LIMA demonstrates that small, curated datasets can be highly effective for instruction tuning and alignment of pretrained language models.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "prompts",
                    "size",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Llama-Nemotron-Post-Training-Dataset is a comprehensive dataset of synthetic SFT and RL samples designed to bolster reasoning, code, math, science, chat, and safety capabilities for NVIDIA&#8217;s Llama-3 Nemotron series. It includes over 33M SFT examples across code, math, science, chat, and safety, plus 56K instruction-following RL examples. Data is sourced from public corpora or synthetically generated, filtered for quality and complexity. Released under CC-BY-4.0, it supports training and evaluation of efficient open-source LLMs offering a flexible accuracy-efficiency tradeoff and transparent development.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LMSYS-Chat-1M is a large-scale dataset of one million real-world LLM conversations, collected from 210K users interacting with 25 models via Chatbot Arena and Vicuna demo (April-August 2023). Each conversation includes model metadata, OpenAI-style JSON formatting, language tags, and moderation labels. Personally identifiable information is redacted. This dataset enables research on LLM alignment, safety, evaluation, and user behavior in the wild, offering unique insights into real-world usage patterns and content moderation challenges in multi-model deployment scenarios.</p>\n\n",
                "matched_terms": [
                    "conversation",
                    "user",
                    "llm",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LongForm is a 27K-example English instruction-following dataset under MIT license, for tasks like table QA, summarization, text generation, question answering. It collects human-written documents from C4 (10K) and Wikipedia (5K), reverse-engineered instructions via LLMs, and structured sources including Stack Exchange (4.4K) and WikiHow (2.5K). It also covers QA, email writing, grammar correction, story/poem generation and summarization from NIv2, Big Bench, BEA-GEC, Enron. Split into 23.6K train, 2K validation and 2K test, it supports instruction tuning and is publicly available.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Math CoT Arabic English Reasoning is a bilingual dataset of 1K-10K meticulously curated English and Arabic math problems with explicit chain-of-thought solutions. Spanning 21 categories from arithmetic to topology and logic, it offers human-verified, step-by-step reasoning examples in parallel languages. Structured in JSON with questions, answers, comprehensive metadata, category labels, and word counts, it supports question-answering, text generation, and mask-filling benchmarks. Licensed under MIT, it&#8217;s ideal for robust multilingual mathematical reasoning research, cross-lingual model evaluation, and educational AI assistant development.</p>\n\n",
                "matched_terms": [
                    "model",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: medical-o1-reasoning-SFT is a supervised fine-tuning dataset designed to enhance advanced medical reasoning in HuatuoGPT-o1. It comprises English and Chinese instruction-response pairs generated by GPT-4o on verifiable clinical problems, validated by a medical verifier. Released under an Apache-2.0 license, the dataset supports question answering and text generation, offering separate configurations for monolingual and mixed-language data. It aims to refine model performance on complex biomedical tasks by leveraging rigorous problem-solving chains, with full details available in the accompanying paper and GitHub repository.</p>\n\n",
                "matched_terms": [
                    "medical",
                    "generated",
                    "generation",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: medical-o1-verifiable-problem is an Apache-2.0 licensed dataset comprising open-ended medical reasoning problems designed to improve large language models&#8217; diagnostic and procedural knowledge. It supports question-answering and text-generation tasks, presenting each instance as a challenging exam-style prompt paired with a verifiable, expert-derived answer. Published in English under a single default configuration with training data provided in JSON format, it allows systematic evaluation and refinement of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "llm",
                    "medical",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Medical-R1-Distill-Data is an Apache-2.0 licensed instruction fine-tuning dataset distilled from Deepseek-R1&#8217;s Full Power Version using medical verifiable problems sourced from HuatuoGPT-o1. It supports English and Chinese, and is tailored for question-answering and text-generation tasks in medical and biology domains. The dataset captures reasoning chains from the native Deepseek-R1 API, facilitating model initialization with robust medical reasoning. A Chinese counterpart is available separately. Methodology and guidelines are provided in the associated paper and GitHub repository. It comprises SFT examples from medical_r1_distill_sft.json.</p>\n\n",
                "matched_terms": [
                    "model",
                    "medical",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: MedReason is a large-scale medical reasoning dataset combining seven clinical question-answer sources with a structured knowledge graph to produce detailed chains of reasoning. It contains 32,682 QA pairs, each annotated with step-by-step explanatory &#8220;thinking paths&#8221; derived from standardized medical KG relations. Designed to enhance the faithfulness and interpretability of medical problem-solving in large language models, MedReason enables fine-tuning of models such as MedReason-8B, which demonstrates state-of-the-art performance. Released under Apache-2.0, this open-source dataset aims to foster transparent medical QA systems.</p>\n\n",
                "matched_terms": [
                    "seven",
                    "medical",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: MedTrinity-25M is a large-scale multimodal medical dataset featuring over 25 million images from 10 imaging modalities. It provides multigranular annotations for 65+ diseases, including textual descriptions, bounding boxes, segmentation masks, and inter-region relationships. Supporting both vision-centric and multimodal tasks like classification, segmentation, and report generation, it facilitates large-scale pretraining for medical foundation models. Public access includes an 18M image-text pair subset. The dataset is organized in shards with structured metadata for scalable research and development.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "medical",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: MMInstruct-GPT4V is a multilingual multi-modal instruction tuning dataset for visual question answering and image captioning, licensed under Apache-2.0. It comprises three configurations&#8212;qa_en, caption_en, and caption_cn&#8212;covering English QA (&#160;216K examples), English captions (&#160;18K examples), and Chinese captions (&#160;144K examples) in JSONL train splits. Total size ranges between 100K and 1M instances. Designed to leverage GPT-4V for high-quality instruction generation, it supports both one-shot and multi-round interactions, enabling robust supervised fine-tuning of vision-language models targeting visual-question-answering and question-answering tasks with enhanced robustness.</p>\n\n",
                "matched_terms": [
                    "size",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: MOSS_002_sft_data is an open-source bilingual conversational dataset designed for fine-tuning MOSS-002. It encompasses over one million samples in English and Chinese across five splits&#8212;helpfulness, honesty and harmlessness&#8212;totaling 2.16 GB of text. User prompts are expanded from human-written seeds via a Self-Instruct-style pipeline, while model responses are synthesized with text-davinci-003. Harmlessness examples in English leverage Anthropic&#8217;s red-teaming attempts. Licensed under CC-BY-4.0, the resource supports text-generation and conversational modeling research within the 1-10 M size category. It is accessible via GitHub and homepage.</p>\n\n",
                "matched_terms": [
                    "size",
                    "user",
                    "prompts",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenAI MRCR (Multi-round co-reference resolution) is a long-context benchmark evaluating LLMs&#8217; ability to find multiple identical requests (&#8220;needles&#8221;) hidden within multi-turn conversations. Inspired by Gemini&#8217;s MRCR, it embeds 2, 4, or 8 duplicate prompts (e.g., &#8220;Write a poem about tapirs&#8221;) among distractors, prompting models to retrieve the i-th instance. It comprises 438 entities, 10 writing formats, and 100 samples per bin across eight token-based bins up to one million tokens. Evaluation uses SequenceMatcher ratio and mandates an alphanumeric hash prefix.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "multiturn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Size</span>: 61 datasets</p>\n\n",
                "matched_terms": [
                    "size",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: NATURAL INSTRUCTIONS is a monolingual English dataset derived from Super-Natural-Instructions, offering 1,600+ NLP tasks for training, validation, and testing. Size ranges between 100 million and one billion examples. Curated by crowdsourced and expert annotators, it covers classification, generation, and reasoning across reading comprehension, commonsense, summarization, arithmetic, logic, and dialog. With over 100 M examples, it provides diverse input-output mappings while enabling deduplication by unique IDs or input fields. Tasks span question answering, text modification, summarization, and beyond, supporting robust instruction-following model development.</p>\n\n",
                "matched_terms": [
                    "model",
                    "size",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The New Yorker Caption Ranking dataset comprises over 250 million massive crowdsourced humor ratings on more than 2.2 million captions collected from eight years of New Yorker cartoon caption contests. Structured into description, ranking, and cartoon subsets, it provides multimodal inputs paired with human preference judgments for training and evaluating creative text-generation models. The dataset supports rigorous benchmark development using human and GPT-4 assessments, showing current fine-tuning methods underperform top human contestants. Licensed under CC-BY-NC-4.0 and accessible via Hugging Face.</p>\n\n",
                "matched_terms": [
                    "human",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: No Robots is a high-quality, human-curated instruction dataset comprising 10,000 examples for supervised fine-tuning of language models. It includes 9,500 training and 500 test instances across ten single-turn categories&#8212;Generation, Open QA, Brainstorm, Chat, Rewrite, Summarize, Coding, Classify, Closed QA, and Extract&#8212;totaling roughly 17 MB of English text under CC-BY-NC-4.0. Each example consists of a prompt with unique ID, structured message history (system, user, assistant), and category labels. It enables models to learn diverse instruction-following behaviors and robustly supports reproducibility.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "user",
                    "singleturn",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: NuminaMath-1.5 is an open-source, large-scale post-training dataset comprising about 900 000 competition-level mathematics problems paired with chain-of-thought solutions. It covers diverse sources&#8212;from Chinese high school exams to US and international Olympiads&#8212;and spans domains like algebra, geometry, number theory, combinatorics, calculus, and puzzles. Each entry includes metadata fields (answer, problem_type, question_type) for verifiable outputs. Recent additions feature manually verified Olympiad references and curated contest data while synthetic problems were removed. Licensed under Apache 2.0, NuminaMath-1.5 supports advanced text-generation research in mathematical reasoning.</p>\n\n",
                "matched_terms": [
                    "number",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenAssistant Conversations (OASST1) is a human-generated, human-annotated corpus with 161,443 messages in 66,497 conversation trees across 35 languages. It includes over 461,000 quality ratings and more than 10,000 fully annotated trees. Each record contains metadata (IDs, timestamps), conversational structure (parent and tree IDs), role and language labels, toxicity and quality scores, emoji labels. Data comes in nested JSONL or flat parquet via HuggingFace, with 84,437 training and 4,401 validation splits, supporting supervised fine-tuning and reward model development. Licensed under Apache-2.0.</p>\n\n",
                "matched_terms": [
                    "model",
                    "oasst1",
                    "conversation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Open Instruction Generalist (OIG) is a large-scale instruction-tuning dataset released under Apache-2.0 license. It comprises 44 million JSONL entries pairing human instructions with model responses for continued pretraining, accompanied by a smaller high-quality subset (OIG-small-chip2) optimized for finetuning. OIG unifies diverse sources&#8212;ranging from Wikipedia dialogs, math problems, and code examples to summarization and question-answering corpora&#8212;into a consistent format. Designed to transform pretrained models into instruction-following agents, it supports scalable development of helpful language systems and targets one trillion tokens of instructions.</p>\n\n",
                "matched_terms": [
                    "entries",
                    "model",
                    "human",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OL-CC is the first open source Chinese conversational instruction dataset collected via crowdsourcing on OpenLabel. It includes 10,006 instruction-answer pairs and 1,649 standalone instructions across tasks such as question-answering, text generation, extraction, rewriting, classification, brainstorming, chit-chat, logic and math. A total of 276 volunteers alternately played user and AI assistant roles to produce the data. Licensed under Apache-2.0 and sized between 10K and 100K examples, OL-CC offers rich, human-generated Chinese instructional dialogues for AI research.</p>\n\n",
                "matched_terms": [
                    "user",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenCodeInstruct is a large-scale open-access instruction tuning dataset for code language models provided under the CC-BY-4.0 license. It comprises five million examples across generic and algorithmic coding tasks, with fields including id, input, output, domain, generation_algorithm, llm_judgement, unit_tests, tests_execution_status, and average_test_score. It supports supervised fine-tuning of code models and is accessible via the HuggingFace datasets library. Developed by NVIDIA for research and use, it accelerates code generation benchmarks and model evaluation.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "domain",
                    "datasets",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenCodeReasoning is a large-scale synthetic dataset designed to distill reasoning capabilities for Python-based competitive programming. It comprises 735,255 samples covering 28,319 unique problems sourced from platforms like CodeForces, AtCoder, and LeetCode. The dataset features two configurations: split_0 includes full problem statements and model responses, while split_1 references external datasets via index placeholders. Each example contains identifiers, source metadata, difficulty labels, and code solutions. Licensed under CC-BY-4.0, OpenCodeReasoning supports supervised fine-tuning of language models for code generation tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model",
                    "generation",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenMathReasoning is a large-scale English math-reasoning dataset (cc-by-4.0) comprising 290K+ olympiad problems with 3.2M chain-of-thought (CoT), 1.7M tool-integrated reasoning (TIR), and 566K GenSelect solution samples. Sourced from AoPS and processed with Qwen2.5-32B, DeepSeek-R1, and QwQ-32B, each record includes problem statements, generated solutions, expected answers, inference modes, metadata, and pass-rate metrics. Available in cot, tir, and genselect splits, it underpins state-of-the-art LLM training and evaluation in question-answering and text-generation.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "llm",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenOrca is an open English dataset licensed under MIT that augments the FLAN Collection with over 4 million GPT-3.5 and GPT-4 responses. It provides system prompts, questions, and AI-generated answers with detailed reasoning traces in tabular format. Tailored for a wide range of tasks including conversational modeling, classification, summarization, question-answering, and zero-shot scenarios. OpenOrca facilitates instruction tuning and reproducible research, powering high-performing models in NLP.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Open-Platypus is a composite English dataset containing 24,926 instruction-input-output examples across logic and reasoning tasks. Sourced from ten benchmarks&#8212;including PRM800K, MATH, ScienceQA, SciBench, ReClor, TheoremQA and Leetcode solutions&#8212;it employs sentence-transformer filtering to ensure &lt;80% question similarity and removes &#160;200 contaminated items. It supports refinement of large language models&#8217; logical reasoning and scientific problem-solving, serving as the core training corpus for Platypus2. License terms vary across components; see individual sources for details.</p>\n\n",
                "matched_terms": [
                    "removes",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenPrompt is a dynamic collection of the most popular prompts curated from OpenPrompt.co, updated daily to reflect trending and effective prompt engineering techniques. The dataset, available in JSON format, captures user preferences and evolving best practices for prompt design across diverse NLP applications.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "user",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Phoenix-sft-data-v1 is a multilingual supervised fine-tuning dataset containing 464,510 samples, combining instruction-following and ChatGPT-distilled conversation data. It includes Alpaca-derived tasks, post-translated multilingual instructions, and user-centered prompts in 40 languages. The dataset also integrates ShareGPT and Discord-sourced dialogues. With nearly 1 million conversation turns and detailed multilingual annotations, it supports multilingual language modeling, alignment, and chat adaptation. English and Chinese dominate the corpus, with broader linguistic diversity represented across the remaining data, enabling robust multilingual model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "conversation",
                    "sharegpt",
                    "prompts",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: PHYBench is a 500-problems physics benchmark evaluating large language models&#8217; physical perception and multi-step reasoning across mechanics, electromagnetism, thermodynamics, optics, modern, and advanced physics. It offers 100 fully-annotated examples with handwritten solutions and 400 question-only items. Problems require symbolic, LaTeX-formatted answers assessed via the novel Expression Edit Distance (EED) metric for partial correctness. A rigorous three-stage validation pipeline ensures originality and clarity. PHYBench reveals substantial gaps between state-of-the-art models and human baselines and supports in-depth error analysis and leaderboard tracking.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: PLM-Video Human is a large-scale human-annotated video understanding dataset for Vision-Language Model training, covering four tasks: fine-grained video question answering (FGQA) with 2.3M QA pairs, region-based video captioning (RCap), dense captioning (RDCap), and temporal localization (RTLoc). Each config provides annotated clip segments with questions, answers, captions, masks, start/end frames, and metadata drawn from diverse open-access sources. Released under CC-BY-4.0, PLM-Video Human supports detailed temporal, spatial, and semantic modeling of complex human activities across diverse realistic dynamic video scenarios.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The PRISM Alignment Dataset is a large-scale human feedback resource designed to assess preference and value alignment in large language models (LLMs). It consists of detailed survey responses from 1,500 participants across 75 countries, followed by multi-turn conversations with 21 LLMs. Participants rate model outputs on a 1-100 scale and provide fine-grained feedback, yielding 8,011 conversation trees and 68,371 scored utterances. The dataset includes four JSONL configurations&#8212;survey, conversations, utterances, and metadata&#8212;licensed under CC-BY and CC-BY-NC for research and educational use.</p>\n\n",
                "matched_terms": [
                    "conversation",
                    "multiturn",
                    "human",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering and Responses Dataset</span>\n</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset facilitates the study of prompt engineering by examining how different prompt types&#8212;questions, commands, and open-ended statements&#8212;influence generated text responses. With over 5,000 records, it enables analysis of prompt effectiveness across natural language generation, conversational agents, and sentiment influence.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "generated",
                    "generation",
                    "analysis",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: PromptGenius is a comprehensive, multilingual prompt dataset structured by usage scenarios, facilitating efficient retrieval across domains like academic research, content creation, and office tasks. It continuously collects popular, high-quality prompts to enhance productivity and offers model output examples to improve prompt design.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Publisher</span>: Prompt Hackers</p>\n\n",
                "matched_terms": [
                    "publisher",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Prompt Hackers is an open platform for sharing prompts categorized across diverse domains including writing, music, marketing, health, gaming, education, coding, and business.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "business"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Prompt-in-context-learning from EgoAlpha Lab offers an open-source engineering guide focused on mastering prompt engineering and in-context learning with large language models like ChatGPT, GPT-3, and FlanT5. Featuring a curated collection of 103 diverse prompts, it provides valuable, up-to-date resources for understanding how contextual prompts influence model behavior and performance.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: PromptSet is a novel dataset containing over 61,000 unique developer-written prompts integrated within open-source Python projects. It highlights the emerging practice of structured prompting as a core component of application logic alongside traditional code.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Size</span>: 660 datasets</p>\n\n",
                "matched_terms": [
                    "size",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: PromptSource is a comprehensive toolkit designed for creating, sharing, and using natural language prompts, facilitating zero-shot and few-shot learning research with large language models. It hosts the Public Pool of Prompts (P3), containing around 2,000 English prompts for over 170 datasets. By providing a simple templating language (Jinja) and API, PromptSource enables reproducible prompt engineering and systematic evaluation, supporting advances in multitask fine-tuning and zero-shot generalization across diverse NLP tasks.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: QuickRef.ME is a prompt-sharing platform that compiles a comprehensive ChatGPT cheatsheet, aggregating prompts and usage tips from global sources. It serves as a practical resource for researchers and practitioners to understand effective prompt formulation and optimize interactions with large language models.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: RedGPT Dataset (V1-CN) offers 50,000 automatically generated multi-turn Chinese dialogues grounded in high-quality factual references from diverse domains such as history, science, law, and culture. Designed to enhance GPT models&#8217; factual accuracy, the dataset enables fine-tuning on realistic, knowledge-rich conversational data without costly manual annotation. It supports research in improving language models&#8217; truthfulness, dialogue generation, and knowledge integration.</p>\n\n",
                "matched_terms": [
                    "multiturn",
                    "generated",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: RepLiQA is a specialized QA dataset of 71,820 human-created Context-Question-Answer triplets from fictitious, natural-looking documents across 17 topics (e.g., local news, folklore, cybersecurity). Designed to test LLMs&#8217; ability to leverage novel reference texts without relying on memorized facts, each document includes five questions with &#160;20% unanswerable. Fields include document IDs, topics, extracted text, questions, answers and long answers. Released under CC-BY-4.0 in four splits, RepLiQA supports question answering, text classification, topic retrieval and selective QA benchmarking.</p>\n\n",
                "matched_terms": [
                    "extracted",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Self-Instruct is an open Apache-2.0-licensed dataset and framework designed to enhance language models&#8217; instruction-following capabilities. It comprises four configurations: a self-generated set of 82K prompt-completion pairs produced via OpenAI&#8217;s davinci engine; 50K samples from Super Natural Instructions; 52K prompts drawn from the P3 public pool; and 252 expert-crafted human evaluation tasks with associated inputs and outputs. All data is in English and supports instruction-tuning by providing diverse natural-language prompts paired with corresponding model or human completions. The dataset facilitates instruction-tuning.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "human",
                    "prompts",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: ShareGPT4Video Captions Dataset offers a comprehensive collection of 4.8 million multimodal video captions generated by GPT4-Vision to improve alignment and fine-grained visual concept understanding in large video-language and text-to-video models. It comprises diverse subsets including the original 40K GPT4-Vision captions, 4,814K ShareCaptioner-Video outputs, and curated VQA and detailed caption mixes for supervised fine-tuning. Released under CC-BY-NC-4.0 in April 2024, it supports research in AIGC, computer vision, NLP, and multimodal AI development, bridging capabilities toward GPT4V and Sora benchmarks open-source releases.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: ShareGPT90K is a dataset of 90,665 conversational threads scraped from the ShareGPT platform. Each example includes a unique id and a sequence of messages, with each message annotated by its origin and its content.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: ShareGPT-Chinese-English-90k is a 90K-instance bilingual parallel human-machine QA dataset covering real and complex user inquiries in both Chinese and English. Licensed under Apache-2.0, it provides semantically aligned Chinese-English QA pairs for robust training of instruction-following dialogue and text-generation models. Unlike synthetic API-simulated corpora, all questions originate from genuine user interactions, preserving realistic instruction distributions. Collected through voluntary sharing, it naturally filters out low-quality exchanges. The dataset supports question-answering and text-generation tasks and can be easily loaded via the Firefly framework.</p>\n\n",
                "matched_terms": [
                    "user",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Skywork-OR1-RL-Data is a large-scale reinforcement learning dataset featuring 105,055 math problems and 14,057 coding questions curated for the Skywork-OR1 model series. Each example includes source attribution, structured prompts with roles, model-aware difficulty ratings for DeepSeek-R1 variants, and a reward model with ground truth and style labels. Problems are rigorously cleaned, deduplicated, and filtered by difficulty per variant. The dataset supports math and code splits totaling 1.5 billion bytes and facilitates robust reasoning training with rule-based RL recipes via curated pipelines efficiently.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Smart ChatGPT Prompts Awesome is a curated repository designed to enhance conversational AI development through carefully selected, effective prompts across diverse domains such as coding, academic writing, learning, and business.</p>\n\n",
                "matched_terms": [
                    "selected",
                    "prompts",
                    "business"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: SocialMaze is a question-answering benchmark designed to evaluate large language models&#8217; social reasoning via hidden role deduction games. Each scenario presents a multi-agent setup where agents (Investigators, Criminal, Rumormongers, Lunatics) make public statements over three rounds. Models receive system prompts and dialogues, then must identify the true Criminal and Player 1&#8217;s actual role. The dataset includes precise QA pairs, chain-of-thought reasoning, and supports easy (6-player) and hard (10-player) splits, facilitating fine-tuning, evaluation, and analysis of complex inference under deception. CC-BY-4.0 licensed.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "where",
                    "analysis",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: SPIRIT is a high-quality system prompt instruction dataset improving large language models&#8217; adherence to complex system prompts. It contains over 24,000 examples, including &#160;3,000 real-world system prompts extracted from open-source GitHub repositories and 21,639 synthetically generated conversation samples via a multi-agent GPT-4-based pipeline. Following the OpenAI message format, SPIRIT ensures compatibility with fine-tuning workflows. Human evaluations show models fine-tuned on SPIRIT outperform instruct baselines in prompt compliance. Released under the MIT License, SPIRIT is ideal for enhancing system prompt following.</p>\n\n",
                "matched_terms": [
                    "conversation",
                    "prompt",
                    "generated",
                    "human",
                    "extracted",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Size</span>: 1616 datasets</p>\n\n",
                "matched_terms": [
                    "size",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Cauldron is a large-scale benchmark that aggregates the training splits of 50 public vision-language datasets. It covers diverse tasks such as general and text-based VQA, chart and figure understanding, table question answering, document OCR, captioning, visual reasoning, screenshot-to-code, and image-pair comparison. Each example comprises one or more images paired with user-assistant dialogues in a conversational Q&amp;A format. Developed for fine-tuning the Idefics2 model, The Cauldron enables unified pretraining of architectures on a broad range of vision-language challenges and applications.</p>\n\n",
                "matched_terms": [
                    "general",
                    "model",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Publisher</span>: The Prompt Index</p>\n\n",
                "matched_terms": [
                    "publisher",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Prompt Index Prompt Database is a user-contributed repository featuring over 500 high-quality prompts spanning multiple domains, including SEO, content writing, coding, and more. This diverse dataset supports research in prompt engineering, cross-domain generalization, and AI-driven content generation.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraChat is an open-source, large-scale multi-round conversational dataset generated using two ChatGPT Turbo APIs under an MIT license. It comprises 1-10 million English dialogue turns across three sectors: world knowledge queries, creative writing and content generation, and assistance on existing materials such as rewriting, summarization, and inference. By simulating user and assistant interactions with carefully designed prompts, UltraChat ensures diverse, high-quality exchanges. Generated conversations undergo rigorous post-processing and filtering to safeguard privacy and maintain robust, realistic dialogue for text-generation research.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "generation",
                    "user",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraFeedback is an MIT-licensed, open-source, large-scale preference dataset designed for training reward and critic models. It contains 64 K prompts drawn from UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA and FLAN, each answered by four out of 17 diverse LLMs under five alignment principles. The result is 256 K responses and 380 K fine-grained annotations covering instruction-following, truthfulness, honesty and helpfulness, all rated by GPT-4. Its scale, diversity and dense numerical plus textual feedback make it ideal for RLHF research and robust reward-model development.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "sharegpt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraMedical is a large-scale English biomedical instruction dataset featuring over 409,000 examples licensed under MIT. Each sample includes an identifier, instruction type, multi-turn conversation pairs between human queries and GPT-generated responses, a ground-truth answer, and a model-evaluated score. The training split comprises roughly 1.2 GB across 410K examples, sourced from both curated public data and synthetic augmentations. UltraMedical aims to support the development of specialized generalist models in biomedicine by providing diverse, high-quality instruction-response instances, and comprehensive evaluation metrics accompany each instance.</p>\n\n",
                "matched_terms": [
                    "conversation",
                    "multiturn",
                    "human",
                    "type",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Unnatural Instructions is a large-scale dataset of automatically generated instruction-input-output triplets designed to facilitate instruction tuning of language models with minimal human effort. It contains over 240,000 examples, including original instructions, associated inputs, outputs, and optional constraints. Each instance also features multiple reformulations&#8212;paraphrased variants of instructions complete with inputs and outputs&#8212;to enhance model robustness. The publicly available training split comprises around 66,000 examples. This dataset supports research in instruction following, prompt paraphrasing, and evaluating model generalization across diverse complex tasks.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "generated",
                    "human",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: WebGLM-QA is an English monolingual dataset designed for question answering and text generation, used to train the WebGLM generator. It contains 43,579 training samples, 1,000 validation examples, and 400 test instances. Each record pairs a user-posed question with a generated answer and a list of reference snippets that support the response. Hosted on Hugging Face, it provides a consistent structure&#8212;question, answer, references&#8212;enabling work on dialogue systems, retrieval-augmented generation, and answer justification.</p>\n\n",
                "matched_terms": [
                    "generated",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Wizard_evol_instruct_196K is a MIT-licensed instruction-tuning dataset comprising 143K evolved QA pairs derived from Alpaca and ShareGPT. It represents an optimized version of the Evol-Instruct data used to train the WizardLM family of models. To assemble the complete instruction set of roughly 196K samples, users must merge this release with the original unfiltered ShareGPT dataset. The refined examples cover diverse conversational and instructional scenarios, facilitating improved alignment and performance in downstream open-source large language models, including structured prompts and responses.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "sharegpt",
                    "represents",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: wonderful-prompts is a curated collection of high-quality Chinese ChatGPT prompts designed to enhance usability and creativity in conversational AI applications. It offers diverse prompt templates covering coding, writing, productivity, art, and specialized expert roles, supporting research on prompt engineering and natural language interaction.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Size</span>: 82 datasets</p>\n\n",
                "matched_terms": [
                    "size",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: xP3 (Crosslingual Public Pool of Prompts) is a multilingual prompt and dataset collection spanning 46 languages and 13+ NLP tasks (e.g., QA, translation, summarization, code generation). Assembled from expert-generated and crowdsourced annotations under an Apache-2.0 license, it supports zero-shot and instruction-tuning for models like BLOOMZ and mT0. The training mixture covers closed-book and extractive QA, multiple-choice, paraphrase identification, program synthesis, sentiment analysis, structure-to-text, summarization, classification and more, totaling over 788 million samples. xP3 streamlines reproducible multilingual finetuning across diverse data scales.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "generation",
                    "analysis",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The ZeroSearch_dataset is a benchmark designed to evaluate and enhance large language models&#8217; search capabilities without performing external retrieval. Released under the Apache-2.0 license, it targets question-answering tasks that require models to infer answers using internal knowledge rather than querying outside sources. Created alongside the ZeroSearch framework, the dataset fuels research on incentivizing retrieval-like reasoning within LLMs. Researchers can obtain the dataset and related materials from the project page to benchmark model performance and spur advances in robust knowledge retrieval.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide the comparision of 3/4/5-grams for all datasets (except <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>, which is displayed in the main paper) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F6\" title=\"Figure 6 &#8227; F.1 Token-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, and the top-5 <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A6.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams comparison across datasets in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F7\" title=\"Figure 7 &#8227; F.1 Token-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams phrases of some datasets include abnormal content (e.g. &#8220;<span class=\"ltx_text ltx_font_italic\">identify which instrument be string</span>&#8221; in <span class=\"ltx_text ltx_font_sansserif\">dolly-15</span> and &#8220;<span class=\"ltx_text ltx_font_italic\">The quick brown fox jumps over the lazy dog</span>&#8221; in <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>), which indicates that there is a lot of repetition in the input content of the template tasks or some instructions used to construct the dataset, which may affect the balance of the dataset.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the complete experimental data for all identified dependency types, along with their proportions in the datasets, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.T5\" title=\"Table 5 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Additionally, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.T6\" title=\"Table 6 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> lists all detected Part-of-Speech tags and their corresponding proportions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F8\" title=\"Figure 8 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> further illustrates the ten most common verbs and their top five direct noun objects found in the prompt datasets except <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>, which are shown in the main paper.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "sharegpt",
                    "medicalo1",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These additional data further support our conclusions.\n<span class=\"ltx_text ltx_font_bold\">(1)</span> The <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> dataset, which consists of professionally crafted medical prompts, exhibits a relatively high proportion of numerical modifiers (nummod, 0.0276) and passive auxiliaries (auxpass, 0.0101) in dependency analysis, as well as a notably high usage of numerals (NUM, 0.0309) in POS tagging. These features reflect a terminology-dense and precision-oriented language style that emphasizes processes and outcomes rather than agents.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> In the <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> dataset, the verb-noun pairs reflect language commonly used in business contexts, such as <span class=\"ltx_text ltx_font_italic\">&#8220;create plan&#8221;</span> and <span class=\"ltx_text ltx_font_italic\">&#8220;create strategy&#8221;</span>. In contrast, the verb-noun pairs observed in <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> suggest more generic and broadly applicable usage scenarios.</p>\n\n",
                "matched_terms": [
                    "medical",
                    "11kbusiness",
                    "business",
                    "selfinstruct",
                    "medicalo1",
                    "analysis",
                    "boredhumans",
                    "oasst1",
                    "prompts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Anomalously, in the <span class=\"ltx_text ltx_font_sansserif\">dolly-15k</span> dataset, the most frequent verb-noun pairs exhibit a skewed distribution, with the highest-frequency nouns overwhelmingly associated with only the top one or two verbs. Moreover, these frequent verb-noun pairs often lack clear task-specific semantics&#8212;for example, <span class=\"ltx_text ltx_font_italic\">&#8220;tell i&#8221;</span>, <span class=\"ltx_text ltx_font_italic\">&#8220;give list&#8221;</span>, and <span class=\"ltx_text ltx_font_italic\">&#8220;classify each&#8221;</span>. This pattern may be attributed to the manual generation process, which is susceptible to the individual linguistic habits of annotators.</p>\n\n",
                "matched_terms": [
                    "dolly15k",
                    "generation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show the distribution of sampled embedding points after PCA for all datasets (except for <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>, which are shown in the main paper) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F9\" title=\"Figure 9 &#8227; F.3 Semantic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "medicalo1",
                    "datasets",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can still observe from the results that datasets with more concentrated topical focus (e.g., <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span>) exhibit clear clustering patterns, whereas those with broader thematic coverage (e.g., <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>) display a more dispersed distribution of data points.</p>\n\n",
                "matched_terms": [
                    "display",
                    "sharegpt",
                    "11kbusiness",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present detailed information for two case studies illustrating the effects of prompt optimization on language model responses. For each case, we include the prompts used both before and after optimization, as well as the responses generated by the language model for each version. The two questions are selected from <span class=\"ltx_text ltx_font_sansserif\">PRM800K</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Lightman et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib37\" title=\"\">2023</a>)</cite> and <span class=\"ltx_text ltx_font_sansserif\">MMLU-Pro</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib66\" title=\"\">2024c</a>)</cite></p>\n\n",
                "matched_terms": [
                    "prompt",
                    "generated",
                    "prompts",
                    "after",
                    "selected",
                    "model"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "Large Language Model Prompt Datasets: An In-depth Analysis and Insights",
        "caption": "Table 2: Top-8 dependency types, with the values indicating their proportions in the dataset. The dependency types represent syntactic relationships between words in a sentence: punctpunctuation marks; prepprepositions; detdeterminers (e.g., \"the\", \"a\"); pobjprepositional objects; dobjdirect objects; nsubjnominal subjects; and ROOTthe sentences main verb or predicate. Note that spaCys (en_core_web_sm) dependency labels do not entirely conform to the Universal Dependencies standard; non-conforming labels are represented with a dash (\"-\") in cross-corpus comparisons. Full data in Table5.",
        "body": "Dependency Type\nEWT\nParTUT\n1.1k-business\nBoredHumans\ndolly-15k\nmedical-o1\nOASST1\nSelf-Instruct\nShareGPT\n\n\n\n\npunct\n0.12\n0.12\n0.1227\n0.1985\n0.1445\n0.1216\n0.1273\n0.1863\n0.1540\n\n\nprep\n-\n-\n0.0759\n0.0672\n0.0866\n0.1013\n0.0816\n0.0676\n0.0764\n\n\ndet\n0.08\n0.09\n0.0518\n0.0692\n0.0961\n0.0906\n0.0841\n0.0838\n0.0693\n\n\npobj\n-\n-\n0.0718\n0.0620\n0.0817\n0.0979\n0.0760\n0.0645\n0.0711\n\n\nnsubj\n0.08\n0.06\n0.0596\n0.0545\n0.0650\n0.0469\n0.0739\n0.0596\n0.0562\n\n\nROOT\n0.07\n0.04\n0.0528\n0.0462\n0.0768\n0.0444\n0.0604\n0.0792\n0.0437\n\n\namod\n0.05\n0.06\n0.0573\n0.0527\n0.0469\n0.1072\n0.0523\n0.0384\n0.0480\n\n\ndobj\n-\n-\n0.0904\n0.0665\n0.0447\n0.0315\n0.0594\n0.0570\n0.0519",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dependency Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">EWT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ParTUT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">1.1k-business</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BoredHumans</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">dolly-15k</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">medical-o1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OASST1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Self-Instruct</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ShareGPT</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">punct</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1227</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.1985</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1445</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1216</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1273</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1863</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1540</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">prep</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.0759</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0672</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0866</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.1013</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0816</td>\n<td class=\"ltx_td ltx_align_center\">0.0676</td>\n<td class=\"ltx_td ltx_align_center\">0.0764</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">det</td>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.09</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0518</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0692</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0961</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0906</td>\n<td class=\"ltx_td ltx_align_center\">0.0841</td>\n<td class=\"ltx_td ltx_align_center\">0.0838</td>\n<td class=\"ltx_td ltx_align_center\">0.0693</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">pobj</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.0718</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0620</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0817</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0979</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0760</td>\n<td class=\"ltx_td ltx_align_center\">0.0645</td>\n<td class=\"ltx_td ltx_align_center\">0.0711</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">nsubj</td>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.06</td>\n<td class=\"ltx_td ltx_align_center\">0.0596</td>\n<td class=\"ltx_td ltx_align_center\">0.0545</td>\n<td class=\"ltx_td ltx_align_center\">0.0650</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0469</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0739</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0596</td>\n<td class=\"ltx_td ltx_align_center\">0.0562</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">ROOT</td>\n<td class=\"ltx_td ltx_align_center\">0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.04</td>\n<td class=\"ltx_td ltx_align_center\">0.0528</td>\n<td class=\"ltx_td ltx_align_center\">0.0462</td>\n<td class=\"ltx_td ltx_align_center\">0.0768</td>\n<td class=\"ltx_td ltx_align_center\">0.0444</td>\n<td class=\"ltx_td ltx_align_center\">0.0604</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0792</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0437</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\">amod</td>\n<td class=\"ltx_td ltx_align_center\">0.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.06</td>\n<td class=\"ltx_td ltx_align_center\">0.0573</td>\n<td class=\"ltx_td ltx_align_center\">0.0527</td>\n<td class=\"ltx_td ltx_align_center\">0.0469</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.1072</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0523</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0384</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0480</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">dobj</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.0904</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0665</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0447</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0315</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0594</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0570</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0519</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "dependency",
            "nonconforming",
            "11kbusiness",
            "main",
            "dash",
            "selfinstruct",
            "medicalo1",
            "data",
            "predicate",
            "oasst1",
            "labels",
            "sentence",
            "rootthe",
            "types",
            "represented",
            "spacys",
            "conform",
            "det",
            "sharegpt",
            "top8",
            "note",
            "verb",
            "entirely",
            "proportions",
            "root",
            "objects",
            "dobjdirect",
            "dobj",
            "nsubjnominal",
            "universal",
            "amod",
            "their",
            "encorewebsm",
            "syntactic",
            "subjects",
            "not",
            "dependencies",
            "crosscorpus",
            "comparisons",
            "prep",
            "pobj",
            "sentences",
            "punct",
            "standard",
            "ewt",
            "relationships",
            "prepprepositions",
            "dataset",
            "indicating",
            "nsubj",
            "represent",
            "partut",
            "dolly15k",
            "pobjprepositional",
            "words",
            "boredhumans",
            "values",
            "marks",
            "between",
            "full",
            "punctpunctuation",
            "type",
            "detdeterminers"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of Results.</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T2\" title=\"Table 2 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the distribution of eight common dependency types across seven prompt datasets and two reference corpora (EWT and ParTUT), revealing three key findings.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">A prompt is a natural language instruction that defines a specific task for a large language model (LLM) and serves as the primary interface for human-LLM interaction. With the growing deployment of LLMs, diverse prompt datasets are emerging from platforms such as GitHub and social media. These datasets span a wide array of applications and content types, facilitating both broader LLM utilization and improved prompt engineering.\nIn this work, we&#8211;for the first time&#8211;have compiled an extensive list of prompt datasets sourced from various channels, representing a spectrum of downstream tasks, languages, engineering techniques, attributes, and modalities. We select key representative datasets for systematic analysis, revealing commonalities and differences in prompt construction across categories, distinguishing them from other text corpora like literature and web.\nWe further propose a prompt optimization approach that leverages syntactic embeddings of part-of-speech and dependency structures. By identifying a centroid representation of prompts and guiding LLMs to rewrite prompts toward this centroid, our method improves the meaningfulness of model outputs.\nWe have made our datasets and code available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/LLM-Prompt-Datasets-7416\" title=\"\">https://anonymous.4open.science/r/LLM-Prompt-Datasets-7416</a>.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "syntactic",
                    "types"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, prior research has largely neglected comprehensive examinations of available prompt datasets. To address this gap, we apply stringent criteria to select, refine, and evaluate datasets that enable analysis of diverse prompts across multiple sources, content types, and target applications. Our survey encompasses over 1.22 TB of data, comprising more than 673M prompt instances from 129 heterogeneous sources. Our first contribution is a hierarchical taxonomy of LLM prompt datasets that serves as a detailed reference for researchers and informs future studies.</p>\n\n",
                "matched_terms": [
                    "types",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we propose a prompt optimization method leveraging part-of-speech and dependency embeddings. By aligning target prompts with a centroid of high-performing syntactic patterns, our approach improves the meaningfulness and quality of LLM responses. This data-driven method provides a foundation for more effective prompt selection and refinement in LLMs.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "syntactic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data discovery guideline.</span> We employ a systematic dataset discovery process across multiple sources to compile a diverse repository of prompt datasets. Our objective is to capture real-world, user-generated prompts, instruction-following interactions, and domain-specific scenarios. In particular, our primary objectives for datasets discovery are three-fold: <span class=\"ltx_text ltx_font_bold\">(1)</span> collecting datasets that are composed of prompts, i.e., natural language instructions that describe a certain task the LLM should perform and guide the LLM towards generating a desired output; <span class=\"ltx_text ltx_font_bold\">(2)</span> ensuring that the extracted data cover various domains, including day-to-day scenarios such as travel planning, professional scenarios such as academic writing, and specialized scenarios such as healthcare and finance; and <span class=\"ltx_text ltx_font_bold\">(3)</span> allowing different forms of prompts, e.g., single instruction, conversations, etc.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data discovery process.</span> We collect publicly available datasets from the following four types of sources. <span class=\"ltx_text ltx_font_bold\">First</span>, we consult <span class=\"ltx_text ltx_font_italic\">dataset collection platforms</span>, including Hugging Face Datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib12\" title=\"\">hug, </a>)</cite>, Kaggle <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib13\" title=\"\">kag, </a>)</cite>, Google Dataset Search <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib11\" title=\"\">goo, </a>)</cite>, and Papers with Code <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib14\" title=\"\">pap, </a>)</cite>. Targeted searches using keywords, e.g., \"prompt dataset\", \"instruction-following dataset\", and \"conversation dataset\" yield 60 prompt datasets. <span class=\"ltx_text ltx_font_bold\">Second</span>, we review the latest <span class=\"ltx_text ltx_font_italic\">academic publications</span>, specifically papers on prompt engineering, natural language understanding, and dialogue systems, published at NeurIPS, ICLR, and ICML between 2023-2024 and identify 73 datasets shared across them. <span class=\"ltx_text ltx_font_bold\">Third</span>, we also examine <span class=\"ltx_text ltx_font_italic\">public repositories</span> by systematically surveying open-source GitHub projects using keywords, e.g., &#8220;prompt collection&#8221;, &#8220;LLM prompts&#8221;, and &#8220;instruction dataset&#8221;. We identify 21 prompt repositories that typically contain curated prompt lists derived from user interactions or synthesized from public APIs. Some of these repositories are &#8220;awesome-lists&#8221;, which are curated collections of high-quality prompts or links to prompt datasets.\nNotable examples include <span class=\"ltx_text ltx_font_sansserif\">Awesome Instruction Datasets</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib46\" title=\"\">Nie, </a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering Guide</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Saravia, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib56\" title=\"\">2022</a>)</cite>, and <span class=\"ltx_text ltx_font_sansserif\">LLMDataHub</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib71\" title=\"\">Zhao, </a>)</cite>. <span class=\"ltx_text ltx_font_bold\">Finally</span>, we extract 14 datasets from <span class=\"ltx_text ltx_font_italic\">popular websites dedicated to prompt-sharing</span>, including <span class=\"ltx_text ltx_font_sansserif\">Prompt Genius</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Pro, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib7\" title=\"\">b</a>)</cite> and <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib10\" title=\"\">bor, </a>)</cite>. These platforms feature user-written prompts for practical purposes.</p>\n\n",
                "matched_terms": [
                    "types",
                    "data",
                    "boredhumans",
                    "between",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data filtering.</span>\nWe remove duplicate entries (e.g., CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib51\" title=\"\">2024</a>)</cite> appears in both Hugging Face and NeurIPS 2024) and then filter the remaining candidates using four quality criteria for inclusion in this paper.\n<span class=\"ltx_text ltx_font_bold\">First</span>, <span class=\"ltx_text ltx_font_italic\">Dataset size.</span> We prioritize datasets containing at least 1K prompts to ensure robustness in diversity and statistical power. In contrast, due to their generally limited scope, user-shared datasets are filtered with a minimum threshold of 50 prompts.\n<span class=\"ltx_text ltx_font_bold\">Second</span>, <span class=\"ltx_text ltx_font_italic\">Data quality.</span> We evaluate the quality of prompts based on their cleanliness. Most datasets (e.g., <span class=\"ltx_text ltx_font_sansserif\">OpenCodeReasoning</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib16\" title=\"\">2025b</a>)</cite>) on data hosting platforms (e.g., Hugging Face and Kaggle) are well-formatted and clean. For the remaining data, we exclude samples with inconsistent formatting or unclear structure. For instance, the <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering Guide</span>&#8211;a resource that offers both curated prompt datasets and instructional examples&#8211;contains many illustrative prompts scattered throughout the material and are thus omitted from our datasets.\n<span class=\"ltx_text ltx_font_bold\">Third</span>, <span class=\"ltx_text ltx_font_italic\">Data relevance.</span> We assess whether the prompts are aligned with our data discovery guidelines, specifically emphasizing on those that represent common usage scenarios for broad audiences (e.g., <span class=\"ltx_text ltx_font_sansserif\">Chinese-DeepSeek-R1-Distill-data-110k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib38\" title=\"\">2025a</a>)</cite>), and tasks from various domains (e.g., <span class=\"ltx_text ltx_font_sansserif\">Medical Verifiable Problems</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">OpenMathReasoning</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Moshkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib44\" title=\"\">2025</a>)</cite>). Datasets that violate our discovery guidelines are omitted. For instance, the <span class=\"ltx_text ltx_font_sansserif\">PersonaHub</span> dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib5\" title=\"\">Per, </a>)</cite> is excluded because it does not meet <span class=\"ltx_text ltx_font_bold\">data discovery guideline (1)</span>, which mandates that prompts be linked to specific, well-defined tasks. Although <span class=\"ltx_text ltx_font_sansserif\">PersonaHub</span> demonstrates the potential of synthetic personas in generating diverse content (e.g., reasoning problems, dialogues, or non-player character behaviors), it predominantly comprises persona descriptions without clear task formulation.\n<span class=\"ltx_text ltx_font_bold\">Fourth</span>, <span class=\"ltx_text ltx_font_italic\">Accessibility.</span> Datasets must be publicly accessible or retrievable via automated crawling, and their licensing terms must permit research use. After filtering, we identify 129 distinct prompt datasets for taxonomic analysis (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S4\" title=\"4 Dataset Taxonomy &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "represent",
                    "their",
                    "data",
                    "dataset",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Publisher</span> denotes the source&#8217;s identity and intent. We distinguish among <span class=\"ltx_text ltx_font_italic\">end users</span> who share prompts for practical tasks like writing/ coding (e.g., <span class=\"ltx_text ltx_font_sansserif\">Prompt Genius</span>), <span class=\"ltx_text ltx_font_italic\">LLM researchers</span> who publish prompts for fine-tuning and benchmarking (e.g., <span class=\"ltx_text ltx_font_sansserif\">OpenMathReasoning</span> by NVIDIA), and <span class=\"ltx_text ltx_font_italic\">domain scientists</span> who use LLMs in their specific fields (e.g., <span class=\"ltx_text ltx_font_sansserif\">ChatGPT Data Science Prompts</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib60\" title=\"\">Tang, </a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "their",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Release channel</span> refers to the platform where a dataset is published. Common platforms include <span class=\"ltx_text ltx_font_italic\">data hosting sites</span> such as GitHub, Hugging Face, and Kaggle, where structured prompt formats (e.g., CSV, JSON) dominate. <span class=\"ltx_text ltx_font_italic\">Personal sites</span> or <span class=\"ltx_text ltx_font_italic\">notes</span> (e.g., Notion workspaces) often host informal, user-oriented prompts. Dedicated <span class=\"ltx_text ltx_font_italic\">prompt sharing websites</span> vary from open-access (e.g., <span class=\"ltx_text ltx_font_sansserif\">QuickRef.ME</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib8\" title=\"\">Qui, </a>)</cite>) to commercial marketplaces (e.g., <span class=\"ltx_text ltx_font_sansserif\">PromptBase</span>). <span class=\"ltx_text ltx_font_italic\">Social media</span>, like Reddit&#8217;s <span class=\"ltx_text ltx_font_sansserif\">r/ChatGPTPromptGenius</span>, also plays a key role in community-driven prompt exchange.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generation process</span> describes how the prompts are created. <span class=\"ltx_text ltx_font_italic\">Human-generated prompts</span> are either manually authored (e.g., <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>)</cite>) or collected from user queries (e.g., <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib36\" title=\"\">Li, </a>)</cite>), <span class=\"ltx_text ltx_font_italic\">Model-generated prompts</span> include those created via self-instruct techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib65\" title=\"\">2023</a>)</cite> (e.g., <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>), multi-agent simulations (e.g., <span class=\"ltx_text ltx_font_sansserif\">AI Society</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib34\" title=\"\">2023a</a>)</cite>), or reverse instruction generation (e.g., <span class=\"ltx_text ltx_font_sansserif\">LongForm</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;ksal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib32\" title=\"\">2023</a>)</cite>). Finally, <span class=\"ltx_text ltx_font_italic\">derivative datasets</span> build on existing resources through task expansion or reformatted aggregation (e.g., <span class=\"ltx_text ltx_font_sansserif\">Flan 2022</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib4\" title=\"\">Fla, </a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">xP3</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib45\" title=\"\">2022</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "sharegpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt engineering</span> methods are critical for enhancing prompt performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Sahoo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib52\" title=\"\">2025</a>)</cite>. Common techniques include <span class=\"ltx_text ltx_font_italic\">few-shot</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Brown et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib20\" title=\"\">2020</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">role playing</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib69\" title=\"\">2018</a>)</cite>, <span class=\"ltx_text ltx_font_italic\">chain-of-thought</span> (CoT) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib67\" title=\"\">2022</a>)</cite>, and <span class=\"ltx_text ltx_font_italic\">rephrase-and-respond</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Deng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib27\" title=\"\">2023</a>)</cite>. Some datasets adopt a single method (e.g., <span class=\"ltx_text ltx_font_sansserif\">awesome-chatgpt-prompts</span> uses role playing), while others combine multiple techniques (e.g., <span class=\"ltx_text ltx_font_sansserif\">PromptBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib72\" title=\"\">2024</a>)</cite> integrates six methods), or leave the strategy unspecified.\nMoreover, datasets may include <span class=\"ltx_text ltx_font_bold\">extra attributes</span>, including <span class=\"ltx_text ltx_font_italic\">labeled data</span> (e.g., response supervision, safety labels), <span class=\"ltx_text ltx_font_italic\">analytical data</span> (e.g., token count, user behavior), <span class=\"ltx_text ltx_font_italic\">structural information</span> (e.g., timestamp, multi-turn context), and <span class=\"ltx_text ltx_font_italic\">ground truth</span> (e.g., gold answers, attribution references)&#8211;as seen in datasets including <span class=\"ltx_text ltx_font_sansserif\">hh-rlhf</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib18\" title=\"\">2022</a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">UltraFeedback</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Cui et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib26\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span>. <span class=\"ltx_text ltx_font_italic\">Additional tags</span> may indicate <span class=\"ltx_text ltx_font_italic\">usage context</span>, such as associated models, cultural regions, or domain specificity.\nFinally, datasets vary in <span class=\"ltx_text ltx_font_bold\">dynamicity</span> (i.e., static vs. dynamically updated) and <span class=\"ltx_text ltx_font_bold\">modality</span> (i.e., single-modal vs. cross-/multi-modal). For example, <span class=\"ltx_text ltx_font_sansserif\">awesome-chatgpt-prompts</span> is a dynamically updated prompts collection, while <span class=\"ltx_text ltx_font_sansserif\">PLM-Video-Human</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Cho et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib24\" title=\"\">2025</a>)</cite> supports multi-modal learning for video understanding.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among these, instruction fine-tuning datasets represent a prominent and widely-used subset of prompt datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib39\" title=\"\">2024</a>)</cite>. These datasets comprise instruction-response pairs, where the \"instruction\" serves as a prompt and the \"response\" represents the target model output. They are primarily employed for supervised fine-tuning to enhance model capability and controllability. As a result, models trained on these datasets exhibit superior alignment with human intent, improved instruction-following, and increased safety characteristics <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib70\" title=\"\">2023</a>)</cite>. Furthermore, the instructions in these datasets often reflect real-world user queries, making them both practical for deployment and valuable for prompt-related research. Notable examples are <span class=\"ltx_text ltx_font_sansserif\">Alpaca</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Taori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib61\" title=\"\">2023</a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;pf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib31\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_sansserif\">FLAN 2022</span>.</p>\n\n",
                "matched_terms": [
                    "represent",
                    "oasst1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to ensure reliable analysis of prompt characteristics, we curate multiple prompt-centric datasets with the following selection principles.\n<span class=\"ltx_text ltx_font_bold\">(1)</span> <span class=\"ltx_text ltx_font_italic\">Language consistency.</span> Only English-language data have been included to ensure uniform linguistic features and avoid cross-linguistic biases.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> <span class=\"ltx_text ltx_font_italic\">Exclusion of benchmark-style prompts.</span> Prompts designed for LLM performance evaluations (e.g., <span class=\"ltx_text ltx_font_sansserif\">PHYBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Qiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib48\" title=\"\">2025</a>)</cite>) are excluded to focus on natural usage scenarios.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> <span class=\"ltx_text ltx_font_italic\">Source and content diversity.</span> To achieve sufficient coverage and reduce sampling bias, we have selected datasets that differ in <span class=\"ltx_text ltx_font_italic\">publisher type</span> (i.e., end user vs. LLM researcher and domain scientist), <span class=\"ltx_text ltx_font_italic\">instruction generation method</span> (i.e., human vs. model generated), and <span class=\"ltx_text ltx_font_italic\">domain scope</span> (i.e., general vs. domain-specific tasks).</p>\n\n",
                "matched_terms": [
                    "type",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following our selection principles, we curated seven representative datasets spanning different user types, instruction methods, and domains. For <span class=\"ltx_text ltx_font_bold\">end users</span>, general-domain prompts include single-turn prompts (<span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span>) and multi-turn conversations (<span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>), while business-domain single-turn prompts are represented by <span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span>. For <span class=\"ltx_text ltx_font_bold\">LLM researchers</span>, human-generated datasets include <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> and <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and model-generated prompts are captured by <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>. For <span class=\"ltx_text ltx_font_bold\">domain scientists</span>, we include model-generated medical prompts from <span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span>. This collection ensures diversity in publisher type, prompt structure, and application domain.</p>\n\n",
                "matched_terms": [
                    "types",
                    "selfinstruct",
                    "represented",
                    "boredhumans",
                    "sharegpt",
                    "oasst1",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span> (<span class=\"ltx_text ltx_font_bold\">1.1k-business</span>). A curated dataset of 1&#8201;235 prompts oriented toward professional and business-related use cases, such as marketing, productivity, and decision-making. It represents structured, domain-specific prompting behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(1., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib1\" title=\"\">1</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> (<span class=\"ltx_text ltx_font_bold\">dolly-15k</span>).\nThis dataset includes 15&#8201;000 human-authored instruction&#8211;response pairs covering a range of everyday tasks. It is single-turn and domain-general, curated to support instruction-following models <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dolly15k",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span> (<span class=\"ltx_text ltx_font_bold\">medical-o1</span>).\nSynthetic data of 90&#8201;120 open-ended questions and GPT-4o generated CoTs and responses. Open-ended questions are reformatted by GPT-4o based on close-set medical examination questions. The dataset is used to fine-tune HuatuoGPT-o1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "medicalo1",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">OASST1.</span>\nThe Open Assistant dataset (OASST1) contains over 30&#8201;000 human-written messages arranged in dialogue trees. It emphasizes cooperative, open-domain assistant behavior and includes branching conversations rather than linear interactions <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;pf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib33\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "oasst1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">Self-Instruct.</span>\nA synthetic dataset with 82&#8201;646 prompts generated by large language models based on a small seed pool of human-written instructions. For every generation step, it samples 6 human-written tasks and 2 model-generated tasks in previous steps to promote diversity <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib65\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform token-level analysis using <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram models to capture local textual patterns <cite class=\"ltx_cite ltx_citemacro_citep\">(Jurafsky &amp; Martin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib30\" title=\"\">2000</a>; Cavnar &amp; Trenkle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib21\" title=\"\">1994</a>; Manning &amp; Schutze, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib42\" title=\"\">2001</a>)</cite>. Initially, all tokens are lemmatized to mitigate inflectional variability, after which we extract 3-gram, 4-gram, and 5-gram sequences to compute their frequency distributions. By analyzing high-frequency <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams, we identify prevalent instruction templates, keyword combinations, and syntactic patterns, laying the groundwork for subsequent syntactic and semantic investigations.</p>\n\n",
                "matched_terms": [
                    "their",
                    "syntactic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> High-frequency <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams reveal domain and prompt-engineering differences, such as role-playing cues in <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> (&#8220;you to act as&#8221;) versus medical reasoning in <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> (&#8220;what be the,&#8221; &#8220;the most likely diagnosis&#8221;). <span class=\"ltx_text ltx_font_bold\">(2)</span> While 3-grams capture general-purpose queries or commands (e.g., &#8220;what be the,&#8221; &#8220;I want to&#8221;), longer <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams (4&#8211;5) reflect task-specific patterns, as in <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> where frequent 5-grams (&#8220;please write in English language,&#8221; &#8220;write a comprehensive reply to&#8221;) highlight its instruction-following orientation. <span class=\"ltx_text ltx_font_bold\">(3)</span> Compared to Google Books 5-grams (e.g., &#8220;at the end of the,&#8221; &#8220;in whole or in part&#8221;) that serve narrative or descriptive purposes, prompt datasets exhibit inquiry- or command-focused <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams, underscoring a clear divergence in linguistic patterns across corpora.</p>\n\n",
                "matched_terms": [
                    "oasst1",
                    "sharegpt",
                    "medicalo1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To gain deeper insights into the linguistic structure of prompts, we perform syntactic analysis from three perspectives: dependency parsing <cite class=\"ltx_cite ltx_citemacro_citep\">(Nivre, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib47\" title=\"\">2003</a>)</cite>, part-of-speech (POS) tagging <cite class=\"ltx_cite ltx_citemacro_citep\">(Brill, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib19\" title=\"\">1992</a>)</cite>, and term frequency-inverse document frequency (TF-IDF) scoring <cite class=\"ltx_cite ltx_citemacro_citep\">(Salton &amp; Buckley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib54\" title=\"\">1988</a>)</cite>. These features are both descriptive and can be aggregated into vector representations for tasks like prompt classification.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "syntactic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparative analysis with non-prompt text datasets, we have used Universal Dependencies\ncorpora for English: EWT <cite class=\"ltx_cite ltx_citemacro_citep\">(Silveira et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib59\" title=\"\">2014</a>)</cite> and ParTUT <cite class=\"ltx_cite ltx_citemacro_citep\">(Sanguinetti &amp; Bosco, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib55\" title=\"\">2014</a>)</cite>, where EWT contains informal contents&#8211;blog, social, reviews, email, and web, and ParTUT contains more formal contents&#8211;legal, news, and wiki.</p>\n\n",
                "matched_terms": [
                    "partut",
                    "dependencies",
                    "ewt",
                    "universal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply the spaCy <span class=\"ltx_text ltx_font_typewriter\">en_core_web_sm</span> parser <cite class=\"ltx_cite ltx_citemacro_citep\">(Honnibal &amp; Montani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib29\" title=\"\">2017</a>)</cite> to extract syntactic dependencies and determine the frequency of key grammatical relations in each dataset. For the EWT and ParTUT corpora, we rely on officially published dependency type annotations. This analysis reveals systematic variations in linguistic style across prompt sources. Additionally, we track verb&#8211;object (dobj) pairs to capture the task-oriented diversity of the prompts (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "dobj",
                    "partut",
                    "encorewebsm",
                    "syntactic",
                    "dependencies",
                    "ewt",
                    "type",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> The <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> dataset is characterized by its high use of adjectival modifiers (amod, 0.11) and low direct object frequency (dobj, 0.03), reflecting a preference for precise, state-oriented descriptions over action-driven narratives, often framed through linking verbs&#8212;typical of medical contexts detailing conditions, symptoms, and diagnoses. <span class=\"ltx_text ltx_font_bold\">(2)</span> In contrast, the <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> dataset favors concise, goal-driven imperatives with bare noun phrases as direct objects (dobj, 0.09) and minimal use of determiners (det, 0.05), aligning with its project-planning focus. <span class=\"ltx_text ltx_font_bold\">(3)</span> Verb&#8211;noun dependency analysis further distinguishes domains: medical instructions cluster around technical, domain-specific pairs like &#8220;have history&#8221; and &#8220;experience pain,&#8221; while datasets such as <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> use broader, generic pairs like &#8220;write answer&#8221; and &#8220;use code&#8221;. These syntactic patterns highlight each corpus&#8217; thematic priorities and inform strategies for domain-aware model training.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "dobj",
                    "11kbusiness",
                    "amod",
                    "medicalo1",
                    "syntactic",
                    "det",
                    "sharegpt",
                    "objects",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We annotate the datasets with POS tags and calculate the distribution of nouns, verbs, adjectives, and adverbs. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T3\" title=\"Table 3 &#8227; 5.3.1 Dependency Parsing &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the functional composition of prompts, contrasting content and function words. For example, a high verb frequency indicates action-oriented prompts, while a predominance of nouns suggests more objective narratives. These distributional differences reveal stylistic and structural variations across sources.</p>\n\n",
                "matched_terms": [
                    "words",
                    "verb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1)</span> Domain-specific datasets such as <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> and <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> exhibit a noun proportion of <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 0.26, surpassing that found in formal corpora like ParTUT. This reflects a concept-driven focus on domain entities and technical terms.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> Additionally, <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> also registers an unusually high adjective ratio (0.11), indicating a repeated emphasis on specifying medical attributes and conditions, consistent with the descriptive nature of clinical reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "partut",
                    "medicalo1",
                    "11kbusiness",
                    "indicating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1) Intra-dataset analysis</span>\ndelineates each dataset&#8217;s lexical focus and stylistic characteristics. For instance, <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> emphasizes business-specific terms like &#8220;content&#8221; and &#8220;email&#8221;, while <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span> features imperatives such as &#8220;act&#8221;, indicative of role-playing instructions. Similarly, <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> shows a dominant TF-IDF score for &#8220;output&#8221; (0.772), highlighting a structural prompt style based on explicit instruction&#8211;response formats.\n<span class=\"ltx_text ltx_font_bold\">(2) Inter-dataset comparison.</span>\nTF-IDF vectors show varying overlaps across datasets. The highest cosine similarity between <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> suggests a similar vocabulary&#8212;likely due to shared human-generation processes. In contrast, <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> is lexically distant from the others, especially <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> and <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span>, reflecting stylistic and domain-specific differences.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "selfinstruct",
                    "medicalo1",
                    "boredhumans",
                    "sharegpt",
                    "oasst1",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze prompt semantics by encoding each prompt into a 384-dimensional dense vector using Sentence-BERT&#8217;s pretrained model <span class=\"ltx_text ltx_font_typewriter\">all-MiniLM-L6-v2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Reimers &amp; Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib50\" title=\"\">2019</a>)</cite>. Each prompt is encoded into a 384-dimensional dense vector that captures its semantic content. These embeddings serve as the foundation for classification, clustering, and visualization analysis. We perform Principal Component Analysis (PCA) to reduce sentence embeddings to two dimensions. For fair comparison, we uniformly at random sample 500 prompts per dataset and visualize their distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Semantic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "sentence",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1) Wide coverage in Self-Instruct:</span> The <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> dataset exhibits the most dispersed and evenly distributed semantic space, suggesting a broad topical coverage. This aligns with the self-instruction paradigm&#8217;s goal of generating diverse instruction types.\n<span class=\"ltx_text ltx_font_bold\">(2) Semantic cohesion in specific domains:</span> Prompts from <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> form more concentrated clusters, indicating domain-specific semantic cohesion.\n<span class=\"ltx_text ltx_font_bold\">(3) Overlap among human-generated sets:</span> The embeddings of <span class=\"ltx_text ltx_font_sansserif\">dolly-15k</span>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> overlap substantially across both PCA dimensions. This suggests that these datasets share stylistic and semantic characteristics, possibly due to their common reliance on human-LLM interactions for data generation.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "types",
                    "selfinstruct",
                    "their",
                    "medicalo1",
                    "data",
                    "oasst1",
                    "sharegpt",
                    "dolly15k",
                    "dataset",
                    "indicating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the above analysis, we propose a new prompt engineering method that leverages structural linguistic features. Specifically, we take the average of the high-dimensional embeddings of POS tags and dependency relations from the analyzed dataset to define a centroid representation. This centroid captures the &#8220;central&#8221; syntactic patterns that are associated with higher-performing prompts.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "syntactic",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each target prompt, we first analyze its POS and dependency embeddings to identify deviations from the centroid. Based on this analysis, a modification plan is generated, specifying how the prompt&#8217;s syntactic structure should be adjusted. The LLM is then guided to rewrite the prompt according to this plan, producing an optimized prompt whose embeddings are closer to the centroid. This process allows peripheral prompts that initially deviate from effective syntactic patterns to be systematically aligned with the central region of the embedding space.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "syntactic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We are committed to ensuring the reproducibility of our results. All code used in this research is publicly available through links in our abstract. The repository includes detailed instructions for dataset preprocessing, and running experiments. We also specify the exact versions of dependencies and libraries used in our experiments. All datasets employed in this study are either publicly accessible or their sources are clearly documented. Random seeds are set for all experiments where applicable to minimize variability. Together, these resources enable researchers to reproduce our analyses and results with minimal effort.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "their",
                    "dependencies"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that the labeled license refers to the licensing information assigned to the dataset based on the publishers&#8217; declared rights. However, certain sub-datasets may remain subject to their original licensing conditions, which could differ from the labeled license.</p>\n\n",
                "matched_terms": [
                    "their",
                    "note",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset features thousands of prompts generated by the teknium/OpenHermes-2p5-Mistral-7B model, each designed to elicit diverse and contextually rich responses. Stored as JSON objects, it enables research in synthetic prompt generation, model creativity evaluation, and downstream fine-tuning.</p>\n\n",
                "matched_terms": [
                    "objects",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Stanford Alpaca dataset comprises 52K high-quality, instruction-following examples generated via a modified Self-Instruct pipeline using text-davinci-003. Designed for fine-tuning LLaMA models, it enables research in alignment, instruction tuning, and synthetic data generation.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Aya Collection is a massive multilingual instruction tuning dataset comprising over 513 million prompt-completion pairs across 115 languages. It integrates three sources: human-crafted instruction templates created by fluent speakers for diverse tasks, machine translations of 19 top-tier datasets into 101 languages via NLLB, and the human-annotated Aya Dataset subset of 204K examples. Split by dataset, each record includes id, inputs, targets, language, script, and task type. Licensed under Apache-2.0, it supports academic and commercial classification, summarization, translation, and QA research.</p>\n\n",
                "matched_terms": [
                    "type",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Baize Chat Data is an instruction-finetuning corpus combining four sources: Alpaca, Medical, Quora, and StackOverflow. It contains about 210,000 conversational examples, each formatted with [|Human|] prompts and [|AI|] responses. Designed to enhance the Baize family of language models, this unified dataset supports interactive text generation and dialogue training. Sourced from the Baize GitHub repository, it provides diverse conversational scenarios ranging from general queries to specialized medical and technical discussions. It is optimized for instruction-following tasks. It enables realistic user interactions.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BELLE_Generated_Chat contains approx. 400k personalized Chinese character dialogues generated by the BELLE project. Each record includes an instruction, an (empty) input, and a generated output. Created by ChatGPT and not strictly verified, the dataset may contain factual inaccuracies. Licensed under GPL-3.0 for research use only. With around 0.4 million entries, it supports text-to-text generation and conversational modeling.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BELLE_Multiturn_Chat is a Chinese multi-turn conversational dataset comprising approximately 0.8 million human-assistant dialogues generated by the BELLE project using ChatGPT. Each record pairs an instruction containing prior context labeled with &#8220;Human:&#8221; and &#8220;Assistant:&#8221; with the assistant&#8217;s subsequent reply. Intended for text-to-text generation tasks, the GPL-3.0-licensed collection covers only Chinese interactions. As this data is automatically generated and unverified, factual errors and inconsistencies may arise. It is provided strictly for non-commercial research under the project&#8217;s usage restrictions; developers should validate outputs and adhere to licensing terms.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Best Chinese Prompt dataset is a comprehensive, well-structured collection of Chinese-language prompts spanning diverse categories such as casual chat, knowledge Q&amp;A, creative planning, copywriting, and code generation. It provides real multi-model response comparisons (e.g., GPT-4, ChatGPT, NewBing, Wenxin) and continuous updates via collaborative platforms.</p>\n\n",
                "matched_terms": [
                    "comparisons",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BoredHumans is a diverse and extensive prompt dataset compiled from multiple sources, including Awesome ChatGPT Prompts, Data Science Prompts, and Tree-of-Thought Prompting, among others. Its rich variety covers numerous domains and prompt styles, enabling comprehensive research on prompt engineering, AI model behavior, and in-context learning strategies.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "boredhumans",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The ChatGPT Prompts for Data Science dataset offers a curated collection of specialized prompts designed to enhance AI applications in data science tasks. It facilitates research on natural language interfaces for data analysis, model explanation, and automation of complex workflows.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Chinese-DeepSeek-R1-Distill-data-110k is a 110K-entry Chinese dataset distilled from DeepSeek-R1, supporting text generation, text2text generation, and question answering under Apache-2.0. It covers four domains: Math (36 568 samples), Exam (2 432), STEM (12 648) and General (58 352). Each record includes input, reasoning content, output, source repo name and model-assigned score. Data originate from diverse math and instruction corpora, distilled via R1 with temperature 0.6, step-by-step math prompts, and validation using Math-Verify and Qwen2.5-72B.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: COIG-CQIA (Chinese Open Instruction Generalist - Quality is All You Need) is a high-quality, open-source Chinese instruction tuning dataset designed to align language models with human interactive behavior. It aggregates over 45,000 manually cleansed, restructured, and reviewed examples spanning social media dialogs, encyclopedic articles, exam questions, finance, medical, legal, traditional culture, and NLP tasks. Each entry includes instruction, optional input, output, task type, domain, and human verification metadata. COIG-CQIA aims to facilitate instruction fine-tuning for Chinese NLP research and applications.</p>\n\n",
                "matched_terms": [
                    "type",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Databricks-dolly-15K is an open-source corpus of over 15,000 human-generated instruction-response pairs created by Databricks employees across eight behavioral categories defined by InstructGPT, including brainstorming, classification, closed and open QA, generation, information extraction, and summarization. Provided under a CC-BY-SA 3.0 license, this English-language dataset supports academic or commercial use. With context passages drawn from Wikipedia when required, it enables training and fine-tuning of large language models, as well as synthetic data generation and data augmentation for robust, scalable instruction-following capabilities.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: DMind_Benchmark is a comprehensive dataset for evaluating large language models on blockchain, cryptocurrency, and Web3 knowledge. It provides objective (multiple choice) and subjective (open ended) questions across nine domains: Fundamentals, Infrastructure, Smart Contracts, DeFi, DAOs, NFTs, Security, Tokenomics, and MEME coins&#8212;organized into CSV and JSONL splits. The benchmark supports diverse question types&#8212;calculations, code audits, risk and scenario analyses&#8212;with automated scoring and evaluation. It features standardized data configurations, leaderboards, and extensible evaluation pipelines for comparative analysis of LLM performance in specialized Web3 tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Dynosaur introduces a dynamic and low-cost paradigm for curating instruction-tuning datasets. It automatically generates diverse instructions by leveraging metadata from HuggingFace datasets, combined with LLM-based instruction synthesis (e.g., via ChatGPT). The result is Dynosaur-full, a large-scale dataset (800K+ samples, generated at &#160; $11.5) that supports dynamic growth and general-purpose instruction-tuning. Empirically, models fine-tuned on Dynosaur outperform Alpaca and GPT-4-Instruct baselines on Super-NI. The project includes: metadata crawling tools, instruction generation pipelines, and fine-tuned T5-3B and LLaMA-7B models. All generated instructions are under Apache 2.0, with task data adhering to original dataset licenses.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Firefly is a Chinese instruction-tuning dataset comprising 1.15 million high-quality examples drawn from 23 common Chinese natural language processing datasets. Each example includes a task type, an input prompt, and a target output, ensuring diverse coverage. Data templates were manually designed for each task to ensure quality and richness. Token length analysis shows that most examples are under 600 tokens. Firefly was used to train the Firefly-1b4 Chinese dialogue LLM, available on GitHub and Hugging Face, fostering reproducibility, community collaboration.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "type",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset aggregates tasks from Flan, T0, Super-Natural Instructions, Chain-of-Thought, and Dialog into a training split. Each task is provided in zero-/few-shot and option/no-option formats as JSONL entries including inputs, targets, and task identifiers. Released under Apache-2.0, it includes scripts for building dependencies, fixing version mismatches, and exporting per-task JSONL data. Mixing ratios can be tuned for optimal downstream performance via guidelines in the associated paper and public GitHub repository.</p>\n\n",
                "matched_terms": [
                    "dependencies",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: GraphWalks is an open-source benchmark dataset designed to evaluate multi-hop reasoning over long graph contexts. Released under the MIT license, it provides directed graphs as edge lists alongside user-specified operations&#8212;such as breadth-first searches or parent retrieval&#8212;for models to execute. Each prompt comprises three demonstration examples, a target graph, and a query, with expected outputs formatted as node ID lists. Accompanying metadata includes prompt character counts and problem types. Standardized extraction and F1-based grading scripts ensure consistent answer parsing and evaluation.</p>\n\n",
                "matched_terms": [
                    "types",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: GSM8K (Grade School Math 8K) is an English monolingual dataset of 8.8K crowd-sourced grade school math word problems paired with multi-step solutions. It contains a main configuration and a Socratic variant, each offering questions and answers with calculator annotations and step-by-step reasoning expressed in natural language. Problems require two to eight elementary arithmetic steps. Split into training (7,473 examples) and test (1,319 examples), GSM8K supports text-to-text generation benchmarks under MIT license. All annotations were crowdsourced via Upwork and Surge AI.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "main"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Human ChatGPT Comparison Corpus (HC3) is the first large-scale bilingual dataset enabling direct comparison of human and ChatGPT-generated text. Spanning English and Chinese samples, it encompasses between 10,000 and 100,000 prompt-response pairs covering tasks such as text classification, question-answering, sentence similarity, and zero-shot classification. Released under a CC-BY-SA license, HC3 supports research in performance evaluation, detection, and analysis of AI-generated content. Accompanying code, models, and benchmarks are available on GitHub, facilitating open science, reproducible experimentation, and collaborative, community-driven global efforts.</p>\n\n",
                "matched_terms": [
                    "between",
                    "sentence",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LaMini-Instruction is an English text-to-text generation dataset comprising 2.58M instruction-response pairs distilled from GPT-3.5-Turbo. Each sample includes an instruction, a corresponding model-generated response, and the instruction&#8217;s provenance&#8212;drawn from sources such as Alpaca, FLAN, P3, and Self-Instruct. Released under CC-BY-NC 4.0, it spans a single training split of over 1.16 GB and supports fine-tuning of compact language models. LaMini-Instruction enables research in instruction-based learning but inherits biases and errors from its GPT-3.5 teacher.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Llama-Nemotron-Post-Training-Dataset is a comprehensive dataset of synthetic SFT and RL samples designed to bolster reasoning, code, math, science, chat, and safety capabilities for NVIDIA&#8217;s Llama-3 Nemotron series. It includes over 33M SFT examples across code, math, science, chat, and safety, plus 56K instruction-following RL examples. Data is sourced from public corpora or synthetically generated, filtered for quality and complexity. Released under CC-BY-4.0, it supports training and evaluation of efficient open-source LLMs offering a flexible accuracy-efficiency tradeoff and transparent development.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LMSYS-Chat-1M is a large-scale dataset of one million real-world LLM conversations, collected from 210K users interacting with 25 models via Chatbot Arena and Vicuna demo (April-August 2023). Each conversation includes model metadata, OpenAI-style JSON formatting, language tags, and moderation labels. Personally identifiable information is redacted. This dataset enables research on LLM alignment, safety, evaluation, and user behavior in the wild, offering unique insights into real-world usage patterns and content moderation challenges in multi-model deployment scenarios.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Math CoT Arabic English Reasoning is a bilingual dataset of 1K-10K meticulously curated English and Arabic math problems with explicit chain-of-thought solutions. Spanning 21 categories from arithmetic to topology and logic, it offers human-verified, step-by-step reasoning examples in parallel languages. Structured in JSON with questions, answers, comprehensive metadata, category labels, and word counts, it supports question-answering, text generation, and mask-filling benchmarks. Licensed under MIT, it&#8217;s ideal for robust multilingual mathematical reasoning research, cross-lingual model evaluation, and educational AI assistant development.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: medical-o1-reasoning-SFT is a supervised fine-tuning dataset designed to enhance advanced medical reasoning in HuatuoGPT-o1. It comprises English and Chinese instruction-response pairs generated by GPT-4o on verifiable clinical problems, validated by a medical verifier. Released under an Apache-2.0 license, the dataset supports question answering and text generation, offering separate configurations for monolingual and mixed-language data. It aims to refine model performance on complex biomedical tasks by leveraging rigorous problem-solving chains, with full details available in the accompanying paper and GitHub repository.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "full",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: medical-o1-verifiable-problem is an Apache-2.0 licensed dataset comprising open-ended medical reasoning problems designed to improve large language models&#8217; diagnostic and procedural knowledge. It supports question-answering and text-generation tasks, presenting each instance as a challenging exam-style prompt paired with a verifiable, expert-derived answer. Published in English under a single default configuration with training data provided in JSON format, it allows systematic evaluation and refinement of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Medical-R1-Distill-Data is an Apache-2.0 licensed instruction fine-tuning dataset distilled from Deepseek-R1&#8217;s Full Power Version using medical verifiable problems sourced from HuatuoGPT-o1. It supports English and Chinese, and is tailored for question-answering and text-generation tasks in medical and biology domains. The dataset captures reasoning chains from the native Deepseek-R1 API, facilitating model initialization with robust medical reasoning. A Chinese counterpart is available separately. Methodology and guidelines are provided in the associated paper and GitHub repository. It comprises SFT examples from medical_r1_distill_sft.json.</p>\n\n",
                "matched_terms": [
                    "full",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: MedTrinity-25M is a large-scale multimodal medical dataset featuring over 25 million images from 10 imaging modalities. It provides multigranular annotations for 65+ diseases, including textual descriptions, bounding boxes, segmentation masks, and inter-region relationships. Supporting both vision-centric and multimodal tasks like classification, segmentation, and report generation, it facilitates large-scale pretraining for medical foundation models. Public access includes an 18M image-text pair subset. The dataset is organized in shards with structured metadata for scalable research and development.</p>\n\n",
                "matched_terms": [
                    "relationships",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: MMInstruct-GPT4V is a multilingual multi-modal instruction tuning dataset for visual question answering and image captioning, licensed under Apache-2.0. It comprises three configurations&#8212;qa_en, caption_en, and caption_cn&#8212;covering English QA (&#160;216K examples), English captions (&#160;18K examples), and Chinese captions (&#160;144K examples) in JSONL train splits. Total size ranges between 100K and 1M instances. Designed to leverage GPT-4V for high-quality instruction generation, it supports both one-shot and multi-round interactions, enabling robust supervised fine-tuning of vision-language models targeting visual-question-answering and question-answering tasks with enhanced robustness.</p>\n\n",
                "matched_terms": [
                    "between",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: NATURAL INSTRUCTIONS is a monolingual English dataset derived from Super-Natural-Instructions, offering 1,600+ NLP tasks for training, validation, and testing. Size ranges between 100 million and one billion examples. Curated by crowdsourced and expert annotators, it covers classification, generation, and reasoning across reading comprehension, commonsense, summarization, arithmetic, logic, and dialog. With over 100 M examples, it provides diverse input-output mappings while enabling deduplication by unique IDs or input fields. Tasks span question answering, text modification, summarization, and beyond, supporting robust instruction-following model development.</p>\n\n",
                "matched_terms": [
                    "between",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Nemotron-CrossThink is a multi-domain reinforcement learning dataset designed to enhance both general-purpose and mathematical reasoning in large language models. It comprises two subsets: Nemotron-CrossThink-QA with high-quality question-answer pairs across STEM, humanities, and sciences, and Nemotron-CrossThink-Math featuring persona-driven, multi-step math problems. Data is curated from CommonCrawl and open-source books, standardized via structured templates into multiple-choice and open-ended formats, filtered for verifiability, and used to train RL policies with Group Relative Policy Optimization. Licensed under CC-BY-4.0, it supports AI development.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: No Robots is a high-quality, human-curated instruction dataset comprising 10,000 examples for supervised fine-tuning of language models. It includes 9,500 training and 500 test instances across ten single-turn categories&#8212;Generation, Open QA, Brainstorm, Chat, Rewrite, Summarize, Coding, Classify, Closed QA, and Extract&#8212;totaling roughly 17 MB of English text under CC-BY-NC-4.0. Each example consists of a prompt with unique ID, structured message history (system, user, assistant), and category labels. It enables models to learn diverse instruction-following behaviors and robustly supports reproducibility.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: NuminaMath-1.5 is an open-source, large-scale post-training dataset comprising about 900 000 competition-level mathematics problems paired with chain-of-thought solutions. It covers diverse sources&#8212;from Chinese high school exams to US and international Olympiads&#8212;and spans domains like algebra, geometry, number theory, combinatorics, calculus, and puzzles. Each entry includes metadata fields (answer, problem_type, question_type) for verifiable outputs. Recent additions feature manually verified Olympiad references and curated contest data while synthetic problems were removed. Licensed under Apache 2.0, NuminaMath-1.5 supports advanced text-generation research in mathematical reasoning.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenAssistant Conversations (OASST1) is a human-generated, human-annotated corpus with 161,443 messages in 66,497 conversation trees across 35 languages. It includes over 461,000 quality ratings and more than 10,000 fully annotated trees. Each record contains metadata (IDs, timestamps), conversational structure (parent and tree IDs), role and language labels, toxicity and quality scores, emoji labels. Data comes in nested JSONL or flat parquet via HuggingFace, with 84,437 training and 4,401 validation splits, supporting supervised fine-tuning and reward model development. Licensed under Apache-2.0.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "oasst1",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OL-CC is the first open source Chinese conversational instruction dataset collected via crowdsourcing on OpenLabel. It includes 10,006 instruction-answer pairs and 1,649 standalone instructions across tasks such as question-answering, text generation, extraction, rewriting, classification, brainstorming, chit-chat, logic and math. A total of 276 volunteers alternately played user and AI assistant roles to produce the data. Licensed under Apache-2.0 and sized between 10K and 100K examples, OL-CC offers rich, human-generated Chinese instructional dialogues for AI research.</p>\n\n",
                "matched_terms": [
                    "between",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenCodeReasoning is a large-scale synthetic dataset designed to distill reasoning capabilities for Python-based competitive programming. It comprises 735,255 samples covering 28,319 unique problems sourced from platforms like CodeForces, AtCoder, and LeetCode. The dataset features two configurations: split_0 includes full problem statements and model responses, while split_1 references external datasets via index placeholders. Each example contains identifiers, source metadata, difficulty labels, and code solutions. Licensed under CC-BY-4.0, OpenCodeReasoning supports supervised fine-tuning of language models for code generation tasks.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "full",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Phoenix-sft-data-v1 is a multilingual supervised fine-tuning dataset containing 464,510 samples, combining instruction-following and ChatGPT-distilled conversation data. It includes Alpaca-derived tasks, post-translated multilingual instructions, and user-centered prompts in 40 languages. The dataset also integrates ShareGPT and Discord-sourced dialogues. With nearly 1 million conversation turns and detailed multilingual annotations, it supports multilingual language modeling, alignment, and chat adaptation. English and Chinese dominate the corpus, with broader linguistic diversity represented across the remaining data, enabling robust multilingual model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "sharegpt",
                    "represented",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: PubMedQA is a biomedical question answering (QA) dataset designed to evaluate systems on their ability to answer yes/no/maybe research questions using corresponding PubMed abstracts. The dataset focuses on factual reasoning within biomedical literature.</p>\n\n",
                "matched_terms": [
                    "their",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: RedGPT Dataset (V1-CN) offers 50,000 automatically generated multi-turn Chinese dialogues grounded in high-quality factual references from diverse domains such as history, science, law, and culture. Designed to enhance GPT models&#8217; factual accuracy, the dataset enables fine-tuning on realistic, knowledge-rich conversational data without costly manual annotation. It supports research in improving language models&#8217; truthfulness, dialogue generation, and knowledge integration.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Self-Instruct is an open Apache-2.0-licensed dataset and framework designed to enhance language models&#8217; instruction-following capabilities. It comprises four configurations: a self-generated set of 82K prompt-completion pairs produced via OpenAI&#8217;s davinci engine; 50K samples from Super Natural Instructions; 52K prompts drawn from the P3 public pool; and 252 expert-crafted human evaluation tasks with associated inputs and outputs. All data is in English and supports instruction-tuning by providing diverse natural-language prompts paired with corresponding model or human completions. The dataset facilitates instruction-tuning.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: ShareGPT90K is a dataset of 90,665 conversational threads scraped from the ShareGPT platform. Each example includes a unique id and a sequence of messages, with each message annotated by its origin and its content.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Skywork-OR1-RL-Data is a large-scale reinforcement learning dataset featuring 105,055 math problems and 14,057 coding questions curated for the Skywork-OR1 model series. Each example includes source attribution, structured prompts with roles, model-aware difficulty ratings for DeepSeek-R1 variants, and a reward model with ground truth and style labels. Problems are rigorously cleaned, deduplicated, and filtered by difficulty per variant. The dataset supports math and code splits totaling 1.5 billion bytes and facilitates robust reasoning training with rule-based RL recipes via curated pipelines efficiently.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraFeedback is an MIT-licensed, open-source, large-scale preference dataset designed for training reward and critic models. It contains 64 K prompts drawn from UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA and FLAN, each answered by four out of 17 diverse LLMs under five alignment principles. The result is 256 K responses and 380 K fine-grained annotations covering instruction-following, truthfulness, honesty and helpfulness, all rated by GPT-4. Its scale, diversity and dense numerical plus textual feedback make it ideal for RLHF research and robust reward-model development.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraMedical is a large-scale English biomedical instruction dataset featuring over 409,000 examples licensed under MIT. Each sample includes an identifier, instruction type, multi-turn conversation pairs between human queries and GPT-generated responses, a ground-truth answer, and a model-evaluated score. The training split comprises roughly 1.2 GB across 410K examples, sourced from both curated public data and synthetic augmentations. UltraMedical aims to support the development of specialized generalist models in biomedicine by providing diverse, high-quality instruction-response instances, and comprehensive evaluation metrics accompany each instance.</p>\n\n",
                "matched_terms": [
                    "between",
                    "dataset",
                    "type",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_sansserif\">Universal Transformers Dataset</span>\n</p>\n\n",
                "matched_terms": [
                    "universal",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Universal Transformer Dataset is a massive, scalable, multimodal resource comprising over one septillion structured datapoints across text, image, video and audio. Designed by the GoX AI Platform, it supports more than 40 NLP, vision, speech, and reinforcement learning tasks, covering over 200 languages. Preprocessed and pre-tokenized for efficient training, it is optimized for LLMs, vision, speech and multimodal architectures. Carefully curated and augmented via advanced AI models, it accelerates pretraining, fine-tuning, and zero-shot learning for cutting-edge AI research.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "universal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Wizard_evol_instruct_196K is a MIT-licensed instruction-tuning dataset comprising 143K evolved QA pairs derived from Alpaca and ShareGPT. It represents an optimized version of the Evol-Instruct data used to train the WizardLM family of models. To assemble the complete instruction set of roughly 196K samples, users must merge this release with the original unfiltered ShareGPT dataset. The refined examples cover diverse conversational and instructional scenarios, facilitating improved alignment and performance in downstream open-source large language models, including structured prompts and responses.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "sharegpt",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: xP3 (Crosslingual Public Pool of Prompts) is a multilingual prompt and dataset collection spanning 46 languages and 13+ NLP tasks (e.g., QA, translation, summarization, code generation). Assembled from expert-generated and crowdsourced annotations under an Apache-2.0 license, it supports zero-shot and instruction-tuning for models like BLOOMZ and mT0. The training mixture covers closed-book and extractive QA, multiple-choice, paraphrase identification, program synthesis, sentiment analysis, structure-to-text, summarization, classification and more, totaling over 788 million samples. xP3 streamlines reproducible multilingual finetuning across diverse data scales.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide the comparision of 3/4/5-grams for all datasets (except <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>, which is displayed in the main paper) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F6\" title=\"Figure 6 &#8227; F.1 Token-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, and the top-5 <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A6.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams comparison across datasets in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F7\" title=\"Figure 7 &#8227; F.1 Token-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "main"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams phrases of some datasets include abnormal content (e.g. &#8220;<span class=\"ltx_text ltx_font_italic\">identify which instrument be string</span>&#8221; in <span class=\"ltx_text ltx_font_sansserif\">dolly-15</span> and &#8220;<span class=\"ltx_text ltx_font_italic\">The quick brown fox jumps over the lazy dog</span>&#8221; in <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>), which indicates that there is a lot of repetition in the input content of the template tasks or some instructions used to construct the dataset, which may affect the balance of the dataset.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the complete experimental data for all identified dependency types, along with their proportions in the datasets, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.T5\" title=\"Table 5 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Additionally, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.T6\" title=\"Table 6 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> lists all detected Part-of-Speech tags and their corresponding proportions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F8\" title=\"Figure 8 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> further illustrates the ten most common verbs and their top five direct noun objects found in the prompt datasets except <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>, which are shown in the main paper.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "types",
                    "main",
                    "their",
                    "medicalo1",
                    "data",
                    "sharegpt",
                    "proportions",
                    "objects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These additional data further support our conclusions.\n<span class=\"ltx_text ltx_font_bold\">(1)</span> The <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> dataset, which consists of professionally crafted medical prompts, exhibits a relatively high proportion of numerical modifiers (nummod, 0.0276) and passive auxiliaries (auxpass, 0.0101) in dependency analysis, as well as a notably high usage of numerals (NUM, 0.0309) in POS tagging. These features reflect a terminology-dense and precision-oriented language style that emphasizes processes and outcomes rather than agents.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> In the <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> dataset, the verb-noun pairs reflect language commonly used in business contexts, such as <span class=\"ltx_text ltx_font_italic\">&#8220;create plan&#8221;</span> and <span class=\"ltx_text ltx_font_italic\">&#8220;create strategy&#8221;</span>. In contrast, the verb-noun pairs observed in <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> suggest more generic and broadly applicable usage scenarios.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "11kbusiness",
                    "selfinstruct",
                    "medicalo1",
                    "data",
                    "boredhumans",
                    "oasst1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Anomalously, in the <span class=\"ltx_text ltx_font_sansserif\">dolly-15k</span> dataset, the most frequent verb-noun pairs exhibit a skewed distribution, with the highest-frequency nouns overwhelmingly associated with only the top one or two verbs. Moreover, these frequent verb-noun pairs often lack clear task-specific semantics&#8212;for example, <span class=\"ltx_text ltx_font_italic\">&#8220;tell i&#8221;</span>, <span class=\"ltx_text ltx_font_italic\">&#8220;give list&#8221;</span>, and <span class=\"ltx_text ltx_font_italic\">&#8220;classify each&#8221;</span>. This pattern may be attributed to the manual generation process, which is susceptible to the individual linguistic habits of annotators.</p>\n\n",
                "matched_terms": [
                    "dolly15k",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show the distribution of sampled embedding points after PCA for all datasets (except for <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>, which are shown in the main paper) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F9\" title=\"Figure 9 &#8227; F.3 Semantic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "medicalo1",
                    "main"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can still observe from the results that datasets with more concentrated topical focus (e.g., <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span>) exhibit clear clustering patterns, whereas those with broader thematic coverage (e.g., <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>) display a more dispersed distribution of data points.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "11kbusiness",
                    "data"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "Large Language Model Prompt Datasets: An In-depth Analysis and Insights",
        "caption": "Table 3: The top-7 Parts-of-Speech, with each value indicating its proportion in a dataset. Full data in Table6.",
        "body": "POS\nEWT\nParTUT\n1.1k-business\nBoredHumans\ndolly-15k\nmedical-o1\nOASST1\nSelf-Instruct\nShareGPT\n\n\n\n\nNOUN\n0.17\n0.21\n0.2637\n0.2103\n0.1899\n0.2590\n0.1946\n0.2027\n0.1944\n\n\nPUNCT\n0.12\n0.12\n0.1094\n0.1942\n0.1435\n0.1158\n0.1231\n0.1839\n0.1450\n\n\nVERB\n0.11\n0.10\n0.1302\n0.1094\n0.0871\n0.0775\n0.1069\n0.0999\n0.0979\n\n\nADP\n0.09\n0.12\n0.0758\n0.0678\n0.0858\n0.0998\n0.0851\n0.0701\n0.0789\n\n\nDET\n0.08\n0.11\n0.0506\n0.0693\n0.0949\n0.0893\n0.0839\n0.0844\n0.0696\n\n\nPRON\n0.09\n0.04\n0.0912\n0.0708\n0.0695\n0.0369\n0.0870\n0.0701\n0.0583\n\n\nADJ\n0.07\n0.08\n0.0588\n0.0543\n0.0538\n0.1104\n0.0632\n0.0498\n0.0563",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">POS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">EWT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ParTUT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">1.1k-business</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BoredHumans</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">dolly-15k</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">medical-o1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OASST1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Self-Instruct</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ShareGPT</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">NOUN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.2637</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.2103</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1899</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.2590</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1946</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.2027</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1944</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">PUNCT</td>\n<td class=\"ltx_td ltx_align_center\">0.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.12</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.1094</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.1942</span></td>\n<td class=\"ltx_td ltx_align_center\">0.1435</td>\n<td class=\"ltx_td ltx_align_center\">0.1158</td>\n<td class=\"ltx_td ltx_align_center\">0.1231</td>\n<td class=\"ltx_td ltx_align_center\">0.1839</td>\n<td class=\"ltx_td ltx_align_center\">0.1450</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">VERB</td>\n<td class=\"ltx_td ltx_align_center\">0.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.10</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.1302</span></td>\n<td class=\"ltx_td ltx_align_center\">0.1094</td>\n<td class=\"ltx_td ltx_align_center\">0.0871</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0775</span></td>\n<td class=\"ltx_td ltx_align_center\">0.1069</td>\n<td class=\"ltx_td ltx_align_center\">0.0999</td>\n<td class=\"ltx_td ltx_align_center\">0.0979</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">ADP</td>\n<td class=\"ltx_td ltx_align_center\">0.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.12</td>\n<td class=\"ltx_td ltx_align_center\">0.0758</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0678</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0858</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0998</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0851</td>\n<td class=\"ltx_td ltx_align_center\">0.0701</td>\n<td class=\"ltx_td ltx_align_center\">0.0789</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">DET</td>\n<td class=\"ltx_td ltx_align_center\">0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.11</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0506</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0693</td>\n<td class=\"ltx_td ltx_align_center\">0.0949</td>\n<td class=\"ltx_td ltx_align_center\">0.0893</td>\n<td class=\"ltx_td ltx_align_center\">0.0839</td>\n<td class=\"ltx_td ltx_align_center\">0.0844</td>\n<td class=\"ltx_td ltx_align_center\">0.0696</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">PRON</td>\n<td class=\"ltx_td ltx_align_center\">0.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.04</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0912</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0708</td>\n<td class=\"ltx_td ltx_align_center\">0.0695</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0369</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0870</td>\n<td class=\"ltx_td ltx_align_center\">0.0701</td>\n<td class=\"ltx_td ltx_align_center\">0.0583</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">ADJ</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0588</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0543</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0538</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.1104</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0632</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0498</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0563</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "11kbusiness",
            "selfinstruct",
            "medicalo1",
            "data",
            "oasst1",
            "partsofspeech",
            "top7",
            "value",
            "adp",
            "det",
            "sharegpt",
            "noun",
            "verb",
            "each",
            "pron",
            "punct",
            "ewt",
            "dataset",
            "indicating",
            "its",
            "partut",
            "pos",
            "proportion",
            "adj",
            "boredhumans",
            "dolly15k",
            "full"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We annotate the datasets with POS tags and calculate the distribution of nouns, verbs, adjectives, and adverbs. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T3\" title=\"Table 3 &#8227; 5.3.1 Dependency Parsing &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the functional composition of prompts, contrasting content and function words. For example, a high verb frequency indicates action-oriented prompts, while a predominance of nouns suggests more objective narratives. These distributional differences reveal stylistic and structural variations across sources.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data discovery guideline.</span> We employ a systematic dataset discovery process across multiple sources to compile a diverse repository of prompt datasets. Our objective is to capture real-world, user-generated prompts, instruction-following interactions, and domain-specific scenarios. In particular, our primary objectives for datasets discovery are three-fold: <span class=\"ltx_text ltx_font_bold\">(1)</span> collecting datasets that are composed of prompts, i.e., natural language instructions that describe a certain task the LLM should perform and guide the LLM towards generating a desired output; <span class=\"ltx_text ltx_font_bold\">(2)</span> ensuring that the extracted data cover various domains, including day-to-day scenarios such as travel planning, professional scenarios such as academic writing, and specialized scenarios such as healthcare and finance; and <span class=\"ltx_text ltx_font_bold\">(3)</span> allowing different forms of prompts, e.g., single instruction, conversations, etc.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data discovery process.</span> We collect publicly available datasets from the following four types of sources. <span class=\"ltx_text ltx_font_bold\">First</span>, we consult <span class=\"ltx_text ltx_font_italic\">dataset collection platforms</span>, including Hugging Face Datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib12\" title=\"\">hug, </a>)</cite>, Kaggle <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib13\" title=\"\">kag, </a>)</cite>, Google Dataset Search <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib11\" title=\"\">goo, </a>)</cite>, and Papers with Code <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib14\" title=\"\">pap, </a>)</cite>. Targeted searches using keywords, e.g., \"prompt dataset\", \"instruction-following dataset\", and \"conversation dataset\" yield 60 prompt datasets. <span class=\"ltx_text ltx_font_bold\">Second</span>, we review the latest <span class=\"ltx_text ltx_font_italic\">academic publications</span>, specifically papers on prompt engineering, natural language understanding, and dialogue systems, published at NeurIPS, ICLR, and ICML between 2023-2024 and identify 73 datasets shared across them. <span class=\"ltx_text ltx_font_bold\">Third</span>, we also examine <span class=\"ltx_text ltx_font_italic\">public repositories</span> by systematically surveying open-source GitHub projects using keywords, e.g., &#8220;prompt collection&#8221;, &#8220;LLM prompts&#8221;, and &#8220;instruction dataset&#8221;. We identify 21 prompt repositories that typically contain curated prompt lists derived from user interactions or synthesized from public APIs. Some of these repositories are &#8220;awesome-lists&#8221;, which are curated collections of high-quality prompts or links to prompt datasets.\nNotable examples include <span class=\"ltx_text ltx_font_sansserif\">Awesome Instruction Datasets</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib46\" title=\"\">Nie, </a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering Guide</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Saravia, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib56\" title=\"\">2022</a>)</cite>, and <span class=\"ltx_text ltx_font_sansserif\">LLMDataHub</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib71\" title=\"\">Zhao, </a>)</cite>. <span class=\"ltx_text ltx_font_bold\">Finally</span>, we extract 14 datasets from <span class=\"ltx_text ltx_font_italic\">popular websites dedicated to prompt-sharing</span>, including <span class=\"ltx_text ltx_font_sansserif\">Prompt Genius</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Pro, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib7\" title=\"\">b</a>)</cite> and <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib10\" title=\"\">bor, </a>)</cite>. These platforms feature user-written prompts for practical purposes.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "boredhumans",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data filtering.</span>\nWe remove duplicate entries (e.g., CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib51\" title=\"\">2024</a>)</cite> appears in both Hugging Face and NeurIPS 2024) and then filter the remaining candidates using four quality criteria for inclusion in this paper.\n<span class=\"ltx_text ltx_font_bold\">First</span>, <span class=\"ltx_text ltx_font_italic\">Dataset size.</span> We prioritize datasets containing at least 1K prompts to ensure robustness in diversity and statistical power. In contrast, due to their generally limited scope, user-shared datasets are filtered with a minimum threshold of 50 prompts.\n<span class=\"ltx_text ltx_font_bold\">Second</span>, <span class=\"ltx_text ltx_font_italic\">Data quality.</span> We evaluate the quality of prompts based on their cleanliness. Most datasets (e.g., <span class=\"ltx_text ltx_font_sansserif\">OpenCodeReasoning</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib16\" title=\"\">2025b</a>)</cite>) on data hosting platforms (e.g., Hugging Face and Kaggle) are well-formatted and clean. For the remaining data, we exclude samples with inconsistent formatting or unclear structure. For instance, the <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering Guide</span>&#8211;a resource that offers both curated prompt datasets and instructional examples&#8211;contains many illustrative prompts scattered throughout the material and are thus omitted from our datasets.\n<span class=\"ltx_text ltx_font_bold\">Third</span>, <span class=\"ltx_text ltx_font_italic\">Data relevance.</span> We assess whether the prompts are aligned with our data discovery guidelines, specifically emphasizing on those that represent common usage scenarios for broad audiences (e.g., <span class=\"ltx_text ltx_font_sansserif\">Chinese-DeepSeek-R1-Distill-data-110k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib38\" title=\"\">2025a</a>)</cite>), and tasks from various domains (e.g., <span class=\"ltx_text ltx_font_sansserif\">Medical Verifiable Problems</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">OpenMathReasoning</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Moshkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib44\" title=\"\">2025</a>)</cite>). Datasets that violate our discovery guidelines are omitted. For instance, the <span class=\"ltx_text ltx_font_sansserif\">PersonaHub</span> dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib5\" title=\"\">Per, </a>)</cite> is excluded because it does not meet <span class=\"ltx_text ltx_font_bold\">data discovery guideline (1)</span>, which mandates that prompts be linked to specific, well-defined tasks. Although <span class=\"ltx_text ltx_font_sansserif\">PersonaHub</span> demonstrates the potential of synthetic personas in generating diverse content (e.g., reasoning problems, dialogues, or non-player character behaviors), it predominantly comprises persona descriptions without clear task formulation.\n<span class=\"ltx_text ltx_font_bold\">Fourth</span>, <span class=\"ltx_text ltx_font_italic\">Accessibility.</span> Datasets must be publicly accessible or retrievable via automated crawling, and their licensing terms must permit research use. After filtering, we identify 129 distinct prompt datasets for taxonomic analysis (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S4\" title=\"4 Dataset Taxonomy &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Release channel</span> refers to the platform where a dataset is published. Common platforms include <span class=\"ltx_text ltx_font_italic\">data hosting sites</span> such as GitHub, Hugging Face, and Kaggle, where structured prompt formats (e.g., CSV, JSON) dominate. <span class=\"ltx_text ltx_font_italic\">Personal sites</span> or <span class=\"ltx_text ltx_font_italic\">notes</span> (e.g., Notion workspaces) often host informal, user-oriented prompts. Dedicated <span class=\"ltx_text ltx_font_italic\">prompt sharing websites</span> vary from open-access (e.g., <span class=\"ltx_text ltx_font_sansserif\">QuickRef.ME</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib8\" title=\"\">Qui, </a>)</cite>) to commercial marketplaces (e.g., <span class=\"ltx_text ltx_font_sansserif\">PromptBase</span>). <span class=\"ltx_text ltx_font_italic\">Social media</span>, like Reddit&#8217;s <span class=\"ltx_text ltx_font_sansserif\">r/ChatGPTPromptGenius</span>, also plays a key role in community-driven prompt exchange.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generation process</span> describes how the prompts are created. <span class=\"ltx_text ltx_font_italic\">Human-generated prompts</span> are either manually authored (e.g., <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>)</cite>) or collected from user queries (e.g., <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib36\" title=\"\">Li, </a>)</cite>), <span class=\"ltx_text ltx_font_italic\">Model-generated prompts</span> include those created via self-instruct techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib65\" title=\"\">2023</a>)</cite> (e.g., <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>), multi-agent simulations (e.g., <span class=\"ltx_text ltx_font_sansserif\">AI Society</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib34\" title=\"\">2023a</a>)</cite>), or reverse instruction generation (e.g., <span class=\"ltx_text ltx_font_sansserif\">LongForm</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;ksal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib32\" title=\"\">2023</a>)</cite>). Finally, <span class=\"ltx_text ltx_font_italic\">derivative datasets</span> build on existing resources through task expansion or reformatted aggregation (e.g., <span class=\"ltx_text ltx_font_sansserif\">Flan 2022</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib4\" title=\"\">Fla, </a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">xP3</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib45\" title=\"\">2022</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "sharegpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following our selection principles, we curated seven representative datasets spanning different user types, instruction methods, and domains. For <span class=\"ltx_text ltx_font_bold\">end users</span>, general-domain prompts include single-turn prompts (<span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span>) and multi-turn conversations (<span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>), while business-domain single-turn prompts are represented by <span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span>. For <span class=\"ltx_text ltx_font_bold\">LLM researchers</span>, human-generated datasets include <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> and <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and model-generated prompts are captured by <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>. For <span class=\"ltx_text ltx_font_bold\">domain scientists</span>, we include model-generated medical prompts from <span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span>. This collection ensures diversity in publisher type, prompt structure, and application domain.</p>\n\n",
                "matched_terms": [
                    "boredhumans",
                    "selfinstruct",
                    "sharegpt",
                    "oasst1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span> (<span class=\"ltx_text ltx_font_bold\">1.1k-business</span>). A curated dataset of 1&#8201;235 prompts oriented toward professional and business-related use cases, such as marketing, productivity, and decision-making. It represents structured, domain-specific prompting behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(1., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib1\" title=\"\">1</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> (<span class=\"ltx_text ltx_font_bold\">dolly-15k</span>).\nThis dataset includes 15&#8201;000 human-authored instruction&#8211;response pairs covering a range of everyday tasks. It is single-turn and domain-general, curated to support instruction-following models <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dolly15k",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span> (<span class=\"ltx_text ltx_font_bold\">medical-o1</span>).\nSynthetic data of 90&#8201;120 open-ended questions and GPT-4o generated CoTs and responses. Open-ended questions are reformatted by GPT-4o based on close-set medical examination questions. The dataset is used to fine-tune HuatuoGPT-o1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "medicalo1",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">OASST1.</span>\nThe Open Assistant dataset (OASST1) contains over 30&#8201;000 human-written messages arranged in dialogue trees. It emphasizes cooperative, open-domain assistant behavior and includes branching conversations rather than linear interactions <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;pf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib33\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "oasst1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">Self-Instruct.</span>\nA synthetic dataset with 82&#8201;646 prompts generated by large language models based on a small seed pool of human-written instructions. For every generation step, it samples 6 human-written tasks and 2 model-generated tasks in previous steps to promote diversity <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib65\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> High-frequency <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams reveal domain and prompt-engineering differences, such as role-playing cues in <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> (&#8220;you to act as&#8221;) versus medical reasoning in <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> (&#8220;what be the,&#8221; &#8220;the most likely diagnosis&#8221;). <span class=\"ltx_text ltx_font_bold\">(2)</span> While 3-grams capture general-purpose queries or commands (e.g., &#8220;what be the,&#8221; &#8220;I want to&#8221;), longer <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams (4&#8211;5) reflect task-specific patterns, as in <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> where frequent 5-grams (&#8220;please write in English language,&#8221; &#8220;write a comprehensive reply to&#8221;) highlight its instruction-following orientation. <span class=\"ltx_text ltx_font_bold\">(3)</span> Compared to Google Books 5-grams (e.g., &#8220;at the end of the,&#8221; &#8220;in whole or in part&#8221;) that serve narrative or descriptive purposes, prompt datasets exhibit inquiry- or command-focused <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams, underscoring a clear divergence in linguistic patterns across corpora.</p>\n\n",
                "matched_terms": [
                    "oasst1",
                    "sharegpt",
                    "medicalo1",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparative analysis with non-prompt text datasets, we have used Universal Dependencies\ncorpora for English: EWT <cite class=\"ltx_cite ltx_citemacro_citep\">(Silveira et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib59\" title=\"\">2014</a>)</cite> and ParTUT <cite class=\"ltx_cite ltx_citemacro_citep\">(Sanguinetti &amp; Bosco, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib55\" title=\"\">2014</a>)</cite>, where EWT contains informal contents&#8211;blog, social, reviews, email, and web, and ParTUT contains more formal contents&#8211;legal, news, and wiki.</p>\n\n",
                "matched_terms": [
                    "partut",
                    "ewt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply the spaCy <span class=\"ltx_text ltx_font_typewriter\">en_core_web_sm</span> parser <cite class=\"ltx_cite ltx_citemacro_citep\">(Honnibal &amp; Montani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib29\" title=\"\">2017</a>)</cite> to extract syntactic dependencies and determine the frequency of key grammatical relations in each dataset. For the EWT and ParTUT corpora, we rely on officially published dependency type annotations. This analysis reveals systematic variations in linguistic style across prompt sources. Additionally, we track verb&#8211;object (dobj) pairs to capture the task-oriented diversity of the prompts (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "each",
                    "partut",
                    "ewt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of Results.</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T2\" title=\"Table 2 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the distribution of eight common dependency types across seven prompt datasets and two reference corpora (EWT and ParTUT), revealing three key findings.</p>\n\n",
                "matched_terms": [
                    "partut",
                    "ewt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> The <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> dataset is characterized by its high use of adjectival modifiers (amod, 0.11) and low direct object frequency (dobj, 0.03), reflecting a preference for precise, state-oriented descriptions over action-driven narratives, often framed through linking verbs&#8212;typical of medical contexts detailing conditions, symptoms, and diagnoses. <span class=\"ltx_text ltx_font_bold\">(2)</span> In contrast, the <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> dataset favors concise, goal-driven imperatives with bare noun phrases as direct objects (dobj, 0.09) and minimal use of determiners (det, 0.05), aligning with its project-planning focus. <span class=\"ltx_text ltx_font_bold\">(3)</span> Verb&#8211;noun dependency analysis further distinguishes domains: medical instructions cluster around technical, domain-specific pairs like &#8220;have history&#8221; and &#8220;experience pain,&#8221; while datasets such as <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> use broader, generic pairs like &#8220;write answer&#8221; and &#8220;use code&#8221;. These syntactic patterns highlight each corpus&#8217; thematic priorities and inform strategies for domain-aware model training.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "its",
                    "medicalo1",
                    "det",
                    "sharegpt",
                    "noun",
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1)</span> Domain-specific datasets such as <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> and <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> exhibit a noun proportion of <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 0.26, surpassing that found in formal corpora like ParTUT. This reflects a concept-driven focus on domain entities and technical terms.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> Additionally, <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> also registers an unusually high adjective ratio (0.11), indicating a repeated emphasis on specifying medical attributes and conditions, consistent with the descriptive nature of clinical reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "partut",
                    "medicalo1",
                    "proportion",
                    "noun",
                    "indicating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze lexical patterns across prompt datasets using TF-IDF. Each dataset&#8217;s prompts are concatenated into a single document (yielding seven corpus-level documents), and a TF-IDF vectorizer (with a 5000-word limit and English stopwords removed) computes sparse term importance representations. We then assess <span class=\"ltx_text ltx_font_bold\">inter-dataset lexical similarity</span> via pairwise cosine similarity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F3.sf3\" title=\"In Figure 3 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>) and extract the top three highest-weight tokens per dataset for <span class=\"ltx_text ltx_font_bold\">intra-dataset characterization</span> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T4\" title=\"Table 4 &#8227; 5.3.2 Part-of-Speech Tagging &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1) Intra-dataset analysis</span>\ndelineates each dataset&#8217;s lexical focus and stylistic characteristics. For instance, <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> emphasizes business-specific terms like &#8220;content&#8221; and &#8220;email&#8221;, while <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span> features imperatives such as &#8220;act&#8221;, indicative of role-playing instructions. Similarly, <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> shows a dominant TF-IDF score for &#8220;output&#8221; (0.772), highlighting a structural prompt style based on explicit instruction&#8211;response formats.\n<span class=\"ltx_text ltx_font_bold\">(2) Inter-dataset comparison.</span>\nTF-IDF vectors show varying overlaps across datasets. The highest cosine similarity between <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> suggests a similar vocabulary&#8212;likely due to shared human-generation processes. In contrast, <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> is lexically distant from the others, especially <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> and <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span>, reflecting stylistic and domain-specific differences.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "selfinstruct",
                    "medicalo1",
                    "boredhumans",
                    "sharegpt",
                    "oasst1",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze prompt semantics by encoding each prompt into a 384-dimensional dense vector using Sentence-BERT&#8217;s pretrained model <span class=\"ltx_text ltx_font_typewriter\">all-MiniLM-L6-v2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Reimers &amp; Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib50\" title=\"\">2019</a>)</cite>. Each prompt is encoded into a 384-dimensional dense vector that captures its semantic content. These embeddings serve as the foundation for classification, clustering, and visualization analysis. We perform Principal Component Analysis (PCA) to reduce sentence embeddings to two dimensions. For fair comparison, we uniformly at random sample 500 prompts per dataset and visualize their distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Semantic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "each",
                    "its",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1) Wide coverage in Self-Instruct:</span> The <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> dataset exhibits the most dispersed and evenly distributed semantic space, suggesting a broad topical coverage. This aligns with the self-instruction paradigm&#8217;s goal of generating diverse instruction types.\n<span class=\"ltx_text ltx_font_bold\">(2) Semantic cohesion in specific domains:</span> Prompts from <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> form more concentrated clusters, indicating domain-specific semantic cohesion.\n<span class=\"ltx_text ltx_font_bold\">(3) Overlap among human-generated sets:</span> The embeddings of <span class=\"ltx_text ltx_font_sansserif\">dolly-15k</span>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> overlap substantially across both PCA dimensions. This suggests that these datasets share stylistic and semantic characteristics, possibly due to their common reliance on human-LLM interactions for data generation.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "selfinstruct",
                    "medicalo1",
                    "data",
                    "oasst1",
                    "sharegpt",
                    "dolly15k",
                    "dataset",
                    "indicating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the above analysis, we propose a new prompt engineering method that leverages structural linguistic features. Specifically, we take the average of the high-dimensional embeddings of POS tags and dependency relations from the analyzed dataset to define a centroid representation. This centroid captures the &#8220;central&#8221; syntactic patterns that are associated with higher-performing prompts.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "pos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each target prompt, we first analyze its POS and dependency embeddings to identify deviations from the centroid. Based on this analysis, a modification plan is generated, specifying how the prompt&#8217;s syntactic structure should be adjusted. The LLM is then guided to rewrite the prompt according to this plan, producing an optimized prompt whose embeddings are closer to the centroid. This process allows peripheral prompts that initially deviate from effective syntactic patterns to be systematically aligned with the central region of the embedding space.</p>\n\n",
                "matched_terms": [
                    "each",
                    "its",
                    "pos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset comprises over 1,000 curated ChatGPT prompt templates in Notion Workspace format, spanning diverse domains such as AI, marketing, education, healthcare, and code generation. Each entry typically includes a prompt, an automatic prompt (system prompt like), and a concise description.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Academic Reasoning and Intuition Chains dataset comprises 1,975 examples of chain-of-thought reasoning distilled from open-access arXiv papers across eight scientific domains, including Biology, Economics, Physics, Mathematics, Computer Science, Finance, Statistics, and Electrical Engineering. Each entry contains comprehensive metadata (arxiv_id, DOI, authors, dates, and categories), interactive model-generated conversations with explicit &lt;think&gt; tags, extensive chain length statistics, and multi-model verifier results with suitability scores. Licensed under Apache-2.0, this resource enables training and evaluation of budgeted chain-of-thought reasoning models with rigorous quality control.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset features thousands of prompts generated by the teknium/OpenHermes-2p5-Mistral-7B model, each designed to elicit diverse and contextually rich responses. Stored as JSON objects, it enables research in synthetic prompt generation, model creativity evaluation, and downstream fine-tuning.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Stanford Alpaca dataset comprises 52K high-quality, instruction-following examples generated via a modified Self-Instruct pipeline using text-davinci-003. Designed for fine-tuning LLaMA models, it enables research in alignment, instruction tuning, and synthetic data generation.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Alpaca_GPT4_data_zh is a Chinese instruction-tuning dataset curated by the Instruction Tuning with GPT-4 project. It comprises 48,818 examples, each featuring an instruction, optional input context, and a GPT-4-generated response, facilitating text-generation and fine-tuning tasks. The dataset occupies 32 MB and is available under a CC-BY-4.0 license for non-commercial research.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: AM-DeepSeek-Distilled-40M is a multilingual (zh/en) reasoning dataset comprising 3.34 million prompts paired with 40 million model-generated responses across code, math, science, instruction-following and general reasoning. Each query includes four samples from three models (1.5B, 7B, and R1), with pass rates computed per model to assign unbiased difficulty scores. Released under CC-BY-NC 4.0, its unified JSONL format supports supervised fine-tuning, preference learning and reinforcement learning applications, enabling selection of subsets by category or difficulty level. It fosters robust LLM development research.</p>\n\n",
                "matched_terms": [
                    "each",
                    "its",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The APIGen-MT-5k dataset comprises 5000 realistic, high-quality, multi-turn function-calling dialogues generated by APIGen-MT, a scalable automated agentic pipeline simulating agent-human interactions. Covering retail and airline domains, each trajectory is verified through format checks, function executions, and semantic validations, achieving a 99% success rate in human evaluation. Provided in ShareGPT-style JSON and licensed under CC-BY-NC-4.0, it supports question-answering, text generation, and reinforcement learning benchmarks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Aya Collection is a massive multilingual instruction tuning dataset comprising over 513 million prompt-completion pairs across 115 languages. It integrates three sources: human-crafted instruction templates created by fluent speakers for diverse tasks, machine translations of 19 top-tier datasets into 101 languages via NLLB, and the human-annotated Aya Dataset subset of 204K examples. Split by dataset, each record includes id, inputs, targets, language, script, and task type. Licensed under Apache-2.0, it supports academic and commercial classification, summarization, translation, and QA research.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Bactrian-X is a multilingual instruction-following dataset containing 3.4 million instruction-input-response triplets across 52 languages. It builds upon 67K unique English prompts drawn from Alpaca and Dolly, automatically translated via Google Translate into 51 languages. For each translated prompt (and optional input), GPT-3.5-Turbo generates a corresponding response, yielding 3.4 million examples. Each record includes an id, instruction, optional input, and model-generated output. Released under CC-BY-NC 4.0, Bactrian-X supports text-generation research, fine-tuning, and evaluation in low-resource and high-resource language settings, covering diverse tasks and domains.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Baize Chat Data is an instruction-finetuning corpus combining four sources: Alpaca, Medical, Quora, and StackOverflow. It contains about 210,000 conversational examples, each formatted with [|Human|] prompts and [|AI|] responses. Designed to enhance the Baize family of language models, this unified dataset supports interactive text generation and dialogue training. Sourced from the Baize GitHub repository, it provides diverse conversational scenarios ranging from general queries to specialized medical and technical discussions. It is optimized for instruction-following tasks. It enables realistic user interactions.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BELLE_Generated_Chat contains approx. 400k personalized Chinese character dialogues generated by the BELLE project. Each record includes an instruction, an (empty) input, and a generated output. Created by ChatGPT and not strictly verified, the dataset may contain factual inaccuracies. Licensed under GPL-3.0 for research use only. With around 0.4 million entries, it supports text-to-text generation and conversational modeling.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BELLE_Multiturn_Chat is a Chinese multi-turn conversational dataset comprising approximately 0.8 million human-assistant dialogues generated by the BELLE project using ChatGPT. Each record pairs an instruction containing prior context labeled with &#8220;Human:&#8221; and &#8220;Assistant:&#8221; with the assistant&#8217;s subsequent reply. Intended for text-to-text generation tasks, the GPL-3.0-licensed collection covers only Chinese interactions. As this data is automatically generated and unverified, factual errors and inconsistencies may arise. It is provided strictly for non-commercial research under the project&#8217;s usage restrictions; developers should validate outputs and adhere to licensing terms.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The BELLE_train_3.5M_CN dataset comprises approximately 3.5 million monolingual Chinese instruction-response pairs generated by the BELLE project, formatted as multi-turn and single-turn dialogues with unique IDs. It includes human-assistant exchanges across 13 instruction categories. Licensed under GPL-3.0, it supports text-to-text generation research exclusively; commercial or harmful use is prohibited. The JSON records each conversation&#8217;s ID and bilingual content.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BoredHumans is a diverse and extensive prompt dataset compiled from multiple sources, including Awesome ChatGPT Prompts, Data Science Prompts, and Tree-of-Thought Prompting, among others. Its rich variety covers numerous domains and prompt styles, enabling comprehensive research on prompt engineering, AI model behavior, and in-context learning strategies.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "boredhumans",
                    "its",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The ChatGPT Prompts for Data Science dataset offers a curated collection of specialized prompts designed to enhance AI applications in data science tasks. It facilitates research on natural language interfaces for data analysis, model explanation, and automation of complex workflows.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The ChatGPT Prompts dataset offers a broad collection of prompts covering diverse topics, designed for use with GPT 3.5. Its value lies in providing versatile, real-world prompt examples that support research on prompt engineering and AI interaction across various domains.</p>\n\n",
                "matched_terms": [
                    "value",
                    "its",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Chinese-DeepSeek-R1-Distill-data-110k is a 110K-entry Chinese dataset distilled from DeepSeek-R1, supporting text generation, text2text generation, and question answering under Apache-2.0. It covers four domains: Math (36 568 samples), Exam (2 432), STEM (12 648) and General (58 352). Each record includes input, reasoning content, output, source repo name and model-assigned score. Data originate from diverse math and instruction corpora, distilled via R1 with temperature 0.6, step-by-step math prompts, and validation using Math-Verify and Qwen2.5-72B.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Licensed under Apache-2.0, Chinese-DeepSeek-R1-Distill-data-110k-SFT is an open-source, Chinese-language instruction-tuning dataset distilled from DeepSeek-R1 outputs, formatted for direct supervised fine-tuning. It comprises 110K examples spanning math (36.6K), exam questions (2.4K), STEM (12.6K), and diverse general prompts (58.4K). Prompts are sourced from multiple Chinese math and STEM repositories, with distillation performed at temperature 0.6 and special step-by-step cues for calculations. Each sample includes integrated reasoning, answers, and model-based scores, facilitating reproducibility of high-performance SFT training. It supports text-generation, text-to-text generation, and question-answering tasks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: COIG-CQIA (Chinese Open Instruction Generalist - Quality is All You Need) is a high-quality, open-source Chinese instruction tuning dataset designed to align language models with human interactive behavior. It aggregates over 45,000 manually cleansed, restructured, and reviewed examples spanning social media dialogs, encyclopedic articles, exam questions, finance, medical, legal, traditional culture, and NLP tasks. Each entry includes instruction, optional input, output, task type, domain, and human verification metadata. COIG-CQIA aims to facilitate instruction fine-tuning for Chinese NLP research and applications.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: CVQA is a culturally diverse, multilingual visual question-answering benchmark featuring over 10,000 image-based questions across 39 country-language pairs. Each sample includes a locally posed query, its English translation, four answer options in both languages, and metadata such as image source, license, category, and a unique ID. Questions span ten thematic categories and images originate from self-contributed and external sources under various licenses. Designed primarily as a test set, CVQA facilitates evaluation of VQA models on nuanced, culturally contextualized visual understanding.</p>\n\n",
                "matched_terms": [
                    "each",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Databricks-dolly-15K is an open-source corpus of over 15,000 human-generated instruction-response pairs created by Databricks employees across eight behavioral categories defined by InstructGPT, including brainstorming, classification, closed and open QA, generation, information extraction, and summarization. Provided under a CC-BY-SA 3.0 license, this English-language dataset supports academic or commercial use. With context passages drawn from Wikipedia when required, it enables training and fine-tuning of large language models, as well as synthetic data generation and data augmentation for robust, scalable instruction-following capabilities.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: DeepMath-103K is a large-scale, MIT-licensed dataset comprising 103K challenging mathematical problems tailored for text-to-text and text-generation tasks. Each example includes a problem statement, a hierarchically classified topic, a numerical difficulty score, three distinct reasoning pathways (R1 solutions), and a verifiable final answer. Designed to support reinforcement learning and supervised fine-tuning, it enables difficulty-aware training, topic-specific evaluation, and robust rule-based reward shaping. Sourced and decontaminated to minimize test leakage, DeepMath-103K drives advances in automated mathematical reasoning research and diverse research areas.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: DMind_Benchmark is a comprehensive dataset for evaluating large language models on blockchain, cryptocurrency, and Web3 knowledge. It provides objective (multiple choice) and subjective (open ended) questions across nine domains: Fundamentals, Infrastructure, Smart Contracts, DeFi, DAOs, NFTs, Security, Tokenomics, and MEME coins&#8212;organized into CSV and JSONL splits. The benchmark supports diverse question types&#8212;calculations, code audits, risk and scenario analyses&#8212;with automated scoring and evaluation. It features standardized data configurations, leaderboards, and extensible evaluation pipelines for comparative analysis of LLM performance in specialized Web3 tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Dynosaur introduces a dynamic and low-cost paradigm for curating instruction-tuning datasets. It automatically generates diverse instructions by leveraging metadata from HuggingFace datasets, combined with LLM-based instruction synthesis (e.g., via ChatGPT). The result is Dynosaur-full, a large-scale dataset (800K+ samples, generated at &#160; $11.5) that supports dynamic growth and general-purpose instruction-tuning. Empirically, models fine-tuned on Dynosaur outperform Alpaca and GPT-4-Instruct baselines on Super-NI. The project includes: metadata crawling tools, instruction generation pipelines, and fine-tuned T5-3B and LLaMA-7B models. All generated instructions are under Apache 2.0, with task data adhering to original dataset licenses.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Firefly is a Chinese instruction-tuning dataset comprising 1.15 million high-quality examples drawn from 23 common Chinese natural language processing datasets. Each example includes a task type, an input prompt, and a target output, ensuring diverse coverage. Data templates were manually designed for each task to ensure quality and richness. Token length analysis shows that most examples are under 600 tokens. Firefly was used to train the Firefly-1b4 Chinese dialogue LLM, available on GitHub and Hugging Face, fostering reproducibility, community collaboration.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset aggregates tasks from Flan, T0, Super-Natural Instructions, Chain-of-Thought, and Dialog into a training split. Each task is provided in zero-/few-shot and option/no-option formats as JSONL entries including inputs, targets, and task identifiers. Released under Apache-2.0, it includes scripts for building dependencies, fixing version mismatches, and exporting per-task JSONL data. Mixing ratios can be tuned for optimal downstream performance via guidelines in the associated paper and public GitHub repository.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Flan-mini is a curated 1.34 M-example subset of the FLAN instruction-tuning collection augmented with code and conversational tasks. It pools 388K Flan2021 instructions, 320K public prompt templates, 200K Natural Instructions v2 instances, 100K chain-of-thought examples, plus code datasets (100K Code Search, 50K Code Contests, 50K APPS). It further integrates 132K ChatGPT-generated examples from GPT-4-Alpaca, Code-Alpaca, and ShareGPT. Each example is randomly paired with handcrafted prompt templates for zero- or few-shot fine-tuning, ensuring diverse task coverage. Released under a permissive CC license.</p>\n\n",
                "matched_terms": [
                    "each",
                    "sharegpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: GraphWalks is an open-source benchmark dataset designed to evaluate multi-hop reasoning over long graph contexts. Released under the MIT license, it provides directed graphs as edge lists alongside user-specified operations&#8212;such as breadth-first searches or parent retrieval&#8212;for models to execute. Each prompt comprises three demonstration examples, a target graph, and a query, with expected outputs formatted as node ID lists. Accompanying metadata includes prompt character counts and problem types. Standardized extraction and F1-based grading scripts ensure consistent answer parsing and evaluation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: GSM8K (Grade School Math 8K) is an English monolingual dataset of 8.8K crowd-sourced grade school math word problems paired with multi-step solutions. It contains a main configuration and a Socratic variant, each offering questions and answers with calculator annotations and step-by-step reasoning expressed in natural language. Problems require two to eight elementary arithmetic steps. Split into training (7,473 examples) and test (1,319 examples), GSM8K supports text-to-text generation benchmarks under MIT license. All annotations were crowdsourced via Upwork and Surge AI.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Intellect-2-RL-Dataset is a large-scale collection of 284,741 training examples, designed for reinforcement learning in mathematical and coding problem solving. Each entry includes a unique problem_id, a task_type label, the problem prompt, verification_info detailing solution validity, and a baseline solve_rate from the Qwen-R1-Distill-7B model. Released under Apache-2.0 license, this dataset supports fine-tuning and evaluation of reasoning-oriented language models, facilitating research on algorithmic proficiency and reward-driven optimization within distributed asynchronous RL frameworks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LaMini-Instruction is an English text-to-text generation dataset comprising 2.58M instruction-response pairs distilled from GPT-3.5-Turbo. Each sample includes an instruction, a corresponding model-generated response, and the instruction&#8217;s provenance&#8212;drawn from sources such as Alpaca, FLAN, P3, and Self-Instruct. Released under CC-BY-NC 4.0, it spans a single training split of over 1.16 GB and supports fine-tuning of compact language models. LaMini-Instruction enables research in instruction-based learning but inherits biases and errors from its GPT-3.5 teacher.</p>\n\n",
                "matched_terms": [
                    "each",
                    "selfinstruct",
                    "its",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Llama-Nemotron-Post-Training-Dataset is a comprehensive dataset of synthetic SFT and RL samples designed to bolster reasoning, code, math, science, chat, and safety capabilities for NVIDIA&#8217;s Llama-3 Nemotron series. It includes over 33M SFT examples across code, math, science, chat, and safety, plus 56K instruction-following RL examples. Data is sourced from public corpora or synthetically generated, filtered for quality and complexity. Released under CC-BY-4.0, it supports training and evaluation of efficient open-source LLMs offering a flexible accuracy-efficiency tradeoff and transparent development.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LMSYS-Chat-1M is a large-scale dataset of one million real-world LLM conversations, collected from 210K users interacting with 25 models via Chatbot Arena and Vicuna demo (April-August 2023). Each conversation includes model metadata, OpenAI-style JSON formatting, language tags, and moderation labels. Personally identifiable information is redacted. This dataset enables research on LLM alignment, safety, evaluation, and user behavior in the wild, offering unique insights into real-world usage patterns and content moderation challenges in multi-model deployment scenarios.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: medical-o1-reasoning-SFT is a supervised fine-tuning dataset designed to enhance advanced medical reasoning in HuatuoGPT-o1. It comprises English and Chinese instruction-response pairs generated by GPT-4o on verifiable clinical problems, validated by a medical verifier. Released under an Apache-2.0 license, the dataset supports question answering and text generation, offering separate configurations for monolingual and mixed-language data. It aims to refine model performance on complex biomedical tasks by leveraging rigorous problem-solving chains, with full details available in the accompanying paper and GitHub repository.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "full",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: medical-o1-verifiable-problem is an Apache-2.0 licensed dataset comprising open-ended medical reasoning problems designed to improve large language models&#8217; diagnostic and procedural knowledge. It supports question-answering and text-generation tasks, presenting each instance as a challenging exam-style prompt paired with a verifiable, expert-derived answer. Published in English under a single default configuration with training data provided in JSON format, it allows systematic evaluation and refinement of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Medical-R1-Distill-Data is an Apache-2.0 licensed instruction fine-tuning dataset distilled from Deepseek-R1&#8217;s Full Power Version using medical verifiable problems sourced from HuatuoGPT-o1. It supports English and Chinese, and is tailored for question-answering and text-generation tasks in medical and biology domains. The dataset captures reasoning chains from the native Deepseek-R1 API, facilitating model initialization with robust medical reasoning. A Chinese counterpart is available separately. Methodology and guidelines are provided in the associated paper and GitHub repository. It comprises SFT examples from medical_r1_distill_sft.json.</p>\n\n",
                "matched_terms": [
                    "full",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: MedReason is a large-scale medical reasoning dataset combining seven clinical question-answer sources with a structured knowledge graph to produce detailed chains of reasoning. It contains 32,682 QA pairs, each annotated with step-by-step explanatory &#8220;thinking paths&#8221; derived from standardized medical KG relations. Designed to enhance the faithfulness and interpretability of medical problem-solving in large language models, MedReason enables fine-tuning of models such as MedReason-8B, which demonstrates state-of-the-art performance. Released under Apache-2.0, this open-source dataset aims to foster transparent medical QA systems.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Nemotron-CrossThink is a multi-domain reinforcement learning dataset designed to enhance both general-purpose and mathematical reasoning in large language models. It comprises two subsets: Nemotron-CrossThink-QA with high-quality question-answer pairs across STEM, humanities, and sciences, and Nemotron-CrossThink-Math featuring persona-driven, multi-step math problems. Data is curated from CommonCrawl and open-source books, standardized via structured templates into multiple-choice and open-ended formats, filtered for verifiability, and used to train RL policies with Group Relative Policy Optimization. Licensed under CC-BY-4.0, it supports AI development.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: No Robots is a high-quality, human-curated instruction dataset comprising 10,000 examples for supervised fine-tuning of language models. It includes 9,500 training and 500 test instances across ten single-turn categories&#8212;Generation, Open QA, Brainstorm, Chat, Rewrite, Summarize, Coding, Classify, Closed QA, and Extract&#8212;totaling roughly 17 MB of English text under CC-BY-NC-4.0. Each example consists of a prompt with unique ID, structured message history (system, user, assistant), and category labels. It enables models to learn diverse instruction-following behaviors and robustly supports reproducibility.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: NuminaMath-1.5 is an open-source, large-scale post-training dataset comprising about 900 000 competition-level mathematics problems paired with chain-of-thought solutions. It covers diverse sources&#8212;from Chinese high school exams to US and international Olympiads&#8212;and spans domains like algebra, geometry, number theory, combinatorics, calculus, and puzzles. Each entry includes metadata fields (answer, problem_type, question_type) for verifiable outputs. Recent additions feature manually verified Olympiad references and curated contest data while synthetic problems were removed. Licensed under Apache 2.0, NuminaMath-1.5 supports advanced text-generation research in mathematical reasoning.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenAssistant Conversations (OASST1) is a human-generated, human-annotated corpus with 161,443 messages in 66,497 conversation trees across 35 languages. It includes over 461,000 quality ratings and more than 10,000 fully annotated trees. Each record contains metadata (IDs, timestamps), conversational structure (parent and tree IDs), role and language labels, toxicity and quality scores, emoji labels. Data comes in nested JSONL or flat parquet via HuggingFace, with 84,437 training and 4,401 validation splits, supporting supervised fine-tuning and reward model development. Licensed under Apache-2.0.</p>\n\n",
                "matched_terms": [
                    "each",
                    "oasst1",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OL-CC is the first open source Chinese conversational instruction dataset collected via crowdsourcing on OpenLabel. It includes 10,006 instruction-answer pairs and 1,649 standalone instructions across tasks such as question-answering, text generation, extraction, rewriting, classification, brainstorming, chit-chat, logic and math. A total of 276 volunteers alternately played user and AI assistant roles to produce the data. Licensed under Apache-2.0 and sized between 10K and 100K examples, OL-CC offers rich, human-generated Chinese instructional dialogues for AI research.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenCodeReasoning is a large-scale synthetic dataset designed to distill reasoning capabilities for Python-based competitive programming. It comprises 735,255 samples covering 28,319 unique problems sourced from platforms like CodeForces, AtCoder, and LeetCode. The dataset features two configurations: split_0 includes full problem statements and model responses, while split_1 references external datasets via index placeholders. Each example contains identifiers, source metadata, difficulty labels, and code solutions. Licensed under CC-BY-4.0, OpenCodeReasoning supports supervised fine-tuning of language models for code generation tasks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "full",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenMathReasoning is a large-scale English math-reasoning dataset (cc-by-4.0) comprising 290K+ olympiad problems with 3.2M chain-of-thought (CoT), 1.7M tool-integrated reasoning (TIR), and 566K GenSelect solution samples. Sourced from AoPS and processed with Qwen2.5-32B, DeepSeek-R1, and QwQ-32B, each record includes problem statements, generated solutions, expected answers, inference modes, metadata, and pass-rate metrics. Available in cot, tir, and genselect splits, it underpins state-of-the-art LLM training and evaluation in question-answering and text-generation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Phoenix-sft-data-v1 is a multilingual supervised fine-tuning dataset containing 464,510 samples, combining instruction-following and ChatGPT-distilled conversation data. It includes Alpaca-derived tasks, post-translated multilingual instructions, and user-centered prompts in 40 languages. The dataset also integrates ShareGPT and Discord-sourced dialogues. With nearly 1 million conversation turns and detailed multilingual annotations, it supports multilingual language modeling, alignment, and chat adaptation. English and Chinese dominate the corpus, with broader linguistic diversity represented across the remaining data, enabling robust multilingual model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "sharegpt",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: PLM-Video Human is a large-scale human-annotated video understanding dataset for Vision-Language Model training, covering four tasks: fine-grained video question answering (FGQA) with 2.3M QA pairs, region-based video captioning (RCap), dense captioning (RDCap), and temporal localization (RTLoc). Each config provides annotated clip segments with questions, answers, captions, masks, start/end frames, and metadata drawn from diverse open-access sources. Released under CC-BY-4.0, PLM-Video Human supports detailed temporal, spatial, and semantic modeling of complex human activities across diverse realistic dynamic video scenarios.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: PolyMath is a multilingual mathematical reasoning benchmark offering parallel problem sets in 18 languages across four difficulty tiers&#8212;K-12 to advanced mathematics&#8212;with splits labeled top, high, medium, and low. Each language contains 125 challenges per level, categorized by thought depth and knowledge breadth. The dataset ensures coverage of problem complexity and wide language representation, spanning over 75% of native speakers. High-quality translations validated by language experts guarantee clarity. PolyMath evaluates large language models&#8217; reasoning capabilities in diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The PRISM Alignment Dataset is a large-scale human feedback resource designed to assess preference and value alignment in large language models (LLMs). It consists of detailed survey responses from 1,500 participants across 75 countries, followed by multi-turn conversations with 21 LLMs. Participants rate model outputs on a 1-100 scale and provide fine-grained feedback, yielding 8,011 conversation trees and 68,371 scored utterances. The dataset includes four JSONL configurations&#8212;survey, conversations, utterances, and metadata&#8212;licensed under CC-BY and CC-BY-NC for research and educational use.</p>\n\n",
                "matched_terms": [
                    "value",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: RedGPT Dataset (V1-CN) offers 50,000 automatically generated multi-turn Chinese dialogues grounded in high-quality factual references from diverse domains such as history, science, law, and culture. Designed to enhance GPT models&#8217; factual accuracy, the dataset enables fine-tuning on realistic, knowledge-rich conversational data without costly manual annotation. It supports research in improving language models&#8217; truthfulness, dialogue generation, and knowledge integration.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: RepLiQA is a specialized QA dataset of 71,820 human-created Context-Question-Answer triplets from fictitious, natural-looking documents across 17 topics (e.g., local news, folklore, cybersecurity). Designed to test LLMs&#8217; ability to leverage novel reference texts without relying on memorized facts, each document includes five questions with &#160;20% unanswerable. Fields include document IDs, topics, extracted text, questions, answers and long answers. Released under CC-BY-4.0 in four splits, RepLiQA supports question answering, text classification, topic retrieval and selective QA benchmarking.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Self-Instruct is an open Apache-2.0-licensed dataset and framework designed to enhance language models&#8217; instruction-following capabilities. It comprises four configurations: a self-generated set of 82K prompt-completion pairs produced via OpenAI&#8217;s davinci engine; 50K samples from Super Natural Instructions; 52K prompts drawn from the P3 public pool; and 252 expert-crafted human evaluation tasks with associated inputs and outputs. All data is in English and supports instruction-tuning by providing diverse natural-language prompts paired with corresponding model or human completions. The dataset facilitates instruction-tuning.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: ShareGPT90K is a dataset of 90,665 conversational threads scraped from the ShareGPT platform. Each example includes a unique id and a sequence of messages, with each message annotated by its origin and its content.</p>\n\n",
                "matched_terms": [
                    "each",
                    "sharegpt",
                    "its",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Skywork-OR1-RL-Data is a large-scale reinforcement learning dataset featuring 105,055 math problems and 14,057 coding questions curated for the Skywork-OR1 model series. Each example includes source attribution, structured prompts with roles, model-aware difficulty ratings for DeepSeek-R1 variants, and a reward model with ground truth and style labels. Problems are rigorously cleaned, deduplicated, and filtered by difficulty per variant. The dataset supports math and code splits totaling 1.5 billion bytes and facilitates robust reasoning training with rule-based RL recipes via curated pipelines efficiently.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: SocialMaze is a question-answering benchmark designed to evaluate large language models&#8217; social reasoning via hidden role deduction games. Each scenario presents a multi-agent setup where agents (Investigators, Criminal, Rumormongers, Lunatics) make public statements over three rounds. Models receive system prompts and dialogues, then must identify the true Criminal and Player 1&#8217;s actual role. The dataset includes precise QA pairs, chain-of-thought reasoning, and supports easy (6-player) and hard (10-player) splits, facilitating fine-tuning, evaluation, and analysis of complex inference under deception. CC-BY-4.0 licensed.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraFeedback is an MIT-licensed, open-source, large-scale preference dataset designed for training reward and critic models. It contains 64 K prompts drawn from UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA and FLAN, each answered by four out of 17 diverse LLMs under five alignment principles. The result is 256 K responses and 380 K fine-grained annotations covering instruction-following, truthfulness, honesty and helpfulness, all rated by GPT-4. Its scale, diversity and dense numerical plus textual feedback make it ideal for RLHF research and robust reward-model development.</p>\n\n",
                "matched_terms": [
                    "each",
                    "sharegpt",
                    "its",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraMedical is a large-scale English biomedical instruction dataset featuring over 409,000 examples licensed under MIT. Each sample includes an identifier, instruction type, multi-turn conversation pairs between human queries and GPT-generated responses, a ground-truth answer, and a model-evaluated score. The training split comprises roughly 1.2 GB across 410K examples, sourced from both curated public data and synthetic augmentations. UltraMedical aims to support the development of specialized generalist models in biomedicine by providing diverse, high-quality instruction-response instances, and comprehensive evaluation metrics accompany each instance.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Unnatural Instructions is a large-scale dataset of automatically generated instruction-input-output triplets designed to facilitate instruction tuning of language models with minimal human effort. It contains over 240,000 examples, including original instructions, associated inputs, outputs, and optional constraints. Each instance also features multiple reformulations&#8212;paraphrased variants of instructions complete with inputs and outputs&#8212;to enhance model robustness. The publicly available training split comprises around 66,000 examples. This dataset supports research in instruction following, prompt paraphrasing, and evaluating model generalization across diverse complex tasks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: WebGLM-QA is an English monolingual dataset designed for question answering and text generation, used to train the WebGLM generator. It contains 43,579 training samples, 1,000 validation examples, and 400 test instances. Each record pairs a user-posed question with a generated answer and a list of reference snippets that support the response. Hosted on Hugging Face, it provides a consistent structure&#8212;question, answer, references&#8212;enabling work on dialogue systems, retrieval-augmented generation, and answer justification.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Wizard_evol_instruct_196K is a MIT-licensed instruction-tuning dataset comprising 143K evolved QA pairs derived from Alpaca and ShareGPT. It represents an optimized version of the Evol-Instruct data used to train the WizardLM family of models. To assemble the complete instruction set of roughly 196K samples, users must merge this release with the original unfiltered ShareGPT dataset. The refined examples cover diverse conversational and instructional scenarios, facilitating improved alignment and performance in downstream open-source large language models, including structured prompts and responses.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "sharegpt",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: xP3 (Crosslingual Public Pool of Prompts) is a multilingual prompt and dataset collection spanning 46 languages and 13+ NLP tasks (e.g., QA, translation, summarization, code generation). Assembled from expert-generated and crowdsourced annotations under an Apache-2.0 license, it supports zero-shot and instruction-tuning for models like BLOOMZ and mT0. The training mixture covers closed-book and extractive QA, multiple-choice, paraphrase identification, program synthesis, sentiment analysis, structure-to-text, summarization, classification and more, totaling over 788 million samples. xP3 streamlines reproducible multilingual finetuning across diverse data scales.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Zhihu-KOL is a large-scale Chinese question-answering dataset derived from the Zhihu platform, designed for training open-domain assistants. It comprises 1,006,218 training instances of instruction-response pairs, each annotated with source and metadata fields.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams phrases of some datasets include abnormal content (e.g. &#8220;<span class=\"ltx_text ltx_font_italic\">identify which instrument be string</span>&#8221; in <span class=\"ltx_text ltx_font_sansserif\">dolly-15</span> and &#8220;<span class=\"ltx_text ltx_font_italic\">The quick brown fox jumps over the lazy dog</span>&#8221; in <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>), which indicates that there is a lot of repetition in the input content of the template tasks or some instructions used to construct the dataset, which may affect the balance of the dataset.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the complete experimental data for all identified dependency types, along with their proportions in the datasets, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.T5\" title=\"Table 5 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Additionally, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.T6\" title=\"Table 6 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> lists all detected Part-of-Speech tags and their corresponding proportions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F8\" title=\"Figure 8 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> further illustrates the ten most common verbs and their top five direct noun objects found in the prompt datasets except <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>, which are shown in the main paper.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "medicalo1",
                    "noun",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These additional data further support our conclusions.\n<span class=\"ltx_text ltx_font_bold\">(1)</span> The <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> dataset, which consists of professionally crafted medical prompts, exhibits a relatively high proportion of numerical modifiers (nummod, 0.0276) and passive auxiliaries (auxpass, 0.0101) in dependency analysis, as well as a notably high usage of numerals (NUM, 0.0309) in POS tagging. These features reflect a terminology-dense and precision-oriented language style that emphasizes processes and outcomes rather than agents.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> In the <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> dataset, the verb-noun pairs reflect language commonly used in business contexts, such as <span class=\"ltx_text ltx_font_italic\">&#8220;create plan&#8221;</span> and <span class=\"ltx_text ltx_font_italic\">&#8220;create strategy&#8221;</span>. In contrast, the verb-noun pairs observed in <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> suggest more generic and broadly applicable usage scenarios.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "selfinstruct",
                    "medicalo1",
                    "data",
                    "pos",
                    "proportion",
                    "boredhumans",
                    "oasst1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Anomalously, in the <span class=\"ltx_text ltx_font_sansserif\">dolly-15k</span> dataset, the most frequent verb-noun pairs exhibit a skewed distribution, with the highest-frequency nouns overwhelmingly associated with only the top one or two verbs. Moreover, these frequent verb-noun pairs often lack clear task-specific semantics&#8212;for example, <span class=\"ltx_text ltx_font_italic\">&#8220;tell i&#8221;</span>, <span class=\"ltx_text ltx_font_italic\">&#8220;give list&#8221;</span>, and <span class=\"ltx_text ltx_font_italic\">&#8220;classify each&#8221;</span>. This pattern may be attributed to the manual generation process, which is susceptible to the individual linguistic habits of annotators.</p>\n\n",
                "matched_terms": [
                    "dolly15k",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show the distribution of sampled embedding points after PCA for all datasets (except for <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>, which are shown in the main paper) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F9\" title=\"Figure 9 &#8227; F.3 Semantic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "medicalo1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can still observe from the results that datasets with more concentrated topical focus (e.g., <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span>) exhibit clear clustering patterns, whereas those with broader thematic coverage (e.g., <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>) display a more dispersed distribution of data points.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "11kbusiness",
                    "data"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "Large Language Model Prompt Datasets: An In-depth Analysis and Insights",
        "caption": "Table 4: Top-3 tokens with the highest TF-IDF weights per dataset",
        "body": "Dataset\nTop-3 tokens\n\n\n\n\n1.1k-business\ncontent (0.308), email (0.284), marketing (0.245)\n\n\nBoredHumans\nact (0.269), want (0.261), write (0.217)\n\n\ndolly-15k\nlist (0.338), given (0.246), following (0.241)\n\n\nmedical-o1\nold (0.427), year (0.367), patient (0.256)\n\n\nOASST1\nwrite (0.307), like (0.216), does (0.207)\n\n\nSelf-Instruct\noutput (0.766), input (0.292), task (0.243)\n\n\nShareGPT\nwrite (0.190), use (0.180), data (0.157)",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Top-3 tokens</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.1k-business</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">content (0.308), email (0.284), marketing (0.245)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">BoredHumans</span></td>\n<td class=\"ltx_td ltx_align_left\">act (0.269), want (0.261), write (0.217)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">dolly-15k</span></td>\n<td class=\"ltx_td ltx_align_left\">list (0.338), given (0.246), following (0.241)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">medical-o1</span></td>\n<td class=\"ltx_td ltx_align_left\">old (0.427), year (0.367), patient (0.256)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">OASST1</span></td>\n<td class=\"ltx_td ltx_align_left\">write (0.307), like (0.216), does (0.207)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Self-Instruct</span></td>\n<td class=\"ltx_td ltx_align_left\">output (0.766), input (0.292), task (0.243)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">ShareGPT</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">write (0.190), use (0.180), data (0.157)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "11kbusiness",
            "old",
            "selfinstruct",
            "medicalo1",
            "data",
            "weights",
            "oasst1",
            "act",
            "given",
            "want",
            "like",
            "sharegpt",
            "use",
            "patient",
            "year",
            "email",
            "marketing",
            "following",
            "write",
            "highest",
            "content",
            "tokens",
            "dataset",
            "input",
            "top3",
            "list",
            "does",
            "task",
            "boredhumans",
            "dolly15k",
            "tfidf",
            "output"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We analyze lexical patterns across prompt datasets using TF-IDF. Each dataset&#8217;s prompts are concatenated into a single document (yielding seven corpus-level documents), and a TF-IDF vectorizer (with a 5000-word limit and English stopwords removed) computes sparse term importance representations. We then assess <span class=\"ltx_text ltx_font_bold\">inter-dataset lexical similarity</span> via pairwise cosine similarity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F3.sf3\" title=\"In Figure 3 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>) and extract the top three highest-weight tokens per dataset for <span class=\"ltx_text ltx_font_bold\">intra-dataset characterization</span> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T4\" title=\"Table 4 &#8227; 5.3.2 Part-of-Speech Tagging &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">A prompt is a natural language instruction that defines a specific task for a large language model (LLM) and serves as the primary interface for human-LLM interaction. With the growing deployment of LLMs, diverse prompt datasets are emerging from platforms such as GitHub and social media. These datasets span a wide array of applications and content types, facilitating both broader LLM utilization and improved prompt engineering.\nIn this work, we&#8211;for the first time&#8211;have compiled an extensive list of prompt datasets sourced from various channels, representing a spectrum of downstream tasks, languages, engineering techniques, attributes, and modalities. We select key representative datasets for systematic analysis, revealing commonalities and differences in prompt construction across categories, distinguishing them from other text corpora like literature and web.\nWe further propose a prompt optimization approach that leverages syntactic embeddings of part-of-speech and dependency structures. By identifying a centroid representation of prompts and guiding LLMs to rewrite prompts toward this centroid, our method improves the meaningfulness of model outputs.\nWe have made our datasets and code available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/LLM-Prompt-Datasets-7416\" title=\"\">https://anonymous.4open.science/r/LLM-Prompt-Datasets-7416</a>.</p>\n\n",
                "matched_terms": [
                    "list",
                    "task",
                    "like",
                    "content"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, prior research has largely neglected comprehensive examinations of available prompt datasets. To address this gap, we apply stringent criteria to select, refine, and evaluate datasets that enable analysis of diverse prompts across multiple sources, content types, and target applications. Our survey encompasses over 1.22 TB of data, comprising more than 673M prompt instances from 129 heterogeneous sources. Our first contribution is a hierarchical taxonomy of LLM prompt datasets that serves as a detailed reference for researchers and informs future studies.</p>\n\n",
                "matched_terms": [
                    "content",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Tools and frameworks for prompt engineering.</span>\nThe prompt report <cite class=\"ltx_cite ltx_citemacro_citep\">(Schulhoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib57\" title=\"\">2024</a>)</cite> offers a thorough survey of prompt engineering techniques, providing detailed scheme definitions and corresponding examples.\nSeveral works focus on developing tools that streamline prompt construction. Both\nPromptAid&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mishra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib43\" title=\"\">2025</a>)</cite> and PromptLandscape&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib63\" title=\"\">2024a</a>)</cite> present visual support systems to simplify the creation and engineering of prompts.\nPEPR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Feffer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib28\" title=\"\">2024</a>)</cite> assesses various prompt combinations to determine the most optimal one for a given scenario.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Saletta &amp; Ferretti (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib53\" title=\"\">2024</a>)</cite> introduce a grammar-based evolutionary method to systematically optimize prompts for specific use cases.\nPromptaware&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib23\" title=\"\">2025</a>)</cite> integrates software engineering principles into the prompt engineering process. There are also works proposing solutions for generating prompts for specific scenarios.\nPromptAgent&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib64\" title=\"\">2024b</a>)</cite> introduces a model that automatically crafts and optimizes prompts with quality on par with those handcrafted by experts.</p>\n\n",
                "matched_terms": [
                    "use",
                    "given"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data discovery guideline.</span> We employ a systematic dataset discovery process across multiple sources to compile a diverse repository of prompt datasets. Our objective is to capture real-world, user-generated prompts, instruction-following interactions, and domain-specific scenarios. In particular, our primary objectives for datasets discovery are three-fold: <span class=\"ltx_text ltx_font_bold\">(1)</span> collecting datasets that are composed of prompts, i.e., natural language instructions that describe a certain task the LLM should perform and guide the LLM towards generating a desired output; <span class=\"ltx_text ltx_font_bold\">(2)</span> ensuring that the extracted data cover various domains, including day-to-day scenarios such as travel planning, professional scenarios such as academic writing, and specialized scenarios such as healthcare and finance; and <span class=\"ltx_text ltx_font_bold\">(3)</span> allowing different forms of prompts, e.g., single instruction, conversations, etc.</p>\n\n",
                "matched_terms": [
                    "output",
                    "dataset",
                    "task",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data discovery process.</span> We collect publicly available datasets from the following four types of sources. <span class=\"ltx_text ltx_font_bold\">First</span>, we consult <span class=\"ltx_text ltx_font_italic\">dataset collection platforms</span>, including Hugging Face Datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib12\" title=\"\">hug, </a>)</cite>, Kaggle <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib13\" title=\"\">kag, </a>)</cite>, Google Dataset Search <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib11\" title=\"\">goo, </a>)</cite>, and Papers with Code <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib14\" title=\"\">pap, </a>)</cite>. Targeted searches using keywords, e.g., \"prompt dataset\", \"instruction-following dataset\", and \"conversation dataset\" yield 60 prompt datasets. <span class=\"ltx_text ltx_font_bold\">Second</span>, we review the latest <span class=\"ltx_text ltx_font_italic\">academic publications</span>, specifically papers on prompt engineering, natural language understanding, and dialogue systems, published at NeurIPS, ICLR, and ICML between 2023-2024 and identify 73 datasets shared across them. <span class=\"ltx_text ltx_font_bold\">Third</span>, we also examine <span class=\"ltx_text ltx_font_italic\">public repositories</span> by systematically surveying open-source GitHub projects using keywords, e.g., &#8220;prompt collection&#8221;, &#8220;LLM prompts&#8221;, and &#8220;instruction dataset&#8221;. We identify 21 prompt repositories that typically contain curated prompt lists derived from user interactions or synthesized from public APIs. Some of these repositories are &#8220;awesome-lists&#8221;, which are curated collections of high-quality prompts or links to prompt datasets.\nNotable examples include <span class=\"ltx_text ltx_font_sansserif\">Awesome Instruction Datasets</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib46\" title=\"\">Nie, </a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering Guide</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Saravia, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib56\" title=\"\">2022</a>)</cite>, and <span class=\"ltx_text ltx_font_sansserif\">LLMDataHub</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib71\" title=\"\">Zhao, </a>)</cite>. <span class=\"ltx_text ltx_font_bold\">Finally</span>, we extract 14 datasets from <span class=\"ltx_text ltx_font_italic\">popular websites dedicated to prompt-sharing</span>, including <span class=\"ltx_text ltx_font_sansserif\">Prompt Genius</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Pro, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib7\" title=\"\">b</a>)</cite> and <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib10\" title=\"\">bor, </a>)</cite>. These platforms feature user-written prompts for practical purposes.</p>\n\n",
                "matched_terms": [
                    "following",
                    "boredhumans",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data filtering.</span>\nWe remove duplicate entries (e.g., CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib51\" title=\"\">2024</a>)</cite> appears in both Hugging Face and NeurIPS 2024) and then filter the remaining candidates using four quality criteria for inclusion in this paper.\n<span class=\"ltx_text ltx_font_bold\">First</span>, <span class=\"ltx_text ltx_font_italic\">Dataset size.</span> We prioritize datasets containing at least 1K prompts to ensure robustness in diversity and statistical power. In contrast, due to their generally limited scope, user-shared datasets are filtered with a minimum threshold of 50 prompts.\n<span class=\"ltx_text ltx_font_bold\">Second</span>, <span class=\"ltx_text ltx_font_italic\">Data quality.</span> We evaluate the quality of prompts based on their cleanliness. Most datasets (e.g., <span class=\"ltx_text ltx_font_sansserif\">OpenCodeReasoning</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib16\" title=\"\">2025b</a>)</cite>) on data hosting platforms (e.g., Hugging Face and Kaggle) are well-formatted and clean. For the remaining data, we exclude samples with inconsistent formatting or unclear structure. For instance, the <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering Guide</span>&#8211;a resource that offers both curated prompt datasets and instructional examples&#8211;contains many illustrative prompts scattered throughout the material and are thus omitted from our datasets.\n<span class=\"ltx_text ltx_font_bold\">Third</span>, <span class=\"ltx_text ltx_font_italic\">Data relevance.</span> We assess whether the prompts are aligned with our data discovery guidelines, specifically emphasizing on those that represent common usage scenarios for broad audiences (e.g., <span class=\"ltx_text ltx_font_sansserif\">Chinese-DeepSeek-R1-Distill-data-110k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib38\" title=\"\">2025a</a>)</cite>), and tasks from various domains (e.g., <span class=\"ltx_text ltx_font_sansserif\">Medical Verifiable Problems</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">OpenMathReasoning</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Moshkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib44\" title=\"\">2025</a>)</cite>). Datasets that violate our discovery guidelines are omitted. For instance, the <span class=\"ltx_text ltx_font_sansserif\">PersonaHub</span> dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib5\" title=\"\">Per, </a>)</cite> is excluded because it does not meet <span class=\"ltx_text ltx_font_bold\">data discovery guideline (1)</span>, which mandates that prompts be linked to specific, well-defined tasks. Although <span class=\"ltx_text ltx_font_sansserif\">PersonaHub</span> demonstrates the potential of synthetic personas in generating diverse content (e.g., reasoning problems, dialogues, or non-player character behaviors), it predominantly comprises persona descriptions without clear task formulation.\n<span class=\"ltx_text ltx_font_bold\">Fourth</span>, <span class=\"ltx_text ltx_font_italic\">Accessibility.</span> Datasets must be publicly accessible or retrievable via automated crawling, and their licensing terms must permit research use. After filtering, we identify 129 distinct prompt datasets for taxonomic analysis (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S4\" title=\"4 Dataset Taxonomy &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "does",
                    "task",
                    "data",
                    "use",
                    "content",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Publisher</span> denotes the source&#8217;s identity and intent. We distinguish among <span class=\"ltx_text ltx_font_italic\">end users</span> who share prompts for practical tasks like writing/ coding (e.g., <span class=\"ltx_text ltx_font_sansserif\">Prompt Genius</span>), <span class=\"ltx_text ltx_font_italic\">LLM researchers</span> who publish prompts for fine-tuning and benchmarking (e.g., <span class=\"ltx_text ltx_font_sansserif\">OpenMathReasoning</span> by NVIDIA), and <span class=\"ltx_text ltx_font_italic\">domain scientists</span> who use LLMs in their specific fields (e.g., <span class=\"ltx_text ltx_font_sansserif\">ChatGPT Data Science Prompts</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib60\" title=\"\">Tang, </a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "like",
                    "use",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Release channel</span> refers to the platform where a dataset is published. Common platforms include <span class=\"ltx_text ltx_font_italic\">data hosting sites</span> such as GitHub, Hugging Face, and Kaggle, where structured prompt formats (e.g., CSV, JSON) dominate. <span class=\"ltx_text ltx_font_italic\">Personal sites</span> or <span class=\"ltx_text ltx_font_italic\">notes</span> (e.g., Notion workspaces) often host informal, user-oriented prompts. Dedicated <span class=\"ltx_text ltx_font_italic\">prompt sharing websites</span> vary from open-access (e.g., <span class=\"ltx_text ltx_font_sansserif\">QuickRef.ME</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib8\" title=\"\">Qui, </a>)</cite>) to commercial marketplaces (e.g., <span class=\"ltx_text ltx_font_sansserif\">PromptBase</span>). <span class=\"ltx_text ltx_font_italic\">Social media</span>, like Reddit&#8217;s <span class=\"ltx_text ltx_font_sansserif\">r/ChatGPTPromptGenius</span>, also plays a key role in community-driven prompt exchange.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "like",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generation process</span> describes how the prompts are created. <span class=\"ltx_text ltx_font_italic\">Human-generated prompts</span> are either manually authored (e.g., <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>)</cite>) or collected from user queries (e.g., <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib36\" title=\"\">Li, </a>)</cite>), <span class=\"ltx_text ltx_font_italic\">Model-generated prompts</span> include those created via self-instruct techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib65\" title=\"\">2023</a>)</cite> (e.g., <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>), multi-agent simulations (e.g., <span class=\"ltx_text ltx_font_sansserif\">AI Society</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib34\" title=\"\">2023a</a>)</cite>), or reverse instruction generation (e.g., <span class=\"ltx_text ltx_font_sansserif\">LongForm</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;ksal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib32\" title=\"\">2023</a>)</cite>). Finally, <span class=\"ltx_text ltx_font_italic\">derivative datasets</span> build on existing resources through task expansion or reformatted aggregation (e.g., <span class=\"ltx_text ltx_font_sansserif\">Flan 2022</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib4\" title=\"\">Fla, </a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">xP3</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib45\" title=\"\">2022</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "task",
                    "sharegpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Content.</span>\nPrompt datasets are characterized by distinct linguistic and structural attributes.\n<span class=\"ltx_text ltx_font_bold\">Linguistically</span>, they may be <span class=\"ltx_text ltx_font_italic\">monolingual</span> or <span class=\"ltx_text ltx_font_italic\">multilingual</span>; in the latter case, datasets are deemed <span class=\"ltx_text ltx_font_italic\">semantically aligned</span> if each entry includes multilingual counterparts with identical semantics, thereby enhancing LLM performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib35\" title=\"\">2023b</a>)</cite>.\nIn terms of <span class=\"ltx_text ltx_font_bold\">display form</span>, prompts appear either as <span class=\"ltx_text ltx_font_italic\">conversation</span> (e.g., single-round, multi-round, or tree-structured) or <span class=\"ltx_text ltx_font_italic\">instruction</span> (e.g., user prompt, system prompt). The prompt <span class=\"ltx_text ltx_font_bold\">format</span>&#8211;ranging from <span class=\"ltx_text ltx_font_italic\">free-form</span> to <span class=\"ltx_text ltx_font_italic\">structured</span> (e.g., JSON, Markdown, HTML), or a combination thereof&#8211;substantially influences LLM response quality <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib40\" title=\"\">2025b</a>)</cite>.\nFinally, datasets differ in their use of <span class=\"ltx_text ltx_font_bold\">placeholders</span>, which allow for text substitution and enable diversified prompt transformations <cite class=\"ltx_cite ltx_citemacro_citep\">(Shin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib58\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "content",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among these, instruction fine-tuning datasets represent a prominent and widely-used subset of prompt datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib39\" title=\"\">2024</a>)</cite>. These datasets comprise instruction-response pairs, where the \"instruction\" serves as a prompt and the \"response\" represents the target model output. They are primarily employed for supervised fine-tuning to enhance model capability and controllability. As a result, models trained on these datasets exhibit superior alignment with human intent, improved instruction-following, and increased safety characteristics <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib70\" title=\"\">2023</a>)</cite>. Furthermore, the instructions in these datasets often reflect real-world user queries, making them both practical for deployment and valuable for prompt-related research. Notable examples are <span class=\"ltx_text ltx_font_sansserif\">Alpaca</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Taori et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib61\" title=\"\">2023</a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;pf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib31\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_sansserif\">FLAN 2022</span>.</p>\n\n",
                "matched_terms": [
                    "output",
                    "oasst1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to ensure reliable analysis of prompt characteristics, we curate multiple prompt-centric datasets with the following selection principles.\n<span class=\"ltx_text ltx_font_bold\">(1)</span> <span class=\"ltx_text ltx_font_italic\">Language consistency.</span> Only English-language data have been included to ensure uniform linguistic features and avoid cross-linguistic biases.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> <span class=\"ltx_text ltx_font_italic\">Exclusion of benchmark-style prompts.</span> Prompts designed for LLM performance evaluations (e.g., <span class=\"ltx_text ltx_font_sansserif\">PHYBench</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Qiu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib48\" title=\"\">2025</a>)</cite>) are excluded to focus on natural usage scenarios.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> <span class=\"ltx_text ltx_font_italic\">Source and content diversity.</span> To achieve sufficient coverage and reduce sampling bias, we have selected datasets that differ in <span class=\"ltx_text ltx_font_italic\">publisher type</span> (i.e., end user vs. LLM researcher and domain scientist), <span class=\"ltx_text ltx_font_italic\">instruction generation method</span> (i.e., human vs. model generated), and <span class=\"ltx_text ltx_font_italic\">domain scope</span> (i.e., general vs. domain-specific tasks).</p>\n\n",
                "matched_terms": [
                    "following",
                    "content",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following our selection principles, we curated seven representative datasets spanning different user types, instruction methods, and domains. For <span class=\"ltx_text ltx_font_bold\">end users</span>, general-domain prompts include single-turn prompts (<span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span>) and multi-turn conversations (<span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>), while business-domain single-turn prompts are represented by <span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span>. For <span class=\"ltx_text ltx_font_bold\">LLM researchers</span>, human-generated datasets include <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> and <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and model-generated prompts are captured by <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>. For <span class=\"ltx_text ltx_font_bold\">domain scientists</span>, we include model-generated medical prompts from <span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span>. This collection ensures diversity in publisher type, prompt structure, and application domain.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "following",
                    "boredhumans",
                    "sharegpt",
                    "oasst1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span> (<span class=\"ltx_text ltx_font_bold\">1.1k-business</span>). A curated dataset of 1&#8201;235 prompts oriented toward professional and business-related use cases, such as marketing, productivity, and decision-making. It represents structured, domain-specific prompting behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(1., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib1\" title=\"\">1</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "use",
                    "11kbusiness",
                    "marketing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">BoredHumans Prompts</span> (<span class=\"ltx_text ltx_font_bold\">BoredHumans</span>).\nA smaller collection of 964 prompts compiled from publicly shared prompts on the boredhumans.com website. Some of the prompts on this site come from other community shared sources (e.g., <span class=\"ltx_text ltx_font_sansserif\">awesome-chatgpt-prompts</span>). It reflects community-created content and captures user creativity and experimentation <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib10\" title=\"\">bor, </a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "boredhumans",
                    "content"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> (<span class=\"ltx_text ltx_font_bold\">dolly-15k</span>).\nThis dataset includes 15&#8201;000 human-authored instruction&#8211;response pairs covering a range of everyday tasks. It is single-turn and domain-general, curated to support instruction-following models <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dolly15k",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span> (<span class=\"ltx_text ltx_font_bold\">medical-o1</span>).\nSynthetic data of 90&#8201;120 open-ended questions and GPT-4o generated CoTs and responses. Open-ended questions are reformatted by GPT-4o based on close-set medical examination questions. The dataset is used to fine-tune HuatuoGPT-o1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "medicalo1",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">OASST1.</span>\nThe Open Assistant dataset (OASST1) contains over 30&#8201;000 human-written messages arranged in dialogue trees. It emphasizes cooperative, open-domain assistant behavior and includes branching conversations rather than linear interactions <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;pf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib33\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "oasst1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">Self-Instruct.</span>\nA synthetic dataset with 82&#8201;646 prompts generated by large language models based on a small seed pool of human-written instructions. For every generation step, it samples 6 human-written tasks and 2 model-generated tasks in previous steps to promote diversity <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib65\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> High-frequency <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams reveal domain and prompt-engineering differences, such as role-playing cues in <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> (&#8220;you to act as&#8221;) versus medical reasoning in <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> (&#8220;what be the,&#8221; &#8220;the most likely diagnosis&#8221;). <span class=\"ltx_text ltx_font_bold\">(2)</span> While 3-grams capture general-purpose queries or commands (e.g., &#8220;what be the,&#8221; &#8220;I want to&#8221;), longer <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams (4&#8211;5) reflect task-specific patterns, as in <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> where frequent 5-grams (&#8220;please write in English language,&#8221; &#8220;write a comprehensive reply to&#8221;) highlight its instruction-following orientation. <span class=\"ltx_text ltx_font_bold\">(3)</span> Compared to Google Books 5-grams (e.g., &#8220;at the end of the,&#8221; &#8220;in whole or in part&#8221;) that serve narrative or descriptive purposes, prompt datasets exhibit inquiry- or command-focused <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams, underscoring a clear divergence in linguistic patterns across corpora.</p>\n\n",
                "matched_terms": [
                    "want",
                    "medicalo1",
                    "oasst1",
                    "sharegpt",
                    "act",
                    "write"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To gain deeper insights into the linguistic structure of prompts, we perform syntactic analysis from three perspectives: dependency parsing <cite class=\"ltx_cite ltx_citemacro_citep\">(Nivre, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib47\" title=\"\">2003</a>)</cite>, part-of-speech (POS) tagging <cite class=\"ltx_cite ltx_citemacro_citep\">(Brill, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib19\" title=\"\">1992</a>)</cite>, and term frequency-inverse document frequency (TF-IDF) scoring <cite class=\"ltx_cite ltx_citemacro_citep\">(Salton &amp; Buckley, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib54\" title=\"\">1988</a>)</cite>. These features are both descriptive and can be aggregated into vector representations for tasks like prompt classification.</p>\n\n",
                "matched_terms": [
                    "tfidf",
                    "like"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> The <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> dataset is characterized by its high use of adjectival modifiers (amod, 0.11) and low direct object frequency (dobj, 0.03), reflecting a preference for precise, state-oriented descriptions over action-driven narratives, often framed through linking verbs&#8212;typical of medical contexts detailing conditions, symptoms, and diagnoses. <span class=\"ltx_text ltx_font_bold\">(2)</span> In contrast, the <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> dataset favors concise, goal-driven imperatives with bare noun phrases as direct objects (dobj, 0.09) and minimal use of determiners (det, 0.05), aligning with its project-planning focus. <span class=\"ltx_text ltx_font_bold\">(3)</span> Verb&#8211;noun dependency analysis further distinguishes domains: medical instructions cluster around technical, domain-specific pairs like &#8220;have history&#8221; and &#8220;experience pain,&#8221; while datasets such as <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> use broader, generic pairs like &#8220;write answer&#8221; and &#8220;use code&#8221;. These syntactic patterns highlight each corpus&#8217; thematic priorities and inform strategies for domain-aware model training.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "medicalo1",
                    "sharegpt",
                    "use",
                    "like",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1)</span> Domain-specific datasets such as <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> and <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> exhibit a noun proportion of <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 0.26, surpassing that found in formal corpora like ParTUT. This reflects a concept-driven focus on domain entities and technical terms.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> Additionally, <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> also registers an unusually high adjective ratio (0.11), indicating a repeated emphasis on specifying medical attributes and conditions, consistent with the descriptive nature of clinical reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "medicalo1",
                    "11kbusiness",
                    "like"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1) Intra-dataset analysis</span>\ndelineates each dataset&#8217;s lexical focus and stylistic characteristics. For instance, <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> emphasizes business-specific terms like &#8220;content&#8221; and &#8220;email&#8221;, while <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span> features imperatives such as &#8220;act&#8221;, indicative of role-playing instructions. Similarly, <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> shows a dominant TF-IDF score for &#8220;output&#8221; (0.772), highlighting a structural prompt style based on explicit instruction&#8211;response formats.\n<span class=\"ltx_text ltx_font_bold\">(2) Inter-dataset comparison.</span>\nTF-IDF vectors show varying overlaps across datasets. The highest cosine similarity between <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> suggests a similar vocabulary&#8212;likely due to shared human-generation processes. In contrast, <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> is lexically distant from the others, especially <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> and <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span>, reflecting stylistic and domain-specific differences.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "selfinstruct",
                    "medicalo1",
                    "boredhumans",
                    "like",
                    "oasst1",
                    "sharegpt",
                    "tfidf",
                    "highest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze prompt semantics by encoding each prompt into a 384-dimensional dense vector using Sentence-BERT&#8217;s pretrained model <span class=\"ltx_text ltx_font_typewriter\">all-MiniLM-L6-v2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Reimers &amp; Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib50\" title=\"\">2019</a>)</cite>. Each prompt is encoded into a 384-dimensional dense vector that captures its semantic content. These embeddings serve as the foundation for classification, clustering, and visualization analysis. We perform Principal Component Analysis (PCA) to reduce sentence embeddings to two dimensions. For fair comparison, we uniformly at random sample 500 prompts per dataset and visualize their distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Semantic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "content",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1) Wide coverage in Self-Instruct:</span> The <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> dataset exhibits the most dispersed and evenly distributed semantic space, suggesting a broad topical coverage. This aligns with the self-instruction paradigm&#8217;s goal of generating diverse instruction types.\n<span class=\"ltx_text ltx_font_bold\">(2) Semantic cohesion in specific domains:</span> Prompts from <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> form more concentrated clusters, indicating domain-specific semantic cohesion.\n<span class=\"ltx_text ltx_font_bold\">(3) Overlap among human-generated sets:</span> The embeddings of <span class=\"ltx_text ltx_font_sansserif\">dolly-15k</span>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> overlap substantially across both PCA dimensions. This suggests that these datasets share stylistic and semantic characteristics, possibly due to their common reliance on human-LLM interactions for data generation.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "selfinstruct",
                    "medicalo1",
                    "data",
                    "oasst1",
                    "sharegpt",
                    "dolly15k",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: \"1100+ ChatGPT Prompts for Business\" is a Notion-based dataset containing 1,235 curated prompts tailored for diverse business scenarios. It spans key domains such as buyer persona development, content strategy, digital marketing, narrative marketing, email campaigns, market research, product innovation, and finance. The collection includes specialized roles like Simulation Specialist, offering practical guidance for professionals, marketers, and entrepreneurs aiming to optimize operations, boost engagement, and enhance strategic decision-making.</p>\n\n",
                "matched_terms": [
                    "email",
                    "marketing",
                    "like",
                    "content",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset comprises over 1,000 curated ChatGPT prompt templates in Notion Workspace format, spanning diverse domains such as AI, marketing, education, healthcare, and code generation. Each entry typically includes a prompt, an automatic prompt (system prompt like), and a concise description.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "like",
                    "marketing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Stanford Alpaca dataset comprises 52K high-quality, instruction-following examples generated via a modified Self-Instruct pipeline using text-davinci-003. Designed for fine-tuning LLaMA models, it enables research in alignment, instruction tuning, and synthetic data generation.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Alpaca_GPT4_data_zh is a Chinese instruction-tuning dataset curated by the Instruction Tuning with GPT-4 project. It comprises 48,818 examples, each featuring an instruction, optional input context, and a GPT-4-generated response, facilitating text-generation and fine-tuning tasks. The dataset occupies 32 MB and is available under a CC-BY-4.0 license for non-commercial research.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Aya Collection is a massive multilingual instruction tuning dataset comprising over 513 million prompt-completion pairs across 115 languages. It integrates three sources: human-crafted instruction templates created by fluent speakers for diverse tasks, machine translations of 19 top-tier datasets into 101 languages via NLLB, and the human-annotated Aya Dataset subset of 204K examples. Split by dataset, each record includes id, inputs, targets, language, script, and task type. Licensed under Apache-2.0, it supports academic and commercial classification, summarization, translation, and QA research.</p>\n\n",
                "matched_terms": [
                    "task",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Aya Dataset is a multilingual, human-annotated instruction fine-tuning resource encompassing 204K prompt-completion pairs across 65 languages and dialects. It includes original annotations, re-annotations, and detailed annotator demographics such as age, gender, and regional background. Collected via the open-science Aya Annotation Platform, it supports diverse linguistic representation from high- to low-resource languages. Released under Apache 2.0, Aya is designed to train, fine-tune, and evaluate large language models on cross-cultural instruction following. It offers train (202K examples) and test splits with tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BABILong is a generative benchmark designed to evaluate large language models&#8217; ability to perform reasoning over extremely long contexts. It embeds the ten bAbI tasks within irrelevant PG19 background text, creating &#8220;needle-in-a-haystack&#8221; scenarios across sequence lengths ranging from 0k to 1M tokens. Each task probes basic reasoning skills&#8212;such as supporting-fact retrieval, negation, and counting&#8212;amidst distractors. BABILong thus challenges models to identify pertinent facts and answer questions accurately.</p>\n\n",
                "matched_terms": [
                    "task",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Bactrian-X is a multilingual instruction-following dataset containing 3.4 million instruction-input-response triplets across 52 languages. It builds upon 67K unique English prompts drawn from Alpaca and Dolly, automatically translated via Google Translate into 51 languages. For each translated prompt (and optional input), GPT-3.5-Turbo generates a corresponding response, yielding 3.4 million examples. Each record includes an id, instruction, optional input, and model-generated output. Released under CC-BY-NC 4.0, Bactrian-X supports text-generation research, fine-tuning, and evaluation in low-resource and high-resource language settings, covering diverse tasks and domains.</p>\n\n",
                "matched_terms": [
                    "output",
                    "dataset",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Baize Chat Data is an instruction-finetuning corpus combining four sources: Alpaca, Medical, Quora, and StackOverflow. It contains about 210,000 conversational examples, each formatted with [|Human|] prompts and [|AI|] responses. Designed to enhance the Baize family of language models, this unified dataset supports interactive text generation and dialogue training. Sourced from the Baize GitHub repository, it provides diverse conversational scenarios ranging from general queries to specialized medical and technical discussions. It is optimized for instruction-following tasks. It enables realistic user interactions.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BELLE_Generated_Chat contains approx. 400k personalized Chinese character dialogues generated by the BELLE project. Each record includes an instruction, an (empty) input, and a generated output. Created by ChatGPT and not strictly verified, the dataset may contain factual inaccuracies. Licensed under GPL-3.0 for research use only. With around 0.4 million entries, it supports text-to-text generation and conversational modeling.</p>\n\n",
                "matched_terms": [
                    "output",
                    "use",
                    "dataset",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BELLE_Multiturn_Chat is a Chinese multi-turn conversational dataset comprising approximately 0.8 million human-assistant dialogues generated by the BELLE project using ChatGPT. Each record pairs an instruction containing prior context labeled with &#8220;Human:&#8221; and &#8220;Assistant:&#8221; with the assistant&#8217;s subsequent reply. Intended for text-to-text generation tasks, the GPL-3.0-licensed collection covers only Chinese interactions. As this data is automatically generated and unverified, factual errors and inconsistencies may arise. It is provided strictly for non-commercial research under the project&#8217;s usage restrictions; developers should validate outputs and adhere to licensing terms.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The BELLE_train_3.5M_CN dataset comprises approximately 3.5 million monolingual Chinese instruction-response pairs generated by the BELLE project, formatted as multi-turn and single-turn dialogues with unique IDs. It includes human-assistant exchanges across 13 instruction categories. Licensed under GPL-3.0, it supports text-to-text generation research exclusively; commercial or harmful use is prohibited. The JSON records each conversation&#8217;s ID and bilingual content.</p>\n\n",
                "matched_terms": [
                    "content",
                    "use",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BoredHumans is a diverse and extensive prompt dataset compiled from multiple sources, including Awesome ChatGPT Prompts, Data Science Prompts, and Tree-of-Thought Prompting, among others. Its rich variety covers numerous domains and prompt styles, enabling comprehensive research on prompt engineering, AI model behavior, and in-context learning strategies.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "boredhumans",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: CAMEL AI Society is a synthetic dialogue corpus comprising 25,000 simulated conversations between GPT-3.5-turbo agents role-playing across 50 distinct user roles and 50 assistant roles on ten tasks per pairing. Available in both chat and instruction formats, each example includes metadata such as role identifiers, original and specified task descriptions, input context, generated responses, and conversation termination reasons. Designed for instruction-tuning and text-generation research, CAMEL is licensed under CC-BY-NC-4.0 and intended solely for non-commercial academic use, acknowledging potential synthetic inaccuracies.</p>\n\n",
                "matched_terms": [
                    "task",
                    "use",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The ChatGPT Prompts for Data Science dataset offers a curated collection of specialized prompts designed to enhance AI applications in data science tasks. It facilitates research on natural language interfaces for data analysis, model explanation, and automation of complex workflows.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The ChatGPT Prompts dataset offers a broad collection of prompts covering diverse topics, designed for use with GPT 3.5. Its value lies in providing versatile, real-world prompt examples that support research on prompt engineering and AI interaction across various domains.</p>\n\n",
                "matched_terms": [
                    "use",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Chinese-DeepSeek-R1-Distill-data-110k is a 110K-entry Chinese dataset distilled from DeepSeek-R1, supporting text generation, text2text generation, and question answering under Apache-2.0. It covers four domains: Math (36 568 samples), Exam (2 432), STEM (12 648) and General (58 352). Each record includes input, reasoning content, output, source repo name and model-assigned score. Data originate from diverse math and instruction corpora, distilled via R1 with temperature 0.6, step-by-step math prompts, and validation using Math-Verify and Qwen2.5-72B.</p>\n\n",
                "matched_terms": [
                    "data",
                    "output",
                    "content",
                    "dataset",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: COIG-CQIA (Chinese Open Instruction Generalist - Quality is All You Need) is a high-quality, open-source Chinese instruction tuning dataset designed to align language models with human interactive behavior. It aggregates over 45,000 manually cleansed, restructured, and reviewed examples spanning social media dialogs, encyclopedic articles, exam questions, finance, medical, legal, traditional culture, and NLP tasks. Each entry includes instruction, optional input, output, task type, domain, and human verification metadata. COIG-CQIA aims to facilitate instruction fine-tuning for Chinese NLP research and applications.</p>\n\n",
                "matched_terms": [
                    "output",
                    "task",
                    "dataset",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Databricks-dolly-15K is an open-source corpus of over 15,000 human-generated instruction-response pairs created by Databricks employees across eight behavioral categories defined by InstructGPT, including brainstorming, classification, closed and open QA, generation, information extraction, and summarization. Provided under a CC-BY-SA 3.0 license, this English-language dataset supports academic or commercial use. With context passages drawn from Wikipedia when required, it enables training and fine-tuning of large language models, as well as synthetic data generation and data augmentation for robust, scalable instruction-following capabilities.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "use",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: DeepSeek-Prover-V1 is a large-scale synthetic proof dataset for Lean 4 theorem proving. It comprises 8 million formal statements and corresponding proofs generated from high-school and undergraduate-level mathematical contest problems. Natural language problems are translated into formal Lean 4 statements, filtered for quality, and paired with automatically generated proofs. Released under the deepseek-license, this dataset enables fine-tuning of large language models, improving whole-proof generation accuracy on benchmarks like miniF2F and FIMO. It supports research in formalized mathematical reasoning, automated theorem proving.</p>\n\n",
                "matched_terms": [
                    "like",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: DialogStudio is a large-scale, unified collection of dialogue datasets curated to advance conversational AI. It integrates a wide range of domains&#8212;such as task-oriented dialogue, open-domain conversation, knowledge-grounded dialogue, and more&#8212;while preserving original metadata and structure. The dataset supports instruction-tuned training and evaluation across over 30 datasets with consistency. It includes model checkpoints (e.g., dialogstudio-t5-base-v1.0) and evaluation scripts using GPT-3.5 for quality metrics like coherence, completeness, and correctness. DialogStudio serves as a robust benchmark for multi-task generalization, instruction-following, and multi-domain dialogue modeling.</p>\n\n",
                "matched_terms": [
                    "like",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: DMind_Benchmark is a comprehensive dataset for evaluating large language models on blockchain, cryptocurrency, and Web3 knowledge. It provides objective (multiple choice) and subjective (open ended) questions across nine domains: Fundamentals, Infrastructure, Smart Contracts, DeFi, DAOs, NFTs, Security, Tokenomics, and MEME coins&#8212;organized into CSV and JSONL splits. The benchmark supports diverse question types&#8212;calculations, code audits, risk and scenario analyses&#8212;with automated scoring and evaluation. It features standardized data configurations, leaderboards, and extensible evaluation pipelines for comparative analysis of LLM performance in specialized Web3 tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Dynosaur introduces a dynamic and low-cost paradigm for curating instruction-tuning datasets. It automatically generates diverse instructions by leveraging metadata from HuggingFace datasets, combined with LLM-based instruction synthesis (e.g., via ChatGPT). The result is Dynosaur-full, a large-scale dataset (800K+ samples, generated at &#160; $11.5) that supports dynamic growth and general-purpose instruction-tuning. Empirically, models fine-tuned on Dynosaur outperform Alpaca and GPT-4-Instruct baselines on Super-NI. The project includes: metadata crawling tools, instruction generation pipelines, and fine-tuned T5-3B and LLaMA-7B models. All generated instructions are under Apache 2.0, with task data adhering to original dataset licenses.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "task",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: \"Exploring the Possibilities of AI Prompts Over 200 Ideas\" is a comprehensive dataset featuring over 200 prompts spanning diverse marketing and content creation domains such as blog writing, email marketing, social media ads, influencer campaigns, and copywriting.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "email",
                    "content",
                    "marketing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Firefly is a Chinese instruction-tuning dataset comprising 1.15 million high-quality examples drawn from 23 common Chinese natural language processing datasets. Each example includes a task type, an input prompt, and a target output, ensuring diverse coverage. Data templates were manually designed for each task to ensure quality and richness. Token length analysis shows that most examples are under 600 tokens. Firefly was used to train the Firefly-1b4 Chinese dialogue LLM, available on GitHub and Hugging Face, fostering reproducibility, community collaboration.</p>\n\n",
                "matched_terms": [
                    "task",
                    "data",
                    "output",
                    "tokens",
                    "dataset",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset aggregates tasks from Flan, T0, Super-Natural Instructions, Chain-of-Thought, and Dialog into a training split. Each task is provided in zero-/few-shot and option/no-option formats as JSONL entries including inputs, targets, and task identifiers. Released under Apache-2.0, it includes scripts for building dependencies, fixing version mismatches, and exporting per-task JSONL data. Mixing ratios can be tuned for optimal downstream performance via guidelines in the associated paper and public GitHub repository.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "task",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Flan-mini is a curated 1.34 M-example subset of the FLAN instruction-tuning collection augmented with code and conversational tasks. It pools 388K Flan2021 instructions, 320K public prompt templates, 200K Natural Instructions v2 instances, 100K chain-of-thought examples, plus code datasets (100K Code Search, 50K Code Contests, 50K APPS). It further integrates 132K ChatGPT-generated examples from GPT-4-Alpaca, Code-Alpaca, and ShareGPT. Each example is randomly paired with handcrafted prompt templates for zero- or few-shot fine-tuning, ensuring diverse task coverage. Released under a permissive CC license.</p>\n\n",
                "matched_terms": [
                    "task",
                    "sharegpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Human ChatGPT Comparison Corpus (HC3) is the first large-scale bilingual dataset enabling direct comparison of human and ChatGPT-generated text. Spanning English and Chinese samples, it encompasses between 10,000 and 100,000 prompt-response pairs covering tasks such as text classification, question-answering, sentence similarity, and zero-shot classification. Released under a CC-BY-SA license, HC3 supports research in performance evaluation, detection, and analysis of AI-generated content. Accompanying code, models, and benchmarks are available on GitHub, facilitating open science, reproducible experimentation, and collaborative, community-driven global efforts.</p>\n\n",
                "matched_terms": [
                    "content",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LaMini-Instruction is an English text-to-text generation dataset comprising 2.58M instruction-response pairs distilled from GPT-3.5-Turbo. Each sample includes an instruction, a corresponding model-generated response, and the instruction&#8217;s provenance&#8212;drawn from sources such as Alpaca, FLAN, P3, and Self-Instruct. Released under CC-BY-NC 4.0, it spans a single training split of over 1.16 GB and supports fine-tuning of compact language models. LaMini-Instruction enables research in instruction-based learning but inherits biases and errors from its GPT-3.5 teacher.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The LIMA dataset contains 1,000 high-quality prompt-response pairs designed to align language models with the style of a helpful AI assistant. Prompts are diverse, sourced from Stack Exchange, wikiHow, WritingPrompts, Natural Instructions, and manually authored examples. Despite limited size (&#160;750K tokens), all responses are stylistically consistent. The dataset includes a 50-example development set and a 300-prompt test set. LIMA demonstrates that small, curated datasets can be highly effective for instruction tuning and alignment of pretrained language models.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Llama-Nemotron-Post-Training-Dataset is a comprehensive dataset of synthetic SFT and RL samples designed to bolster reasoning, code, math, science, chat, and safety capabilities for NVIDIA&#8217;s Llama-3 Nemotron series. It includes over 33M SFT examples across code, math, science, chat, and safety, plus 56K instruction-following RL examples. Data is sourced from public corpora or synthetically generated, filtered for quality and complexity. Released under CC-BY-4.0, it supports training and evaluation of efficient open-source LLMs offering a flexible accuracy-efficiency tradeoff and transparent development.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LMSYS-Chat-1M is a large-scale dataset of one million real-world LLM conversations, collected from 210K users interacting with 25 models via Chatbot Arena and Vicuna demo (April-August 2023). Each conversation includes model metadata, OpenAI-style JSON formatting, language tags, and moderation labels. Personally identifiable information is redacted. This dataset enables research on LLM alignment, safety, evaluation, and user behavior in the wild, offering unique insights into real-world usage patterns and content moderation challenges in multi-model deployment scenarios.</p>\n\n",
                "matched_terms": [
                    "content",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LongForm is a 27K-example English instruction-following dataset under MIT license, for tasks like table QA, summarization, text generation, question answering. It collects human-written documents from C4 (10K) and Wikipedia (5K), reverse-engineered instructions via LLMs, and structured sources including Stack Exchange (4.4K) and WikiHow (2.5K). It also covers QA, email writing, grammar correction, story/poem generation and summarization from NIv2, Big Bench, BEA-GEC, Enron. Split into 23.6K train, 2K validation and 2K test, it supports instruction tuning and is publicly available.</p>\n\n",
                "matched_terms": [
                    "email",
                    "like",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: medical-o1-reasoning-SFT is a supervised fine-tuning dataset designed to enhance advanced medical reasoning in HuatuoGPT-o1. It comprises English and Chinese instruction-response pairs generated by GPT-4o on verifiable clinical problems, validated by a medical verifier. Released under an Apache-2.0 license, the dataset supports question answering and text generation, offering separate configurations for monolingual and mixed-language data. It aims to refine model performance on complex biomedical tasks by leveraging rigorous problem-solving chains, with full details available in the accompanying paper and GitHub repository.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: medical-o1-verifiable-problem is an Apache-2.0 licensed dataset comprising open-ended medical reasoning problems designed to improve large language models&#8217; diagnostic and procedural knowledge. It supports question-answering and text-generation tasks, presenting each instance as a challenging exam-style prompt paired with a verifiable, expert-derived answer. Published in English under a single default configuration with training data provided in JSON format, it allows systematic evaluation and refinement of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: MedTrinity-25M is a large-scale multimodal medical dataset featuring over 25 million images from 10 imaging modalities. It provides multigranular annotations for 65+ diseases, including textual descriptions, bounding boxes, segmentation masks, and inter-region relationships. Supporting both vision-centric and multimodal tasks like classification, segmentation, and report generation, it facilitates large-scale pretraining for medical foundation models. Public access includes an 18M image-text pair subset. The dataset is organized in shards with structured metadata for scalable research and development.</p>\n\n",
                "matched_terms": [
                    "like",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: NATURAL INSTRUCTIONS is a monolingual English dataset derived from Super-Natural-Instructions, offering 1,600+ NLP tasks for training, validation, and testing. Size ranges between 100 million and one billion examples. Curated by crowdsourced and expert annotators, it covers classification, generation, and reasoning across reading comprehension, commonsense, summarization, arithmetic, logic, and dialog. With over 100 M examples, it provides diverse input-output mappings while enabling deduplication by unique IDs or input fields. Tasks span question answering, text modification, summarization, and beyond, supporting robust instruction-following model development.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Nemotron-CrossThink is a multi-domain reinforcement learning dataset designed to enhance both general-purpose and mathematical reasoning in large language models. It comprises two subsets: Nemotron-CrossThink-QA with high-quality question-answer pairs across STEM, humanities, and sciences, and Nemotron-CrossThink-Math featuring persona-driven, multi-step math problems. Data is curated from CommonCrawl and open-source books, standardized via structured templates into multiple-choice and open-ended formats, filtered for verifiability, and used to train RL policies with Group Relative Policy Optimization. Licensed under CC-BY-4.0, it supports AI development.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: NuminaMath-1.5 is an open-source, large-scale post-training dataset comprising about 900 000 competition-level mathematics problems paired with chain-of-thought solutions. It covers diverse sources&#8212;from Chinese high school exams to US and international Olympiads&#8212;and spans domains like algebra, geometry, number theory, combinatorics, calculus, and puzzles. Each entry includes metadata fields (answer, problem_type, question_type) for verifiable outputs. Recent additions feature manually verified Olympiad references and curated contest data while synthetic problems were removed. Licensed under Apache 2.0, NuminaMath-1.5 supports advanced text-generation research in mathematical reasoning.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "like",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenAssistant Conversations (OASST1) is a human-generated, human-annotated corpus with 161,443 messages in 66,497 conversation trees across 35 languages. It includes over 461,000 quality ratings and more than 10,000 fully annotated trees. Each record contains metadata (IDs, timestamps), conversational structure (parent and tree IDs), role and language labels, toxicity and quality scores, emoji labels. Data comes in nested JSONL or flat parquet via HuggingFace, with 84,437 training and 4,401 validation splits, supporting supervised fine-tuning and reward model development. Licensed under Apache-2.0.</p>\n\n",
                "matched_terms": [
                    "oasst1",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Open Instruction Generalist (OIG) is a large-scale instruction-tuning dataset released under Apache-2.0 license. It comprises 44 million JSONL entries pairing human instructions with model responses for continued pretraining, accompanied by a smaller high-quality subset (OIG-small-chip2) optimized for finetuning. OIG unifies diverse sources&#8212;ranging from Wikipedia dialogs, math problems, and code examples to summarization and question-answering corpora&#8212;into a consistent format. Designed to transform pretrained models into instruction-following agents, it supports scalable development of helpful language systems and targets one trillion tokens of instructions.</p>\n\n",
                "matched_terms": [
                    "tokens",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OL-CC is the first open source Chinese conversational instruction dataset collected via crowdsourcing on OpenLabel. It includes 10,006 instruction-answer pairs and 1,649 standalone instructions across tasks such as question-answering, text generation, extraction, rewriting, classification, brainstorming, chit-chat, logic and math. A total of 276 volunteers alternately played user and AI assistant roles to produce the data. Licensed under Apache-2.0 and sized between 10K and 100K examples, OL-CC offers rich, human-generated Chinese instructional dialogues for AI research.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenCodeInstruct is a large-scale open-access instruction tuning dataset for code language models provided under the CC-BY-4.0 license. It comprises five million examples across generic and algorithmic coding tasks, with fields including id, input, output, domain, generation_algorithm, llm_judgement, unit_tests, tests_execution_status, and average_test_score. It supports supervised fine-tuning of code models and is accessible via the HuggingFace datasets library. Developed by NVIDIA for research and use, it accelerates code generation benchmarks and model evaluation.</p>\n\n",
                "matched_terms": [
                    "output",
                    "use",
                    "dataset",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenCodeReasoning is a large-scale synthetic dataset designed to distill reasoning capabilities for Python-based competitive programming. It comprises 735,255 samples covering 28,319 unique problems sourced from platforms like CodeForces, AtCoder, and LeetCode. The dataset features two configurations: split_0 includes full problem statements and model responses, while split_1 references external datasets via index placeholders. Each example contains identifiers, source metadata, difficulty labels, and code solutions. Licensed under CC-BY-4.0, OpenCodeReasoning supports supervised fine-tuning of language models for code generation tasks.</p>\n\n",
                "matched_terms": [
                    "like",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Phoenix-sft-data-v1 is a multilingual supervised fine-tuning dataset containing 464,510 samples, combining instruction-following and ChatGPT-distilled conversation data. It includes Alpaca-derived tasks, post-translated multilingual instructions, and user-centered prompts in 40 languages. The dataset also integrates ShareGPT and Discord-sourced dialogues. With nearly 1 million conversation turns and detailed multilingual annotations, it supports multilingual language modeling, alignment, and chat adaptation. English and Chinese dominate the corpus, with broader linguistic diversity represented across the remaining data, enabling robust multilingual model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "sharegpt",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The PRISM Alignment Dataset is a large-scale human feedback resource designed to assess preference and value alignment in large language models (LLMs). It consists of detailed survey responses from 1,500 participants across 75 countries, followed by multi-turn conversations with 21 LLMs. Participants rate model outputs on a 1-100 scale and provide fine-grained feedback, yielding 8,011 conversation trees and 68,371 scored utterances. The dataset includes four JSONL configurations&#8212;survey, conversations, utterances, and metadata&#8212;licensed under CC-BY and CC-BY-NC for research and educational use.</p>\n\n",
                "matched_terms": [
                    "use",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: PromptGenius is a comprehensive, multilingual prompt dataset structured by usage scenarios, facilitating efficient retrieval across domains like academic research, content creation, and office tasks. It continuously collects popular, high-quality prompts to enhance productivity and offers model output examples to improve prompt design.</p>\n\n",
                "matched_terms": [
                    "output",
                    "like",
                    "content",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: RedGPT Dataset (V1-CN) offers 50,000 automatically generated multi-turn Chinese dialogues grounded in high-quality factual references from diverse domains such as history, science, law, and culture. Designed to enhance GPT models&#8217; factual accuracy, the dataset enables fine-tuning on realistic, knowledge-rich conversational data without costly manual annotation. It supports research in improving language models&#8217; truthfulness, dialogue generation, and knowledge integration.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Self-Instruct is an open Apache-2.0-licensed dataset and framework designed to enhance language models&#8217; instruction-following capabilities. It comprises four configurations: a self-generated set of 82K prompt-completion pairs produced via OpenAI&#8217;s davinci engine; 50K samples from Super Natural Instructions; 52K prompts drawn from the P3 public pool; and 252 expert-crafted human evaluation tasks with associated inputs and outputs. All data is in English and supports instruction-tuning by providing diverse natural-language prompts paired with corresponding model or human completions. The dataset facilitates instruction-tuning.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: ShareGPT90K is a dataset of 90,665 conversational threads scraped from the ShareGPT platform. Each example includes a unique id and a sequence of messages, with each message annotated by its origin and its content.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "content",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: SPIRIT is a high-quality system prompt instruction dataset improving large language models&#8217; adherence to complex system prompts. It contains over 24,000 examples, including &#160;3,000 real-world system prompts extracted from open-source GitHub repositories and 21,639 synthetically generated conversation samples via a multi-agent GPT-4-based pipeline. Following the OpenAI message format, SPIRIT ensures compatibility with fine-tuning workflows. Human evaluations show models fine-tuned on SPIRIT outperform instruct baselines in prompt compliance. Released under the MIT License, SPIRIT is ideal for enhancing system prompt following.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: SUPER-NATURAL INSTRUCTIONS is a benchmark dataset designed to evaluate large language models&#8217; ability to generalize across diverse unseen tasks by leveraging natural language instructions. It emphasizes the importance of clear, comprehensive task descriptions to enable models to understand and perform novel tasks without additional training.</p>\n\n",
                "matched_terms": [
                    "task",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Prompt Index Prompt Database is a user-contributed repository featuring over 500 high-quality prompts spanning multiple domains, including SEO, content writing, coding, and more. This diverse dataset supports research in prompt engineering, cross-domain generalization, and AI-driven content generation.</p>\n\n",
                "matched_terms": [
                    "content",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraChat is an open-source, large-scale multi-round conversational dataset generated using two ChatGPT Turbo APIs under an MIT license. It comprises 1-10 million English dialogue turns across three sectors: world knowledge queries, creative writing and content generation, and assistance on existing materials such as rewriting, summarization, and inference. By simulating user and assistant interactions with carefully designed prompts, UltraChat ensures diverse, high-quality exchanges. Generated conversations undergo rigorous post-processing and filtering to safeguard privacy and maintain robust, realistic dialogue for text-generation research.</p>\n\n",
                "matched_terms": [
                    "content",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraFeedback is an MIT-licensed, open-source, large-scale preference dataset designed for training reward and critic models. It contains 64 K prompts drawn from UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA and FLAN, each answered by four out of 17 diverse LLMs under five alignment principles. The result is 256 K responses and 380 K fine-grained annotations covering instruction-following, truthfulness, honesty and helpfulness, all rated by GPT-4. Its scale, diversity and dense numerical plus textual feedback make it ideal for RLHF research and robust reward-model development.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraMedical is a large-scale English biomedical instruction dataset featuring over 409,000 examples licensed under MIT. Each sample includes an identifier, instruction type, multi-turn conversation pairs between human queries and GPT-generated responses, a ground-truth answer, and a model-evaluated score. The training split comprises roughly 1.2 GB across 410K examples, sourced from both curated public data and synthetic augmentations. UltraMedical aims to support the development of specialized generalist models in biomedicine by providing diverse, high-quality instruction-response instances, and comprehensive evaluation metrics accompany each instance.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Unnatural Instructions is a large-scale dataset of automatically generated instruction-input-output triplets designed to facilitate instruction tuning of language models with minimal human effort. It contains over 240,000 examples, including original instructions, associated inputs, outputs, and optional constraints. Each instance also features multiple reformulations&#8212;paraphrased variants of instructions complete with inputs and outputs&#8212;to enhance model robustness. The publicly available training split comprises around 66,000 examples. This dataset supports research in instruction following, prompt paraphrasing, and evaluating model generalization across diverse complex tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: WebGLM-QA is an English monolingual dataset designed for question answering and text generation, used to train the WebGLM generator. It contains 43,579 training samples, 1,000 validation examples, and 400 test instances. Each record pairs a user-posed question with a generated answer and a list of reference snippets that support the response. Hosted on Hugging Face, it provides a consistent structure&#8212;question, answer, references&#8212;enabling work on dialogue systems, retrieval-augmented generation, and answer justification.</p>\n\n",
                "matched_terms": [
                    "list",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Wizard_evol_instruct_196K is a MIT-licensed instruction-tuning dataset comprising 143K evolved QA pairs derived from Alpaca and ShareGPT. It represents an optimized version of the Evol-Instruct data used to train the WizardLM family of models. To assemble the complete instruction set of roughly 196K samples, users must merge this release with the original unfiltered ShareGPT dataset. The refined examples cover diverse conversational and instructional scenarios, facilitating improved alignment and performance in downstream open-source large language models, including structured prompts and responses.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "sharegpt",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: xP3 (Crosslingual Public Pool of Prompts) is a multilingual prompt and dataset collection spanning 46 languages and 13+ NLP tasks (e.g., QA, translation, summarization, code generation). Assembled from expert-generated and crowdsourced annotations under an Apache-2.0 license, it supports zero-shot and instruction-tuning for models like BLOOMZ and mT0. The training mixture covers closed-book and extractive QA, multiple-choice, paraphrase identification, program synthesis, sentiment analysis, structure-to-text, summarization, classification and more, totaling over 788 million samples. xP3 streamlines reproducible multilingual finetuning across diverse data scales.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "like",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams phrases of some datasets include abnormal content (e.g. &#8220;<span class=\"ltx_text ltx_font_italic\">identify which instrument be string</span>&#8221; in <span class=\"ltx_text ltx_font_sansserif\">dolly-15</span> and &#8220;<span class=\"ltx_text ltx_font_italic\">The quick brown fox jumps over the lazy dog</span>&#8221; in <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>), which indicates that there is a lot of repetition in the input content of the template tasks or some instructions used to construct the dataset, which may affect the balance of the dataset.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "content",
                    "dataset",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the complete experimental data for all identified dependency types, along with their proportions in the datasets, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.T5\" title=\"Table 5 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Additionally, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.T6\" title=\"Table 6 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> lists all detected Part-of-Speech tags and their corresponding proportions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F8\" title=\"Figure 8 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> further illustrates the ten most common verbs and their top five direct noun objects found in the prompt datasets except <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>, which are shown in the main paper.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "medicalo1",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These additional data further support our conclusions.\n<span class=\"ltx_text ltx_font_bold\">(1)</span> The <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> dataset, which consists of professionally crafted medical prompts, exhibits a relatively high proportion of numerical modifiers (nummod, 0.0276) and passive auxiliaries (auxpass, 0.0101) in dependency analysis, as well as a notably high usage of numerals (NUM, 0.0309) in POS tagging. These features reflect a terminology-dense and precision-oriented language style that emphasizes processes and outcomes rather than agents.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> In the <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> dataset, the verb-noun pairs reflect language commonly used in business contexts, such as <span class=\"ltx_text ltx_font_italic\">&#8220;create plan&#8221;</span> and <span class=\"ltx_text ltx_font_italic\">&#8220;create strategy&#8221;</span>. In contrast, the verb-noun pairs observed in <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> suggest more generic and broadly applicable usage scenarios.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "selfinstruct",
                    "medicalo1",
                    "data",
                    "boredhumans",
                    "oasst1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Anomalously, in the <span class=\"ltx_text ltx_font_sansserif\">dolly-15k</span> dataset, the most frequent verb-noun pairs exhibit a skewed distribution, with the highest-frequency nouns overwhelmingly associated with only the top one or two verbs. Moreover, these frequent verb-noun pairs often lack clear task-specific semantics&#8212;for example, <span class=\"ltx_text ltx_font_italic\">&#8220;tell i&#8221;</span>, <span class=\"ltx_text ltx_font_italic\">&#8220;give list&#8221;</span>, and <span class=\"ltx_text ltx_font_italic\">&#8220;classify each&#8221;</span>. This pattern may be attributed to the manual generation process, which is susceptible to the individual linguistic habits of annotators.</p>\n\n",
                "matched_terms": [
                    "dolly15k",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show the distribution of sampled embedding points after PCA for all datasets (except for <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>, which are shown in the main paper) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F9\" title=\"Figure 9 &#8227; F.3 Semantic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "medicalo1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can still observe from the results that datasets with more concentrated topical focus (e.g., <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span>) exhibit clear clustering patterns, whereas those with broader thematic coverage (e.g., <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>) display a more dispersed distribution of data points.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "11kbusiness",
                    "data"
                ]
            }
        ]
    },
    "A6.T5": {
        "source_file": "Large Language Model Prompt Datasets: An In-depth Analysis and Insights",
        "caption": "Table 5: All detected dependency types, with the values indicating their proportions in the dataset. - means the Dependency Type not detected in the dataset.",
        "body": "Dependency Type\n1.1k-business\nBoredHumans\ndolly-15k\nmedical-o1\nOASST1\nSelf-Instruct\nShareGPT\n\n\n\n\npunct\n0.1227\n0.1985\n0.1445\n0.1216\n0.1273\n0.1863\n0.154\n\n\nprep\n0.0759\n0.0672\n0.0866\n0.1013\n0.0816\n0.0676\n0.0764\n\n\ndet\n0.0518\n0.0692\n0.0961\n0.0906\n0.0841\n0.0838\n0.0693\n\n\npobj\n0.0718\n0.062\n0.0817\n0.0979\n0.076\n0.0645\n0.0711\n\n\nnsubj\n0.0596\n0.0545\n0.065\n0.0469\n0.0739\n0.0596\n0.0562\n\n\nROOT\n0.0528\n0.0462\n0.0768\n0.0444\n0.0604\n0.0792\n0.0437\n\n\namod\n0.0573\n0.0527\n0.0469\n0.1072\n0.0523\n0.0384\n0.048\n\n\ndobj\n0.0904\n0.0665\n0.0447\n0.0315\n0.0594\n0.057\n0.0519\n\n\ncompound\n0.0742\n0.0471\n0.0719\n0.0716\n0.0436\n0.023\n0.0576\n\n\nconj\n0.0457\n0.0494\n0.0569\n0.0391\n0.0343\n0.0359\n0.0371\n\n\naux\n0.0642\n0.0425\n0.0257\n0.0143\n0.0495\n0.0302\n0.0355\n\n\ndep\n0.0095\n0.0306\n0.007\n0.0183\n0.0218\n0.0611\n0.0577\n\n\ncc\n0.04\n0.0297\n0.0203\n0.0291\n0.0289\n0.0203\n0.0287\n\n\nadvmod\n0.0269\n0.0273\n0.0263\n0.023\n0.0383\n0.0224\n0.0299\n\n\nposs\n0.0401\n0.0183\n0.0084\n0.0132\n0.0118\n0.0124\n0.0116\n\n\nappos\n0.003\n0.0223\n0.017\n0.0099\n0.0129\n0.0207\n0.0238\n\n\nattr\n0.0044\n0.005\n0.0306\n0.014\n0.0163\n0.0113\n0.0083\n\n\nnummod\n0.003\n0.0073\n0.0096\n0.0276\n0.0093\n0.0136\n0.0155\n\n\nnmod\n0.0252\n0.0126\n0.0042\n0.0095\n0.0068\n0.0043\n0.0126\n\n\nccomp\n0.0058\n0.0129\n0.0089\n0.0044\n0.013\n0.0142\n0.013\n\n\nrelcl\n0.0146\n0.0088\n0.0097\n0.0072\n0.0109\n0.0097\n0.0094\n\n\nxcomp\n0.0194\n0.0104\n0.0042\n0.0042\n0.0106\n0.0079\n0.0099\n\n\nadvcl\n0.0104\n0.0109\n0.0056\n0.0066\n0.0115\n0.0086\n0.0122\n\n\nnpadvmod\n0.0026\n0.0068\n0.0052\n0.0141\n0.0051\n0.0052\n0.0066\n\n\nacomp\n0.0034\n0.0041\n0.0059\n0.007\n0.0086\n0.0091\n0.0068\n\n\nmark\n0.002\n0.0057\n0.0038\n0.0033\n0.0087\n0.0114\n0.0095\n\n\nacl\n0.0038\n0.0062\n0.0055\n0.0077\n0.0056\n0.0079\n0.0063\n\n\nauxpass\n0.0016\n0.0017\n0.0062\n0.0101\n0.0055\n0.0052\n0.0059\n\n\npcomp\n0.008\n0.0048\n0.0036\n0.0053\n0.0057\n0.0035\n0.0051\n\n\nnsubjpass\n0.0014\n0.0015\n0.005\n0.009\n0.0046\n0.0048\n0.0051\n\n\nneg\n0.0015\n0.0031\n0.0016\n0.002\n0.0043\n0.0033\n0.0045\n\n\ncase\n0.0035\n0.0013\n0.0032\n0.0024\n0.002\n0.0011\n0.0023\n\n\ndative\n0.0003\n0.0029\n0.0041\n0.0001\n0.0035\n0.0023\n0.0016\n\n\nprt\n0.0014\n0.0018\n0.0014\n0.0006\n0.0025\n0.0047\n0.0025\n\n\nintj\n0.0004\n0.0038\n0.0012\n0.0002\n0.0033\n0.0016\n0.0025\n\n\nagent\n0.0002\n0.0003\n0.001\n0.002\n0.001\n0.0015\n0.0012\n\n\nexpl\n0.0\n0.0002\n0.0004\n0.0007\n0.0016\n0.0014\n0.0011\n\n\nquantmod\n0.0\n0.0002\n0.0005\n0.0007\n0.0009\n0.0014\n0.0016\n\n\nmeta\n0.0001\n0.0016\n0.0001\n0.0\n0.0003\n0.0018\n0.0012\n\n\noprd\n0.0002\n0.0009\n0.0009\n0.0007\n0.0009\n0.0004\n0.0008\n\n\npredet\n-\n0.0003\n0.0005\n0.0001\n0.0006\n0.0009\n0.0005\n\n\ncsubj\n0.0005\n0.0001\n0.0004\n0.0004\n0.0005\n0.0001\n0.0007\n\n\nparataxis\n-\n0.0009\n0.0001\n0.0\n0.0003\n0.0002\n0.0006\n\n\npreconj\n0.0\n0.0001\n0.0008\n0.0002\n0.0002\n0.0002\n0.0003\n\n\ncsubjpass\n0.0\n-\n0.0001\n0.0\n0.0\n0.0\n0.0",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dependency Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">1.1k-business</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BoredHumans</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">dolly-15k</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">medical-o1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OASST1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Self-Instruct</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ShareGPT</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">punct</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1227</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.1985</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1445</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.1216</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1273</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1863</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.154</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">prep</th>\n<td class=\"ltx_td ltx_align_center\">0.0759</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0672</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0866</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.1013</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0816</td>\n<td class=\"ltx_td ltx_align_center\">0.0676</td>\n<td class=\"ltx_td ltx_align_center\">0.0764</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">det</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0518</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0692</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0961</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0906</td>\n<td class=\"ltx_td ltx_align_center\">0.0841</td>\n<td class=\"ltx_td ltx_align_center\">0.0838</td>\n<td class=\"ltx_td ltx_align_center\">0.0693</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">pobj</th>\n<td class=\"ltx_td ltx_align_center\">0.0718</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.062</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0817</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0979</span></td>\n<td class=\"ltx_td ltx_align_center\">0.076</td>\n<td class=\"ltx_td ltx_align_center\">0.0645</td>\n<td class=\"ltx_td ltx_align_center\">0.0711</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">nsubj</th>\n<td class=\"ltx_td ltx_align_center\">0.0596</td>\n<td class=\"ltx_td ltx_align_center\">0.0545</td>\n<td class=\"ltx_td ltx_align_center\">0.065</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0469</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0739</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0596</td>\n<td class=\"ltx_td ltx_align_center\">0.0562</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">ROOT</th>\n<td class=\"ltx_td ltx_align_center\">0.0528</td>\n<td class=\"ltx_td ltx_align_center\">0.0462</td>\n<td class=\"ltx_td ltx_align_center\">0.0768</td>\n<td class=\"ltx_td ltx_align_center\">0.0444</td>\n<td class=\"ltx_td ltx_align_center\">0.0604</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0792</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0437</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">amod</th>\n<td class=\"ltx_td ltx_align_center\">0.0573</td>\n<td class=\"ltx_td ltx_align_center\">0.0527</td>\n<td class=\"ltx_td ltx_align_center\">0.0469</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.1072</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0523</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0384</span></td>\n<td class=\"ltx_td ltx_align_center\">0.048</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">dobj</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0904</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0665</td>\n<td class=\"ltx_td ltx_align_center\">0.0447</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0315</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0594</td>\n<td class=\"ltx_td ltx_align_center\">0.057</td>\n<td class=\"ltx_td ltx_align_center\">0.0519</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">compound</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0742</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0471</td>\n<td class=\"ltx_td ltx_align_center\">0.0719</td>\n<td class=\"ltx_td ltx_align_center\">0.0716</td>\n<td class=\"ltx_td ltx_align_center\">0.0436</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.023</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0576</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">conj</th>\n<td class=\"ltx_td ltx_align_center\">0.0457</td>\n<td class=\"ltx_td ltx_align_center\">0.0494</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0569</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0391</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0343</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0359</td>\n<td class=\"ltx_td ltx_align_center\">0.0371</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">aux</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0642</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0425</td>\n<td class=\"ltx_td ltx_align_center\">0.0257</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0143</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0495</td>\n<td class=\"ltx_td ltx_align_center\">0.0302</td>\n<td class=\"ltx_td ltx_align_center\">0.0355</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">dep</th>\n<td class=\"ltx_td ltx_align_center\">0.0095</td>\n<td class=\"ltx_td ltx_align_center\">0.0306</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.007</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0183</td>\n<td class=\"ltx_td ltx_align_center\">0.0218</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0611</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0577</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">cc</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.04</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0297</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0203</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0291</td>\n<td class=\"ltx_td ltx_align_center\">0.0289</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0203</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0287</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">advmod</th>\n<td class=\"ltx_td ltx_align_center\">0.0269</td>\n<td class=\"ltx_td ltx_align_center\">0.0273</td>\n<td class=\"ltx_td ltx_align_center\">0.0263</td>\n<td class=\"ltx_td ltx_align_center\">0.023</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0383</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0224</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0299</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">poss</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0401</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0183</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0084</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0132</td>\n<td class=\"ltx_td ltx_align_center\">0.0118</td>\n<td class=\"ltx_td ltx_align_center\">0.0124</td>\n<td class=\"ltx_td ltx_align_center\">0.0116</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">appos</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.003</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0223</td>\n<td class=\"ltx_td ltx_align_center\">0.017</td>\n<td class=\"ltx_td ltx_align_center\">0.0099</td>\n<td class=\"ltx_td ltx_align_center\">0.0129</td>\n<td class=\"ltx_td ltx_align_center\">0.0207</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0238</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">attr</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0044</span></td>\n<td class=\"ltx_td ltx_align_center\">0.005</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0306</span></td>\n<td class=\"ltx_td ltx_align_center\">0.014</td>\n<td class=\"ltx_td ltx_align_center\">0.0163</td>\n<td class=\"ltx_td ltx_align_center\">0.0113</td>\n<td class=\"ltx_td ltx_align_center\">0.0083</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">nummod</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.003</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0073</td>\n<td class=\"ltx_td ltx_align_center\">0.0096</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0276</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0093</td>\n<td class=\"ltx_td ltx_align_center\">0.0136</td>\n<td class=\"ltx_td ltx_align_center\">0.0155</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">nmod</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0252</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0126</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0042</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0095</td>\n<td class=\"ltx_td ltx_align_center\">0.0068</td>\n<td class=\"ltx_td ltx_align_center\">0.0043</td>\n<td class=\"ltx_td ltx_align_center\">0.0126</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">ccomp</th>\n<td class=\"ltx_td ltx_align_center\">0.0058</td>\n<td class=\"ltx_td ltx_align_center\">0.0129</td>\n<td class=\"ltx_td ltx_align_center\">0.0089</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0044</span></td>\n<td class=\"ltx_td ltx_align_center\">0.013</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0142</span></td>\n<td class=\"ltx_td ltx_align_center\">0.013</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">relcl</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0146</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0088</td>\n<td class=\"ltx_td ltx_align_center\">0.0097</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0072</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0109</td>\n<td class=\"ltx_td ltx_align_center\">0.0097</td>\n<td class=\"ltx_td ltx_align_center\">0.0094</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">xcomp</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0194</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0104</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0042</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0042</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0106</td>\n<td class=\"ltx_td ltx_align_center\">0.0079</td>\n<td class=\"ltx_td ltx_align_center\">0.0099</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">advcl</th>\n<td class=\"ltx_td ltx_align_center\">0.0104</td>\n<td class=\"ltx_td ltx_align_center\">0.0109</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0056</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0066</td>\n<td class=\"ltx_td ltx_align_center\">0.0115</td>\n<td class=\"ltx_td ltx_align_center\">0.0086</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0122</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">npadvmod</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0026</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0068</td>\n<td class=\"ltx_td ltx_align_center\">0.0052</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0141</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0051</td>\n<td class=\"ltx_td ltx_align_center\">0.0052</td>\n<td class=\"ltx_td ltx_align_center\">0.0066</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">acomp</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0034</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0041</td>\n<td class=\"ltx_td ltx_align_center\">0.0059</td>\n<td class=\"ltx_td ltx_align_center\">0.007</td>\n<td class=\"ltx_td ltx_align_center\">0.0086</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0091</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0068</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">mark</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.002</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0057</td>\n<td class=\"ltx_td ltx_align_center\">0.0038</td>\n<td class=\"ltx_td ltx_align_center\">0.0033</td>\n<td class=\"ltx_td ltx_align_center\">0.0087</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0114</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0095</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">acl</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0038</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0062</td>\n<td class=\"ltx_td ltx_align_center\">0.0055</td>\n<td class=\"ltx_td ltx_align_center\">0.0077</td>\n<td class=\"ltx_td ltx_align_center\">0.0056</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0079</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0063</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">auxpass</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0016</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0017</td>\n<td class=\"ltx_td ltx_align_center\">0.0062</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0101</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0055</td>\n<td class=\"ltx_td ltx_align_center\">0.0052</td>\n<td class=\"ltx_td ltx_align_center\">0.0059</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">pcomp</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.008</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0048</td>\n<td class=\"ltx_td ltx_align_center\">0.0036</td>\n<td class=\"ltx_td ltx_align_center\">0.0053</td>\n<td class=\"ltx_td ltx_align_center\">0.0057</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0035</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0051</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">nsubjpass</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0014</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0015</td>\n<td class=\"ltx_td ltx_align_center\">0.005</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.009</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0046</td>\n<td class=\"ltx_td ltx_align_center\">0.0048</td>\n<td class=\"ltx_td ltx_align_center\">0.0051</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">neg</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0015</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0031</td>\n<td class=\"ltx_td ltx_align_center\">0.0016</td>\n<td class=\"ltx_td ltx_align_center\">0.002</td>\n<td class=\"ltx_td ltx_align_center\">0.0043</td>\n<td class=\"ltx_td ltx_align_center\">0.0033</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0045</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">case</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0035</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0013</td>\n<td class=\"ltx_td ltx_align_center\">0.0032</td>\n<td class=\"ltx_td ltx_align_center\">0.0024</td>\n<td class=\"ltx_td ltx_align_center\">0.002</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0011</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0023</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">dative</th>\n<td class=\"ltx_td ltx_align_center\">0.0003</td>\n<td class=\"ltx_td ltx_align_center\">0.0029</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0041</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0001</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0035</td>\n<td class=\"ltx_td ltx_align_center\">0.0023</td>\n<td class=\"ltx_td ltx_align_center\">0.0016</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">prt</th>\n<td class=\"ltx_td ltx_align_center\">0.0014</td>\n<td class=\"ltx_td ltx_align_center\">0.0018</td>\n<td class=\"ltx_td ltx_align_center\">0.0014</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0006</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0025</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0047</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0025</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">intj</th>\n<td class=\"ltx_td ltx_align_center\">0.0004</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0038</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0012</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0002</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0033</td>\n<td class=\"ltx_td ltx_align_center\">0.0016</td>\n<td class=\"ltx_td ltx_align_center\">0.0025</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">agent</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0002</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0003</td>\n<td class=\"ltx_td ltx_align_center\">0.001</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.002</span></td>\n<td class=\"ltx_td ltx_align_center\">0.001</td>\n<td class=\"ltx_td ltx_align_center\">0.0015</td>\n<td class=\"ltx_td ltx_align_center\">0.0012</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">expl</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0002</td>\n<td class=\"ltx_td ltx_align_center\">0.0004</td>\n<td class=\"ltx_td ltx_align_center\">0.0007</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0016</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0014</td>\n<td class=\"ltx_td ltx_align_center\">0.0011</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">quantmod</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0002</td>\n<td class=\"ltx_td ltx_align_center\">0.0005</td>\n<td class=\"ltx_td ltx_align_center\">0.0007</td>\n<td class=\"ltx_td ltx_align_center\">0.0009</td>\n<td class=\"ltx_td ltx_align_center\">0.0014</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0016</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">meta</th>\n<td class=\"ltx_td ltx_align_center\">0.0001</td>\n<td class=\"ltx_td ltx_align_center\">0.0016</td>\n<td class=\"ltx_td ltx_align_center\">0.0001</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0003</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0018</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0012</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">oprd</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0002</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0009</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0009</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0007</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0009</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0004</td>\n<td class=\"ltx_td ltx_align_center\">0.0008</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">predet</th>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.0003</td>\n<td class=\"ltx_td ltx_align_center\">0.0005</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0001</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0006</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0009</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0005</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">csubj</th>\n<td class=\"ltx_td ltx_align_center\">0.0005</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0001</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0004</td>\n<td class=\"ltx_td ltx_align_center\">0.0004</td>\n<td class=\"ltx_td ltx_align_center\">0.0005</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0001</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0007</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">parataxis</th>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0009</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0001</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0003</td>\n<td class=\"ltx_td ltx_align_center\">0.0002</td>\n<td class=\"ltx_td ltx_align_center\">0.0006</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">preconj</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0001</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0008</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0002</td>\n<td class=\"ltx_td ltx_align_center\">0.0002</td>\n<td class=\"ltx_td ltx_align_center\">0.0002</td>\n<td class=\"ltx_td ltx_align_center\">0.0003</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">csubjpass</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.0001</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "dependency",
            "attr",
            "csubjpass",
            "11kbusiness",
            "poss",
            "csubj",
            "selfinstruct",
            "oprd",
            "nsubjpass",
            "medicalo1",
            "xcomp",
            "nmod",
            "preconj",
            "oasst1",
            "advcl",
            "npadvmod",
            "pcomp",
            "intj",
            "meta",
            "parataxis",
            "mark",
            "predet",
            "types",
            "advmod",
            "root",
            "det",
            "sharegpt",
            "prt",
            "proportions",
            "conj",
            "dep",
            "dobj",
            "case",
            "dative",
            "expl",
            "means",
            "amod",
            "their",
            "ccomp",
            "compound",
            "prep",
            "pobj",
            "punct",
            "aux",
            "detected",
            "dataset",
            "indicating",
            "nsubj",
            "agent",
            "all",
            "relcl",
            "appos",
            "nummod",
            "boredhumans",
            "auxpass",
            "quantmod",
            "values",
            "acomp",
            "dolly15k",
            "acl",
            "type",
            "neg",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In this section, we present the complete experimental data for all identified dependency types, along with their proportions in the datasets, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.T5\" title=\"Table 5 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Additionally, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.T6\" title=\"Table 6 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> lists all detected Part-of-Speech tags and their corresponding proportions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F8\" title=\"Figure 8 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> further illustrates the ten most common verbs and their top five direct noun objects found in the prompt datasets except <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>, which are shown in the main paper.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">A prompt is a natural language instruction that defines a specific task for a large language model (LLM) and serves as the primary interface for human-LLM interaction. With the growing deployment of LLMs, diverse prompt datasets are emerging from platforms such as GitHub and social media. These datasets span a wide array of applications and content types, facilitating both broader LLM utilization and improved prompt engineering.\nIn this work, we&#8211;for the first time&#8211;have compiled an extensive list of prompt datasets sourced from various channels, representing a spectrum of downstream tasks, languages, engineering techniques, attributes, and modalities. We select key representative datasets for systematic analysis, revealing commonalities and differences in prompt construction across categories, distinguishing them from other text corpora like literature and web.\nWe further propose a prompt optimization approach that leverages syntactic embeddings of part-of-speech and dependency structures. By identifying a centroid representation of prompts and guiding LLMs to rewrite prompts toward this centroid, our method improves the meaningfulness of model outputs.\nWe have made our datasets and code available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/LLM-Prompt-Datasets-7416\" title=\"\">https://anonymous.4open.science/r/LLM-Prompt-Datasets-7416</a>.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "types"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data discovery process.</span> We collect publicly available datasets from the following four types of sources. <span class=\"ltx_text ltx_font_bold\">First</span>, we consult <span class=\"ltx_text ltx_font_italic\">dataset collection platforms</span>, including Hugging Face Datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib12\" title=\"\">hug, </a>)</cite>, Kaggle <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib13\" title=\"\">kag, </a>)</cite>, Google Dataset Search <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib11\" title=\"\">goo, </a>)</cite>, and Papers with Code <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib14\" title=\"\">pap, </a>)</cite>. Targeted searches using keywords, e.g., \"prompt dataset\", \"instruction-following dataset\", and \"conversation dataset\" yield 60 prompt datasets. <span class=\"ltx_text ltx_font_bold\">Second</span>, we review the latest <span class=\"ltx_text ltx_font_italic\">academic publications</span>, specifically papers on prompt engineering, natural language understanding, and dialogue systems, published at NeurIPS, ICLR, and ICML between 2023-2024 and identify 73 datasets shared across them. <span class=\"ltx_text ltx_font_bold\">Third</span>, we also examine <span class=\"ltx_text ltx_font_italic\">public repositories</span> by systematically surveying open-source GitHub projects using keywords, e.g., &#8220;prompt collection&#8221;, &#8220;LLM prompts&#8221;, and &#8220;instruction dataset&#8221;. We identify 21 prompt repositories that typically contain curated prompt lists derived from user interactions or synthesized from public APIs. Some of these repositories are &#8220;awesome-lists&#8221;, which are curated collections of high-quality prompts or links to prompt datasets.\nNotable examples include <span class=\"ltx_text ltx_font_sansserif\">Awesome Instruction Datasets</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib46\" title=\"\">Nie, </a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering Guide</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Saravia, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib56\" title=\"\">2022</a>)</cite>, and <span class=\"ltx_text ltx_font_sansserif\">LLMDataHub</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib71\" title=\"\">Zhao, </a>)</cite>. <span class=\"ltx_text ltx_font_bold\">Finally</span>, we extract 14 datasets from <span class=\"ltx_text ltx_font_italic\">popular websites dedicated to prompt-sharing</span>, including <span class=\"ltx_text ltx_font_sansserif\">Prompt Genius</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Pro, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib7\" title=\"\">b</a>)</cite> and <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib10\" title=\"\">bor, </a>)</cite>. These platforms feature user-written prompts for practical purposes.</p>\n\n",
                "matched_terms": [
                    "boredhumans",
                    "types",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data filtering.</span>\nWe remove duplicate entries (e.g., CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib51\" title=\"\">2024</a>)</cite> appears in both Hugging Face and NeurIPS 2024) and then filter the remaining candidates using four quality criteria for inclusion in this paper.\n<span class=\"ltx_text ltx_font_bold\">First</span>, <span class=\"ltx_text ltx_font_italic\">Dataset size.</span> We prioritize datasets containing at least 1K prompts to ensure robustness in diversity and statistical power. In contrast, due to their generally limited scope, user-shared datasets are filtered with a minimum threshold of 50 prompts.\n<span class=\"ltx_text ltx_font_bold\">Second</span>, <span class=\"ltx_text ltx_font_italic\">Data quality.</span> We evaluate the quality of prompts based on their cleanliness. Most datasets (e.g., <span class=\"ltx_text ltx_font_sansserif\">OpenCodeReasoning</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib16\" title=\"\">2025b</a>)</cite>) on data hosting platforms (e.g., Hugging Face and Kaggle) are well-formatted and clean. For the remaining data, we exclude samples with inconsistent formatting or unclear structure. For instance, the <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering Guide</span>&#8211;a resource that offers both curated prompt datasets and instructional examples&#8211;contains many illustrative prompts scattered throughout the material and are thus omitted from our datasets.\n<span class=\"ltx_text ltx_font_bold\">Third</span>, <span class=\"ltx_text ltx_font_italic\">Data relevance.</span> We assess whether the prompts are aligned with our data discovery guidelines, specifically emphasizing on those that represent common usage scenarios for broad audiences (e.g., <span class=\"ltx_text ltx_font_sansserif\">Chinese-DeepSeek-R1-Distill-data-110k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib38\" title=\"\">2025a</a>)</cite>), and tasks from various domains (e.g., <span class=\"ltx_text ltx_font_sansserif\">Medical Verifiable Problems</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">OpenMathReasoning</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Moshkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib44\" title=\"\">2025</a>)</cite>). Datasets that violate our discovery guidelines are omitted. For instance, the <span class=\"ltx_text ltx_font_sansserif\">PersonaHub</span> dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib5\" title=\"\">Per, </a>)</cite> is excluded because it does not meet <span class=\"ltx_text ltx_font_bold\">data discovery guideline (1)</span>, which mandates that prompts be linked to specific, well-defined tasks. Although <span class=\"ltx_text ltx_font_sansserif\">PersonaHub</span> demonstrates the potential of synthetic personas in generating diverse content (e.g., reasoning problems, dialogues, or non-player character behaviors), it predominantly comprises persona descriptions without clear task formulation.\n<span class=\"ltx_text ltx_font_bold\">Fourth</span>, <span class=\"ltx_text ltx_font_italic\">Accessibility.</span> Datasets must be publicly accessible or retrievable via automated crawling, and their licensing terms must permit research use. After filtering, we identify 129 distinct prompt datasets for taxonomic analysis (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S4\" title=\"4 Dataset Taxonomy &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "their",
                    "dataset",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generation process</span> describes how the prompts are created. <span class=\"ltx_text ltx_font_italic\">Human-generated prompts</span> are either manually authored (e.g., <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>)</cite>) or collected from user queries (e.g., <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib36\" title=\"\">Li, </a>)</cite>), <span class=\"ltx_text ltx_font_italic\">Model-generated prompts</span> include those created via self-instruct techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib65\" title=\"\">2023</a>)</cite> (e.g., <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>), multi-agent simulations (e.g., <span class=\"ltx_text ltx_font_sansserif\">AI Society</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib34\" title=\"\">2023a</a>)</cite>), or reverse instruction generation (e.g., <span class=\"ltx_text ltx_font_sansserif\">LongForm</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;ksal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib32\" title=\"\">2023</a>)</cite>). Finally, <span class=\"ltx_text ltx_font_italic\">derivative datasets</span> build on existing resources through task expansion or reformatted aggregation (e.g., <span class=\"ltx_text ltx_font_sansserif\">Flan 2022</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib4\" title=\"\">Fla, </a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">xP3</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib45\" title=\"\">2022</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "sharegpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Content.</span>\nPrompt datasets are characterized by distinct linguistic and structural attributes.\n<span class=\"ltx_text ltx_font_bold\">Linguistically</span>, they may be <span class=\"ltx_text ltx_font_italic\">monolingual</span> or <span class=\"ltx_text ltx_font_italic\">multilingual</span>; in the latter case, datasets are deemed <span class=\"ltx_text ltx_font_italic\">semantically aligned</span> if each entry includes multilingual counterparts with identical semantics, thereby enhancing LLM performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib35\" title=\"\">2023b</a>)</cite>.\nIn terms of <span class=\"ltx_text ltx_font_bold\">display form</span>, prompts appear either as <span class=\"ltx_text ltx_font_italic\">conversation</span> (e.g., single-round, multi-round, or tree-structured) or <span class=\"ltx_text ltx_font_italic\">instruction</span> (e.g., user prompt, system prompt). The prompt <span class=\"ltx_text ltx_font_bold\">format</span>&#8211;ranging from <span class=\"ltx_text ltx_font_italic\">free-form</span> to <span class=\"ltx_text ltx_font_italic\">structured</span> (e.g., JSON, Markdown, HTML), or a combination thereof&#8211;substantially influences LLM response quality <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib40\" title=\"\">2025b</a>)</cite>.\nFinally, datasets differ in their use of <span class=\"ltx_text ltx_font_bold\">placeholders</span>, which allow for text substitution and enable diversified prompt transformations <cite class=\"ltx_cite ltx_citemacro_citep\">(Shin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib58\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "case",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following our selection principles, we curated seven representative datasets spanning different user types, instruction methods, and domains. For <span class=\"ltx_text ltx_font_bold\">end users</span>, general-domain prompts include single-turn prompts (<span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span>) and multi-turn conversations (<span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>), while business-domain single-turn prompts are represented by <span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span>. For <span class=\"ltx_text ltx_font_bold\">LLM researchers</span>, human-generated datasets include <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> and <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and model-generated prompts are captured by <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>. For <span class=\"ltx_text ltx_font_bold\">domain scientists</span>, we include model-generated medical prompts from <span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span>. This collection ensures diversity in publisher type, prompt structure, and application domain.</p>\n\n",
                "matched_terms": [
                    "types",
                    "selfinstruct",
                    "boredhumans",
                    "sharegpt",
                    "oasst1",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span> (<span class=\"ltx_text ltx_font_bold\">1.1k-business</span>). A curated dataset of 1&#8201;235 prompts oriented toward professional and business-related use cases, such as marketing, productivity, and decision-making. It represents structured, domain-specific prompting behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(1., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib1\" title=\"\">1</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> (<span class=\"ltx_text ltx_font_bold\">dolly-15k</span>).\nThis dataset includes 15&#8201;000 human-authored instruction&#8211;response pairs covering a range of everyday tasks. It is single-turn and domain-general, curated to support instruction-following models <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dolly15k",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span> (<span class=\"ltx_text ltx_font_bold\">medical-o1</span>).\nSynthetic data of 90&#8201;120 open-ended questions and GPT-4o generated CoTs and responses. Open-ended questions are reformatted by GPT-4o based on close-set medical examination questions. The dataset is used to fine-tune HuatuoGPT-o1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "medicalo1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">OASST1.</span>\nThe Open Assistant dataset (OASST1) contains over 30&#8201;000 human-written messages arranged in dialogue trees. It emphasizes cooperative, open-domain assistant behavior and includes branching conversations rather than linear interactions <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;pf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib33\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "oasst1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">Self-Instruct.</span>\nA synthetic dataset with 82&#8201;646 prompts generated by large language models based on a small seed pool of human-written instructions. For every generation step, it samples 6 human-written tasks and 2 model-generated tasks in previous steps to promote diversity <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib65\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform token-level analysis using <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram models to capture local textual patterns <cite class=\"ltx_cite ltx_citemacro_citep\">(Jurafsky &amp; Martin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib30\" title=\"\">2000</a>; Cavnar &amp; Trenkle, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib21\" title=\"\">1994</a>; Manning &amp; Schutze, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib42\" title=\"\">2001</a>)</cite>. Initially, all tokens are lemmatized to mitigate inflectional variability, after which we extract 3-gram, 4-gram, and 5-gram sequences to compute their frequency distributions. By analyzing high-frequency <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams, we identify prevalent instruction templates, keyword combinations, and syntactic patterns, laying the groundwork for subsequent syntactic and semantic investigations.</p>\n\n",
                "matched_terms": [
                    "their",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> High-frequency <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams reveal domain and prompt-engineering differences, such as role-playing cues in <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> (&#8220;you to act as&#8221;) versus medical reasoning in <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> (&#8220;what be the,&#8221; &#8220;the most likely diagnosis&#8221;). <span class=\"ltx_text ltx_font_bold\">(2)</span> While 3-grams capture general-purpose queries or commands (e.g., &#8220;what be the,&#8221; &#8220;I want to&#8221;), longer <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams (4&#8211;5) reflect task-specific patterns, as in <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> where frequent 5-grams (&#8220;please write in English language,&#8221; &#8220;write a comprehensive reply to&#8221;) highlight its instruction-following orientation. <span class=\"ltx_text ltx_font_bold\">(3)</span> Compared to Google Books 5-grams (e.g., &#8220;at the end of the,&#8221; &#8220;in whole or in part&#8221;) that serve narrative or descriptive purposes, prompt datasets exhibit inquiry- or command-focused <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams, underscoring a clear divergence in linguistic patterns across corpora.</p>\n\n",
                "matched_terms": [
                    "oasst1",
                    "sharegpt",
                    "medicalo1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply the spaCy <span class=\"ltx_text ltx_font_typewriter\">en_core_web_sm</span> parser <cite class=\"ltx_cite ltx_citemacro_citep\">(Honnibal &amp; Montani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib29\" title=\"\">2017</a>)</cite> to extract syntactic dependencies and determine the frequency of key grammatical relations in each dataset. For the EWT and ParTUT corpora, we rely on officially published dependency type annotations. This analysis reveals systematic variations in linguistic style across prompt sources. Additionally, we track verb&#8211;object (dobj) pairs to capture the task-oriented diversity of the prompts (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "dobj",
                    "type",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of Results.</span>\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T2\" title=\"Table 2 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the distribution of eight common dependency types across seven prompt datasets and two reference corpora (EWT and ParTUT), revealing three key findings.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "types"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> The <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> dataset is characterized by its high use of adjectival modifiers (amod, 0.11) and low direct object frequency (dobj, 0.03), reflecting a preference for precise, state-oriented descriptions over action-driven narratives, often framed through linking verbs&#8212;typical of medical contexts detailing conditions, symptoms, and diagnoses. <span class=\"ltx_text ltx_font_bold\">(2)</span> In contrast, the <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> dataset favors concise, goal-driven imperatives with bare noun phrases as direct objects (dobj, 0.09) and minimal use of determiners (det, 0.05), aligning with its project-planning focus. <span class=\"ltx_text ltx_font_bold\">(3)</span> Verb&#8211;noun dependency analysis further distinguishes domains: medical instructions cluster around technical, domain-specific pairs like &#8220;have history&#8221; and &#8220;experience pain,&#8221; while datasets such as <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> use broader, generic pairs like &#8220;write answer&#8221; and &#8220;use code&#8221;. These syntactic patterns highlight each corpus&#8217; thematic priorities and inform strategies for domain-aware model training.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "dobj",
                    "11kbusiness",
                    "amod",
                    "medicalo1",
                    "det",
                    "sharegpt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1)</span> Domain-specific datasets such as <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> and <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> exhibit a noun proportion of <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 0.26, surpassing that found in formal corpora like ParTUT. This reflects a concept-driven focus on domain entities and technical terms.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> Additionally, <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> also registers an unusually high adjective ratio (0.11), indicating a repeated emphasis on specifying medical attributes and conditions, consistent with the descriptive nature of clinical reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "medicalo1",
                    "11kbusiness",
                    "indicating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1) Intra-dataset analysis</span>\ndelineates each dataset&#8217;s lexical focus and stylistic characteristics. For instance, <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> emphasizes business-specific terms like &#8220;content&#8221; and &#8220;email&#8221;, while <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span> features imperatives such as &#8220;act&#8221;, indicative of role-playing instructions. Similarly, <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> shows a dominant TF-IDF score for &#8220;output&#8221; (0.772), highlighting a structural prompt style based on explicit instruction&#8211;response formats.\n<span class=\"ltx_text ltx_font_bold\">(2) Inter-dataset comparison.</span>\nTF-IDF vectors show varying overlaps across datasets. The highest cosine similarity between <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> suggests a similar vocabulary&#8212;likely due to shared human-generation processes. In contrast, <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> is lexically distant from the others, especially <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> and <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span>, reflecting stylistic and domain-specific differences.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "selfinstruct",
                    "medicalo1",
                    "boredhumans",
                    "sharegpt",
                    "oasst1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze prompt semantics by encoding each prompt into a 384-dimensional dense vector using Sentence-BERT&#8217;s pretrained model <span class=\"ltx_text ltx_font_typewriter\">all-MiniLM-L6-v2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Reimers &amp; Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib50\" title=\"\">2019</a>)</cite>. Each prompt is encoded into a 384-dimensional dense vector that captures its semantic content. These embeddings serve as the foundation for classification, clustering, and visualization analysis. We perform Principal Component Analysis (PCA) to reduce sentence embeddings to two dimensions. For fair comparison, we uniformly at random sample 500 prompts per dataset and visualize their distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Semantic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "their",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1) Wide coverage in Self-Instruct:</span> The <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> dataset exhibits the most dispersed and evenly distributed semantic space, suggesting a broad topical coverage. This aligns with the self-instruction paradigm&#8217;s goal of generating diverse instruction types.\n<span class=\"ltx_text ltx_font_bold\">(2) Semantic cohesion in specific domains:</span> Prompts from <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> form more concentrated clusters, indicating domain-specific semantic cohesion.\n<span class=\"ltx_text ltx_font_bold\">(3) Overlap among human-generated sets:</span> The embeddings of <span class=\"ltx_text ltx_font_sansserif\">dolly-15k</span>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> overlap substantially across both PCA dimensions. This suggests that these datasets share stylistic and semantic characteristics, possibly due to their common reliance on human-LLM interactions for data generation.</p>\n\n",
                "matched_terms": [
                    "types",
                    "11kbusiness",
                    "selfinstruct",
                    "their",
                    "medicalo1",
                    "oasst1",
                    "sharegpt",
                    "dolly15k",
                    "dataset",
                    "indicating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the above analysis, we propose a new prompt engineering method that leverages structural linguistic features. Specifically, we take the average of the high-dimensional embeddings of POS tags and dependency relations from the analyzed dataset to define a centroid representation. This centroid captures the &#8220;central&#8221; syntactic patterns that are associated with higher-performing prompts.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We are committed to ensuring the reproducibility of our results. All code used in this research is publicly available through links in our abstract. The repository includes detailed instructions for dataset preprocessing, and running experiments. We also specify the exact versions of dependencies and libraries used in our experiments. All datasets employed in this study are either publicly accessible or their sources are clearly documented. Random seeds are set for all experiments where applicable to minimize variability. Together, these resources enable researchers to reproduce our analyses and results with minimal effort.</p>\n\n",
                "matched_terms": [
                    "their",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that the labeled license refers to the licensing information assigned to the dataset based on the publishers&#8217; declared rights. However, certain sub-datasets may remain subject to their original licensing conditions, which could differ from the labeled license.</p>\n\n",
                "matched_terms": [
                    "their",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Stanford Alpaca dataset comprises 52K high-quality, instruction-following examples generated via a modified Self-Instruct pipeline using text-davinci-003. Designed for fine-tuning LLaMA models, it enables research in alignment, instruction tuning, and synthetic data generation.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Aya Collection is a massive multilingual instruction tuning dataset comprising over 513 million prompt-completion pairs across 115 languages. It integrates three sources: human-crafted instruction templates created by fluent speakers for diverse tasks, machine translations of 19 top-tier datasets into 101 languages via NLLB, and the human-annotated Aya Dataset subset of 204K examples. Split by dataset, each record includes id, inputs, targets, language, script, and task type. Licensed under Apache-2.0, it supports academic and commercial classification, summarization, translation, and QA research.</p>\n\n",
                "matched_terms": [
                    "type",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BELLE_Generated_Chat contains approx. 400k personalized Chinese character dialogues generated by the BELLE project. Each record includes an instruction, an (empty) input, and a generated output. Created by ChatGPT and not strictly verified, the dataset may contain factual inaccuracies. Licensed under GPL-3.0 for research use only. With around 0.4 million entries, it supports text-to-text generation and conversational modeling.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BoredHumans is a diverse and extensive prompt dataset compiled from multiple sources, including Awesome ChatGPT Prompts, Data Science Prompts, and Tree-of-Thought Prompting, among others. Its rich variety covers numerous domains and prompt styles, enabling comprehensive research on prompt engineering, AI model behavior, and in-context learning strategies.</p>\n\n",
                "matched_terms": [
                    "boredhumans",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: COIG-CQIA (Chinese Open Instruction Generalist - Quality is All You Need) is a high-quality, open-source Chinese instruction tuning dataset designed to align language models with human interactive behavior. It aggregates over 45,000 manually cleansed, restructured, and reviewed examples spanning social media dialogs, encyclopedic articles, exam questions, finance, medical, legal, traditional culture, and NLP tasks. Each entry includes instruction, optional input, output, task type, domain, and human verification metadata. COIG-CQIA aims to facilitate instruction fine-tuning for Chinese NLP research and applications.</p>\n\n",
                "matched_terms": [
                    "type",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Dynosaur introduces a dynamic and low-cost paradigm for curating instruction-tuning datasets. It automatically generates diverse instructions by leveraging metadata from HuggingFace datasets, combined with LLM-based instruction synthesis (e.g., via ChatGPT). The result is Dynosaur-full, a large-scale dataset (800K+ samples, generated at &#160; $11.5) that supports dynamic growth and general-purpose instruction-tuning. Empirically, models fine-tuned on Dynosaur outperform Alpaca and GPT-4-Instruct baselines on Super-NI. The project includes: metadata crawling tools, instruction generation pipelines, and fine-tuned T5-3B and LLaMA-7B models. All generated instructions are under Apache 2.0, with task data adhering to original dataset licenses.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Firefly is a Chinese instruction-tuning dataset comprising 1.15 million high-quality examples drawn from 23 common Chinese natural language processing datasets. Each example includes a task type, an input prompt, and a target output, ensuring diverse coverage. Data templates were manually designed for each task to ensure quality and richness. Token length analysis shows that most examples are under 600 tokens. Firefly was used to train the Firefly-1b4 Chinese dialogue LLM, available on GitHub and Hugging Face, fostering reproducibility, community collaboration.</p>\n\n",
                "matched_terms": [
                    "type",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: GraphWalks is an open-source benchmark dataset designed to evaluate multi-hop reasoning over long graph contexts. Released under the MIT license, it provides directed graphs as edge lists alongside user-specified operations&#8212;such as breadth-first searches or parent retrieval&#8212;for models to execute. Each prompt comprises three demonstration examples, a target graph, and a query, with expected outputs formatted as node ID lists. Accompanying metadata includes prompt character counts and problem types. Standardized extraction and F1-based grading scripts ensure consistent answer parsing and evaluation.</p>\n\n",
                "matched_terms": [
                    "types",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: GSM8K (Grade School Math 8K) is an English monolingual dataset of 8.8K crowd-sourced grade school math word problems paired with multi-step solutions. It contains a main configuration and a Socratic variant, each offering questions and answers with calculator annotations and step-by-step reasoning expressed in natural language. Problems require two to eight elementary arithmetic steps. Split into training (7,473 examples) and test (1,319 examples), GSM8K supports text-to-text generation benchmarks under MIT license. All annotations were crowdsourced via Upwork and Surge AI.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LaMini-Instruction is an English text-to-text generation dataset comprising 2.58M instruction-response pairs distilled from GPT-3.5-Turbo. Each sample includes an instruction, a corresponding model-generated response, and the instruction&#8217;s provenance&#8212;drawn from sources such as Alpaca, FLAN, P3, and Self-Instruct. Released under CC-BY-NC 4.0, it spans a single training split of over 1.16 GB and supports fine-tuning of compact language models. LaMini-Instruction enables research in instruction-based learning but inherits biases and errors from its GPT-3.5 teacher.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The LIMA dataset contains 1,000 high-quality prompt-response pairs designed to align language models with the style of a helpful AI assistant. Prompts are diverse, sourced from Stack Exchange, wikiHow, WritingPrompts, Natural Instructions, and manually authored examples. Despite limited size (&#160;750K tokens), all responses are stylistically consistent. The dataset includes a 50-example development set and a 300-prompt test set. LIMA demonstrates that small, curated datasets can be highly effective for instruction tuning and alignment of pretrained language models.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Phoenix-sft-data-v1 is a multilingual supervised fine-tuning dataset containing 464,510 samples, combining instruction-following and ChatGPT-distilled conversation data. It includes Alpaca-derived tasks, post-translated multilingual instructions, and user-centered prompts in 40 languages. The dataset also integrates ShareGPT and Discord-sourced dialogues. With nearly 1 million conversation turns and detailed multilingual annotations, it supports multilingual language modeling, alignment, and chat adaptation. English and Chinese dominate the corpus, with broader linguistic diversity represented across the remaining data, enabling robust multilingual model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: PubMedQA is a biomedical question answering (QA) dataset designed to evaluate systems on their ability to answer yes/no/maybe research questions using corresponding PubMed abstracts. The dataset focuses on factual reasoning within biomedical literature.</p>\n\n",
                "matched_terms": [
                    "their",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Self-Instruct is an open Apache-2.0-licensed dataset and framework designed to enhance language models&#8217; instruction-following capabilities. It comprises four configurations: a self-generated set of 82K prompt-completion pairs produced via OpenAI&#8217;s davinci engine; 50K samples from Super Natural Instructions; 52K prompts drawn from the P3 public pool; and 252 expert-crafted human evaluation tasks with associated inputs and outputs. All data is in English and supports instruction-tuning by providing diverse natural-language prompts paired with corresponding model or human completions. The dataset facilitates instruction-tuning.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: ShareGPT90K is a dataset of 90,665 conversational threads scraped from the ShareGPT platform. Each example includes a unique id and a sequence of messages, with each message annotated by its origin and its content.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: ShareGPT-Chinese-English-90k is a 90K-instance bilingual parallel human-machine QA dataset covering real and complex user inquiries in both Chinese and English. Licensed under Apache-2.0, it provides semantically aligned Chinese-English QA pairs for robust training of instruction-following dialogue and text-generation models. Unlike synthetic API-simulated corpora, all questions originate from genuine user interactions, preserving realistic instruction distributions. Collected through voluntary sharing, it naturally filters out low-quality exchanges. The dataset supports question-answering and text-generation tasks and can be easily loaded via the Firefly framework.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraFeedback is an MIT-licensed, open-source, large-scale preference dataset designed for training reward and critic models. It contains 64 K prompts drawn from UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA and FLAN, each answered by four out of 17 diverse LLMs under five alignment principles. The result is 256 K responses and 380 K fine-grained annotations covering instruction-following, truthfulness, honesty and helpfulness, all rated by GPT-4. Its scale, diversity and dense numerical plus textual feedback make it ideal for RLHF research and robust reward-model development.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraMedical is a large-scale English biomedical instruction dataset featuring over 409,000 examples licensed under MIT. Each sample includes an identifier, instruction type, multi-turn conversation pairs between human queries and GPT-generated responses, a ground-truth answer, and a model-evaluated score. The training split comprises roughly 1.2 GB across 410K examples, sourced from both curated public data and synthetic augmentations. UltraMedical aims to support the development of specialized generalist models in biomedicine by providing diverse, high-quality instruction-response instances, and comprehensive evaluation metrics accompany each instance.</p>\n\n",
                "matched_terms": [
                    "type",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Wizard_evol_instruct_196K is a MIT-licensed instruction-tuning dataset comprising 143K evolved QA pairs derived from Alpaca and ShareGPT. It represents an optimized version of the Evol-Instruct data used to train the WizardLM family of models. To assemble the complete instruction set of roughly 196K samples, users must merge this release with the original unfiltered ShareGPT dataset. The refined examples cover diverse conversational and instructional scenarios, facilitating improved alignment and performance in downstream open-source large language models, including structured prompts and responses.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide the comparision of 3/4/5-grams for all datasets (except <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>, which is displayed in the main paper) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F6\" title=\"Figure 6 &#8227; F.1 Token-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, and the top-5 <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A6.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams comparison across datasets in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F7\" title=\"Figure 7 &#8227; F.1 Token-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams phrases of some datasets include abnormal content (e.g. &#8220;<span class=\"ltx_text ltx_font_italic\">identify which instrument be string</span>&#8221; in <span class=\"ltx_text ltx_font_sansserif\">dolly-15</span> and &#8220;<span class=\"ltx_text ltx_font_italic\">The quick brown fox jumps over the lazy dog</span>&#8221; in <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>), which indicates that there is a lot of repetition in the input content of the template tasks or some instructions used to construct the dataset, which may affect the balance of the dataset.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These additional data further support our conclusions.\n<span class=\"ltx_text ltx_font_bold\">(1)</span> The <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> dataset, which consists of professionally crafted medical prompts, exhibits a relatively high proportion of numerical modifiers (nummod, 0.0276) and passive auxiliaries (auxpass, 0.0101) in dependency analysis, as well as a notably high usage of numerals (NUM, 0.0309) in POS tagging. These features reflect a terminology-dense and precision-oriented language style that emphasizes processes and outcomes rather than agents.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> In the <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> dataset, the verb-noun pairs reflect language commonly used in business contexts, such as <span class=\"ltx_text ltx_font_italic\">&#8220;create plan&#8221;</span> and <span class=\"ltx_text ltx_font_italic\">&#8220;create strategy&#8221;</span>. In contrast, the verb-noun pairs observed in <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> suggest more generic and broadly applicable usage scenarios.</p>\n\n",
                "matched_terms": [
                    "dependency",
                    "11kbusiness",
                    "selfinstruct",
                    "medicalo1",
                    "nummod",
                    "boredhumans",
                    "auxpass",
                    "oasst1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Anomalously, in the <span class=\"ltx_text ltx_font_sansserif\">dolly-15k</span> dataset, the most frequent verb-noun pairs exhibit a skewed distribution, with the highest-frequency nouns overwhelmingly associated with only the top one or two verbs. Moreover, these frequent verb-noun pairs often lack clear task-specific semantics&#8212;for example, <span class=\"ltx_text ltx_font_italic\">&#8220;tell i&#8221;</span>, <span class=\"ltx_text ltx_font_italic\">&#8220;give list&#8221;</span>, and <span class=\"ltx_text ltx_font_italic\">&#8220;classify each&#8221;</span>. This pattern may be attributed to the manual generation process, which is susceptible to the individual linguistic habits of annotators.</p>\n\n",
                "matched_terms": [
                    "dolly15k",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show the distribution of sampled embedding points after PCA for all datasets (except for <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>, which are shown in the main paper) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F9\" title=\"Figure 9 &#8227; F.3 Semantic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "medicalo1",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can still observe from the results that datasets with more concentrated topical focus (e.g., <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span>) exhibit clear clustering patterns, whereas those with broader thematic coverage (e.g., <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>) display a more dispersed distribution of data points.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "11kbusiness"
                ]
            }
        ]
    },
    "A6.T6": {
        "source_file": "Large Language Model Prompt Datasets: An In-depth Analysis and Insights",
        "caption": "Table 6: All detected Parts-of-Speech Tags, with each value indicating its proportion in a dataset. - means the POS tag not detected in the dataset.",
        "body": "POS\n1.1k-business\nBoredHumans\ndolly-15k\nmedical-o1\nOASST1\nSelf-Instruct\nShareGPT\n\n\n\n\nNOUN\n0.2637\n0.2103\n0.1899\n0.259\n0.1946\n0.2027\n0.1944\n\n\nPUNCT\n0.1094\n0.1942\n0.1435\n0.1158\n0.1231\n0.1839\n0.145\n\n\nVERB\n0.1302\n0.1094\n0.0871\n0.0775\n0.1069\n0.0999\n0.0979\n\n\nADP\n0.0758\n0.0678\n0.0858\n0.0998\n0.0851\n0.0701\n0.0789\n\n\nDET\n0.0506\n0.0693\n0.0949\n0.0893\n0.0839\n0.0844\n0.0696\n\n\nPRON\n0.0912\n0.0708\n0.0695\n0.0369\n0.0869\n0.0701\n0.0583\n\n\nADJ\n0.0588\n0.0543\n0.0538\n0.1104\n0.0632\n0.0498\n0.0563\n\n\nPROPN\n0.0219\n0.0372\n0.1272\n0.0515\n0.0471\n0.0294\n0.0703\n\n\nAUX\n0.0458\n0.0379\n0.0608\n0.0382\n0.0644\n0.0453\n0.0423\n\n\nCCONJ\n0.0399\n0.0294\n0.0209\n0.0291\n0.0288\n0.0204\n0.0286\n\n\nSPACE\n-\n0.0267\n0.0053\n0.0175\n0.019\n0.0504\n0.0517\n\n\nPART\n0.0358\n0.0223\n0.013\n0.01\n0.0222\n0.0172\n0.0213\n\n\nNUM\n0.0041\n0.0146\n0.014\n0.0309\n0.0153\n0.0282\n0.0273\n\n\nADV\n0.0097\n0.0259\n0.0107\n0.0209\n0.0238\n0.0153\n0.0247\n\n\nSCONJ\n0.0199\n0.0105\n0.0198\n0.007\n0.0242\n0.0197\n0.0152\n\n\nX\n0.035\n0.0128\n0.0015\n0.0005\n0.0044\n0.0075\n0.0082\n\n\nSYM\n0.008\n0.0037\n0.0008\n0.0049\n0.0035\n0.0031\n0.0073\n\n\nINTJ\n0.0002\n0.003\n0.0012\n0.001\n0.0037\n0.0027\n0.0028",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">POS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">1.1k-business</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BoredHumans</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">dolly-15k</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">medical-o1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">OASST1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Self-Instruct</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ShareGPT</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">NOUN</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.2637</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.2103</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.1899</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.259</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1946</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.2027</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1944</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">PUNCT</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.1094</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.1942</span></td>\n<td class=\"ltx_td ltx_align_center\">0.1435</td>\n<td class=\"ltx_td ltx_align_center\">0.1158</td>\n<td class=\"ltx_td ltx_align_center\">0.1231</td>\n<td class=\"ltx_td ltx_align_center\">0.1839</td>\n<td class=\"ltx_td ltx_align_center\">0.145</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">VERB</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.1302</span></td>\n<td class=\"ltx_td ltx_align_center\">0.1094</td>\n<td class=\"ltx_td ltx_align_center\">0.0871</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0775</span></td>\n<td class=\"ltx_td ltx_align_center\">0.1069</td>\n<td class=\"ltx_td ltx_align_center\">0.0999</td>\n<td class=\"ltx_td ltx_align_center\">0.0979</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">ADP</th>\n<td class=\"ltx_td ltx_align_center\">0.0758</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0678</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0858</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0998</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0851</td>\n<td class=\"ltx_td ltx_align_center\">0.0701</td>\n<td class=\"ltx_td ltx_align_center\">0.0789</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">DET</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0506</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0693</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0949</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0893</td>\n<td class=\"ltx_td ltx_align_center\">0.0839</td>\n<td class=\"ltx_td ltx_align_center\">0.0844</td>\n<td class=\"ltx_td ltx_align_center\">0.0696</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">PRON</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0912</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0708</td>\n<td class=\"ltx_td ltx_align_center\">0.0695</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0369</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0869</td>\n<td class=\"ltx_td ltx_align_center\">0.0701</td>\n<td class=\"ltx_td ltx_align_center\">0.0583</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">ADJ</th>\n<td class=\"ltx_td ltx_align_center\">0.0588</td>\n<td class=\"ltx_td ltx_align_center\">0.0543</td>\n<td class=\"ltx_td ltx_align_center\">0.0538</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.1104</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0632</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0498</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0563</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">PROPN</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0219</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0372</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.1272</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0515</td>\n<td class=\"ltx_td ltx_align_center\">0.0471</td>\n<td class=\"ltx_td ltx_align_center\">0.0294</td>\n<td class=\"ltx_td ltx_align_center\">0.0703</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">AUX</th>\n<td class=\"ltx_td ltx_align_center\">0.0458</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0379</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0608</td>\n<td class=\"ltx_td ltx_align_center\">0.0382</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0644</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0453</td>\n<td class=\"ltx_td ltx_align_center\">0.0423</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CCONJ</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0399</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0294</td>\n<td class=\"ltx_td ltx_align_center\">0.0209</td>\n<td class=\"ltx_td ltx_align_center\">0.0291</td>\n<td class=\"ltx_td ltx_align_center\">0.0288</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0204</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0286</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SPACE</th>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">0.0267</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0053</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0175</td>\n<td class=\"ltx_td ltx_align_center\">0.019</td>\n<td class=\"ltx_td ltx_align_center\">0.0504</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0517</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">PART</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0358</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0223</td>\n<td class=\"ltx_td ltx_align_center\">0.013</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.01</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0222</td>\n<td class=\"ltx_td ltx_align_center\">0.0172</td>\n<td class=\"ltx_td ltx_align_center\">0.0213</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">NUM</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0041</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0146</td>\n<td class=\"ltx_td ltx_align_center\">0.014</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0309</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0153</td>\n<td class=\"ltx_td ltx_align_center\">0.0282</td>\n<td class=\"ltx_td ltx_align_center\">0.0273</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">ADV</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0097</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0259</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0107</td>\n<td class=\"ltx_td ltx_align_center\">0.0209</td>\n<td class=\"ltx_td ltx_align_center\">0.0238</td>\n<td class=\"ltx_td ltx_align_center\">0.0153</td>\n<td class=\"ltx_td ltx_align_center\">0.0247</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SCONJ</th>\n<td class=\"ltx_td ltx_align_center\">0.0199</td>\n<td class=\"ltx_td ltx_align_center\">0.0105</td>\n<td class=\"ltx_td ltx_align_center\">0.0198</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.007</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0242</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0197</td>\n<td class=\"ltx_td ltx_align_center\">0.0152</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">X</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.035</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0128</td>\n<td class=\"ltx_td ltx_align_center\">0.0015</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0005</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0044</td>\n<td class=\"ltx_td ltx_align_center\">0.0075</td>\n<td class=\"ltx_td ltx_align_center\">0.0082</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SYM</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.008</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0037</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0008</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0049</td>\n<td class=\"ltx_td ltx_align_center\">0.0035</td>\n<td class=\"ltx_td ltx_align_center\">0.0031</td>\n<td class=\"ltx_td ltx_align_center\">0.0073</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">INTJ</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">0.0002</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.003</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0012</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.001</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.0037</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0027</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0028</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "11kbusiness",
            "selfinstruct",
            "medicalo1",
            "num",
            "oasst1",
            "partsofspeech",
            "intj",
            "space",
            "value",
            "adp",
            "det",
            "sharegpt",
            "noun",
            "verb",
            "each",
            "part",
            "propn",
            "means",
            "tag",
            "tags",
            "pron",
            "punct",
            "adv",
            "aux",
            "detected",
            "dataset",
            "indicating",
            "sym",
            "its",
            "all",
            "sconj",
            "pos",
            "proportion",
            "adj",
            "boredhumans",
            "dolly15k",
            "cconj",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In this section, we present the complete experimental data for all identified dependency types, along with their proportions in the datasets, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.T5\" title=\"Table 5 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Additionally, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.T6\" title=\"Table 6 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> lists all detected Part-of-Speech tags and their corresponding proportions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F8\" title=\"Figure 8 &#8227; F.2 Syntactic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> further illustrates the ten most common verbs and their top five direct noun objects found in the prompt datasets except <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>, which are shown in the main paper.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data discovery process.</span> We collect publicly available datasets from the following four types of sources. <span class=\"ltx_text ltx_font_bold\">First</span>, we consult <span class=\"ltx_text ltx_font_italic\">dataset collection platforms</span>, including Hugging Face Datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib12\" title=\"\">hug, </a>)</cite>, Kaggle <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib13\" title=\"\">kag, </a>)</cite>, Google Dataset Search <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib11\" title=\"\">goo, </a>)</cite>, and Papers with Code <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib14\" title=\"\">pap, </a>)</cite>. Targeted searches using keywords, e.g., \"prompt dataset\", \"instruction-following dataset\", and \"conversation dataset\" yield 60 prompt datasets. <span class=\"ltx_text ltx_font_bold\">Second</span>, we review the latest <span class=\"ltx_text ltx_font_italic\">academic publications</span>, specifically papers on prompt engineering, natural language understanding, and dialogue systems, published at NeurIPS, ICLR, and ICML between 2023-2024 and identify 73 datasets shared across them. <span class=\"ltx_text ltx_font_bold\">Third</span>, we also examine <span class=\"ltx_text ltx_font_italic\">public repositories</span> by systematically surveying open-source GitHub projects using keywords, e.g., &#8220;prompt collection&#8221;, &#8220;LLM prompts&#8221;, and &#8220;instruction dataset&#8221;. We identify 21 prompt repositories that typically contain curated prompt lists derived from user interactions or synthesized from public APIs. Some of these repositories are &#8220;awesome-lists&#8221;, which are curated collections of high-quality prompts or links to prompt datasets.\nNotable examples include <span class=\"ltx_text ltx_font_sansserif\">Awesome Instruction Datasets</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib46\" title=\"\">Nie, </a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering Guide</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Saravia, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib56\" title=\"\">2022</a>)</cite>, and <span class=\"ltx_text ltx_font_sansserif\">LLMDataHub</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib71\" title=\"\">Zhao, </a>)</cite>. <span class=\"ltx_text ltx_font_bold\">Finally</span>, we extract 14 datasets from <span class=\"ltx_text ltx_font_italic\">popular websites dedicated to prompt-sharing</span>, including <span class=\"ltx_text ltx_font_sansserif\">Prompt Genius</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Pro, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib7\" title=\"\">b</a>)</cite> and <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib10\" title=\"\">bor, </a>)</cite>. These platforms feature user-written prompts for practical purposes.</p>\n\n",
                "matched_terms": [
                    "boredhumans",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data filtering.</span>\nWe remove duplicate entries (e.g., CVQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Romero et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib51\" title=\"\">2024</a>)</cite> appears in both Hugging Face and NeurIPS 2024) and then filter the remaining candidates using four quality criteria for inclusion in this paper.\n<span class=\"ltx_text ltx_font_bold\">First</span>, <span class=\"ltx_text ltx_font_italic\">Dataset size.</span> We prioritize datasets containing at least 1K prompts to ensure robustness in diversity and statistical power. In contrast, due to their generally limited scope, user-shared datasets are filtered with a minimum threshold of 50 prompts.\n<span class=\"ltx_text ltx_font_bold\">Second</span>, <span class=\"ltx_text ltx_font_italic\">Data quality.</span> We evaluate the quality of prompts based on their cleanliness. Most datasets (e.g., <span class=\"ltx_text ltx_font_sansserif\">OpenCodeReasoning</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib16\" title=\"\">2025b</a>)</cite>) on data hosting platforms (e.g., Hugging Face and Kaggle) are well-formatted and clean. For the remaining data, we exclude samples with inconsistent formatting or unclear structure. For instance, the <span class=\"ltx_text ltx_font_sansserif\">Prompt Engineering Guide</span>&#8211;a resource that offers both curated prompt datasets and instructional examples&#8211;contains many illustrative prompts scattered throughout the material and are thus omitted from our datasets.\n<span class=\"ltx_text ltx_font_bold\">Third</span>, <span class=\"ltx_text ltx_font_italic\">Data relevance.</span> We assess whether the prompts are aligned with our data discovery guidelines, specifically emphasizing on those that represent common usage scenarios for broad audiences (e.g., <span class=\"ltx_text ltx_font_sansserif\">Chinese-DeepSeek-R1-Distill-data-110k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib38\" title=\"\">2025a</a>)</cite>), and tasks from various domains (e.g., <span class=\"ltx_text ltx_font_sansserif\">Medical Verifiable Problems</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">OpenMathReasoning</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Moshkov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib44\" title=\"\">2025</a>)</cite>). Datasets that violate our discovery guidelines are omitted. For instance, the <span class=\"ltx_text ltx_font_sansserif\">PersonaHub</span> dataset <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib5\" title=\"\">Per, </a>)</cite> is excluded because it does not meet <span class=\"ltx_text ltx_font_bold\">data discovery guideline (1)</span>, which mandates that prompts be linked to specific, well-defined tasks. Although <span class=\"ltx_text ltx_font_sansserif\">PersonaHub</span> demonstrates the potential of synthetic personas in generating diverse content (e.g., reasoning problems, dialogues, or non-player character behaviors), it predominantly comprises persona descriptions without clear task formulation.\n<span class=\"ltx_text ltx_font_bold\">Fourth</span>, <span class=\"ltx_text ltx_font_italic\">Accessibility.</span> Datasets must be publicly accessible or retrievable via automated crawling, and their licensing terms must permit research use. After filtering, we identify 129 distinct prompt datasets for taxonomic analysis (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S4\" title=\"4 Dataset Taxonomy &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generation process</span> describes how the prompts are created. <span class=\"ltx_text ltx_font_italic\">Human-generated prompts</span> are either manually authored (e.g., <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>)</cite>) or collected from user queries (e.g., <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib36\" title=\"\">Li, </a>)</cite>), <span class=\"ltx_text ltx_font_italic\">Model-generated prompts</span> include those created via self-instruct techniques <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib65\" title=\"\">2023</a>)</cite> (e.g., <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>), multi-agent simulations (e.g., <span class=\"ltx_text ltx_font_sansserif\">AI Society</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib34\" title=\"\">2023a</a>)</cite>), or reverse instruction generation (e.g., <span class=\"ltx_text ltx_font_sansserif\">LongForm</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;ksal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib32\" title=\"\">2023</a>)</cite>). Finally, <span class=\"ltx_text ltx_font_italic\">derivative datasets</span> build on existing resources through task expansion or reformatted aggregation (e.g., <span class=\"ltx_text ltx_font_sansserif\">Flan 2022</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib4\" title=\"\">Fla, </a>)</cite>, <span class=\"ltx_text ltx_font_sansserif\">xP3</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Muennighoff et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib45\" title=\"\">2022</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "sharegpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following our selection principles, we curated seven representative datasets spanning different user types, instruction methods, and domains. For <span class=\"ltx_text ltx_font_bold\">end users</span>, general-domain prompts include single-turn prompts (<span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span>) and multi-turn conversations (<span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>), while business-domain single-turn prompts are represented by <span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span>. For <span class=\"ltx_text ltx_font_bold\">LLM researchers</span>, human-generated datasets include <span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> and <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and model-generated prompts are captured by <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>. For <span class=\"ltx_text ltx_font_bold\">domain scientists</span>, we include model-generated medical prompts from <span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span>. This collection ensures diversity in publisher type, prompt structure, and application domain.</p>\n\n",
                "matched_terms": [
                    "boredhumans",
                    "selfinstruct",
                    "sharegpt",
                    "oasst1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">1100+ ChatGPT Prompts for Business</span> (<span class=\"ltx_text ltx_font_bold\">1.1k-business</span>). A curated dataset of 1&#8201;235 prompts oriented toward professional and business-related use cases, such as marketing, productivity, and decision-making. It represents structured, domain-specific prompting behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(1., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib1\" title=\"\">1</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">databricks-dolly-15k</span> (<span class=\"ltx_text ltx_font_bold\">dolly-15k</span>).\nThis dataset includes 15&#8201;000 human-authored instruction&#8211;response pairs covering a range of everyday tasks. It is single-turn and domain-general, curated to support instruction-following models <cite class=\"ltx_cite ltx_citemacro_citep\">(Conover et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib25\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dolly15k",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">medical-o1-reasoning-SFT</span> (<span class=\"ltx_text ltx_font_bold\">medical-o1</span>).\nSynthetic data of 90&#8201;120 open-ended questions and GPT-4o generated CoTs and responses. Open-ended questions are reformatted by GPT-4o based on close-set medical examination questions. The dataset is used to fine-tune HuatuoGPT-o1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib22\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "medicalo1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">OASST1.</span>\nThe Open Assistant dataset (OASST1) contains over 30&#8201;000 human-written messages arranged in dialogue trees. It emphasizes cooperative, open-domain assistant behavior and includes branching conversations rather than linear interactions <cite class=\"ltx_cite ltx_citemacro_citep\">(K&#246;pf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib33\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "oasst1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_sansserif\">Self-Instruct.</span>\nA synthetic dataset with 82&#8201;646 prompts generated by large language models based on a small seed pool of human-written instructions. For every generation step, it samples 6 human-written tasks and 2 model-generated tasks in previous steps to promote diversity <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib65\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> High-frequency <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams reveal domain and prompt-engineering differences, such as role-playing cues in <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> (&#8220;you to act as&#8221;) versus medical reasoning in <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> (&#8220;what be the,&#8221; &#8220;the most likely diagnosis&#8221;). <span class=\"ltx_text ltx_font_bold\">(2)</span> While 3-grams capture general-purpose queries or commands (e.g., &#8220;what be the,&#8221; &#8220;I want to&#8221;), longer <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams (4&#8211;5) reflect task-specific patterns, as in <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> where frequent 5-grams (&#8220;please write in English language,&#8221; &#8220;write a comprehensive reply to&#8221;) highlight its instruction-following orientation. <span class=\"ltx_text ltx_font_bold\">(3)</span> Compared to Google Books 5-grams (e.g., &#8220;at the end of the,&#8221; &#8220;in whole or in part&#8221;) that serve narrative or descriptive purposes, prompt datasets exhibit inquiry- or command-focused <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams, underscoring a clear divergence in linguistic patterns across corpora.</p>\n\n",
                "matched_terms": [
                    "oasst1",
                    "sharegpt",
                    "medicalo1",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply the spaCy <span class=\"ltx_text ltx_font_typewriter\">en_core_web_sm</span> parser <cite class=\"ltx_cite ltx_citemacro_citep\">(Honnibal &amp; Montani, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib29\" title=\"\">2017</a>)</cite> to extract syntactic dependencies and determine the frequency of key grammatical relations in each dataset. For the EWT and ParTUT corpora, we rely on officially published dependency type annotations. This analysis reveals systematic variations in linguistic style across prompt sources. Additionally, we track verb&#8211;object (dobj) pairs to capture the task-oriented diversity of the prompts (see Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F3\" title=\"Figure 3 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">(1)</span> The <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> dataset is characterized by its high use of adjectival modifiers (amod, 0.11) and low direct object frequency (dobj, 0.03), reflecting a preference for precise, state-oriented descriptions over action-driven narratives, often framed through linking verbs&#8212;typical of medical contexts detailing conditions, symptoms, and diagnoses. <span class=\"ltx_text ltx_font_bold\">(2)</span> In contrast, the <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> dataset favors concise, goal-driven imperatives with bare noun phrases as direct objects (dobj, 0.09) and minimal use of determiners (det, 0.05), aligning with its project-planning focus. <span class=\"ltx_text ltx_font_bold\">(3)</span> Verb&#8211;noun dependency analysis further distinguishes domains: medical instructions cluster around technical, domain-specific pairs like &#8220;have history&#8221; and &#8220;experience pain,&#8221; while datasets such as <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> use broader, generic pairs like &#8220;write answer&#8221; and &#8220;use code&#8221;. These syntactic patterns highlight each corpus&#8217; thematic priorities and inform strategies for domain-aware model training.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "its",
                    "medicalo1",
                    "det",
                    "sharegpt",
                    "noun",
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We annotate the datasets with POS tags and calculate the distribution of nouns, verbs, adjectives, and adverbs. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T3\" title=\"Table 3 &#8227; 5.3.1 Dependency Parsing &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> summarizes the functional composition of prompts, contrasting content and function words. For example, a high verb frequency indicates action-oriented prompts, while a predominance of nouns suggests more objective narratives. These distributional differences reveal stylistic and structural variations across sources.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "tags",
                    "verb"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1)</span> Domain-specific datasets such as <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> and <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> exhibit a noun proportion of <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS2.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 0.26, surpassing that found in formal corpora like ParTUT. This reflects a concept-driven focus on domain entities and technical terms.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> Additionally, <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> also registers an unusually high adjective ratio (0.11), indicating a repeated emphasis on specifying medical attributes and conditions, consistent with the descriptive nature of clinical reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "medicalo1",
                    "proportion",
                    "noun",
                    "indicating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze lexical patterns across prompt datasets using TF-IDF. Each dataset&#8217;s prompts are concatenated into a single document (yielding seven corpus-level documents), and a TF-IDF vectorizer (with a 5000-word limit and English stopwords removed) computes sparse term importance representations. We then assess <span class=\"ltx_text ltx_font_bold\">inter-dataset lexical similarity</span> via pairwise cosine similarity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F3.sf3\" title=\"In Figure 3 &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">3(c)</span></a>) and extract the top three highest-weight tokens per dataset for <span class=\"ltx_text ltx_font_bold\">intra-dataset characterization</span> (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.T4\" title=\"Table 4 &#8227; 5.3.2 Part-of-Speech Tagging &#8227; 5.3 Syntactic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1) Intra-dataset analysis</span>\ndelineates each dataset&#8217;s lexical focus and stylistic characteristics. For instance, <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> emphasizes business-specific terms like &#8220;content&#8221; and &#8220;email&#8221;, while <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span> features imperatives such as &#8220;act&#8221;, indicative of role-playing instructions. Similarly, <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> shows a dominant TF-IDF score for &#8220;output&#8221; (0.772), highlighting a structural prompt style based on explicit instruction&#8211;response formats.\n<span class=\"ltx_text ltx_font_bold\">(2) Inter-dataset comparison.</span>\nTF-IDF vectors show varying overlaps across datasets. The highest cosine similarity between <span class=\"ltx_text ltx_font_sansserif\">OASST1</span> and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> suggests a similar vocabulary&#8212;likely due to shared human-generation processes. In contrast, <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> is lexically distant from the others, especially <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> and <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span>, reflecting stylistic and domain-specific differences.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "selfinstruct",
                    "medicalo1",
                    "boredhumans",
                    "sharegpt",
                    "oasst1",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze prompt semantics by encoding each prompt into a 384-dimensional dense vector using Sentence-BERT&#8217;s pretrained model <span class=\"ltx_text ltx_font_typewriter\">all-MiniLM-L6-v2</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Reimers &amp; Gurevych, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#bib.bib50\" title=\"\">2019</a>)</cite>. Each prompt is encoded into a 384-dimensional dense vector that captures its semantic content. These embeddings serve as the foundation for classification, clustering, and visualization analysis. We perform Principal Component Analysis (PCA) to reduce sentence embeddings to two dimensions. For fair comparison, we uniformly at random sample 500 prompts per dataset and visualize their distribution (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Semantic-level Analysis &#8227; 5 Prompt Data Analysis &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).</p>\n\n",
                "matched_terms": [
                    "each",
                    "its",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Analysis of results.</span>\n<span class=\"ltx_text ltx_font_bold\">(1) Wide coverage in Self-Instruct:</span> The <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> dataset exhibits the most dispersed and evenly distributed semantic space, suggesting a broad topical coverage. This aligns with the self-instruction paradigm&#8217;s goal of generating diverse instruction types.\n<span class=\"ltx_text ltx_font_bold\">(2) Semantic cohesion in specific domains:</span> Prompts from <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> form more concentrated clusters, indicating domain-specific semantic cohesion.\n<span class=\"ltx_text ltx_font_bold\">(3) Overlap among human-generated sets:</span> The embeddings of <span class=\"ltx_text ltx_font_sansserif\">dolly-15k</span>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span> overlap substantially across both PCA dimensions. This suggests that these datasets share stylistic and semantic characteristics, possibly due to their common reliance on human-LLM interactions for data generation.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "selfinstruct",
                    "dolly15k",
                    "medicalo1",
                    "oasst1",
                    "sharegpt",
                    "dataset",
                    "space",
                    "indicating"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the above analysis, we propose a new prompt engineering method that leverages structural linguistic features. Specifically, we take the average of the high-dimensional embeddings of POS tags and dependency relations from the analyzed dataset to define a centroid representation. This centroid captures the &#8220;central&#8221; syntactic patterns that are associated with higher-performing prompts.</p>\n\n",
                "matched_terms": [
                    "pos",
                    "dataset",
                    "tags"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each target prompt, we first analyze its POS and dependency embeddings to identify deviations from the centroid. Based on this analysis, a modification plan is generated, specifying how the prompt&#8217;s syntactic structure should be adjusted. The LLM is then guided to rewrite the prompt according to this plan, producing an optimized prompt whose embeddings are closer to the centroid. This process allows peripheral prompts that initially deviate from effective syntactic patterns to be systematically aligned with the central region of the embedding space.</p>\n\n",
                "matched_terms": [
                    "each",
                    "its",
                    "space",
                    "pos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We are committed to ensuring the reproducibility of our results. All code used in this research is publicly available through links in our abstract. The repository includes detailed instructions for dataset preprocessing, and running experiments. We also specify the exact versions of dependencies and libraries used in our experiments. All datasets employed in this study are either publicly accessible or their sources are clearly documented. Random seeds are set for all experiments where applicable to minimize variability. Together, these resources enable researchers to reproduce our analyses and results with minimal effort.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset comprises over 1,000 curated ChatGPT prompt templates in Notion Workspace format, spanning diverse domains such as AI, marketing, education, healthcare, and code generation. Each entry typically includes a prompt, an automatic prompt (system prompt like), and a concise description.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Academic Reasoning and Intuition Chains dataset comprises 1,975 examples of chain-of-thought reasoning distilled from open-access arXiv papers across eight scientific domains, including Biology, Economics, Physics, Mathematics, Computer Science, Finance, Statistics, and Electrical Engineering. Each entry contains comprehensive metadata (arxiv_id, DOI, authors, dates, and categories), interactive model-generated conversations with explicit &lt;think&gt; tags, extensive chain length statistics, and multi-model verifier results with suitability scores. Licensed under Apache-2.0, this resource enables training and evaluation of budgeted chain-of-thought reasoning models with rigorous quality control.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "tags"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset features thousands of prompts generated by the teknium/OpenHermes-2p5-Mistral-7B model, each designed to elicit diverse and contextually rich responses. Stored as JSON objects, it enables research in synthetic prompt generation, model creativity evaluation, and downstream fine-tuning.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The Stanford Alpaca dataset comprises 52K high-quality, instruction-following examples generated via a modified Self-Instruct pipeline using text-davinci-003. Designed for fine-tuning LLaMA models, it enables research in alignment, instruction tuning, and synthetic data generation.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Alpaca_GPT4_data_zh is a Chinese instruction-tuning dataset curated by the Instruction Tuning with GPT-4 project. It comprises 48,818 examples, each featuring an instruction, optional input context, and a GPT-4-generated response, facilitating text-generation and fine-tuning tasks. The dataset occupies 32 MB and is available under a CC-BY-4.0 license for non-commercial research.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: AM-DeepSeek-Distilled-40M is a multilingual (zh/en) reasoning dataset comprising 3.34 million prompts paired with 40 million model-generated responses across code, math, science, instruction-following and general reasoning. Each query includes four samples from three models (1.5B, 7B, and R1), with pass rates computed per model to assign unbiased difficulty scores. Released under CC-BY-NC 4.0, its unified JSONL format supports supervised fine-tuning, preference learning and reinforcement learning applications, enabling selection of subsets by category or difficulty level. It fosters robust LLM development research.</p>\n\n",
                "matched_terms": [
                    "each",
                    "its",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The APIGen-MT-5k dataset comprises 5000 realistic, high-quality, multi-turn function-calling dialogues generated by APIGen-MT, a scalable automated agentic pipeline simulating agent-human interactions. Covering retail and airline domains, each trajectory is verified through format checks, function executions, and semantic validations, achieving a 99% success rate in human evaluation. Provided in ShareGPT-style JSON and licensed under CC-BY-NC-4.0, it supports question-answering, text generation, and reinforcement learning benchmarks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Aya Collection is a massive multilingual instruction tuning dataset comprising over 513 million prompt-completion pairs across 115 languages. It integrates three sources: human-crafted instruction templates created by fluent speakers for diverse tasks, machine translations of 19 top-tier datasets into 101 languages via NLLB, and the human-annotated Aya Dataset subset of 204K examples. Split by dataset, each record includes id, inputs, targets, language, script, and task type. Licensed under Apache-2.0, it supports academic and commercial classification, summarization, translation, and QA research.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Bactrian-X is a multilingual instruction-following dataset containing 3.4 million instruction-input-response triplets across 52 languages. It builds upon 67K unique English prompts drawn from Alpaca and Dolly, automatically translated via Google Translate into 51 languages. For each translated prompt (and optional input), GPT-3.5-Turbo generates a corresponding response, yielding 3.4 million examples. Each record includes an id, instruction, optional input, and model-generated output. Released under CC-BY-NC 4.0, Bactrian-X supports text-generation research, fine-tuning, and evaluation in low-resource and high-resource language settings, covering diverse tasks and domains.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Baize Chat Data is an instruction-finetuning corpus combining four sources: Alpaca, Medical, Quora, and StackOverflow. It contains about 210,000 conversational examples, each formatted with [|Human|] prompts and [|AI|] responses. Designed to enhance the Baize family of language models, this unified dataset supports interactive text generation and dialogue training. Sourced from the Baize GitHub repository, it provides diverse conversational scenarios ranging from general queries to specialized medical and technical discussions. It is optimized for instruction-following tasks. It enables realistic user interactions.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BELLE_Generated_Chat contains approx. 400k personalized Chinese character dialogues generated by the BELLE project. Each record includes an instruction, an (empty) input, and a generated output. Created by ChatGPT and not strictly verified, the dataset may contain factual inaccuracies. Licensed under GPL-3.0 for research use only. With around 0.4 million entries, it supports text-to-text generation and conversational modeling.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BELLE_Multiturn_Chat is a Chinese multi-turn conversational dataset comprising approximately 0.8 million human-assistant dialogues generated by the BELLE project using ChatGPT. Each record pairs an instruction containing prior context labeled with &#8220;Human:&#8221; and &#8220;Assistant:&#8221; with the assistant&#8217;s subsequent reply. Intended for text-to-text generation tasks, the GPL-3.0-licensed collection covers only Chinese interactions. As this data is automatically generated and unverified, factual errors and inconsistencies may arise. It is provided strictly for non-commercial research under the project&#8217;s usage restrictions; developers should validate outputs and adhere to licensing terms.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The BELLE_train_3.5M_CN dataset comprises approximately 3.5 million monolingual Chinese instruction-response pairs generated by the BELLE project, formatted as multi-turn and single-turn dialogues with unique IDs. It includes human-assistant exchanges across 13 instruction categories. Licensed under GPL-3.0, it supports text-to-text generation research exclusively; commercial or harmful use is prohibited. The JSON records each conversation&#8217;s ID and bilingual content.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: BoredHumans is a diverse and extensive prompt dataset compiled from multiple sources, including Awesome ChatGPT Prompts, Data Science Prompts, and Tree-of-Thought Prompting, among others. Its rich variety covers numerous domains and prompt styles, enabling comprehensive research on prompt engineering, AI model behavior, and in-context learning strategies.</p>\n\n",
                "matched_terms": [
                    "boredhumans",
                    "its",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The ChatGPT Prompts dataset offers a broad collection of prompts covering diverse topics, designed for use with GPT 3.5. Its value lies in providing versatile, real-world prompt examples that support research on prompt engineering and AI interaction across various domains.</p>\n\n",
                "matched_terms": [
                    "value",
                    "its",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Chinese-DeepSeek-R1-Distill-data-110k is a 110K-entry Chinese dataset distilled from DeepSeek-R1, supporting text generation, text2text generation, and question answering under Apache-2.0. It covers four domains: Math (36 568 samples), Exam (2 432), STEM (12 648) and General (58 352). Each record includes input, reasoning content, output, source repo name and model-assigned score. Data originate from diverse math and instruction corpora, distilled via R1 with temperature 0.6, step-by-step math prompts, and validation using Math-Verify and Qwen2.5-72B.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Licensed under Apache-2.0, Chinese-DeepSeek-R1-Distill-data-110k-SFT is an open-source, Chinese-language instruction-tuning dataset distilled from DeepSeek-R1 outputs, formatted for direct supervised fine-tuning. It comprises 110K examples spanning math (36.6K), exam questions (2.4K), STEM (12.6K), and diverse general prompts (58.4K). Prompts are sourced from multiple Chinese math and STEM repositories, with distillation performed at temperature 0.6 and special step-by-step cues for calculations. Each sample includes integrated reasoning, answers, and model-based scores, facilitating reproducibility of high-performance SFT training. It supports text-generation, text-to-text generation, and question-answering tasks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: COIG-CQIA (Chinese Open Instruction Generalist - Quality is All You Need) is a high-quality, open-source Chinese instruction tuning dataset designed to align language models with human interactive behavior. It aggregates over 45,000 manually cleansed, restructured, and reviewed examples spanning social media dialogs, encyclopedic articles, exam questions, finance, medical, legal, traditional culture, and NLP tasks. Each entry includes instruction, optional input, output, task type, domain, and human verification metadata. COIG-CQIA aims to facilitate instruction fine-tuning for Chinese NLP research and applications.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: CVQA is a culturally diverse, multilingual visual question-answering benchmark featuring over 10,000 image-based questions across 39 country-language pairs. Each sample includes a locally posed query, its English translation, four answer options in both languages, and metadata such as image source, license, category, and a unique ID. Questions span ten thematic categories and images originate from self-contributed and external sources under various licenses. Designed primarily as a test set, CVQA facilitates evaluation of VQA models on nuanced, culturally contextualized visual understanding.</p>\n\n",
                "matched_terms": [
                    "each",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: DeepMath-103K is a large-scale, MIT-licensed dataset comprising 103K challenging mathematical problems tailored for text-to-text and text-generation tasks. Each example includes a problem statement, a hierarchically classified topic, a numerical difficulty score, three distinct reasoning pathways (R1 solutions), and a verifiable final answer. Designed to support reinforcement learning and supervised fine-tuning, it enables difficulty-aware training, topic-specific evaluation, and robust rule-based reward shaping. Sourced and decontaminated to minimize test leakage, DeepMath-103K drives advances in automated mathematical reasoning research and diverse research areas.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Dynosaur introduces a dynamic and low-cost paradigm for curating instruction-tuning datasets. It automatically generates diverse instructions by leveraging metadata from HuggingFace datasets, combined with LLM-based instruction synthesis (e.g., via ChatGPT). The result is Dynosaur-full, a large-scale dataset (800K+ samples, generated at &#160; $11.5) that supports dynamic growth and general-purpose instruction-tuning. Empirically, models fine-tuned on Dynosaur outperform Alpaca and GPT-4-Instruct baselines on Super-NI. The project includes: metadata crawling tools, instruction generation pipelines, and fine-tuned T5-3B and LLaMA-7B models. All generated instructions are under Apache 2.0, with task data adhering to original dataset licenses.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Firefly is a Chinese instruction-tuning dataset comprising 1.15 million high-quality examples drawn from 23 common Chinese natural language processing datasets. Each example includes a task type, an input prompt, and a target output, ensuring diverse coverage. Data templates were manually designed for each task to ensure quality and richness. Token length analysis shows that most examples are under 600 tokens. Firefly was used to train the Firefly-1b4 Chinese dialogue LLM, available on GitHub and Hugging Face, fostering reproducibility, community collaboration.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: This dataset aggregates tasks from Flan, T0, Super-Natural Instructions, Chain-of-Thought, and Dialog into a training split. Each task is provided in zero-/few-shot and option/no-option formats as JSONL entries including inputs, targets, and task identifiers. Released under Apache-2.0, it includes scripts for building dependencies, fixing version mismatches, and exporting per-task JSONL data. Mixing ratios can be tuned for optimal downstream performance via guidelines in the associated paper and public GitHub repository.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Flan-mini is a curated 1.34 M-example subset of the FLAN instruction-tuning collection augmented with code and conversational tasks. It pools 388K Flan2021 instructions, 320K public prompt templates, 200K Natural Instructions v2 instances, 100K chain-of-thought examples, plus code datasets (100K Code Search, 50K Code Contests, 50K APPS). It further integrates 132K ChatGPT-generated examples from GPT-4-Alpaca, Code-Alpaca, and ShareGPT. Each example is randomly paired with handcrafted prompt templates for zero- or few-shot fine-tuning, ensuring diverse task coverage. Released under a permissive CC license.</p>\n\n",
                "matched_terms": [
                    "each",
                    "sharegpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: GraphWalks is an open-source benchmark dataset designed to evaluate multi-hop reasoning over long graph contexts. Released under the MIT license, it provides directed graphs as edge lists alongside user-specified operations&#8212;such as breadth-first searches or parent retrieval&#8212;for models to execute. Each prompt comprises three demonstration examples, a target graph, and a query, with expected outputs formatted as node ID lists. Accompanying metadata includes prompt character counts and problem types. Standardized extraction and F1-based grading scripts ensure consistent answer parsing and evaluation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: GSM8K (Grade School Math 8K) is an English monolingual dataset of 8.8K crowd-sourced grade school math word problems paired with multi-step solutions. It contains a main configuration and a Socratic variant, each offering questions and answers with calculator annotations and step-by-step reasoning expressed in natural language. Problems require two to eight elementary arithmetic steps. Split into training (7,473 examples) and test (1,319 examples), GSM8K supports text-to-text generation benchmarks under MIT license. All annotations were crowdsourced via Upwork and Surge AI.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Intellect-2-RL-Dataset is a large-scale collection of 284,741 training examples, designed for reinforcement learning in mathematical and coding problem solving. Each entry includes a unique problem_id, a task_type label, the problem prompt, verification_info detailing solution validity, and a baseline solve_rate from the Qwen-R1-Distill-7B model. Released under Apache-2.0 license, this dataset supports fine-tuning and evaluation of reasoning-oriented language models, facilitating research on algorithmic proficiency and reward-driven optimization within distributed asynchronous RL frameworks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LaMini-Instruction is an English text-to-text generation dataset comprising 2.58M instruction-response pairs distilled from GPT-3.5-Turbo. Each sample includes an instruction, a corresponding model-generated response, and the instruction&#8217;s provenance&#8212;drawn from sources such as Alpaca, FLAN, P3, and Self-Instruct. Released under CC-BY-NC 4.0, it spans a single training split of over 1.16 GB and supports fine-tuning of compact language models. LaMini-Instruction enables research in instruction-based learning but inherits biases and errors from its GPT-3.5 teacher.</p>\n\n",
                "matched_terms": [
                    "each",
                    "selfinstruct",
                    "its",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The LIMA dataset contains 1,000 high-quality prompt-response pairs designed to align language models with the style of a helpful AI assistant. Prompts are diverse, sourced from Stack Exchange, wikiHow, WritingPrompts, Natural Instructions, and manually authored examples. Despite limited size (&#160;750K tokens), all responses are stylistically consistent. The dataset includes a 50-example development set and a 300-prompt test set. LIMA demonstrates that small, curated datasets can be highly effective for instruction tuning and alignment of pretrained language models.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: LMSYS-Chat-1M is a large-scale dataset of one million real-world LLM conversations, collected from 210K users interacting with 25 models via Chatbot Arena and Vicuna demo (April-August 2023). Each conversation includes model metadata, OpenAI-style JSON formatting, language tags, and moderation labels. Personally identifiable information is redacted. This dataset enables research on LLM alignment, safety, evaluation, and user behavior in the wild, offering unique insights into real-world usage patterns and content moderation challenges in multi-model deployment scenarios.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset",
                    "tags"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: medical-o1-verifiable-problem is an Apache-2.0 licensed dataset comprising open-ended medical reasoning problems designed to improve large language models&#8217; diagnostic and procedural knowledge. It supports question-answering and text-generation tasks, presenting each instance as a challenging exam-style prompt paired with a verifiable, expert-derived answer. Published in English under a single default configuration with training data provided in JSON format, it allows systematic evaluation and refinement of LLM outputs.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: MedReason is a large-scale medical reasoning dataset combining seven clinical question-answer sources with a structured knowledge graph to produce detailed chains of reasoning. It contains 32,682 QA pairs, each annotated with step-by-step explanatory &#8220;thinking paths&#8221; derived from standardized medical KG relations. Designed to enhance the faithfulness and interpretability of medical problem-solving in large language models, MedReason enables fine-tuning of models such as MedReason-8B, which demonstrates state-of-the-art performance. Released under Apache-2.0, this open-source dataset aims to foster transparent medical QA systems.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: No Robots is a high-quality, human-curated instruction dataset comprising 10,000 examples for supervised fine-tuning of language models. It includes 9,500 training and 500 test instances across ten single-turn categories&#8212;Generation, Open QA, Brainstorm, Chat, Rewrite, Summarize, Coding, Classify, Closed QA, and Extract&#8212;totaling roughly 17 MB of English text under CC-BY-NC-4.0. Each example consists of a prompt with unique ID, structured message history (system, user, assistant), and category labels. It enables models to learn diverse instruction-following behaviors and robustly supports reproducibility.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: NuminaMath-1.5 is an open-source, large-scale post-training dataset comprising about 900 000 competition-level mathematics problems paired with chain-of-thought solutions. It covers diverse sources&#8212;from Chinese high school exams to US and international Olympiads&#8212;and spans domains like algebra, geometry, number theory, combinatorics, calculus, and puzzles. Each entry includes metadata fields (answer, problem_type, question_type) for verifiable outputs. Recent additions feature manually verified Olympiad references and curated contest data while synthetic problems were removed. Licensed under Apache 2.0, NuminaMath-1.5 supports advanced text-generation research in mathematical reasoning.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenAssistant Conversations (OASST1) is a human-generated, human-annotated corpus with 161,443 messages in 66,497 conversation trees across 35 languages. It includes over 461,000 quality ratings and more than 10,000 fully annotated trees. Each record contains metadata (IDs, timestamps), conversational structure (parent and tree IDs), role and language labels, toxicity and quality scores, emoji labels. Data comes in nested JSONL or flat parquet via HuggingFace, with 84,437 training and 4,401 validation splits, supporting supervised fine-tuning and reward model development. Licensed under Apache-2.0.</p>\n\n",
                "matched_terms": [
                    "each",
                    "oasst1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenCodeReasoning is a large-scale synthetic dataset designed to distill reasoning capabilities for Python-based competitive programming. It comprises 735,255 samples covering 28,319 unique problems sourced from platforms like CodeForces, AtCoder, and LeetCode. The dataset features two configurations: split_0 includes full problem statements and model responses, while split_1 references external datasets via index placeholders. Each example contains identifiers, source metadata, difficulty labels, and code solutions. Licensed under CC-BY-4.0, OpenCodeReasoning supports supervised fine-tuning of language models for code generation tasks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: OpenMathReasoning is a large-scale English math-reasoning dataset (cc-by-4.0) comprising 290K+ olympiad problems with 3.2M chain-of-thought (CoT), 1.7M tool-integrated reasoning (TIR), and 566K GenSelect solution samples. Sourced from AoPS and processed with Qwen2.5-32B, DeepSeek-R1, and QwQ-32B, each record includes problem statements, generated solutions, expected answers, inference modes, metadata, and pass-rate metrics. Available in cot, tir, and genselect splits, it underpins state-of-the-art LLM training and evaluation in question-answering and text-generation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Phoenix-sft-data-v1 is a multilingual supervised fine-tuning dataset containing 464,510 samples, combining instruction-following and ChatGPT-distilled conversation data. It includes Alpaca-derived tasks, post-translated multilingual instructions, and user-centered prompts in 40 languages. The dataset also integrates ShareGPT and Discord-sourced dialogues. With nearly 1 million conversation turns and detailed multilingual annotations, it supports multilingual language modeling, alignment, and chat adaptation. English and Chinese dominate the corpus, with broader linguistic diversity represented across the remaining data, enabling robust multilingual model training and evaluation.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: PLM-Video Human is a large-scale human-annotated video understanding dataset for Vision-Language Model training, covering four tasks: fine-grained video question answering (FGQA) with 2.3M QA pairs, region-based video captioning (RCap), dense captioning (RDCap), and temporal localization (RTLoc). Each config provides annotated clip segments with questions, answers, captions, masks, start/end frames, and metadata drawn from diverse open-access sources. Released under CC-BY-4.0, PLM-Video Human supports detailed temporal, spatial, and semantic modeling of complex human activities across diverse realistic dynamic video scenarios.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: PolyMath is a multilingual mathematical reasoning benchmark offering parallel problem sets in 18 languages across four difficulty tiers&#8212;K-12 to advanced mathematics&#8212;with splits labeled top, high, medium, and low. Each language contains 125 challenges per level, categorized by thought depth and knowledge breadth. The dataset ensures coverage of problem complexity and wide language representation, spanning over 75% of native speakers. High-quality translations validated by language experts guarantee clarity. PolyMath evaluates large language models&#8217; reasoning capabilities in diverse linguistic contexts.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: The PRISM Alignment Dataset is a large-scale human feedback resource designed to assess preference and value alignment in large language models (LLMs). It consists of detailed survey responses from 1,500 participants across 75 countries, followed by multi-turn conversations with 21 LLMs. Participants rate model outputs on a 1-100 scale and provide fine-grained feedback, yielding 8,011 conversation trees and 68,371 scored utterances. The dataset includes four JSONL configurations&#8212;survey, conversations, utterances, and metadata&#8212;licensed under CC-BY and CC-BY-NC for research and educational use.</p>\n\n",
                "matched_terms": [
                    "value",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: RepLiQA is a specialized QA dataset of 71,820 human-created Context-Question-Answer triplets from fictitious, natural-looking documents across 17 topics (e.g., local news, folklore, cybersecurity). Designed to test LLMs&#8217; ability to leverage novel reference texts without relying on memorized facts, each document includes five questions with &#160;20% unanswerable. Fields include document IDs, topics, extracted text, questions, answers and long answers. Released under CC-BY-4.0 in four splits, RepLiQA supports question answering, text classification, topic retrieval and selective QA benchmarking.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Self-Instruct is an open Apache-2.0-licensed dataset and framework designed to enhance language models&#8217; instruction-following capabilities. It comprises four configurations: a self-generated set of 82K prompt-completion pairs produced via OpenAI&#8217;s davinci engine; 50K samples from Super Natural Instructions; 52K prompts drawn from the P3 public pool; and 252 expert-crafted human evaluation tasks with associated inputs and outputs. All data is in English and supports instruction-tuning by providing diverse natural-language prompts paired with corresponding model or human completions. The dataset facilitates instruction-tuning.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: ShareGPT90K is a dataset of 90,665 conversational threads scraped from the ShareGPT platform. Each example includes a unique id and a sequence of messages, with each message annotated by its origin and its content.</p>\n\n",
                "matched_terms": [
                    "each",
                    "sharegpt",
                    "its",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: ShareGPT-Chinese-English-90k is a 90K-instance bilingual parallel human-machine QA dataset covering real and complex user inquiries in both Chinese and English. Licensed under Apache-2.0, it provides semantically aligned Chinese-English QA pairs for robust training of instruction-following dialogue and text-generation models. Unlike synthetic API-simulated corpora, all questions originate from genuine user interactions, preserving realistic instruction distributions. Collected through voluntary sharing, it naturally filters out low-quality exchanges. The dataset supports question-answering and text-generation tasks and can be easily loaded via the Firefly framework.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Skywork-OR1-RL-Data is a large-scale reinforcement learning dataset featuring 105,055 math problems and 14,057 coding questions curated for the Skywork-OR1 model series. Each example includes source attribution, structured prompts with roles, model-aware difficulty ratings for DeepSeek-R1 variants, and a reward model with ground truth and style labels. Problems are rigorously cleaned, deduplicated, and filtered by difficulty per variant. The dataset supports math and code splits totaling 1.5 billion bytes and facilitates robust reasoning training with rule-based RL recipes via curated pipelines efficiently.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: SocialMaze is a question-answering benchmark designed to evaluate large language models&#8217; social reasoning via hidden role deduction games. Each scenario presents a multi-agent setup where agents (Investigators, Criminal, Rumormongers, Lunatics) make public statements over three rounds. Models receive system prompts and dialogues, then must identify the true Criminal and Player 1&#8217;s actual role. The dataset includes precise QA pairs, chain-of-thought reasoning, and supports easy (6-player) and hard (10-player) splits, facilitating fine-tuning, evaluation, and analysis of complex inference under deception. CC-BY-4.0 licensed.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraFeedback is an MIT-licensed, open-source, large-scale preference dataset designed for training reward and critic models. It contains 64 K prompts drawn from UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA and FLAN, each answered by four out of 17 diverse LLMs under five alignment principles. The result is 256 K responses and 380 K fine-grained annotations covering instruction-following, truthfulness, honesty and helpfulness, all rated by GPT-4. Its scale, diversity and dense numerical plus textual feedback make it ideal for RLHF research and robust reward-model development.</p>\n\n",
                "matched_terms": [
                    "its",
                    "all",
                    "sharegpt",
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: UltraMedical is a large-scale English biomedical instruction dataset featuring over 409,000 examples licensed under MIT. Each sample includes an identifier, instruction type, multi-turn conversation pairs between human queries and GPT-generated responses, a ground-truth answer, and a model-evaluated score. The training split comprises roughly 1.2 GB across 410K examples, sourced from both curated public data and synthetic augmentations. UltraMedical aims to support the development of specialized generalist models in biomedicine by providing diverse, high-quality instruction-response instances, and comprehensive evaluation metrics accompany each instance.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Unnatural Instructions is a large-scale dataset of automatically generated instruction-input-output triplets designed to facilitate instruction tuning of language models with minimal human effort. It contains over 240,000 examples, including original instructions, associated inputs, outputs, and optional constraints. Each instance also features multiple reformulations&#8212;paraphrased variants of instructions complete with inputs and outputs&#8212;to enhance model robustness. The publicly available training split comprises around 66,000 examples. This dataset supports research in instruction following, prompt paraphrasing, and evaluating model generalization across diverse complex tasks.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: WebGLM-QA is an English monolingual dataset designed for question answering and text generation, used to train the WebGLM generator. It contains 43,579 training samples, 1,000 validation examples, and 400 test instances. Each record pairs a user-posed question with a generated answer and a list of reference snippets that support the response. Hosted on Hugging Face, it provides a consistent structure&#8212;question, answer, references&#8212;enabling work on dialogue systems, retrieval-augmented generation, and answer justification.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Wizard_evol_instruct_196K is a MIT-licensed instruction-tuning dataset comprising 143K evolved QA pairs derived from Alpaca and ShareGPT. It represents an optimized version of the Evol-Instruct data used to train the WizardLM family of models. To assemble the complete instruction set of roughly 196K samples, users must merge this release with the original unfiltered ShareGPT dataset. The refined examples cover diverse conversational and instructional scenarios, facilitating improved alignment and performance in downstream open-source large language models, including structured prompts and responses.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description</span>: Zhihu-KOL is a large-scale Chinese question-answering dataset derived from the Zhihu platform, designed for training open-domain assistants. It comprises 1,006,218 training instances of instruction-response pairs, each annotated with source and metadata fields.</p>\n\n",
                "matched_terms": [
                    "each",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide the comparision of 3/4/5-grams for all datasets (except <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>, which is displayed in the main paper) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F6\" title=\"Figure 6 &#8227; F.1 Token-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, and the top-5 <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A6.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams comparison across datasets in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F7\" title=\"Figure 7 &#8227; F.1 Token-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-grams phrases of some datasets include abnormal content (e.g. &#8220;<span class=\"ltx_text ltx_font_italic\">identify which instrument be string</span>&#8221; in <span class=\"ltx_text ltx_font_sansserif\">dolly-15</span> and &#8220;<span class=\"ltx_text ltx_font_italic\">The quick brown fox jumps over the lazy dog</span>&#8221; in <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>), which indicates that there is a lot of repetition in the input content of the template tasks or some instructions used to construct the dataset, which may affect the balance of the dataset.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These additional data further support our conclusions.\n<span class=\"ltx_text ltx_font_bold\">(1)</span> The <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> dataset, which consists of professionally crafted medical prompts, exhibits a relatively high proportion of numerical modifiers (nummod, 0.0276) and passive auxiliaries (auxpass, 0.0101) in dependency analysis, as well as a notably high usage of numerals (NUM, 0.0309) in POS tagging. These features reflect a terminology-dense and precision-oriented language style that emphasizes processes and outcomes rather than agents.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> In the <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span> dataset, the verb-noun pairs reflect language commonly used in business contexts, such as <span class=\"ltx_text ltx_font_italic\">&#8220;create plan&#8221;</span> and <span class=\"ltx_text ltx_font_italic\">&#8220;create strategy&#8221;</span>. In contrast, the verb-noun pairs observed in <span class=\"ltx_text ltx_font_sansserif\">BoredHumans</span>, <span class=\"ltx_text ltx_font_sansserif\">OASST1</span>, and <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span> suggest more generic and broadly applicable usage scenarios.</p>\n\n",
                "matched_terms": [
                    "11kbusiness",
                    "selfinstruct",
                    "medicalo1",
                    "pos",
                    "proportion",
                    "num",
                    "boredhumans",
                    "oasst1",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Anomalously, in the <span class=\"ltx_text ltx_font_sansserif\">dolly-15k</span> dataset, the most frequent verb-noun pairs exhibit a skewed distribution, with the highest-frequency nouns overwhelmingly associated with only the top one or two verbs. Moreover, these frequent verb-noun pairs often lack clear task-specific semantics&#8212;for example, <span class=\"ltx_text ltx_font_italic\">&#8220;tell i&#8221;</span>, <span class=\"ltx_text ltx_font_italic\">&#8220;give list&#8221;</span>, and <span class=\"ltx_text ltx_font_italic\">&#8220;classify each&#8221;</span>. This pattern may be attributed to the manual generation process, which is susceptible to the individual linguistic habits of annotators.</p>\n\n",
                "matched_terms": [
                    "dolly15k",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we show the distribution of sampled embedding points after PCA for all datasets (except for <span class=\"ltx_text ltx_font_sansserif\">medical-o1</span> and <span class=\"ltx_text ltx_font_sansserif\">Self-Instruct</span>, which are shown in the main paper) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.09316v1#A6.F9\" title=\"Figure 9 &#8227; F.3 Semantic-level Analysis &#8227; Appendix F Additional Experimental Results &#8227; Large Language Model Prompt Datasets: An In-depth Analysis and Insights\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "selfinstruct",
                    "medicalo1",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We can still observe from the results that datasets with more concentrated topical focus (e.g., <span class=\"ltx_text ltx_font_sansserif\">1.1k-business</span>) exhibit clear clustering patterns, whereas those with broader thematic coverage (e.g., <span class=\"ltx_text ltx_font_sansserif\">ShareGPT</span>) display a more dispersed distribution of data points.</p>\n\n",
                "matched_terms": [
                    "sharegpt",
                    "11kbusiness"
                ]
            }
        ]
    }
}