{
    "S4.T1": {
        "caption": "Table 1: \nSupported backend models in our system. ASR and dialogue components use publicly available pretrained models. SVS models were trained and released by us on Hugging Face.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Component</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model Name</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Source</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">ASR</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>)</cite> (small, medium, large-v3, large-v3-turbo)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">OpenAI</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">Paraformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao22b_interspeech</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gao2023funasr</span>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\">Alibaba</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">LLM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Gemini 2.5 Flash&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gemini2025</span>)</cite>, Gemma 2 2B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">team2024gemma</span>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Google</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">Llama 3.2 3B Instruct, Llama 3.1 8B Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">grattafiori2024llama</span>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\">Meta</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">Qwen3 8B, Qwen3 30B A3B&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yang2025qwen3</span>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\">Alibaba</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_left\">MiniMax-Text-01&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">minimax2025minimax01scalingfoundationmodels</span>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_left\">MiniMaxAI</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SVS</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">VISinger 2 (CN, multi-speaker)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Ours (Hugging Face)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_b\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">VISinger 2 (CN/JP, multi-speaker)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">Ours (Hugging Face)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "released",
            "dialogue",
            "largev3",
            "source",
            "yang2025qwen3",
            "gemini2025",
            "minimax2025minimax01scalingfoundationmodels",
            "our",
            "instruct",
            "llm",
            "alibaba",
            "face",
            "paraformer",
            "gao22binterspeech",
            "meta",
            "small",
            "30b",
            "cnjp",
            "system",
            "largev3turbo",
            "trained",
            "a3b",
            "openai",
            "component",
            "medium",
            "gemma",
            "radford2023robust",
            "llama",
            "minimaxai",
            "google",
            "asr",
            "available",
            "backend",
            "name",
            "hugging",
            "ours",
            "pretrained",
            "components",
            "grattafiori2024llama",
            "gemini",
            "visinger",
            "use",
            "model",
            "team2024gemma",
            "multispeaker",
            "flash",
            "minimaxtext01",
            "whisper",
            "qwen3",
            "supported",
            "publicly",
            "svs",
            "gao2023funasr"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With recent advances in automatic speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS) technologies, spoken dialogue systems (SDS) have become widely accessible.\nHowever, most existing SDS are limited to conventional spoken responses.\nWe present <span class=\"ltx_text ltx_font_bold\">SingingSDS</span>, a cascaded SDS that responds through singing rather than speaking, fostering more affective, memorable, and pleasurable interactions in character-based roleplay and interactive entertainment scenarios. SingingSDS employs a modular ASR-LLM-SVS pipeline and supports a wide range of configurations across character personas, ASR and LLM backends, SVS models, melody sources, and voice profiles, tailored to different needs in terms of latency, quality, and musical style.\nSingingSDS is available as a plug-and-play web demo, featuring modular, open-source code that supports customization and extension.\nDemo: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/espnet/SingingSDS\" title=\"\">https://huggingface.co/spaces/espnet/SingingSDS</a>. Code: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SingingSDS/SingingSDS\" title=\"\">https://github.com/SingingSDS/SingingSDS</a>. Video: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://youtube.com/playlist?list=PLZpUJJbwp2WvtPBenG5D3h09qKIrt24ui&amp;si=7CSLWAYWcfkTEdqe\" title=\"\">https://youtube.com/playlist?list=PLZpUJJbwp2WvtPBenG5D3h09qKIrt24ui&amp;si=7CSLWAYWcfkTEdqe</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "dialogue",
                    "llm",
                    "asr",
                    "available",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Singing, as a communicative modality, combines linguistic content with melody and rhythm, offering enhanced memorability and pleasure compared to speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">haiduk2020song</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gold2019musical</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zatorre2013perception</span>)</cite>, which can enrich interactive entertainment experiences. Despite significant progress in singing voice synthesis (SVS) and song generation models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024muskits-espnet</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024toksing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2024singomd</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2024visinger2+</span>)</cite>, these systems are essentially non-interactive: largely operate on predefined lyrics, lacking mechanisms for dynamic responses to user input.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we introduce <span class=\"ltx_text ltx_font_bold\">SingingSDS</span>, the first open-source system supporting speech-in, singing-out roleplay interactions for entertainment and character-driven scenarios.\nSingingSDS integrates automatic speech recognition (ASR), character-consistent response generation using large language models (LLMs), melody control with optional structural constraints, and singing voice synthesis (SVS).\nThe system is modular and configurable, including 5 ASR models, 7 LLMs, our released bilingual (Chinese-Japanese) and monolingual (Chinese-only) SVS models, and 5 melody control settings, resulting in 350 possible system configurations.\nWe conduct systematic assessment of both audio quality and user perception, supporting reproducible research on interactive singing dialogue. The system is fully open-sourced and provides an interactive web demo and a command-line interface for the creation and evaluation of speech-to-singing dialogues with fictional characters. These features support reproducible research and structured experimentation with interactive singing dialogues.</p>\n\n",
                "matched_terms": [
                    "models",
                    "released",
                    "dialogue",
                    "asr",
                    "system",
                    "svs",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingingSDS establishes a foundation for investigating singing as an interactive response modality beyond conventional spoken dialogue. The system has potential applications in VR concerts and other virtual performances, interactive music games and theme park attractions, and live streaming with audience participation. Through singing responses, SingingSDS can enhance these applications, offering more memorable and enjoyable user experiences, while also providing a platform for empirical studies of singing-based dialogue.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In parallel, SVS has progressed significantly in recent years with the development of neural models such as TokSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024toksing</span>)</cite>,\nDiffSinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022diffsinger</span>)</cite>, and VISinger2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>, which enable high-fidelity singing generation by modeling pitch, duration, and timbre.\nDespite the progress in both SDS and SVS, to the best of our knowledge, no prior work has integrated singing voice synthesis into an interactive spoken dialogue system.\nOur work presents the first attempt to bridge these two domains, enabling an LLM-based dialogue agent to sing its responses to the user via SVS techniques.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "dialogue",
                    "system",
                    "svs",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, model-based metrics such as Meta AudioBox Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tjandra2025meta</span>)</cite> did not consistently align with human preferences, and in some cases favored randomly generated, inharmonic note sequences over well-structured melodies. To better capture the aspects of engagement and enjoyability, we conducted human evaluations focusing on perceived enjoyment. Additionally, we report coarse melodic statistics, such as the large jump ratio, to quantify pitch dynamics in the generated singing outputs. Together, these complementary metrics offer a more holistic perspective on melody-conditioned dialogue generation (<span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:metrics).</p>\n\n",
                "matched_terms": [
                    "meta",
                    "dialogue",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the requirements, our system adopts a cascaded ASR-LLM-SVS pipeline with reference melodies (<span class=\"ltx_ERROR undefined\">\\figureref</span>fig:pipeline).\nAdditional architectural considerations and design trade-offs are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A1\" title=\"Appendix A System Design Considerations &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a user speech input <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and the specified language <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>, the system first transcribes the utterance using an ASR module:</p>\n\n",
                "matched_terms": [
                    "asr",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math> is explicitly provided to avoid errors from language identification capabilities of an ASR backend and improves recognition accuracy within a dialogue.</p>\n\n",
                "matched_terms": [
                    "backend",
                    "dialogue",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The transcription <math alttext=\"s_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">s_{t}</annotation></semantics></math> is then passed to a LLM, which generates an in-character reply conditioned on the user&#8217;s utterance, the virtual character&#8217;s profile <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, and optional structural constraints <math alttext=\"\\mathcal{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><annotation encoding=\"application/x-tex\">\\mathcal{C}</annotation></semantics></math>. The model is prompted with a system prompt containing <math alttext=\"(c,\\mathcal{C})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(c,\\mathcal{C})</annotation></semantics></math> and a user prompt containing <math alttext=\"s_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">s_{t}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "model",
                    "llm",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The character&#8217;s profile <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> largely follows the standard persona format used in OmniCharacter&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2025omnicharacter</span>)</cite>, with adaptations tailored to lyrical dialogue in our system. The structural constraint <math alttext=\"\\mathcal{C}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119966;</mi><annotation encoding=\"application/x-tex\">\\mathcal{C}</annotation></semantics></math> is derived from the melody controller when phrase annotations are available. Full prompt templates are provided in <span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:prompt.</p>\n\n",
                "matched_terms": [
                    "available",
                    "system",
                    "dialogue",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To encourage structural alignment between the generated textual response and the melody, the system constructs an LLM prompt specifying the required syllable count per musical phrase when phrase annotations are available (e.g., in the KiSing dataset and our self-constructed melody datasets synthesized with Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>)</cite>).\nThis alignment serves as a soft constraint for the LLM, encouraging outputs that match the expected number of musical events and exhibit more coherent phrase-level structure.\nDetails on the phrase-constrained prompt used for LLM generation are provided in <span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:melody_prompt.</p>\n\n",
                "matched_terms": [
                    "available",
                    "system",
                    "llm",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The generated lyrical response <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> is normalized and converted into phonemes <math alttext=\"l_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">l_{\\phi}</annotation></semantics></math> with grapheme-to-phoneme (G2P) system. Along with a music score <math alttext=\"\\mathcal{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><annotation encoding=\"application/x-tex\">\\mathcal{N}</annotation></semantics></math> created by the melody controller module and speaker information <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, either speaker embodding or speaker identity\ndepending on the model, the inputs are passed to an SVS model, producing the final sung output:</p>\n\n",
                "matched_terms": [
                    "svs",
                    "model",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingingSDS adopts a modular architecture with registry-based components that enable flexible integration of models, datasets, and character personas. As shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:package_structure, each core function, such as ASR, LLM, SVS, and melody loading and handling, is encapsulated as an independent module. This design supports rapid iteration, systematic benchmarking, and seamless extensibility.</p>\n\n",
                "matched_terms": [
                    "models",
                    "components",
                    "llm",
                    "asr",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system supports multiple backends for ASR, LLM, and SVS, all integrated through a registry-based modular architecture. Most ASR and LLM modules are community-pretrained. The supported SVS models are trained by us.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "models",
                    "llm",
                    "asr",
                    "system",
                    "supported",
                    "trained",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide two multi-speaker VISinger 2 models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>:\n(1) a Chinese SVS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain\" title=\"\">https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain</a></span></span></span>\ntrained on the ACE-Opencpop dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2024singing</span>)</cite>, and\n(2) a bilingual Mandarin-Japanese SVS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/visinger2-zh-jp-multisinger-svs\" title=\"\">https://huggingface.co/espnet/visinger2-zh-jp-multisinger-svs</a></span></span></span>\ntrained on a mixture of publicly available singing datasets, including OpenCpop&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2021opencpop</span>)</cite>, KiSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>, ACE-KiSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2024singing</span>)</cite>, M4Singer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022m4singer</span>)</cite>, Kiritan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ogawa2021tohoku</span>)</cite>, Onikuru Kurumi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">onikuru_kurumi_db</span>)</cite>, PJS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koguchi2020pjs</span>)</cite>, and Namine Ritsu&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">namineritsu_db</span>)</cite>. Details of the training configuration are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A3\" title=\"Appendix C SVS Model Training Details &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "models",
                    "visinger",
                    "multispeaker",
                    "available",
                    "publicly",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A full list of supported models is summarized in <span class=\"ltx_ERROR undefined\">\\tableref</span>tab:supported-models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "supported"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system supports two original singing characters, Limei and Yaoyin, each defined by a prompt-based persona, as specified in <span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:character_prompt. Both characters are drawn from our original fictional universe, <span class=\"ltx_text ltx_font_italic\">Changge Plains</span>, designed to support immersive roleplay interaction and storytelling. New characters can be added by specifying prompt configurations.</p>\n\n",
                "matched_terms": [
                    "system",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingingSDS is available as an interactive web demo hosted on Hugging Face Spaces.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/espnet/SingingSDS\" title=\"\">https://huggingface.co/spaces/espnet/SingingSDS</a></span></span></span>\nUsers can initiate dialogue by speaking into a microphone. The system transcribes the input, generates an in-character lyrical response, and synthesizes a singing reply. The interface displays synchronized lyrics, character portraits, and playback controls, shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:demo_ui. Users can switch between characters (e.g., Limei and Yaoyin) and different model and melody configurations.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "model",
                    "face",
                    "system",
                    "available",
                    "hugging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation module is fully integrated into the system and can be triggered directly through the user interface or computed with our CLI command.</p>\n\n",
                "matched_terms": [
                    "system",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are run on single NVIDIA v100 GPU using the cascaded pipeline shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:pipeline. We evaluate two ASR models: <span class=\"ltx_text ltx_font_typewriter\">whisper-medium</span> (OpenAI) and <span class=\"ltx_text ltx_font_typewriter\">paraformer-zh</span> (Alibaba), and two LLMs: <span class=\"ltx_text ltx_font_typewriter\">Llama-3.1-8B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-flash</span>.\nFor brevity, we refer to them as Whisper, Paraformer, Llama 3, and Gemini in the rest of this paper.\nFor singing voice synthesis (SVS), we use our bilingual pretrained VISinger 2 model.</p>\n\n",
                "matched_terms": [
                    "openai",
                    "models",
                    "pretrained",
                    "gemini",
                    "llama",
                    "visinger",
                    "use",
                    "model",
                    "alibaba",
                    "paraformer",
                    "whisper",
                    "asr",
                    "svs",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our system under multiple configurations of the ASR, LLM, and melody generation modules. <span class=\"ltx_ERROR undefined\">\\tableref</span>tab:eval_results presents the performance across combinations of Whisper and Paraformer (ASR), LLaMA 3 and Gemini (LLM), and KiSing and Touhou (melody). The Whisper + Gemini configuration achieves the highest overall perceptual quality and entertainment value, as indicated by automatic singing quality scores (SingMOS) and human evaluations of novelty and fun (N&amp;F), character consistency (Char. Cons.), and lyric quality (Lyric Qual.). In contrast, the Paraformer + LLaMA 3 setting yields the lowest system latency, making it more suitable for interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "llama",
                    "llm",
                    "paraformer",
                    "whisper",
                    "asr",
                    "system",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presents SingingSDS, a modular spoken dialogue system with melody-conditioned singing responses to user input in Chinese and Japanese. The system combines ASR, LLM, and SVS components through a prompting scheme that aligns lyric structure with melodic phrasing, without requiring model fine-tuning.</p>\n\n",
                "matched_terms": [
                    "components",
                    "dialogue",
                    "llm",
                    "model",
                    "asr",
                    "system",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support future work, we release the pretrained SVS model used in SingingSDS, along with scripts for evaluation and dataset construction. Although other components are based on open-source APIs, the pipeline remains modular and extensible, allowing substitution of melody sources, LLMs, or SVS backends for controlled experimentation.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "pretrained",
                    "components",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingingSDS constitutes the first fully implemented pipeline for interactive dialogue system with singing virtual characters. By bridging conversational AI and singing synthesis, it enables a novel form of interactive response grounded in melody and persona. Our system opens new research directions in controllable singing generation, expressive speech interfaces, and musical human-computer interaction.</p>\n\n",
                "matched_terms": [
                    "system",
                    "dialogue",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We acknowledge illustrator Zihe Zhou for the creation of Yaoyin&#8217;s character artwork, which is included in the demo page shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:demo_ui. The artwork was commissioned exclusively for the SingingSDS project and may be used for direct derivatives of SingingSDS, such as project-related posts or usage videos, without additional permission. Any other use requires express permission from the illustrator. Use of the artwork for training or fine-tuning artificial intelligence or machine learning models is strictly prohibited.</p>\n\n",
                "matched_terms": [
                    "models",
                    "use"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Parts of the experiments of this work used the Bridges2 system at PSC through allocations CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services &amp; Support (ACCESS) program, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.</p>\n\n",
                "matched_terms": [
                    "supported",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initially considered direct text-to-song generation, where singing audio is synthesized end-to-end from LLM responses without a predefined melody. However, existing music generation models introduce substantial latency; for example, in our test, Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>)</cite> required over 40 seconds to generate a 5-second audio clip on a T4 GPU, making such methods impractical for interactive use.</p>\n\n",
                "matched_terms": [
                    "models",
                    "use",
                    "llm",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure responsiveness while preserving musical phrasing, we reformulate the task as melody-constrained singing response generation. Instead of relying on end-to-end text-to-song models with multi-second latencies, the system employs a lightweight melody-conditioned SVS module, achieving an SVS synthesis latency of approximately 0.16&#8211;0.19&#8201;s across our evaluated configurations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "system",
                    "svs",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both SVS systems adopt the VISinger&#160;2 architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>. Unless otherwise specified, the two models share the same hyperparameter settings, as detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A3.T3\" title=\"Table 3 &#8227; C.1 Model Architectures &#8227; Appendix C SVS Model Training Details &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs",
                    "visinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Chinese model uses speaker ID (SID) conditioning for multi-singer\nmodeling. The exact configuration corresponds to the released model on Hugging\nFace.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain\" title=\"\">https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "released",
                    "model",
                    "hugging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The bilingual model differs from the Chinese system only in its conditioning mechanisms. Specifically, it uses:</p>\n\n",
                "matched_terms": [
                    "model",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The full configuration matches the released model on Hugging\nFace.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/visinger2-zh-jp-multisinger-svs\" title=\"\">https://huggingface.co/espnet/visinger2-zh-jp-multisinger-svs</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "released",
                    "model",
                    "hugging"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both SVS models were trained using the ESPnet GAN-SVS recipe&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>. All experiments use a waveform sampling rate of 44.1&#160;kHz. Key\ntraining hyperparameters are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A3.T4\" title=\"Table 4 &#8227; C.2 Training Procedure &#8227; Appendix C SVS Model Training Details &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "models",
                    "use",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We self-constructed a Chinese music score corpus with lyric-level annotation, covering a total of 305 music genres. The vocal data is generated using a pipeline involving two models: lyrics and genre prompts are first produced by DeepSeek&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024deepseek</span>)</cite>, conditioned on a specified music genre; then, the music&#8212;including separate vocal and instrumental tracks&#8212;is synthesized using the YuE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>)</cite> model, which adopts a track-decoupled next-token prediction strategy. This allows direct access to clean vocal data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report SingMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2024singmos</span>)</cite>, a model-predicted score trained on crowd-annotated singing data. It reflects vocal quality, articulation, and how closely the output resembles natural singing. SingMOS enables consistent comparison across different SVS backends without requiring additional annotation.</p>\n\n",
                "matched_terms": [
                    "trained",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use phoneme error rate (PER) to measure how accurately the system preserves linguistic content. Outputs are transcribed using Whisper-turbo and aligned at the phoneme level with ground-truth references. PER is preferred over character error rate for singing, which often involves pitch variation and extended vowels.</p>\n\n",
                "matched_terms": [
                    "use",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report end-to-end wall-clock latency (Lat.) from user input to synthesized audio, including all components (ASR, LLM, SVS). To account for variable output durations, latency is normalized by the number of input tokens. All measurements are conducted on NVIDIA L40S GPUs.</p>\n\n",
                "matched_terms": [
                    "components",
                    "svs",
                    "llm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments are run on NVIDIA L40S GPUs using the cascaded pipeline shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:pipeline. For singing voice synthesis (SVS), we use our bilingual pretrained VISinger 2 model. We compare three SVS variants based on melody selection: (1) <span class=\"ltx_text ltx_font_bold\">SVS-1</span>, with randomly generated durations and pitch contours; (2) <span class=\"ltx_text ltx_font_bold\">SVS-2</span>, with melodies retrieved from the KiSing dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>; and (3) <span class=\"ltx_text ltx_font_bold\">SVS-3</span>, using main melodies retrieved from a curated Touhou MIDI archive.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/AyHa1810/touhou-midi-collection\" title=\"\">https://github.com/AyHa1810/touhou-midi-collection</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "visinger",
                    "use",
                    "model",
                    "svs",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ASR component uses Whisper model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span> (16kHz)</span></span></span>, and the LLM is <span class=\"ltx_text ltx_font_typewriter\">gemma-2-2b</span>. SVS outputs are synthesized at 44.1kHz and downsampled to 16&#160;kHz for PER evaluation. Latency is reported as end-to-end wall-clock time. All models are used as-is without fine-tuning during experimentation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "component",
                    "model",
                    "llm",
                    "whisper",
                    "asr",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">tab:kdconv summarizes performance on our sampled KdConv test sets. All SVS variants outperform the TTS baseline in perceived naturalness (SingMOS), with minor differences in intelligibility (PER within 4 percentage points).</p>\n\n",
                "matched_terms": [
                    "svs",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We emphasize transparency and user control. The web demo is publicly accessible via Hugging Face Spaces, and by default it does not collect or store any user data. All audio and text inputs are processed locally in memory and discarded after response generation. The system does not log, transmit, or retain user data without explicit user awareness. If future researchers extend the system with logging or evaluation tools, they are responsible for obtaining appropriate consent from participants.</p>\n\n",
                "matched_terms": [
                    "face",
                    "hugging",
                    "publicly",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models used in the system are publicly available, including pretrained components for ASR and LLM, as well as our own SVS models. The SVS models are trained exclusively on open datasets with appropriate usage licenses. We encourage responsible and transparent use of SingingSDS for creative, educational, and research purposes.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "models",
                    "pretrained",
                    "components",
                    "use",
                    "llm",
                    "asr",
                    "system",
                    "available",
                    "publicly",
                    "trained",
                    "our"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Evaluation results of different ASR (Whisper, Paraformer), LLM (Llama 3, Gemini) and melody (KiSing, Touhou) configurations. ↑\\uparrow indicates higher is better; ↓\\downarrow indicates lower is better. Note that the Large Jump Ratio (Jump R.) reflects melody dynamics and does not necessarily favor lower values. Detailed metric definitions can be found in \\appendixrefapd:metrics.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ASR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">LLM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Melody</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">SingMOS</span><span class=\"ltx_text\" style=\"font-size:70%;\">&#8201;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">PER (%)</span><span class=\"ltx_text\" style=\"font-size:70%;\">&#8201;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Jump R.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">N&amp;F</span><span class=\"ltx_text\" style=\"font-size:70%;\">&#8201;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Char. Cons.</span><span class=\"ltx_text\" style=\"font-size:70%;\">&#8201;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Lyric Qual.</span><span class=\"ltx_text\" style=\"font-size:70%;\">&#8201;</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ASR Lat.</span><span class=\"ltx_text\" style=\"font-size:70%;\">&#8201;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">LLM Lat.</span><span class=\"ltx_text\" style=\"font-size:70%;\">&#8201;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.4pt 4.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">SVS Lat.</span><span class=\"ltx_text\" style=\"font-size:70%;\">&#8201;</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.700em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Whisper</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Llama 3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">KiSing</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">3.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">1.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.19</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Paraformer</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Llama 3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">KiSing</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.47</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.12</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">3.41</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.44</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">1.79</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Whisper</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Gemini</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">KiSing</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.59</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.48</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.21</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">4.19</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">3.86</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.55</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">5.79</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Whisper</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Llama 3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">Touhou</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">4.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">3.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">2.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding:0.4pt 4.0pt;\"><span class=\"ltx_text\" style=\"font-size:70%;\">0.19</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "does",
            "configurations",
            "singmos",
            "reflects",
            "melody",
            "evaluation",
            "detailed",
            "dynamics",
            "↓downarrow",
            "appendixrefapdmetrics",
            "not",
            "qual",
            "svs",
            "found",
            "llm",
            "paraformer",
            "kising",
            "ratio",
            "favor",
            "necessarily",
            "llama",
            "note",
            "definitions",
            "higher",
            "asr",
            "indicates",
            "results",
            "lower",
            "metric",
            "lyric",
            "jump",
            "values",
            "↑uparrow",
            "cons",
            "gemini",
            "better",
            "large",
            "lat",
            "char",
            "whisper",
            "touhou",
            "different"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With recent advances in automatic speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS) technologies, spoken dialogue systems (SDS) have become widely accessible.\nHowever, most existing SDS are limited to conventional spoken responses.\nWe present <span class=\"ltx_text ltx_font_bold\">SingingSDS</span>, a cascaded SDS that responds through singing rather than speaking, fostering more affective, memorable, and pleasurable interactions in character-based roleplay and interactive entertainment scenarios. SingingSDS employs a modular ASR-LLM-SVS pipeline and supports a wide range of configurations across character personas, ASR and LLM backends, SVS models, melody sources, and voice profiles, tailored to different needs in terms of latency, quality, and musical style.\nSingingSDS is available as a plug-and-play web demo, featuring modular, open-source code that supports customization and extension.\nDemo: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/espnet/SingingSDS\" title=\"\">https://huggingface.co/spaces/espnet/SingingSDS</a>. Code: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SingingSDS/SingingSDS\" title=\"\">https://github.com/SingingSDS/SingingSDS</a>. Video: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://youtube.com/playlist?list=PLZpUJJbwp2WvtPBenG5D3h09qKIrt24ui&amp;si=7CSLWAYWcfkTEdqe\" title=\"\">https://youtube.com/playlist?list=PLZpUJJbwp2WvtPBenG5D3h09qKIrt24ui&amp;si=7CSLWAYWcfkTEdqe</a>.</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "llm",
                    "large",
                    "melody",
                    "asr",
                    "different",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Singing, as a communicative modality, combines linguistic content with melody and rhythm, offering enhanced memorability and pleasure compared to speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">haiduk2020song</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gold2019musical</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zatorre2013perception</span>)</cite>, which can enrich interactive entertainment experiences. Despite significant progress in singing voice synthesis (SVS) and song generation models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024muskits-espnet</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024toksing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2024singomd</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2024visinger2+</span>)</cite>, these systems are essentially non-interactive: largely operate on predefined lyrics, lacking mechanisms for dynamic responses to user input.</p>\n\n",
                "matched_terms": [
                    "melody",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we introduce <span class=\"ltx_text ltx_font_bold\">SingingSDS</span>, the first open-source system supporting speech-in, singing-out roleplay interactions for entertainment and character-driven scenarios.\nSingingSDS integrates automatic speech recognition (ASR), character-consistent response generation using large language models (LLMs), melody control with optional structural constraints, and singing voice synthesis (SVS).\nThe system is modular and configurable, including 5 ASR models, 7 LLMs, our released bilingual (Chinese-Japanese) and monolingual (Chinese-only) SVS models, and 5 melody control settings, resulting in 350 possible system configurations.\nWe conduct systematic assessment of both audio quality and user perception, supporting reproducible research on interactive singing dialogue. The system is fully open-sourced and provides an interactive web demo and a command-line interface for the creation and evaluation of speech-to-singing dialogues with fictional characters. These features support reproducible research and structured experimentation with interactive singing dialogues.</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "large",
                    "melody",
                    "evaluation",
                    "asr",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, model-based metrics such as Meta AudioBox Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tjandra2025meta</span>)</cite> did not consistently align with human preferences, and in some cases favored randomly generated, inharmonic note sequences over well-structured melodies. To better capture the aspects of engagement and enjoyability, we conducted human evaluations focusing on perceived enjoyment. Additionally, we report coarse melodic statistics, such as the large jump ratio, to quantify pitch dynamics in the generated singing outputs. Together, these complementary metrics offer a more holistic perspective on melody-conditioned dialogue generation (<span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:metrics).</p>\n\n",
                "matched_terms": [
                    "better",
                    "large",
                    "note",
                    "dynamics",
                    "appendixrefapdmetrics",
                    "ratio",
                    "not",
                    "jump"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The melody controller provides note-level constraints in the form of a sequence <math alttext=\"\\mathcal{N}={(p_{i},\\tau_{i}^{s},\\tau_{i}^{e})}_{i=1}^{n}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo>=</mo><msubsup><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>&#964;</mi><mi>i</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mi>&#964;</mi><mi>i</mi><mi>e</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}={(p_{i},\\tau_{i}^{s},\\tau_{i}^{e})}_{i=1}^{n}</annotation></semantics></math>, where <math alttext=\"p_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">p_{i}</annotation></semantics></math> denotes pitch, and <math alttext=\"\\tau_{i}^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>&#964;</mi><mi>i</mi><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">\\tau_{i}^{s}</annotation></semantics></math>, <math alttext=\"\\tau_{i}^{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>&#964;</mi><mi>i</mi><mi>e</mi></msubsup><annotation encoding=\"application/x-tex\">\\tau_{i}^{e}</annotation></semantics></math> indicate the start and end times&#160;(in seconds) of each note. Optional phrase annotations define the boundaries of musical phrases.</p>\n\n",
                "matched_terms": [
                    "melody",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We support two types of melody sources.\nThe first setting consists of randomly synthesized melodies and serves as a baseline.\nThese are generated on the fly by sampling pitch and duration values uniformly, without rests or phrase-level structure. Since no reference alignment is available, a simple forced alignment is applied, assigning one syllable per note.\nThe second is sampled melodies drawn from existing song datasets. For these, we support two alignment strategies. In pitch-based alignment, each syllable is mapped one-to-one to a note in the melody. In lyric-aware alignment, one-to-many mappings are preserved: when a syllable spans multiple notes in the original song, the same structure is retained in the output, as illustrated in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:alignment_example.</p>\n\n",
                "matched_terms": [
                    "melody",
                    "values",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To encourage structural alignment between the generated textual response and the melody, the system constructs an LLM prompt specifying the required syllable count per musical phrase when phrase annotations are available (e.g., in the KiSing dataset and our self-constructed melody datasets synthesized with Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>)</cite>).\nThis alignment serves as a soft constraint for the LLM, encouraging outputs that match the expected number of musical events and exhibit more coherent phrase-level structure.\nDetails on the phrase-constrained prompt used for LLM generation are provided in <span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:melody_prompt.</p>\n\n",
                "matched_terms": [
                    "melody",
                    "llm",
                    "kising"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The generated lyrical response <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> is normalized and converted into phonemes <math alttext=\"l_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">l_{\\phi}</annotation></semantics></math> with grapheme-to-phoneme (G2P) system. Along with a music score <math alttext=\"\\mathcal{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><annotation encoding=\"application/x-tex\">\\mathcal{N}</annotation></semantics></math> created by the melody controller module and speaker information <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, either speaker embodding or speaker identity\ndepending on the model, the inputs are passed to an SVS model, producing the final sung output:</p>\n\n",
                "matched_terms": [
                    "melody",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingingSDS adopts a modular architecture with registry-based components that enable flexible integration of models, datasets, and character personas. As shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:package_structure, each core function, such as ASR, LLM, SVS, and melody loading and handling, is encapsulated as an independent module. This design supports rapid iteration, systematic benchmarking, and seamless extensibility.</p>\n\n",
                "matched_terms": [
                    "melody",
                    "svs",
                    "llm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system supports multiple backends for ASR, LLM, and SVS, all integrated through a registry-based modular architecture. Most ASR and LLM modules are community-pretrained. The supported SVS models are trained by us.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "llm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide two multi-speaker VISinger 2 models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>:\n(1) a Chinese SVS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain\" title=\"\">https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain</a></span></span></span>\ntrained on the ACE-Opencpop dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2024singing</span>)</cite>, and\n(2) a bilingual Mandarin-Japanese SVS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/visinger2-zh-jp-multisinger-svs\" title=\"\">https://huggingface.co/espnet/visinger2-zh-jp-multisinger-svs</a></span></span></span>\ntrained on a mixture of publicly available singing datasets, including OpenCpop&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2021opencpop</span>)</cite>, KiSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>, ACE-KiSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2024singing</span>)</cite>, M4Singer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022m4singer</span>)</cite>, Kiritan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ogawa2021tohoku</span>)</cite>, Onikuru Kurumi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">onikuru_kurumi_db</span>)</cite>, PJS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koguchi2020pjs</span>)</cite>, and Namine Ritsu&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">namineritsu_db</span>)</cite>. Details of the training configuration are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A3\" title=\"Appendix C SVS Model Training Details &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "kising"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The system supports retrieval from three melody dataset in addition to randomly generated melodies: KiSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>, a Touhou MIDI collection<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/AyHa1810/touhou-midi-collection\" title=\"\">https://github.com/AyHa1810/touhou-midi-collection</a></span></span></span>, and a synthesized dataset of 499 songs generated using Yue, constructed to expand the melody database (see <span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:synthesized_melody for details). These datasets provide melodies that condition the singing output. A registry-based handler module loads and converts each melody into a format suitable for synthesis, allowing new datasets to be integrated with minimal effort.</p>\n\n",
                "matched_terms": [
                    "melody",
                    "kising",
                    "touhou"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingingSDS is available as an interactive web demo hosted on Hugging Face Spaces.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/espnet/SingingSDS\" title=\"\">https://huggingface.co/spaces/espnet/SingingSDS</a></span></span></span>\nUsers can initiate dialogue by speaking into a microphone. The system transcribes the input, generates an in-character lyrical response, and synthesizes a singing reply. The interface displays synchronized lyrics, character portraits, and playback controls, shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:demo_ui. Users can switch between characters (e.g., Limei and Yaoyin) and different model and melody configurations.</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "melody",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the system from multiple perspectives, including perceptual quality, linguistic accuracy, melodic structure, and runtime efficiency through automated or human evaluation. Detailed explanations on evaluation setups can be found in <span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:eval.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "found",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluated using a subset of KdConv dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhou2020kdconv</span>)</cite>, a multi-domain multi-turn dialogue corpus, to simulate user interactions.\nThe experimental setup and results for KdConv sampled data can be found in <span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:kdconv.</p>\n\n",
                "matched_terms": [
                    "found",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio outputs are resampled to 16 kHz for ASR-based intelligibility evaluation (i.e., PER) and kept at 44.1 kHz for subjective MOS testing. For melody selection, we use scores retrieved from the KiSing dataset and a curated archive of Touhou MIDI files.</p>\n\n",
                "matched_terms": [
                    "melody",
                    "evaluation",
                    "kising",
                    "touhou"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are run on single NVIDIA v100 GPU using the cascaded pipeline shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:pipeline. We evaluate two ASR models: <span class=\"ltx_text ltx_font_typewriter\">whisper-medium</span> (OpenAI) and <span class=\"ltx_text ltx_font_typewriter\">paraformer-zh</span> (Alibaba), and two LLMs: <span class=\"ltx_text ltx_font_typewriter\">Llama-3.1-8B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-flash</span>.\nFor brevity, we refer to them as Whisper, Paraformer, Llama 3, and Gemini in the rest of this paper.\nFor singing voice synthesis (SVS), we use our bilingual pretrained VISinger 2 model.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "llama",
                    "paraformer",
                    "whisper",
                    "asr",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our system under multiple configurations of the ASR, LLM, and melody generation modules. <span class=\"ltx_ERROR undefined\">\\tableref</span>tab:eval_results presents the performance across combinations of Whisper and Paraformer (ASR), LLaMA 3 and Gemini (LLM), and KiSing and Touhou (melody). The Whisper + Gemini configuration achieves the highest overall perceptual quality and entertainment value, as indicated by automatic singing quality scores (SingMOS) and human evaluations of novelty and fun (N&amp;F), character consistency (Char. Cons.), and lyric quality (Lyric Qual.). In contrast, the Paraformer + LLaMA 3 setting yields the lowest system latency, making it more suitable for interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "cons",
                    "gemini",
                    "configurations",
                    "singmos",
                    "llama",
                    "llm",
                    "melody",
                    "paraformer",
                    "char",
                    "kising",
                    "asr",
                    "whisper",
                    "touhou",
                    "lyric",
                    "qual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presents SingingSDS, a modular spoken dialogue system with melody-conditioned singing responses to user input in Chinese and Japanese. The system combines ASR, LLM, and SVS components through a prompting scheme that aligns lyric structure with melodic phrasing, without requiring model fine-tuning.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "lyric",
                    "llm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluation across perceptual quality, intelligibility, latency, and melodic dynamics confirms the feasibility of singing-based interaction in dialogue systems. Subjective ratings indicate that appropriate melody selection improves perceived entertainment value without compromising intelligibility.</p>\n\n",
                "matched_terms": [
                    "melody",
                    "evaluation",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support future work, we release the pretrained SVS model used in SingingSDS, along with scripts for evaluation and dataset construction. Although other components are based on open-source APIs, the pipeline remains modular and extensible, allowing substitution of melody sources, LLMs, or SVS backends for controlled experimentation.</p>\n\n",
                "matched_terms": [
                    "melody",
                    "evaluation",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initially considered direct text-to-song generation, where singing audio is synthesized end-to-end from LLM responses without a predefined melody. However, existing music generation models introduce substantial latency; for example, in our test, Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>)</cite> required over 40 seconds to generate a 5-second audio clip on a T4 GPU, making such methods impractical for interactive use.</p>\n\n",
                "matched_terms": [
                    "melody",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure responsiveness while preserving musical phrasing, we reformulate the task as melody-constrained singing response generation. Instead of relying on end-to-end text-to-song models with multi-second latencies, the system employs a lightweight melody-conditioned SVS module, achieving an SVS synthesis latency of approximately 0.16&#8211;0.19&#8201;s across our evaluated configurations.</p>\n\n",
                "matched_terms": [
                    "configurations",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both SVS systems adopt the VISinger&#160;2 architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>. Unless otherwise specified, the two models share the same hyperparameter settings, as detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A3.T3\" title=\"Table 3 &#8227; C.1 Model Architectures &#8227; Appendix C SVS Model Training Details &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report SingMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2024singmos</span>)</cite>, a model-predicted score trained on crowd-annotated singing data. It reflects vocal quality, articulation, and how closely the output resembles natural singing. SingMOS enables consistent comparison across different SVS backends without requiring additional annotation.</p>\n\n",
                "matched_terms": [
                    "different",
                    "singmos",
                    "svs",
                    "reflects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation to assess the perceived quality and entertainment value of each sung response. Six listeners participated in a blind listening evaluation after providing informed consent. Participants were instructed to evaluate the samples independently, without discussion or influence from others, based on their individual perceptual judgments. Participants rate each sample on a 5-point Likert scale across three dimensions: Novelty and Fun (N&amp;F), Character Consistency (Char. Cons.), and Lyric Quality (Lyric Qual.). These criteria are designed to capture both the expressive and contextual aspects of singing dialogue. Specifically, raters assess (1) how engaging and novel the singing-based interaction feels, (2) whether the lyrical content aligns with the character&#8217;s profile and persona, and (3) the linguistic fluency, coherence, and poetic rhythm of the lyrics. This evaluation framework enables nuanced analysis of singing responses beyond vocal quality alone, with a particular focus on creativity, role embodiment, and lyricism.</p>\n\n",
                "matched_terms": [
                    "cons",
                    "char",
                    "evaluation",
                    "lyric",
                    "qual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report end-to-end wall-clock latency (Lat.) from user input to synthesized audio, including all components (ASR, LLM, SVS). To account for variable output durations, latency is normalized by the number of input tokens. All measurements are conducted on NVIDIA L40S GPUs.</p>\n\n",
                "matched_terms": [
                    "lat",
                    "svs",
                    "llm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify pitch movement, we compute the large jump ratio (Jump R.), the proportion of adjacent notes differing by more than five semitones:</p>\n\n",
                "matched_terms": [
                    "ratio",
                    "jump",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"p_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS0.SSS0.Px5.p1.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">p_{i}</annotation></semantics></math> is the MIDI pitch of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS0.SSS0.Px5.p1.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th note and <math alttext=\"L\" class=\"ltx_Math\" display=\"inline\" id=\"A5.SS0.SSS0.Px5.p1.m3\" intent=\":literal\"><semantics><mi>L</mi><annotation encoding=\"application/x-tex\">L</annotation></semantics></math> is the number of notes. This metric reflects melodic smoothness, with higher values indicating more abrupt pitch transitions.</p>\n\n",
                "matched_terms": [
                    "reflects",
                    "note",
                    "higher",
                    "metric",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments are run on NVIDIA L40S GPUs using the cascaded pipeline shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:pipeline. For singing voice synthesis (SVS), we use our bilingual pretrained VISinger 2 model. We compare three SVS variants based on melody selection: (1) <span class=\"ltx_text ltx_font_bold\">SVS-1</span>, with randomly generated durations and pitch contours; (2) <span class=\"ltx_text ltx_font_bold\">SVS-2</span>, with melodies retrieved from the KiSing dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>; and (3) <span class=\"ltx_text ltx_font_bold\">SVS-3</span>, using main melodies retrieved from a curated Touhou MIDI archive.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/AyHa1810/touhou-midi-collection\" title=\"\">https://github.com/AyHa1810/touhou-midi-collection</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "melody",
                    "svs",
                    "kising",
                    "touhou"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ASR component uses Whisper model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span> (16kHz)</span></span></span>, and the LLM is <span class=\"ltx_text ltx_font_typewriter\">gemma-2-2b</span>. SVS outputs are synthesized at 44.1kHz and downsampled to 16&#160;kHz for PER evaluation. Latency is reported as end-to-end wall-clock time. All models are used as-is without fine-tuning during experimentation.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "evaluation",
                    "whisper",
                    "asr",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">tab:kdconv summarizes performance on our sampled KdConv test sets. All SVS variants outperform the TTS baseline in perceived naturalness (SingMOS), with minor differences in intelligibility (PER within 4 percentage points).</p>\n\n",
                "matched_terms": [
                    "singmos",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On <span class=\"ltx_text ltx_font_bold\">KdConv</span>, <span class=\"ltx_text ltx_font_bold\">SVS-1</span> (random melody) achieves the highest SingMOS and lowest PER.\nThis suggests that, for general domain utterances, randomly generated melodic patterns are sufficient to produce appealing singing output.\nHowever, its melodic contours are more varied, resulting in larger pitch jumps.</p>\n\n",
                "matched_terms": [
                    "singmos",
                    "melody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SVS-2</span> (KiSing) yields the smoothest melodic transitions but shows higher PER, possibly due to slower note progressions that affect phoneme clarity. This trade-off suggests that melody selection should be context-aware: expressive, wide-range melodies may enhance persona-rich dialogue, while flatter contours may suit more neutral interactions.</p>\n\n",
                "matched_terms": [
                    "melody",
                    "higher",
                    "kising",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We emphasize transparency and user control. The web demo is publicly accessible via Hugging Face Spaces, and by default it does not collect or store any user data. All audio and text inputs are processed locally in memory and discarded after response generation. The system does not log, transmit, or retain user data without explicit user awareness. If future researchers extend the system with logging or evaluation tools, they are responsible for obtaining appropriate consent from participants.</p>\n\n",
                "matched_terms": [
                    "not",
                    "evaluation",
                    "does"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models used in the system are publicly available, including pretrained components for ASR and LLM, as well as our own SVS models. The SVS models are trained exclusively on open datasets with appropriate usage licenses. We encourage responsible and transparent use of SingingSDS for creative, educational, and research purposes.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "llm",
                    "asr"
                ]
            }
        ]
    },
    "A3.T3": {
        "caption": "Table 3: Model architecture parameters shared by both SVS models (VISinger 2).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Parameter</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Value</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Hidden Dimension (<math alttext=\"D_{model}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m1\" intent=\":literal\"><semantics><msub><mi>D</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">D_{model}</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">192</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Text Encoder Layers</td>\n<td class=\"ltx_td ltx_align_center\">6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Posterior Encoder Layers</td>\n<td class=\"ltx_td ltx_align_center\">8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Attention Heads (<math alttext=\"N_{head}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m2\" intent=\":literal\"><semantics><msub><mi>N</mi><mrow><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">N_{head}</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FFN Expansion Factor (4x)</td>\n<td class=\"ltx_td ltx_align_center\">768</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Encoder Dropout Rate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "hidden",
            "expansion",
            "attention",
            "ffn",
            "parameter",
            "heads",
            "rate",
            "dm​o​d​e​ldmodel",
            "architecture",
            "encoder",
            "value",
            "text",
            "posterior",
            "shared",
            "nh​e​a​dnhead",
            "both",
            "factor",
            "parameters",
            "layers",
            "visinger",
            "model",
            "dimension",
            "dropout",
            "svs"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Both SVS systems adopt the VISinger&#160;2 architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>. Unless otherwise specified, the two models share the same hyperparameter settings, as detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A3.T3\" title=\"Table 3 &#8227; C.1 Model Architectures &#8227; Appendix C SVS Model Training Details &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With recent advances in automatic speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS) technologies, spoken dialogue systems (SDS) have become widely accessible.\nHowever, most existing SDS are limited to conventional spoken responses.\nWe present <span class=\"ltx_text ltx_font_bold\">SingingSDS</span>, a cascaded SDS that responds through singing rather than speaking, fostering more affective, memorable, and pleasurable interactions in character-based roleplay and interactive entertainment scenarios. SingingSDS employs a modular ASR-LLM-SVS pipeline and supports a wide range of configurations across character personas, ASR and LLM backends, SVS models, melody sources, and voice profiles, tailored to different needs in terms of latency, quality, and musical style.\nSingingSDS is available as a plug-and-play web demo, featuring modular, open-source code that supports customization and extension.\nDemo: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/espnet/SingingSDS\" title=\"\">https://huggingface.co/spaces/espnet/SingingSDS</a>. Code: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SingingSDS/SingingSDS\" title=\"\">https://github.com/SingingSDS/SingingSDS</a>. Video: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://youtube.com/playlist?list=PLZpUJJbwp2WvtPBenG5D3h09qKIrt24ui&amp;si=7CSLWAYWcfkTEdqe\" title=\"\">https://youtube.com/playlist?list=PLZpUJJbwp2WvtPBenG5D3h09qKIrt24ui&amp;si=7CSLWAYWcfkTEdqe</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Singing, as a communicative modality, combines linguistic content with melody and rhythm, offering enhanced memorability and pleasure compared to speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">haiduk2020song</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gold2019musical</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zatorre2013perception</span>)</cite>, which can enrich interactive entertainment experiences. Despite significant progress in singing voice synthesis (SVS) and song generation models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024muskits-espnet</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024toksing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2024singomd</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2024visinger2+</span>)</cite>, these systems are essentially non-interactive: largely operate on predefined lyrics, lacking mechanisms for dynamic responses to user input.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we introduce <span class=\"ltx_text ltx_font_bold\">SingingSDS</span>, the first open-source system supporting speech-in, singing-out roleplay interactions for entertainment and character-driven scenarios.\nSingingSDS integrates automatic speech recognition (ASR), character-consistent response generation using large language models (LLMs), melody control with optional structural constraints, and singing voice synthesis (SVS).\nThe system is modular and configurable, including 5 ASR models, 7 LLMs, our released bilingual (Chinese-Japanese) and monolingual (Chinese-only) SVS models, and 5 melody control settings, resulting in 350 possible system configurations.\nWe conduct systematic assessment of both audio quality and user perception, supporting reproducible research on interactive singing dialogue. The system is fully open-sourced and provides an interactive web demo and a command-line interface for the creation and evaluation of speech-to-singing dialogues with fictional characters. These features support reproducible research and structured experimentation with interactive singing dialogues.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In parallel, SVS has progressed significantly in recent years with the development of neural models such as TokSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024toksing</span>)</cite>,\nDiffSinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022diffsinger</span>)</cite>, and VISinger2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>, which enable high-fidelity singing generation by modeling pitch, duration, and timbre.\nDespite the progress in both SDS and SVS, to the best of our knowledge, no prior work has integrated singing voice synthesis into an interactive spoken dialogue system.\nOur work presents the first attempt to bridge these two domains, enabling an LLM-based dialogue agent to sing its responses to the user via SVS techniques.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The generated lyrical response <math alttext=\"l\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m1\" intent=\":literal\"><semantics><mi>l</mi><annotation encoding=\"application/x-tex\">l</annotation></semantics></math> is normalized and converted into phonemes <math alttext=\"l_{\\phi}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m2\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>&#981;</mi></msub><annotation encoding=\"application/x-tex\">l_{\\phi}</annotation></semantics></math> with grapheme-to-phoneme (G2P) system. Along with a music score <math alttext=\"\\mathcal{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><annotation encoding=\"application/x-tex\">\\mathcal{N}</annotation></semantics></math> created by the melody controller module and speaker information <math alttext=\"v\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS0.SSS0.Px4.p1.m4\" intent=\":literal\"><semantics><mi>v</mi><annotation encoding=\"application/x-tex\">v</annotation></semantics></math>, either speaker embodding or speaker identity\ndepending on the model, the inputs are passed to an SVS model, producing the final sung output:</p>\n\n",
                "matched_terms": [
                    "svs",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingingSDS adopts a modular architecture with registry-based components that enable flexible integration of models, datasets, and character personas. As shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:package_structure, each core function, such as ASR, LLM, SVS, and melody loading and handling, is encapsulated as an independent module. This design supports rapid iteration, systematic benchmarking, and seamless extensibility.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system supports multiple backends for ASR, LLM, and SVS, all integrated through a registry-based modular architecture. Most ASR and LLM modules are community-pretrained. The supported SVS models are trained by us.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide two multi-speaker VISinger 2 models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>:\n(1) a Chinese SVS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain\" title=\"\">https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain</a></span></span></span>\ntrained on the ACE-Opencpop dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2024singing</span>)</cite>, and\n(2) a bilingual Mandarin-Japanese SVS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/visinger2-zh-jp-multisinger-svs\" title=\"\">https://huggingface.co/espnet/visinger2-zh-jp-multisinger-svs</a></span></span></span>\ntrained on a mixture of publicly available singing datasets, including OpenCpop&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2021opencpop</span>)</cite>, KiSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>, ACE-KiSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2024singing</span>)</cite>, M4Singer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022m4singer</span>)</cite>, Kiritan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ogawa2021tohoku</span>)</cite>, Onikuru Kurumi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">onikuru_kurumi_db</span>)</cite>, PJS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koguchi2020pjs</span>)</cite>, and Namine Ritsu&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">namineritsu_db</span>)</cite>. Details of the training configuration are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A3\" title=\"Appendix C SVS Model Training Details &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs",
                    "visinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are run on single NVIDIA v100 GPU using the cascaded pipeline shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:pipeline. We evaluate two ASR models: <span class=\"ltx_text ltx_font_typewriter\">whisper-medium</span> (OpenAI) and <span class=\"ltx_text ltx_font_typewriter\">paraformer-zh</span> (Alibaba), and two LLMs: <span class=\"ltx_text ltx_font_typewriter\">Llama-3.1-8B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-flash</span>.\nFor brevity, we refer to them as Whisper, Paraformer, Llama 3, and Gemini in the rest of this paper.\nFor singing voice synthesis (SVS), we use our bilingual pretrained VISinger 2 model.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "models",
                    "model",
                    "visinger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presents SingingSDS, a modular spoken dialogue system with melody-conditioned singing responses to user input in Chinese and Japanese. The system combines ASR, LLM, and SVS components through a prompting scheme that aligns lyric structure with melodic phrasing, without requiring model fine-tuning.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support future work, we release the pretrained SVS model used in SingingSDS, along with scripts for evaluation and dataset construction. Although other components are based on open-source APIs, the pipeline remains modular and extensible, allowing substitution of melody sources, LLMs, or SVS backends for controlled experimentation.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure responsiveness while preserving musical phrasing, we reformulate the task as melody-constrained singing response generation. Instead of relying on end-to-end text-to-song models with multi-second latencies, the system employs a lightweight melody-conditioned SVS module, achieving an SVS synthesis latency of approximately 0.16&#8211;0.19&#8201;s across our evaluated configurations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both SVS models were trained using the ESPnet GAN-SVS recipe&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>. All experiments use a waveform sampling rate of 44.1&#160;kHz. Key\ntraining hyperparameters are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A3.T4\" title=\"Table 4 &#8227; C.2 Training Procedure &#8227; Appendix C SVS Model Training Details &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "models",
                    "svs",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We self-constructed a Chinese music score corpus with lyric-level annotation, covering a total of 305 music genres. The vocal data is generated using a pipeline involving two models: lyrics and genre prompts are first produced by DeepSeek&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2024deepseek</span>)</cite>, conditioned on a specified music genre; then, the music&#8212;including separate vocal and instrumental tracks&#8212;is synthesized using the YuE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>)</cite> model, which adopts a track-decoupled next-token prediction strategy. This allows direct access to clean vocal data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation to assess the perceived quality and entertainment value of each sung response. Six listeners participated in a blind listening evaluation after providing informed consent. Participants were instructed to evaluate the samples independently, without discussion or influence from others, based on their individual perceptual judgments. Participants rate each sample on a 5-point Likert scale across three dimensions: Novelty and Fun (N&amp;F), Character Consistency (Char. Cons.), and Lyric Quality (Lyric Qual.). These criteria are designed to capture both the expressive and contextual aspects of singing dialogue. Specifically, raters assess (1) how engaging and novel the singing-based interaction feels, (2) whether the lyrical content aligns with the character&#8217;s profile and persona, and (3) the linguistic fluency, coherence, and poetic rhythm of the lyrics. This evaluation framework enables nuanced analysis of singing responses beyond vocal quality alone, with a particular focus on creativity, role embodiment, and lyricism.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "value",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments are run on NVIDIA L40S GPUs using the cascaded pipeline shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:pipeline. For singing voice synthesis (SVS), we use our bilingual pretrained VISinger 2 model. We compare three SVS variants based on melody selection: (1) <span class=\"ltx_text ltx_font_bold\">SVS-1</span>, with randomly generated durations and pitch contours; (2) <span class=\"ltx_text ltx_font_bold\">SVS-2</span>, with melodies retrieved from the KiSing dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>; and (3) <span class=\"ltx_text ltx_font_bold\">SVS-3</span>, using main melodies retrieved from a curated Touhou MIDI archive.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/AyHa1810/touhou-midi-collection\" title=\"\">https://github.com/AyHa1810/touhou-midi-collection</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "svs",
                    "visinger",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ASR component uses Whisper model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span> (16kHz)</span></span></span>, and the LLM is <span class=\"ltx_text ltx_font_typewriter\">gemma-2-2b</span>. SVS outputs are synthesized at 44.1kHz and downsampled to 16&#160;kHz for PER evaluation. Latency is reported as end-to-end wall-clock time. All models are used as-is without fine-tuning during experimentation.</p>\n\n",
                "matched_terms": [
                    "svs",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models used in the system are publicly available, including pretrained components for ASR and LLM, as well as our own SVS models. The SVS models are trained exclusively on open datasets with appropriate usage licenses. We encourage responsible and transparent use of SingingSDS for creative, educational, and research purposes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            }
        ]
    },
    "A3.T4": {
        "caption": "Table 4: Training hyperparameters shared by both SVS models.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Parameter</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Value</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Max epochs</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">500</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Batch size</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Optimizer</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">AdamW</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Learning rate</span></th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T4.m1\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2\\times 10^{-4}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Scheduler</span></th>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Exponential LR (</span><math alttext=\"\\gamma=0.998\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T4.m2\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">&#947;</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">0.998</mn></mrow><annotation encoding=\"application/x-tex\">\\gamma=0.998</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Adversarial loss</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">MSE GAN</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Mel loss weight</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">45.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Pitch loss weight</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Duration loss weight</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">KL loss weight</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "pitch",
            "optimizer",
            "exponential",
            "size",
            "parameter",
            "rate",
            "max",
            "batch",
            "learning",
            "loss",
            "training",
            "hyperparameters",
            "scheduler",
            "value",
            "γ0998gamma0998",
            "shared",
            "adamw",
            "both",
            "weight",
            "2×10−42times",
            "adversarial",
            "mse",
            "mel",
            "gan",
            "duration",
            "epochs",
            "svs"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Both SVS models were trained using the ESPnet GAN-SVS recipe&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>. All experiments use a waveform sampling rate of 44.1&#160;kHz. Key\ntraining hyperparameters are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A3.T4\" title=\"Table 4 &#8227; C.2 Training Procedure &#8227; Appendix C SVS Model Training Details &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With recent advances in automatic speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS) technologies, spoken dialogue systems (SDS) have become widely accessible.\nHowever, most existing SDS are limited to conventional spoken responses.\nWe present <span class=\"ltx_text ltx_font_bold\">SingingSDS</span>, a cascaded SDS that responds through singing rather than speaking, fostering more affective, memorable, and pleasurable interactions in character-based roleplay and interactive entertainment scenarios. SingingSDS employs a modular ASR-LLM-SVS pipeline and supports a wide range of configurations across character personas, ASR and LLM backends, SVS models, melody sources, and voice profiles, tailored to different needs in terms of latency, quality, and musical style.\nSingingSDS is available as a plug-and-play web demo, featuring modular, open-source code that supports customization and extension.\nDemo: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/espnet/SingingSDS\" title=\"\">https://huggingface.co/spaces/espnet/SingingSDS</a>. Code: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SingingSDS/SingingSDS\" title=\"\">https://github.com/SingingSDS/SingingSDS</a>. Video: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://youtube.com/playlist?list=PLZpUJJbwp2WvtPBenG5D3h09qKIrt24ui&amp;si=7CSLWAYWcfkTEdqe\" title=\"\">https://youtube.com/playlist?list=PLZpUJJbwp2WvtPBenG5D3h09qKIrt24ui&amp;si=7CSLWAYWcfkTEdqe</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Singing, as a communicative modality, combines linguistic content with melody and rhythm, offering enhanced memorability and pleasure compared to speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">haiduk2020song</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gold2019musical</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zatorre2013perception</span>)</cite>, which can enrich interactive entertainment experiences. Despite significant progress in singing voice synthesis (SVS) and song generation models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024muskits-espnet</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024toksing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2024singomd</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2024visinger2+</span>)</cite>, these systems are essentially non-interactive: largely operate on predefined lyrics, lacking mechanisms for dynamic responses to user input.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we introduce <span class=\"ltx_text ltx_font_bold\">SingingSDS</span>, the first open-source system supporting speech-in, singing-out roleplay interactions for entertainment and character-driven scenarios.\nSingingSDS integrates automatic speech recognition (ASR), character-consistent response generation using large language models (LLMs), melody control with optional structural constraints, and singing voice synthesis (SVS).\nThe system is modular and configurable, including 5 ASR models, 7 LLMs, our released bilingual (Chinese-Japanese) and monolingual (Chinese-only) SVS models, and 5 melody control settings, resulting in 350 possible system configurations.\nWe conduct systematic assessment of both audio quality and user perception, supporting reproducible research on interactive singing dialogue. The system is fully open-sourced and provides an interactive web demo and a command-line interface for the creation and evaluation of speech-to-singing dialogues with fictional characters. These features support reproducible research and structured experimentation with interactive singing dialogues.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In parallel, SVS has progressed significantly in recent years with the development of neural models such as TokSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024toksing</span>)</cite>,\nDiffSinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022diffsinger</span>)</cite>, and VISinger2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>, which enable high-fidelity singing generation by modeling pitch, duration, and timbre.\nDespite the progress in both SDS and SVS, to the best of our knowledge, no prior work has integrated singing voice synthesis into an interactive spoken dialogue system.\nOur work presents the first attempt to bridge these two domains, enabling an LLM-based dialogue agent to sing its responses to the user via SVS techniques.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "pitch",
                    "duration",
                    "both",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We support two types of melody sources.\nThe first setting consists of randomly synthesized melodies and serves as a baseline.\nThese are generated on the fly by sampling pitch and duration values uniformly, without rests or phrase-level structure. Since no reference alignment is available, a simple forced alignment is applied, assigning one syllable per note.\nThe second is sampled melodies drawn from existing song datasets. For these, we support two alignment strategies. In pitch-based alignment, each syllable is mapped one-to-one to a note in the melody. In lyric-aware alignment, one-to-many mappings are preserved: when a syllable spans multiple notes in the original song, the same structure is retained in the output, as illustrated in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:alignment_example.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingingSDS adopts a modular architecture with registry-based components that enable flexible integration of models, datasets, and character personas. As shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:package_structure, each core function, such as ASR, LLM, SVS, and melody loading and handling, is encapsulated as an independent module. This design supports rapid iteration, systematic benchmarking, and seamless extensibility.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system supports multiple backends for ASR, LLM, and SVS, all integrated through a registry-based modular architecture. Most ASR and LLM modules are community-pretrained. The supported SVS models are trained by us.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide two multi-speaker VISinger 2 models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>:\n(1) a Chinese SVS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain\" title=\"\">https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain</a></span></span></span>\ntrained on the ACE-Opencpop dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2024singing</span>)</cite>, and\n(2) a bilingual Mandarin-Japanese SVS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/visinger2-zh-jp-multisinger-svs\" title=\"\">https://huggingface.co/espnet/visinger2-zh-jp-multisinger-svs</a></span></span></span>\ntrained on a mixture of publicly available singing datasets, including OpenCpop&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2021opencpop</span>)</cite>, KiSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>, ACE-KiSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2024singing</span>)</cite>, M4Singer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022m4singer</span>)</cite>, Kiritan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ogawa2021tohoku</span>)</cite>, Onikuru Kurumi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">onikuru_kurumi_db</span>)</cite>, PJS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koguchi2020pjs</span>)</cite>, and Namine Ritsu&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">namineritsu_db</span>)</cite>. Details of the training configuration are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A3\" title=\"Appendix C SVS Model Training Details &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments are run on single NVIDIA v100 GPU using the cascaded pipeline shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:pipeline. We evaluate two ASR models: <span class=\"ltx_text ltx_font_typewriter\">whisper-medium</span> (OpenAI) and <span class=\"ltx_text ltx_font_typewriter\">paraformer-zh</span> (Alibaba), and two LLMs: <span class=\"ltx_text ltx_font_typewriter\">Llama-3.1-8B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-flash</span>.\nFor brevity, we refer to them as Whisper, Paraformer, Llama 3, and Gemini in the rest of this paper.\nFor singing voice synthesis (SVS), we use our bilingual pretrained VISinger 2 model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We acknowledge illustrator Zihe Zhou for the creation of Yaoyin&#8217;s character artwork, which is included in the demo page shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:demo_ui. The artwork was commissioned exclusively for the SingingSDS project and may be used for direct derivatives of SingingSDS, such as project-related posts or usage videos, without additional permission. Any other use requires express permission from the illustrator. Use of the artwork for training or fine-tuning artificial intelligence or machine learning models is strictly prohibited.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "models",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure responsiveness while preserving musical phrasing, we reformulate the task as melody-constrained singing response generation. Instead of relying on end-to-end text-to-song models with multi-second latencies, the system employs a lightweight melody-conditioned SVS module, achieving an SVS synthesis latency of approximately 0.16&#8211;0.19&#8201;s across our evaluated configurations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both SVS systems adopt the VISinger&#160;2 architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>. Unless otherwise specified, the two models share the same hyperparameter settings, as detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A3.T3\" title=\"Table 3 &#8227; C.1 Model Architectures &#8227; Appendix C SVS Model Training Details &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation to assess the perceived quality and entertainment value of each sung response. Six listeners participated in a blind listening evaluation after providing informed consent. Participants were instructed to evaluate the samples independently, without discussion or influence from others, based on their individual perceptual judgments. Participants rate each sample on a 5-point Likert scale across three dimensions: Novelty and Fun (N&amp;F), Character Consistency (Char. Cons.), and Lyric Quality (Lyric Qual.). These criteria are designed to capture both the expressive and contextual aspects of singing dialogue. Specifically, raters assess (1) how engaging and novel the singing-based interaction feels, (2) whether the lyrical content aligns with the character&#8217;s profile and persona, and (3) the linguistic fluency, coherence, and poetic rhythm of the lyrics. This evaluation framework enables nuanced analysis of singing responses beyond vocal quality alone, with a particular focus on creativity, role embodiment, and lyricism.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "value",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use phoneme error rate (PER) to measure how accurately the system preserves linguistic content. Outputs are transcribed using Whisper-turbo and aligned at the phoneme level with ground-truth references. PER is preferred over character error rate for singing, which often involves pitch variation and extended vowels.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments are run on NVIDIA L40S GPUs using the cascaded pipeline shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:pipeline. For singing voice synthesis (SVS), we use our bilingual pretrained VISinger 2 model. We compare three SVS variants based on melody selection: (1) <span class=\"ltx_text ltx_font_bold\">SVS-1</span>, with randomly generated durations and pitch contours; (2) <span class=\"ltx_text ltx_font_bold\">SVS-2</span>, with melodies retrieved from the KiSing dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>; and (3) <span class=\"ltx_text ltx_font_bold\">SVS-3</span>, using main melodies retrieved from a curated Touhou MIDI archive.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/AyHa1810/touhou-midi-collection\" title=\"\">https://github.com/AyHa1810/touhou-midi-collection</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "svs",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ASR component uses Whisper model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span> (16kHz)</span></span></span>, and the LLM is <span class=\"ltx_text ltx_font_typewriter\">gemma-2-2b</span>. SVS outputs are synthesized at 44.1kHz and downsampled to 16&#160;kHz for PER evaluation. Latency is reported as end-to-end wall-clock time. All models are used as-is without fine-tuning during experimentation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models used in the system are publicly available, including pretrained components for ASR and LLM, as well as our own SVS models. The SVS models are trained exclusively on open datasets with appropriate usage licenses. We encourage responsible and transparent use of SingingSDS for creative, educational, and research purposes.</p>\n\n",
                "matched_terms": [
                    "models",
                    "svs"
                ]
            }
        ]
    },
    "A6.T5": {
        "caption": "Table 5: Evaluation on KdConv (450 utterances). All singing systems outperform the TTS baseline in SingMOS while maintaining comparable intelligibility. MOS scores are pending human evaluation.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">System</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">SingMOS</span>&#8201;<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">PER (%)</span>&#8201;<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Latency (s)</span>&#8201;<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Jump Ratio (%)</span>&#8201;<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T5.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SVS-1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SVS-2 KiSing</td>\n<td class=\"ltx_td ltx_align_center\">4.27</td>\n<td class=\"ltx_td ltx_align_center\">36</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.02</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\">SVS-3 Touhou</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">4.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">0.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">12</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "singing",
            "while",
            "singmos",
            "utterances",
            "evaluation",
            "↓downarrow",
            "comparable",
            "mos",
            "baseline",
            "tts",
            "systems",
            "kising",
            "system",
            "ratio",
            "scores",
            "pending",
            "svs3",
            "jump",
            "↑uparrow",
            "svs1",
            "all",
            "kdconv",
            "outperform",
            "human",
            "latency",
            "svs2",
            "touhou",
            "maintaining",
            "intelligibility"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With recent advances in automatic speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS) technologies, spoken dialogue systems (SDS) have become widely accessible.\nHowever, most existing SDS are limited to conventional spoken responses.\nWe present <span class=\"ltx_text ltx_font_bold\">SingingSDS</span>, a cascaded SDS that responds through singing rather than speaking, fostering more affective, memorable, and pleasurable interactions in character-based roleplay and interactive entertainment scenarios. SingingSDS employs a modular ASR-LLM-SVS pipeline and supports a wide range of configurations across character personas, ASR and LLM backends, SVS models, melody sources, and voice profiles, tailored to different needs in terms of latency, quality, and musical style.\nSingingSDS is available as a plug-and-play web demo, featuring modular, open-source code that supports customization and extension.\nDemo: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/espnet/SingingSDS\" title=\"\">https://huggingface.co/spaces/espnet/SingingSDS</a>. Code: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/SingingSDS/SingingSDS\" title=\"\">https://github.com/SingingSDS/SingingSDS</a>. Video: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://youtube.com/playlist?list=PLZpUJJbwp2WvtPBenG5D3h09qKIrt24ui&amp;si=7CSLWAYWcfkTEdqe\" title=\"\">https://youtube.com/playlist?list=PLZpUJJbwp2WvtPBenG5D3h09qKIrt24ui&amp;si=7CSLWAYWcfkTEdqe</a>.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "latency",
                    "tts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Singing, as a communicative modality, combines linguistic content with melody and rhythm, offering enhanced memorability and pleasure compared to speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">haiduk2020song</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">gold2019musical</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zatorre2013perception</span>)</cite>, which can enrich interactive entertainment experiences. Despite significant progress in singing voice synthesis (SVS) and song generation models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024muskits-espnet</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024toksing</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2024singomd</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yu2024visinger2+</span>)</cite>, these systems are essentially non-interactive: largely operate on predefined lyrics, lacking mechanisms for dynamic responses to user input.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we introduce <span class=\"ltx_text ltx_font_bold\">SingingSDS</span>, the first open-source system supporting speech-in, singing-out roleplay interactions for entertainment and character-driven scenarios.\nSingingSDS integrates automatic speech recognition (ASR), character-consistent response generation using large language models (LLMs), melody control with optional structural constraints, and singing voice synthesis (SVS).\nThe system is modular and configurable, including 5 ASR models, 7 LLMs, our released bilingual (Chinese-Japanese) and monolingual (Chinese-only) SVS models, and 5 melody control settings, resulting in 350 possible system configurations.\nWe conduct systematic assessment of both audio quality and user perception, supporting reproducible research on interactive singing dialogue. The system is fully open-sourced and provides an interactive web demo and a command-line interface for the creation and evaluation of speech-to-singing dialogues with fictional characters. These features support reproducible research and structured experimentation with interactive singing dialogues.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "evaluation",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingingSDS establishes a foundation for investigating singing as an interactive response modality beyond conventional spoken dialogue. The system has potential applications in VR concerts and other virtual performances, interactive music games and theme park attractions, and live streaming with audience participation. Through singing responses, SingingSDS can enhance these applications, offering more memorable and enjoyable user experiences, while also providing a platform for empirical studies of singing-based dialogue.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "while",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In parallel, SVS has progressed significantly in recent years with the development of neural models such as TokSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wu2024toksing</span>)</cite>,\nDiffSinger&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">liu2022diffsinger</span>)</cite>, and VISinger2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>, which enable high-fidelity singing generation by modeling pitch, duration, and timbre.\nDespite the progress in both SDS and SVS, to the best of our knowledge, no prior work has integrated singing voice synthesis into an interactive spoken dialogue system.\nOur work presents the first attempt to bridge these two domains, enabling an LLM-based dialogue agent to sing its responses to the user via SVS techniques.\n</p>\n\n",
                "matched_terms": [
                    "singing",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One of the key challenges in equipping LLM-based spoken dialogue systems with singing capabilities lies in evaluation. While various metrics have been proposed to assess synthesized speech and singing quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">utmos</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">umbert2015expression</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2024singmos</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2025versa</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2024versaversatileevaluationtoolkit</span>)</cite>, existing tools often fail to account for the entertainment value conveyed through singing or speech.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "evaluation",
                    "while",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, model-based metrics such as Meta AudioBox Aesthetics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tjandra2025meta</span>)</cite> did not consistently align with human preferences, and in some cases favored randomly generated, inharmonic note sequences over well-structured melodies. To better capture the aspects of engagement and enjoyability, we conducted human evaluations focusing on perceived enjoyment. Additionally, we report coarse melodic statistics, such as the large jump ratio, to quantify pitch dynamics in the generated singing outputs. Together, these complementary metrics offer a more holistic perspective on melody-conditioned dialogue generation (<span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:metrics).</p>\n\n",
                "matched_terms": [
                    "singing",
                    "ratio",
                    "human",
                    "jump"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To encourage structural alignment between the generated textual response and the melody, the system constructs an LLM prompt specifying the required syllable count per musical phrase when phrase annotations are available (e.g., in the KiSing dataset and our self-constructed melody datasets synthesized with Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>)</cite>).\nThis alignment serves as a soft constraint for the LLM, encouraging outputs that match the expected number of musical events and exhibit more coherent phrase-level structure.\nDetails on the phrase-constrained prompt used for LLM generation are provided in <span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:melody_prompt.</p>\n\n",
                "matched_terms": [
                    "kising",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system supports multiple backends for ASR, LLM, and SVS, all integrated through a registry-based modular architecture. Most ASR and LLM modules are community-pretrained. The supported SVS models are trained by us.</p>\n\n",
                "matched_terms": [
                    "all",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide two multi-speaker VISinger 2 models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022visinger2</span>)</cite>:\n(1) a Chinese SVS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain\" title=\"\">https://huggingface.co/espnet/aceopencpop_svs_visinger2_40singer_pretrain</a></span></span></span>\ntrained on the ACE-Opencpop dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2024singing</span>)</cite>, and\n(2) a bilingual Mandarin-Japanese SVS model<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/visinger2-zh-jp-multisinger-svs\" title=\"\">https://huggingface.co/espnet/visinger2-zh-jp-multisinger-svs</a></span></span></span>\ntrained on a mixture of publicly available singing datasets, including OpenCpop&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">huang2021opencpop</span>)</cite>, KiSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>, ACE-KiSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2024singing</span>)</cite>, M4Singer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhang2022m4singer</span>)</cite>, Kiritan&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ogawa2021tohoku</span>)</cite>, Onikuru Kurumi&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">onikuru_kurumi_db</span>)</cite>, PJS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koguchi2020pjs</span>)</cite>, and Namine Ritsu&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">namineritsu_db</span>)</cite>. Details of the training configuration are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.20972v1#A3\" title=\"Appendix C SVS Model Training Details &#8227; 6 Conclusion &#8227; 5.3 Results and Discussion &#8227; 5 Evaluation &#8227; 4.4 Deployment and Access &#8227; 4 Demonstration &#8227; SVS. &#8227; Melody Control. &#8227; LLM. &#8227; ASR. &#8227; 3 System Design &#8227; SingingSDS: A Singing-Capable Spoken Dialogue System for Conversational Roleplay Applications\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "kising"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The system supports retrieval from three melody dataset in addition to randomly generated melodies: KiSing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>, a Touhou MIDI collection<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/AyHa1810/touhou-midi-collection\" title=\"\">https://github.com/AyHa1810/touhou-midi-collection</a></span></span></span>, and a synthesized dataset of 499 songs generated using Yue, constructed to expand the melody database (see <span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:synthesized_melody for details). These datasets provide melodies that condition the singing output. A registry-based handler module loads and converts each melody into a format suitable for synthesis, allowing new datasets to be integrated with minimal effort.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "kising",
                    "touhou",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system supports two original singing characters, Limei and Yaoyin, each defined by a prompt-based persona, as specified in <span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:character_prompt. Both characters are drawn from our original fictional universe, <span class=\"ltx_text ltx_font_italic\">Changge Plains</span>, designed to support immersive roleplay interaction and storytelling. New characters can be added by specifying prompt configurations.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingingSDS is available as an interactive web demo hosted on Hugging Face Spaces.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/espnet/SingingSDS\" title=\"\">https://huggingface.co/spaces/espnet/SingingSDS</a></span></span></span>\nUsers can initiate dialogue by speaking into a microphone. The system transcribes the input, generates an in-character lyrical response, and synthesizes a singing reply. The interface displays synchronized lyrics, character portraits, and playback controls, shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:demo_ui. Users can switch between characters (e.g., Limei and Yaoyin) and different model and melody configurations.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the system from multiple perspectives, including perceptual quality, linguistic accuracy, melodic structure, and runtime efficiency through automated or human evaluation. Detailed explanations on evaluation setups can be found in <span class=\"ltx_ERROR undefined\">\\appendixref</span>apd:eval.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The evaluation module is fully integrated into the system and can be triggered directly through the user interface or computed with our CLI command.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All audio outputs are resampled to 16 kHz for ASR-based intelligibility evaluation (i.e., PER) and kept at 44.1 kHz for subjective MOS testing. For melody selection, we use scores retrieved from the KiSing dataset and a curated archive of Touhou MIDI files.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "all",
                    "evaluation",
                    "kising",
                    "touhou",
                    "intelligibility",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our system under multiple configurations of the ASR, LLM, and melody generation modules. <span class=\"ltx_ERROR undefined\">\\tableref</span>tab:eval_results presents the performance across combinations of Whisper and Paraformer (ASR), LLaMA 3 and Gemini (LLM), and KiSing and Touhou (melody). The Whisper + Gemini configuration achieves the highest overall perceptual quality and entertainment value, as indicated by automatic singing quality scores (SingMOS) and human evaluations of novelty and fun (N&amp;F), character consistency (Char. Cons.), and lyric quality (Lyric Qual.). In contrast, the Paraformer + LLaMA 3 setting yields the lowest system latency, making it more suitable for interactive scenarios.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "singmos",
                    "latency",
                    "kising",
                    "human",
                    "system",
                    "touhou",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presents SingingSDS, a modular spoken dialogue system with melody-conditioned singing responses to user input in Chinese and Japanese. The system combines ASR, LLM, and SVS components through a prompting scheme that aligns lyric structure with melodic phrasing, without requiring model fine-tuning.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluation across perceptual quality, intelligibility, latency, and melodic dynamics confirms the feasibility of singing-based interaction in dialogue systems. Subjective ratings indicate that appropriate melody selection improves perceived entertainment value without compromising intelligibility.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "intelligibility",
                    "latency",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SingingSDS constitutes the first fully implemented pipeline for interactive dialogue system with singing virtual characters. By bridging conversational AI and singing synthesis, it enables a novel form of interactive response grounded in melody and persona. Our system opens new research directions in controllable singing generation, expressive speech interfaces, and musical human-computer interaction.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The system is designed for character-based voiced interactive experiences, in which virtual characters respond to user prompts by singing. This requires generating semantically appropriate replies and synthesizing them as singing audio with melodic structure and consistent character voice.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initially considered direct text-to-song generation, where singing audio is synthesized end-to-end from LLM responses without a predefined melody. However, existing music generation models introduce substantial latency; for example, in our test, Yue&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yuan2025yue</span>)</cite> required over 40 seconds to generate a 5-second audio clip on a T4 GPU, making such methods impractical for interactive use.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure responsiveness while preserving musical phrasing, we reformulate the task as melody-constrained singing response generation. Instead of relying on end-to-end text-to-song models with multi-second latencies, the system employs a lightweight melody-conditioned SVS module, achieving an SVS synthesis latency of approximately 0.16&#8211;0.19&#8201;s across our evaluated configurations.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "while",
                    "latency",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate SingingSDS across four dimensions: intelligibility, latency, melodic dynamics, and perceptual quality. The last is further divided into two distinct aspects: singing naturalness, overall content quality and entertainment.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "intelligibility",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report SingMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tang2024singmos</span>)</cite>, a model-predicted score trained on crowd-annotated singing data. It reflects vocal quality, articulation, and how closely the output resembles natural singing. SingMOS enables consistent comparison across different SVS backends without requiring additional annotation.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "singmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a human evaluation to assess the perceived quality and entertainment value of each sung response. Six listeners participated in a blind listening evaluation after providing informed consent. Participants were instructed to evaluate the samples independently, without discussion or influence from others, based on their individual perceptual judgments. Participants rate each sample on a 5-point Likert scale across three dimensions: Novelty and Fun (N&amp;F), Character Consistency (Char. Cons.), and Lyric Quality (Lyric Qual.). These criteria are designed to capture both the expressive and contextual aspects of singing dialogue. Specifically, raters assess (1) how engaging and novel the singing-based interaction feels, (2) whether the lyrical content aligns with the character&#8217;s profile and persona, and (3) the linguistic fluency, coherence, and poetic rhythm of the lyrics. This evaluation framework enables nuanced analysis of singing responses beyond vocal quality alone, with a particular focus on creativity, role embodiment, and lyricism.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use phoneme error rate (PER) to measure how accurately the system preserves linguistic content. Outputs are transcribed using Whisper-turbo and aligned at the phoneme level with ground-truth references. PER is preferred over character error rate for singing, which often involves pitch variation and extended vowels.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We report end-to-end wall-clock latency (Lat.) from user input to synthesized audio, including all components (ASR, LLM, SVS). To account for variable output durations, latency is normalized by the number of input tokens. All measurements are conducted on NVIDIA L40S GPUs.</p>\n\n",
                "matched_terms": [
                    "all",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify pitch movement, we compute the large jump ratio (Jump R.), the proportion of adjacent notes differing by more than five semitones:</p>\n\n",
                "matched_terms": [
                    "ratio",
                    "jump"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We sample 450 questions from the KdConv dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">zhou2020kdconv</span>)</cite>&#8217;s test split and synthesize the audio with a VITS-based TTS system<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/espnet/kan-bayashi_csmsc_vits\" title=\"\">https://huggingface.co/espnet/kan-bayashi_csmsc_vits</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "kdconv",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments are run on NVIDIA L40S GPUs using the cascaded pipeline shown in <span class=\"ltx_ERROR undefined\">\\figureref</span>fig:pipeline. For singing voice synthesis (SVS), we use our bilingual pretrained VISinger 2 model. We compare three SVS variants based on melody selection: (1) <span class=\"ltx_text ltx_font_bold\">SVS-1</span>, with randomly generated durations and pitch contours; (2) <span class=\"ltx_text ltx_font_bold\">SVS-2</span>, with melodies retrieved from the KiSing dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">shi2022muskits</span>)</cite>; and (3) <span class=\"ltx_text ltx_font_bold\">SVS-3</span>, using main melodies retrieved from a curated Touhou MIDI archive.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/AyHa1810/touhou-midi-collection\" title=\"\">https://github.com/AyHa1810/touhou-midi-collection</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "singing",
                    "svs1",
                    "svs3",
                    "all",
                    "kising",
                    "touhou",
                    "svs2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ASR component uses Whisper model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span> (16kHz)</span></span></span>, and the LLM is <span class=\"ltx_text ltx_font_typewriter\">gemma-2-2b</span>. SVS outputs are synthesized at 44.1kHz and downsampled to 16&#160;kHz for PER evaluation. Latency is reported as end-to-end wall-clock time. All models are used as-is without fine-tuning during experimentation.</p>\n\n",
                "matched_terms": [
                    "all",
                    "evaluation",
                    "latency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">tab:kdconv summarizes performance on our sampled KdConv test sets. All SVS variants outperform the TTS baseline in perceived naturalness (SingMOS), with minor differences in intelligibility (PER within 4 percentage points).</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "tts",
                    "singmos",
                    "all",
                    "kdconv",
                    "outperform",
                    "intelligibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On <span class=\"ltx_text ltx_font_bold\">KdConv</span>, <span class=\"ltx_text ltx_font_bold\">SVS-1</span> (random melody) achieves the highest SingMOS and lowest PER.\nThis suggests that, for general domain utterances, randomly generated melodic patterns are sufficient to produce appealing singing output.\nHowever, its melodic contours are more varied, resulting in larger pitch jumps.</p>\n\n",
                "matched_terms": [
                    "singing",
                    "singmos",
                    "svs1",
                    "kdconv",
                    "utterances"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SVS-2</span> (KiSing) yields the smoothest melodic transitions but shows higher PER, possibly due to slower note progressions that affect phoneme clarity. This trade-off suggests that melody selection should be context-aware: expressive, wide-range melodies may enhance persona-rich dialogue, while flatter contours may suit more neutral interactions.</p>\n\n",
                "matched_terms": [
                    "svs2",
                    "while",
                    "kising"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We emphasize transparency and user control. The web demo is publicly accessible via Hugging Face Spaces, and by default it does not collect or store any user data. All audio and text inputs are processed locally in memory and discarded after response generation. The system does not log, transmit, or retain user data without explicit user awareness. If future researchers extend the system with logging or evaluation tools, they are responsible for obtaining appropriate consent from participants.</p>\n\n",
                "matched_terms": [
                    "all",
                    "evaluation",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models used in the system are publicly available, including pretrained components for ASR and LLM, as well as our own SVS models. The SVS models are trained exclusively on open datasets with appropriate usage licenses. We encourage responsible and transparent use of SingingSDS for creative, educational, and research purposes.</p>\n\n",
                "matched_terms": [
                    "all",
                    "system"
                ]
            }
        ]
    }
}