{
    "S4.T1": {
        "source_file": "Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models",
        "caption": "Table 1: Pruning experiments for different thresholds (Œ∏w\\theta_{w}) : Œ∏w1={max{T1,T2}\\theta_{w}^{1}=\\{\\max\\{T_{1},T_{2}\\} for layers 0-5; T2T_{2} for layers 6-11} ; Œ∏w3={0.1œÉ\\theta_{w}^{3}=\\{0.1\\sigma for ùí´1\\mathcal{P}_{1}; 0.1‚ÄãT10.1T_{1} for ATT; 0.2‚ÄãT20.2T_{2} for FC}; Œ∏w5={max{T1,T2}\\theta_{w}^{5}=\\{\\max\\{T_{1},T_{2}\\} for layers 0-4; T2T_{2} for layers 5-23}",
        "body": "Encoder\nDecoder\nPruning\nWER\n\n\n\n\nNet-1\n\n\nT3T_{3}\nT3T_{3}\n41.7%\n26.24\n\n\nT2T_{2}\nT3T_{3}\n44.9%\n26.75\n\n\nŒ∏w1\\theta_{w}^{1}\nT3T_{3}\n35.4%\n24.84\n\n\n\n\nNet-2\n\n\nT3T_{3}\nT3T_{3}\n38.4%\n38.18\n\n\nŒ∏w2\\theta_{w}^{2}\nT3T_{3}\n32.2%\n32.86\n\n\nŒ∏w3\\theta_{w}^{3}\nT3T_{3}\n28.5%\n34.35\n\n\nŒ∏w4\\theta_{w}^{4}\nT3T_{3}\n32.7%\n33.0\n\n\n\n\nNet-3\n\n\nT3T_{3}\nT3T_{3}\n31.0%\n19.58\n\n\nT2T_{2}\nT3T_{3}\n38.9%\n21.15\n\n\nŒ∏w5\\theta_{w}^{5}\nT2T_{2}\n51.5%\n27.8\n\n\n\n\n\n\n\n\nŒ∏w2={0.1‚ÄãœÉ;ùí´10.1‚ÄãT1;Enc-ATT0.1‚ÄãT2;Dec-ATT0.2‚ÄãT1;Enc-FCT2;Dec-FC‚ÄãŒ∏w4={0.1‚ÄãœÉ;ùí´10.1‚ÄãT1;Enc-ATT0.5‚ÄãT2;Dec-ATT0.2‚ÄãT1;Enc-FCT2;Dec-FC\\theta_{w}^{2}=\\begin{cases}0.1\\sigma&\\text{;$\\mathcal{P}_{1}$}\\\\\n0.1T_{1}&\\text{;Enc-ATT}\\\\\n0.1T_{2}&\\text{;Dec-ATT}\\\\\n0.2T_{1}&\\text{;Enc-FC}\\\\\nT_{2}&\\text{;Dec-FC}\\\\\n\\end{cases}\\quad\\theta_{w}^{4}=\\begin{cases}0.1\\sigma&\\text{;$\\mathcal{P}_{1}$}\\\\\n0.1T_{1}&\\text{;Enc-ATT}\\\\\n0.5T_{2}&\\text{;Dec-ATT}\\\\\n0.2T_{1}&\\text{;Enc-FC}\\\\\nT_{2}&\\text{;Dec-FC}\\\\\n\\end{cases}\n\n\n\n\n\n\nWhisper-small with IMP[10]\n\n16.7%\n37.15\n\n\nWhisper-medium with IMP[10]\n\n20.29%\n30.49",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Encoder</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Decoder</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Pruning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:24.2pt;vertical-align:-8.7pt;\"><span class=\"ltx_transformed_inner\" style=\"width:24.2pt;transform:translate(-8.7pt,-8.7pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">Net-1</p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">41.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">26.24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">T_{2}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">44.9%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">26.75</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"\\theta_{w}^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><msubsup><mi>&#952;</mi><mi>w</mi><mn>1</mn></msubsup><annotation encoding=\"application/x-tex\">\\theta_{w}^{1}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">35.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">24.84</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"4\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:24.2pt;vertical-align:-8.7pt;\"><span class=\"ltx_transformed_inner\" style=\"width:24.2pt;transform:translate(-8.7pt,-8.7pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">Net-2</p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">38.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">38.18</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"\\theta_{w}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><msubsup><mi>&#952;</mi><mi>w</mi><mn>2</mn></msubsup><annotation encoding=\"application/x-tex\">\\theta_{w}^{2}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m10\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"\\theta_{w}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m11\" intent=\":literal\"><semantics><msubsup><mi>&#952;</mi><mi>w</mi><mn>3</mn></msubsup><annotation encoding=\"application/x-tex\">\\theta_{w}^{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m12\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">28.5%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">34.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"\\theta_{w}^{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m13\" intent=\":literal\"><semantics><msubsup><mi>&#952;</mi><mi>w</mi><mn>4</mn></msubsup><annotation encoding=\"application/x-tex\">\\theta_{w}^{4}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m14\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">33.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:24.2pt;vertical-align:-8.7pt;\"><span class=\"ltx_transformed_inner\" style=\"width:24.2pt;transform:translate(-8.7pt,-8.7pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">Net-3</p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m15\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m16\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">31.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">19.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m17\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">T_{2}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m18\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">38.9%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">21.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"\\theta_{w}^{5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m19\" intent=\":literal\"><semantics><msubsup><mi>&#952;</mi><mi>w</mi><mn>5</mn></msubsup><annotation encoding=\"application/x-tex\">\\theta_{w}^{5}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math alttext=\"T_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m20\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">T_{2}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">51.5%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">27.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_l ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" colspan=\"4\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_middle\" style=\"width:346.9pt;\">\n<span class=\"ltx_equation ltx_eqn_table\" id=\"S4.Ex1\">\n<span><span class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<span class=\"ltx_eqn_cell ltx_eqn_center_padleft\"/>\n<span class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"\\theta_{w}^{2}=\\begin{cases}0.1\\sigma&amp;\\text{;$\\mathcal{P}_{1}$}\\\\&#10;0.1T_{1}&amp;\\text{;Enc-ATT}\\\\&#10;0.1T_{2}&amp;\\text{;Dec-ATT}\\\\&#10;0.2T_{1}&amp;\\text{;Enc-FC}\\\\&#10;T_{2}&amp;\\text{;Dec-FC}\\\\&#10;\\end{cases}\\quad\\theta_{w}^{4}=\\begin{cases}0.1\\sigma&amp;\\text{;$\\mathcal{P}_{1}$}\\\\&#10;0.1T_{1}&amp;\\text{;Enc-ATT}\\\\&#10;0.5T_{2}&amp;\\text{;Dec-ATT}\\\\&#10;0.2T_{1}&amp;\\text{;Enc-FC}\\\\&#10;T_{2}&amp;\\text{;Dec-FC}\\\\&#10;\\end{cases}\" class=\"ltx_Math\" display=\"block\" id=\"S4.Ex1.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi mathsize=\"0.800em\">&#952;</mi><mi mathsize=\"0.800em\">w</mi><mn mathsize=\"0.800em\">2</mn></msubsup><mo mathsize=\"0.800em\">=</mo><mrow><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mn mathsize=\"0.800em\">0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.800em\">&#963;</mi></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mtext mathsize=\"0.800em\">;</mtext><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.800em\">&#119979;</mi><mn mathsize=\"0.800em\">1</mn></msub></mrow></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mn mathsize=\"0.800em\">0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi mathsize=\"0.800em\">T</mi><mn mathsize=\"0.800em\">1</mn></msub></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mtext mathsize=\"0.800em\">;Enc-ATT</mtext></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mn mathsize=\"0.800em\">0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi mathsize=\"0.800em\">T</mi><mn mathsize=\"0.800em\">2</mn></msub></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mtext mathsize=\"0.800em\">;Dec-ATT</mtext></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mn mathsize=\"0.800em\">0.2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi mathsize=\"0.800em\">T</mi><mn mathsize=\"0.800em\">1</mn></msub></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mtext mathsize=\"0.800em\">;Enc-FC</mtext></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><msub><mi mathsize=\"0.800em\">T</mi><mn mathsize=\"0.800em\">2</mn></msub></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mtext mathsize=\"0.800em\">;Dec-FC</mtext></mtd></mtr></mtable></mrow><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi mathsize=\"0.800em\">&#952;</mi><mi mathsize=\"0.800em\">w</mi><mn mathsize=\"0.800em\">4</mn></msubsup></mrow><mo mathsize=\"0.800em\">=</mo><mrow><mo>{</mo><mtable columnspacing=\"5pt\" displaystyle=\"true\" rowspacing=\"0pt\"><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mn mathsize=\"0.800em\">0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.800em\">&#963;</mi></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mtext mathsize=\"0.800em\">;</mtext><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.800em\">&#119979;</mi><mn mathsize=\"0.800em\">1</mn></msub></mrow></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mn mathsize=\"0.800em\">0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi mathsize=\"0.800em\">T</mi><mn mathsize=\"0.800em\">1</mn></msub></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mtext mathsize=\"0.800em\">;Enc-ATT</mtext></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mn mathsize=\"0.800em\">0.5</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi mathsize=\"0.800em\">T</mi><mn mathsize=\"0.800em\">2</mn></msub></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mtext mathsize=\"0.800em\">;Dec-ATT</mtext></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><mrow><mn mathsize=\"0.800em\">0.2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi mathsize=\"0.800em\">T</mi><mn mathsize=\"0.800em\">1</mn></msub></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mtext mathsize=\"0.800em\">;Enc-FC</mtext></mtd></mtr><mtr><mtd class=\"ltx_align_left\" columnalign=\"left\"><msub><mi mathsize=\"0.800em\">T</mi><mn mathsize=\"0.800em\">2</mn></msub></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\"><mtext mathsize=\"0.800em\">;Dec-FC</mtext></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\">\\theta_{w}^{2}=\\begin{cases}0.1\\sigma&amp;\\text{;$\\mathcal{P}_{1}$}\\\\\n0.1T_{1}&amp;\\text{;Enc-ATT}\\\\\n0.1T_{2}&amp;\\text{;Dec-ATT}\\\\\n0.2T_{1}&amp;\\text{;Enc-FC}\\\\\nT_{2}&amp;\\text{;Dec-FC}\\\\\n\\end{cases}\\quad\\theta_{w}^{4}=\\begin{cases}0.1\\sigma&amp;\\text{;$\\mathcal{P}_{1}$}\\\\\n0.1T_{1}&amp;\\text{;Enc-ATT}\\\\\n0.5T_{2}&amp;\\text{;Dec-ATT}\\\\\n0.2T_{1}&amp;\\text{;Enc-FC}\\\\\nT_{2}&amp;\\text{;Dec-FC}\\\\\n\\end{cases}</annotation></semantics></math></span>\n<span class=\"ltx_eqn_cell ltx_eqn_center_padright\"/></span></span>\n</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" colspan=\"3\">Whisper-small with IMP<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib10\" title=\"\">10</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">37.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" colspan=\"3\">Whisper-medium with IMP<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib10\" title=\"\">10</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">20.29%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">30.49</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "Œ∏wthetaw",
            "01t2textdecatt",
            "t2t2",
            "t3t3",
            "Œ∏w4thetaw4",
            "net2",
            "Œ∏w301œÉthetaw301sigma",
            "encoder",
            "02t1textencfc",
            "whispersmall",
            "decoder",
            "net3",
            "Œ∏w5thetaw5",
            "endcases",
            "Œ∏w5maxt1t2thetaw5maxt1t2",
            "net1",
            "wer",
            "t2textdecfc",
            "Œ∏w2thetaw2",
            "layers",
            "endcasesquadthetaw4begincases01sigmatextmathcalp1",
            "01t1textencatt",
            "01‚Äãt101t1",
            "05t2textdecatt",
            "02‚Äãt202t2",
            "Œ∏w1thetaw1",
            "Œ∏w3thetaw3",
            "experiments",
            "different",
            "Œ∏w201‚ÄãœÉùí´101‚Äãt1encatt01‚Äãt2decatt02‚Äãt1encfct2decfc‚ÄãŒ∏w401‚ÄãœÉùí´101‚Äãt1encatt05‚Äãt2decatt02‚Äãt1encfct2decfcthetaw2begincases01sigmatextmathcalp1",
            "pruning",
            "Œ∏w1maxt1t2thetaw1maxt1t2",
            "thresholds",
            "imp10",
            "whispermedium",
            "att",
            "ùí´1mathcalp1"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our method obtains WER of 23.32, 23.94 and 15.86, on Net-1, 2, 3, respectively, with SGL regularization before pruning. We conduct more experiments with various combinations of <math alttext=\"T_{1},T_{2},T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">T_{1},T_{2},T_{3}</annotation></semantics></math> to set <math alttext=\"\\theta_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{w}</annotation></semantics></math> for the Net-1,2, 3 to provide the tradeoff between WER and pruning <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>. A few experimental results are reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S4.T1\" title=\"Table 1 &#8227; 4 RESULTS AND DISCUSSION &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Net-1 gives lowest WER of <math alttext=\"24.84\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\"><semantics><mn>24.84</mn><annotation encoding=\"application/x-tex\">24.84</annotation></semantics></math> with 35.4% reduction in parameters. The &#8221;lowest WER&#8221; refers to the smallest WER obtained among all the experiments conducted for a model. The lowest WER reported for Net-2 is 32.86 with 32.2% reduction in parameters. This indicates that all layers must not be regularized as non-FC layers capture significant information about the ASR learning. Hence, we refrain from performing regularization on all layers of Whisper-medium, but rather only on FC layers. Net-3 gives lowest WER of 19.58 when being pruned by <math alttext=\"31\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\"><semantics><mrow><mn>31</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">31\\%</annotation></semantics></math> of parameters. All our models substantially outperform the baseline IMP-based method both in terms of pruning % and WER. Specifically, Net-1 prunes 18.7% more parameters with a WER reduction of 12.31 when compared to IMP on Whisper-small. Similarly, on Whisper-medium, Net-3 achieves a WER reduction by 10.91 with 10.71% more pruning. Further, in all our three networks, one can achieve a greater pruning <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m6\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> of upto <math alttext=\"44.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m7\" intent=\":literal\"><semantics><mn>44.9</mn><annotation encoding=\"application/x-tex\">44.9</annotation></semantics></math>, <math alttext=\"38.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m8\" intent=\":literal\"><semantics><mn>38.4</mn><annotation encoding=\"application/x-tex\">38.4</annotation></semantics></math>, and <math alttext=\"51.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m9\" intent=\":literal\"><semantics><mn>51.5</mn><annotation encoding=\"application/x-tex\">51.5</annotation></semantics></math> respectively, provided the increase in WER is tolerable for the ASR application considered. In all experiments, WERs shown are computed using our proposed custom Hindi normalizer. However, we also compare with other normalizers in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S4.T3\" title=\"Table 3 &#8227; 4 RESULTS AND DISCUSSION &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Whisper models have achieved remarkable progress in speech recognition; yet their large size remains a bottleneck for deployment on resource-constrained edge devices. This paper proposes a framework to design fine-tuned variants of Whisper which address the above problem. Structured sparsity is enforced via the Sparse Group LASSO penalty as a loss regularizer, to reduce the number of FLOating Point operations (FLOPs). Further, a weight statistics aware pruning algorithm is proposed. We also design our custom text normalizer for WER evaluation. On Common Voice 11.0 Hindi dataset, we obtain, without degrading WER, (a) 35.4% reduction in model parameters, 14.25% lower memory consumption and 18.5% fewer FLOPs on Whisper-small, and (b) 31% reduction in model parameters, 15.29% lower memory consumption and 16.95% fewer FLOPs on Whisper-medium; and, (c) substantially outperform the state-of-the-art Iterative Magnitude Pruning based method by pruning 18.7% more parameters along with a 12.31 reduction in WER.</p>\n\n",
                "matched_terms": [
                    "whispermedium",
                    "whispersmall",
                    "pruning",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There has been active research on fine-tuning large pre-trained models like Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib1\" title=\"\">1</a>]</cite> for Automatic Speech Recognition (ASR) in different low-resource languages, such as Amharic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib2\" title=\"\">2</a>]</cite>, Nepali <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib3\" title=\"\">3</a>]</cite>, Swiss German <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib4\" title=\"\">4</a>]</cite>, Afrikaans, Belarusian, Icelandic, Kazakh, Marathi, Nepali, and Swahili <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib5\" title=\"\">5</a>]</cite>. The IndicWhisper model in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib6\" title=\"\">6</a>]</cite> fine-tunes Whisper-medium on <math alttext=\"10,700\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo>,</mo><mn>700</mn></mrow><annotation encoding=\"application/x-tex\">10,700</annotation></semantics></math> hours of data belonging to <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m2\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> Indian languages, including Hindi. In edge applications limited by compute and memory, one would typically like to prune these models.</p>\n\n",
                "matched_terms": [
                    "different",
                    "whispermedium"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pruning of neural networks has been studied in literature, starting from the landmark paper<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib7\" title=\"\">7</a>]</cite> which demonstrated pruning without loss of accuracy for a simple neural network based on second order loss derivatives. The authors of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib8\" title=\"\">8</a>]</cite> demonstrated that through iterative magnitude pruning (IMP), one can recover a subnetwork that reaches an accuracy comparable to that of the full network, known as the lottery ticket hypothesis (LTH). Based on this hypothesis, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib9\" title=\"\">9</a>]</cite> pruned a transformer model with IMP. Following this, the authors of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib10\" title=\"\">10</a>]</cite> applied the pruning methodology of LTH to the Whisper model and obtained results comparable to the unpruned model. However, the existing works have three major limitations: (1) the pruning methods are iterative<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib10\" title=\"\">10</a>]</cite>, (2) they require additional adapters plugged into the model layers, increasing the inference time complexity<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib10\" title=\"\">10</a>]</cite>, and, (3) pruning with IMP does not reduce the number of FLOPs as the pruning is unstructured.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to address these shortcomings, this paper proposes a fine-tuning and pruning method, called TSPAR (fine<span class=\"ltx_text ltx_font_bold\">T</span>uning for <span class=\"ltx_text ltx_font_bold\">S</span>tructured s<span class=\"ltx_text ltx_font_bold\">P</span>arsity and <span class=\"ltx_text ltx_font_bold\">A</span>dapti-ve p<span class=\"ltx_text ltx_font_bold\">R</span>uning), which is applied on the Whisper-small and Whisper-medium models. In pruning literature, loss regularization has traditionally been used to sparsify the network during training <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib13\" title=\"\">13</a>]</cite>. However, imposing a penalty on the weight magnitudes only imposes unstructured sparsity, which does not reduce the number of FLOating Point operations (FLOPs). Using the Sparse Group LASSO (SGL)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib14\" title=\"\">14</a>]</cite> penalty function to regularize loss, one can achieve structured sparsity wherever it exists and unstructured sparsity elsewhere, which saves both FLOPs and memory utilization. We employ SGL regularized fine-tuning on Whisper, and prune according to the weight statistics of the regularized model, achieving comparable results to the full model. Ours is a non-iterative approach, which prunes up to <math alttext=\"51.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>51.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">51.5\\%</annotation></semantics></math> parameters in one shot with a considerable reduction in FLOPs, without requiring any additional adapters.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "pruning",
                    "whispermedium"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WER metric has traditionally been used to evaluate ASR models. Existing normalizers such as Whisper&#8217;s default &#8220;Non-English&#8221; normalizer<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib1\" title=\"\">1</a>]</cite> or the Hindi normalizer from Indic NLP library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://indic-nlp-library.readthedocs.io/en/latest/</span></span></span> are not ideal: Whisper&#8217;s normalization is overly aggressive and discards meaningful diacritics, while Indic normalization preserves all constructs.\nThis makes WER values across languages unfair; Hindi ends up with a more misleading WER than less phonemically complex languages like English <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib15\" title=\"\">15</a>]</cite>.\nWe propose a more balanced text normalization for Hindi, and adopt it in our WER computation. In summary, our main contributions are threefold: (1) imposing structured sparsity on fine-tuned Whisper models using the SGL penalty function, thereby reducing both memory footprint and FLOPs; (2) proposing a weight-statistics&#8211;driven pruning strategy that adapts dynamically to model parameters; and (3) designing a custom Hindi text normalizer for fair WER evaluation.</p>\n\n",
                "matched_terms": [
                    "pruning",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Loss regularization techniques such as weight decay or <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> norm minimization involves adding a penalty term on the magnitude of the weights to regularize the cross entropy function <math alttext=\"\\mathcal{L}_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{CE}</annotation></semantics></math>. This falls in the category of regularized loss minimization, while just using <math alttext=\"\\mathcal{L}_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{CE}</annotation></semantics></math> is empirical risk minimization. The sparsity induced by the <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> mechanism is unstructured, as the weights being driven to zero are randomly spread across the weight matrices. Pruning these parameters reduces memory requirement, but does not reduce FLOPs.\nIn the context of statistical literature, Sparse Group LASSO (SGL) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib14\" title=\"\">14</a>]</cite> was introduced where, an additional grouped loss regularization imposes structured sparsity on a group of weights as a whole. We exploit this SGL penalty function in our work to drive certain columns of weight matrices to zero.\nLet <math alttext=\"\\mathbf{W}_{jl}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}</annotation></semantics></math> be the <math alttext=\"j^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msup><mi>j</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">j^{th}</annotation></semantics></math> weight matrix of the <math alttext=\"l^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msup><mi>l</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">l^{th}</annotation></semantics></math> layer, with a total of <math alttext=\"J_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>J</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">J_{l}</annotation></semantics></math> matrices for <math alttext=\"1\\leq l\\leq L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8804;</mo><mi>l</mi><mo>&#8804;</mo><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">1\\leq l\\leq L</annotation></semantics></math> layers. Let <math alttext=\"\\mathbf{W}_{jl}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}^{i}</annotation></semantics></math> denote the <math alttext=\"i^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m11\" intent=\":literal\"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">i^{th}</annotation></semantics></math> column of <math alttext=\"\\mathbf{W}_{jl}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m12\" intent=\":literal\"><semantics><msub><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}</annotation></semantics></math> and <math alttext=\"I_{J_{l}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m13\" intent=\":literal\"><semantics><msub><mi>I</mi><msub><mi>J</mi><mi>l</mi></msub></msub><annotation encoding=\"application/x-tex\">I_{J_{l}}</annotation></semantics></math> denote the number of columns of each matrix <math alttext=\"\\mathbf{W}_{jl}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m14\" intent=\":literal\"><semantics><msub><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}</annotation></semantics></math>. Each such column <math alttext=\"\\mathbf{W}_{jl}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m15\" intent=\":literal\"><semantics><msubsup><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}^{i}</annotation></semantics></math> represents the set of outgoing connections from a neuron in layer <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m16\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> to every neuron in the next layer <math alttext=\"j+1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m17\" intent=\":literal\"><semantics><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">j+1</annotation></semantics></math>. By imposing column-wise structured sparsity, the entire column can be pruned. These pruned columns are excluded from matrix multiplication, resulting in the reduction of FLOPs. Hence, the total loss is defined as,</p>\n\n",
                "matched_terms": [
                    "layers",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where superscripts E and D on <math alttext=\"\\mathcal{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m18\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><annotation encoding=\"application/x-tex\">\\mathcal{L}</annotation></semantics></math> denote the encoder and decoder, while subscripts <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m19\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> and <math alttext=\"L_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m20\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">L_{2}</annotation></semantics></math> denote the regularization terms that induce unstructured and structured sparsity, respectively.</p>\n\n",
                "matched_terms": [
                    "decoder",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a two-pass pruning algorithm, which is given in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#alg1\" title=\"Algorithm 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The structured <math alttext=\"L_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">L_{2}</annotation></semantics></math> loss regularization drives entire columns to zero, and hence in the first pass (lines 3-7) we prune columns which are approximately sparse. The columns with high density of &#8220;almost zero weights&#8221; are called &#8221;approximate sparse columns&#8221;. A weight, <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mi>w</mi><annotation encoding=\"application/x-tex\">w</annotation></semantics></math> is defined as an &#8220;almost zero weight&#8221; if <math alttext=\"|w|&lt;\\theta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>w</mi><mo stretchy=\"false\">|</mo></mrow><mo>&lt;</mo><msub><mi>&#952;</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">|w|&lt;\\theta_{0}</annotation></semantics></math> for some positive constant <math alttext=\"\\theta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\theta_{0}</annotation></semantics></math>. A column <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> of a weight matrix is defined as &#8220;approximately sparse&#8221; if the approximate sparsity of the column, <math alttext=\"\\hat{S}_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>S</mi><mo>^</mo></mover><mi>c</mi></msub><annotation encoding=\"application/x-tex\">\\hat{S}_{c}</annotation></semantics></math> is greater than a positive threshold <math alttext=\"\\theta_{\\hat{S}_{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><msub><mover accent=\"true\"><mi>S</mi><mo>^</mo></mover><mi>c</mi></msub></msub><annotation encoding=\"application/x-tex\">\\theta_{\\hat{S}_{c}}</annotation></semantics></math>. Approximate sparsity is defined as <math alttext=\"\\hat{S}_{c}(w;\\theta_{0})=\\frac{\\mathbf{1}_{|w|&lt;\\theta_{0}}(C)}{|C|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mover accent=\"true\"><mi>S</mi><mo>^</mo></mover><mi>c</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo>;</mo><msub><mi>&#952;</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mn>&#120783;</mn><mrow><mrow><mo stretchy=\"false\">|</mo><mi>w</mi><mo stretchy=\"false\">|</mo></mrow><mo>&lt;</mo><msub><mi>&#952;</mi><mn>0</mn></msub></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><mi>C</mi><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\hat{S}_{c}(w;\\theta_{0})=\\frac{\\mathbf{1}_{|w|&lt;\\theta_{0}}(C)}{|C|}</annotation></semantics></math> where, <math alttext=\"\\mathbf{1}(.)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><mrow><mn>&#120783;</mn><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0.167em\">.</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{1}(.)</annotation></semantics></math> is an indicator function, and <math alttext=\"|C|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>C</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|C|</annotation></semantics></math> is the length of column <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m11\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>. During the second pass of pruning (lines 8&#8211;12), we prune individual weights that have been driven to zero by the <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m12\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> loss regularization with a threshold <math alttext=\"\\theta_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m13\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{w}</annotation></semantics></math>.\nWhile pruning thresholds are typically tuned by trial and error, we propose to analyze the weight statistics of the model and derive pruning thresholds <math alttext=\"\\theta_{0},\\theta_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m14\" intent=\":literal\"><semantics><mrow><msub><mi>&#952;</mi><mn>0</mn></msub><mo>,</mo><msub><mi>&#952;</mi><mi>c</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\theta_{0},\\theta_{c}</annotation></semantics></math>, and <math alttext=\"\\theta_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m15\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{w}</annotation></semantics></math>. The detailed analysis is provided in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S3\" title=\"3 ANALYSIS OF WEIGHT STATISTICS &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "thresholds",
                    "Œ∏wthetaw",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we analyze the weight statistics of three fine-tuned models when regularization is applied on: (a) only fully connected (FC) layers of Whisper-small, (b) all layers of Whisper-small, and (c) only FC layers of Whisper-medium. These models are referred to as Net-1, Net-2, and Net-3, respectively. In all models, the mean <math alttext=\"\\mu\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>&#956;</mi><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math> and standard deviation <math alttext=\"\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>&#963;</mi><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math> of weights are in the order of <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-4}</annotation></semantics></math> and <math alttext=\"10^{-2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>2</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-2}</annotation></semantics></math>, respectively, in all layers (except LayerNorm in Net-2). We observe three main types of weight distributions, namely (A) highly spiked (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf1\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>), (B) narrowly spread (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf2\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>), and (C) widely spread (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf3\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(c)</span></a>) distributions around mean. In highly spiked distributions (type A), the weights between the <math alttext=\"25^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><msup><mn>25</mn><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">25^{th}</annotation></semantics></math> and <math alttext=\"75^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><msup><mn>75</mn><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">75^{th}</annotation></semantics></math> percentiles, denoted by <math alttext=\"Q_{2},Q_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mn>2</mn></msub><mo>,</mo><msub><mi>Q</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">Q_{2},Q_{3}</annotation></semantics></math>, are in the order of <math alttext=\"10^{-11}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m8\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>11</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-11}</annotation></semantics></math>. This implies that <math alttext=\"50\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m9\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">50\\%</annotation></semantics></math> of the weights are very close to zero, and hence, can be pruned immediately. However, we prune all weights between <math alttext=\"\\mu-0.1\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m10\" intent=\":literal\"><semantics><mrow><mi>&#956;</mi><mo>&#8722;</mo><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mu-0.1\\sigma</annotation></semantics></math> and <math alttext=\"\\mu+0.1\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m11\" intent=\":literal\"><semantics><mrow><mi>&#956;</mi><mo>+</mo><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mu+0.1\\sigma</annotation></semantics></math> in order to prune more weights than those in the interquartile range. So, we set the threshold function as <math alttext=\"T_{1}=0.1\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m12\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>=</mo><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">T_{1}=0.1\\sigma</annotation></semantics></math>. In narrowly spread distributions (type B), <math alttext=\"Q_{2},Q_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m13\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mn>2</mn></msub><mo>,</mo><msub><mi>Q</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">Q_{2},Q_{3}</annotation></semantics></math> are at <math alttext=\"\\sim(\\mu\\pm 0.3\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m14\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#956;</mi><mo>&#177;</mo><mrow><mn>0.3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim(\\mu\\pm 0.3\\sigma)</annotation></semantics></math>. To prune slightly more weights than those in the inter-quartile range, the threshold function is set as <math alttext=\"T_{2}=\\max\\{|Q_{2}|,|Q_{3}|\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m15\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>2</mn></msub><mo>=</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>Q</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>Q</mi><mn>3</mn></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">T_{2}=\\max\\{|Q_{2}|,|Q_{3}|\\}</annotation></semantics></math>. The widely spread distributions (type C), have <math alttext=\"Q_{2},Q_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m16\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mn>2</mn></msub><mo>,</mo><msub><mi>Q</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">Q_{2},Q_{3}</annotation></semantics></math> locations at <math alttext=\"\\sim(\\mu\\pm 0.67\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m17\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#956;</mi><mo>&#177;</mo><mrow><mn>0.67</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim(\\mu\\pm 0.67\\sigma)</annotation></semantics></math>. We can not apply threshold to be <math alttext=\"\\max\\{|Q_{2}|,|Q_{3}|\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m18\" intent=\":literal\"><semantics><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>Q</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>Q</mi><mn>3</mn></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\max\\{|Q_{2}|,|Q_{3}|\\}</annotation></semantics></math> as it will prune too many weights. Hence, we use the widely adopted threshold function, <math alttext=\"T_{3}=\\eta W_{max}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m19\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>3</mn></msub><mo>=</mo><mrow><mi>&#951;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>W</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">T_{3}=\\eta W_{max}</annotation></semantics></math> where <math alttext=\"W_{max}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m20\" intent=\":literal\"><semantics><msub><mi>W</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msub><annotation encoding=\"application/x-tex\">W_{max}</annotation></semantics></math> is the maximum absolute weight of matrix <math alttext=\"W\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m21\" intent=\":literal\"><semantics><mi>W</mi><annotation encoding=\"application/x-tex\">W</annotation></semantics></math> and <math alttext=\"\\eta&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m22\" intent=\":literal\"><semantics><mrow><mi>&#951;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\eta&gt;0</annotation></semantics></math> is a hyperparameter <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib13\" title=\"\">13</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "layers",
                    "net3",
                    "whispermedium",
                    "net2",
                    "net1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze the weight distributions of the columns of weight matrices in different layers. The columns that follow highly spiked distributions (type A), have weights very close to mean (<math alttext=\"\\sim 0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0</annotation></semantics></math>) i.e., almost zero weights. To identify these weights, <math alttext=\"\\theta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\theta_{0}</annotation></semantics></math> is set as <math alttext=\"0.1\\sigma_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#963;</mi><mi>c</mi></msub></mrow><annotation encoding=\"application/x-tex\">0.1\\sigma_{c}</annotation></semantics></math>. To prune columns with 90% approximate sparsity, <math alttext=\"\\theta_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{c}</annotation></semantics></math> is set as <math alttext=\"0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m5\" intent=\":literal\"><semantics><mn>0.9</mn><annotation encoding=\"application/x-tex\">0.9</annotation></semantics></math> (see a sample pruned column weight distribution in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf4\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(d)</span></a>). The weights of the unpruned columns (a sample weight distribution shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf5\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(e)</span></a>) of a layer are further pruned in the second pass of pruning using a threshold <math alttext=\"\\theta_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m6\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{w}</annotation></semantics></math>, which is set as <math alttext=\"T_{1},T_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m7\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">T_{1},T_{2}</annotation></semantics></math> or <math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m8\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math>, depending on whether the layer follows distribution type A, B, or C. We observe the following weight distributions in Net-1, 2, and 3.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Net-1:</span> The first 6 FC layers of the encoder have type A distributions, remaining encoder layers follow type B, while the decoder layers follow type C distributions.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Net-2:</span> There are 3 FC + 7 attention (ATT) + 1 convolution (CONV) layers in the encoder, and 14 ATT + 2 FC layers in the decoder; i.e., a total of 27 layers (denoted by <math alttext=\"\\mathcal{P}_{1})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.p2.m9\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{1})</annotation></semantics></math>, which exhibit distributions of type A. The remaining layers follow distributions of type C. The LayerNorm layers of both the encoder and decoder have significantly larger weights (<math alttext=\"\\mu\\sim 1.0,\\sigma\\sim 10^{-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m10\" intent=\":literal\"><semantics><mrow><mrow><mi>&#956;</mi><mo>&#8764;</mo><mn>1.0</mn></mrow><mo>,</mo><mrow><mi>&#963;</mi><mo>&#8764;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mu\\sim 1.0,\\sigma\\sim 10^{-1}</annotation></semantics></math>) and hence, cannot be pruned.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Net-3:</span> The first 5 layers of the encoder follow type A, while all the remaining layers of the encoder and decoder follow type B distribution. We apply the derived thresholds and provide results in the next section.</p>\n\n",
                "matched_terms": [
                    "Œ∏wthetaw",
                    "layers",
                    "decoder",
                    "different",
                    "net3",
                    "pruning",
                    "t3t3",
                    "thresholds",
                    "net2",
                    "net1",
                    "att",
                    "ùí´1mathcalp1",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Hindi is the third largest (609M total speakers) language in the world <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://www.ethnologue.com/insights/most-spoken-language/</span></span></span>, we consider the benchmark Common Voice 11.0 Hindi dataset for our experiments. Whisper-small and Whisper-medium are selected for our experiments as WERs of these models are substantially better than Whisper-tiny, base for Hindi data (see Appendix D.2.2, D.2.4 in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib1\" title=\"\">1</a>]</cite>). Our aim is to design models for resource constrained edges, and thus we do not choose the large, large v2 versions of Whisper. For fine-tuning, the pre-trained Whisper is trained with 5000 steps (24.39 epochs) with training batch-size and evaluation batch-size of 8, AdamW optimizer, cosine learning rate scheduler, warmup ratio of 0.2, and weight decay of 0.01 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib16\" title=\"\">16</a>]</cite>.\nWe set <math alttext=\"\\lambda_{1}=\\lambda_{2}=10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mn>1</mn></msub><mo>=</mo><msub><mi>&#955;</mi><mn>2</mn></msub><mo>=</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\lambda_{1}=\\lambda_{2}=10^{-5}</annotation></semantics></math> (for <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> loss) and <math alttext=\"\\lambda_{3}=\\lambda_{4}=10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#955;</mi><mn>3</mn></msub><mo>=</mo><msub><mi>&#955;</mi><mn>4</mn></msub><mo>=</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\lambda_{3}=\\lambda_{4}=10^{-4}</annotation></semantics></math> (for <math alttext=\"L_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">L_{2}</annotation></semantics></math> loss) by tuning independently.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "experiments",
                    "whispermedium"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VWS, VWM refer to Whisper-small, Whisper-medium, respectively, when fine-tuned without any regularization. Net-3 (15.86) has improved upon the state-of-the-art IndicWhisper (16.64). Note that IndicWhisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib18\" title=\"\">18</a>]</cite> was fine-tuned with 2150 hours of Hindi data, while we use only 13 hours of data. Net-1 maintains its WER (24.84 after pruning), comparable to its full model (23.32), and VWS (22.88). Net-3 (19.58), after pruning, is also fairly competitive with full IndicWhisper (16.64). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S4.T3\" title=\"Table 3 &#8227; 4 RESULTS AND DISCUSSION &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (columns &#8216;Whisper&#8217;, &#8216;Indic&#8217;, &#8216;Ours&#8217;), WERs obtained with our normalizer lie between WERs with Whisper and Indic, showing it balances the two extremes.</p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "net3",
                    "pruning",
                    "whispermedium",
                    "net1",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a framework TSPAR to obtain compact fine-tuned versions of Whisper. TSPAR applies SGL loss regularization to induce both column-wise structured sparsity and element-wise unstructured sparsity. Structured sparsity gives us a significant reduction in FLOPs and model weights. TSPAR then performs a two-pass pruning adaptive to weight statistics, exploiting the induced sparsity.\nAs a result, we outperform the state-of-the-art IMP method both in pruning and WER performance, making our models suitable for resource constrained edge device applications.\nWe also design our custom Hindi text normalizer balancing the over-simplification by Whisper and over-preservation rules by Indic normalizers.\nA future direction is to leverage our TSPAR method to design low-complex variants of other state-of-the-art models under resource constraints.\n</p>\n\n",
                "matched_terms": [
                    "pruning",
                    "wer"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models",
        "caption": "Table 2: Memory costs (in MB) and FLOPs",
        "body": "Model\n\n\nBefore pruning\nAfter pruning\n\n\nMemory\nFLOPs\nMemory\nFLOPs\n\n\n\n\nNet-1\n\n\n536.62\n1.65e16\n460.15\n1.34e16\n\n\n\n\nNet-2\n\n\n536.62\n1.65e16\n413.65\n1.18e16\n\n\n\n\nNet-3\n\n\n1557.11\n4.95e16\n1318.97\n4.11e16",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Before pruning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">After pruning</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Memory</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FLOPs</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Memory</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FLOPs</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">Net-1</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">536.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.65e16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">460.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.34e16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">Net-2</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">536.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.65e16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">413.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.18e16</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:28.5pt;\">Net-3</span>\n</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1557.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">4.95e16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1318.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">4.11e16</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "165e16",
            "model",
            "before",
            "costs",
            "134e16",
            "495e16",
            "411e16",
            "flops",
            "pruning",
            "118e16",
            "net3",
            "net1",
            "net2",
            "memory",
            "after"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">From Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S4.T2\" title=\"Table 2 &#8227; 4 RESULTS AND DISCUSSION &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we observe that memory consumption is reduced by 14.25%, 22.91%, 15.29%, and computation costs (FLOPs) are decreased by 18.5%, 28.31%, 16.95% in Net-1,2,3, respectively, for the lowest WER reported. The FLOPs are computed according to the methodology outlined in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib17\" title=\"\">17</a>]</cite>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Whisper models have achieved remarkable progress in speech recognition; yet their large size remains a bottleneck for deployment on resource-constrained edge devices. This paper proposes a framework to design fine-tuned variants of Whisper which address the above problem. Structured sparsity is enforced via the Sparse Group LASSO penalty as a loss regularizer, to reduce the number of FLOating Point operations (FLOPs). Further, a weight statistics aware pruning algorithm is proposed. We also design our custom text normalizer for WER evaluation. On Common Voice 11.0 Hindi dataset, we obtain, without degrading WER, (a) 35.4% reduction in model parameters, 14.25% lower memory consumption and 18.5% fewer FLOPs on Whisper-small, and (b) 31% reduction in model parameters, 15.29% lower memory consumption and 16.95% fewer FLOPs on Whisper-medium; and, (c) substantially outperform the state-of-the-art Iterative Magnitude Pruning based method by pruning 18.7% more parameters along with a 12.31 reduction in WER.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "model",
                    "flops",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There has been active research on fine-tuning large pre-trained models like Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib1\" title=\"\">1</a>]</cite> for Automatic Speech Recognition (ASR) in different low-resource languages, such as Amharic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib2\" title=\"\">2</a>]</cite>, Nepali <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib3\" title=\"\">3</a>]</cite>, Swiss German <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib4\" title=\"\">4</a>]</cite>, Afrikaans, Belarusian, Icelandic, Kazakh, Marathi, Nepali, and Swahili <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib5\" title=\"\">5</a>]</cite>. The IndicWhisper model in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib6\" title=\"\">6</a>]</cite> fine-tunes Whisper-medium on <math alttext=\"10,700\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo>,</mo><mn>700</mn></mrow><annotation encoding=\"application/x-tex\">10,700</annotation></semantics></math> hours of data belonging to <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m2\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> Indian languages, including Hindi. In edge applications limited by compute and memory, one would typically like to prune these models.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pruning of neural networks has been studied in literature, starting from the landmark paper<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib7\" title=\"\">7</a>]</cite> which demonstrated pruning without loss of accuracy for a simple neural network based on second order loss derivatives. The authors of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib8\" title=\"\">8</a>]</cite> demonstrated that through iterative magnitude pruning (IMP), one can recover a subnetwork that reaches an accuracy comparable to that of the full network, known as the lottery ticket hypothesis (LTH). Based on this hypothesis, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib9\" title=\"\">9</a>]</cite> pruned a transformer model with IMP. Following this, the authors of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib10\" title=\"\">10</a>]</cite> applied the pruning methodology of LTH to the Whisper model and obtained results comparable to the unpruned model. However, the existing works have three major limitations: (1) the pruning methods are iterative<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib10\" title=\"\">10</a>]</cite>, (2) they require additional adapters plugged into the model layers, increasing the inference time complexity<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib10\" title=\"\">10</a>]</cite>, and, (3) pruning with IMP does not reduce the number of FLOPs as the pruning is unstructured.</p>\n\n",
                "matched_terms": [
                    "model",
                    "flops",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to address these shortcomings, this paper proposes a fine-tuning and pruning method, called TSPAR (fine<span class=\"ltx_text ltx_font_bold\">T</span>uning for <span class=\"ltx_text ltx_font_bold\">S</span>tructured s<span class=\"ltx_text ltx_font_bold\">P</span>arsity and <span class=\"ltx_text ltx_font_bold\">A</span>dapti-ve p<span class=\"ltx_text ltx_font_bold\">R</span>uning), which is applied on the Whisper-small and Whisper-medium models. In pruning literature, loss regularization has traditionally been used to sparsify the network during training <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib13\" title=\"\">13</a>]</cite>. However, imposing a penalty on the weight magnitudes only imposes unstructured sparsity, which does not reduce the number of FLOating Point operations (FLOPs). Using the Sparse Group LASSO (SGL)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib14\" title=\"\">14</a>]</cite> penalty function to regularize loss, one can achieve structured sparsity wherever it exists and unstructured sparsity elsewhere, which saves both FLOPs and memory utilization. We employ SGL regularized fine-tuning on Whisper, and prune according to the weight statistics of the regularized model, achieving comparable results to the full model. Ours is a non-iterative approach, which prunes up to <math alttext=\"51.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>51.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">51.5\\%</annotation></semantics></math> parameters in one shot with a considerable reduction in FLOPs, without requiring any additional adapters.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "model",
                    "flops",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WER metric has traditionally been used to evaluate ASR models. Existing normalizers such as Whisper&#8217;s default &#8220;Non-English&#8221; normalizer<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib1\" title=\"\">1</a>]</cite> or the Hindi normalizer from Indic NLP library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://indic-nlp-library.readthedocs.io/en/latest/</span></span></span> are not ideal: Whisper&#8217;s normalization is overly aggressive and discards meaningful diacritics, while Indic normalization preserves all constructs.\nThis makes WER values across languages unfair; Hindi ends up with a more misleading WER than less phonemically complex languages like English <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib15\" title=\"\">15</a>]</cite>.\nWe propose a more balanced text normalization for Hindi, and adopt it in our WER computation. In summary, our main contributions are threefold: (1) imposing structured sparsity on fine-tuned Whisper models using the SGL penalty function, thereby reducing both memory footprint and FLOPs; (2) proposing a weight-statistics&#8211;driven pruning strategy that adapts dynamically to model parameters; and (3) designing a custom Hindi text normalizer for fair WER evaluation.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "model",
                    "flops",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Loss regularization techniques such as weight decay or <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> norm minimization involves adding a penalty term on the magnitude of the weights to regularize the cross entropy function <math alttext=\"\\mathcal{L}_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{CE}</annotation></semantics></math>. This falls in the category of regularized loss minimization, while just using <math alttext=\"\\mathcal{L}_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{CE}</annotation></semantics></math> is empirical risk minimization. The sparsity induced by the <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> mechanism is unstructured, as the weights being driven to zero are randomly spread across the weight matrices. Pruning these parameters reduces memory requirement, but does not reduce FLOPs.\nIn the context of statistical literature, Sparse Group LASSO (SGL) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib14\" title=\"\">14</a>]</cite> was introduced where, an additional grouped loss regularization imposes structured sparsity on a group of weights as a whole. We exploit this SGL penalty function in our work to drive certain columns of weight matrices to zero.\nLet <math alttext=\"\\mathbf{W}_{jl}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}</annotation></semantics></math> be the <math alttext=\"j^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msup><mi>j</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">j^{th}</annotation></semantics></math> weight matrix of the <math alttext=\"l^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msup><mi>l</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">l^{th}</annotation></semantics></math> layer, with a total of <math alttext=\"J_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>J</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">J_{l}</annotation></semantics></math> matrices for <math alttext=\"1\\leq l\\leq L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8804;</mo><mi>l</mi><mo>&#8804;</mo><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">1\\leq l\\leq L</annotation></semantics></math> layers. Let <math alttext=\"\\mathbf{W}_{jl}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}^{i}</annotation></semantics></math> denote the <math alttext=\"i^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m11\" intent=\":literal\"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">i^{th}</annotation></semantics></math> column of <math alttext=\"\\mathbf{W}_{jl}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m12\" intent=\":literal\"><semantics><msub><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}</annotation></semantics></math> and <math alttext=\"I_{J_{l}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m13\" intent=\":literal\"><semantics><msub><mi>I</mi><msub><mi>J</mi><mi>l</mi></msub></msub><annotation encoding=\"application/x-tex\">I_{J_{l}}</annotation></semantics></math> denote the number of columns of each matrix <math alttext=\"\\mathbf{W}_{jl}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m14\" intent=\":literal\"><semantics><msub><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}</annotation></semantics></math>. Each such column <math alttext=\"\\mathbf{W}_{jl}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m15\" intent=\":literal\"><semantics><msubsup><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}^{i}</annotation></semantics></math> represents the set of outgoing connections from a neuron in layer <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m16\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> to every neuron in the next layer <math alttext=\"j+1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m17\" intent=\":literal\"><semantics><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">j+1</annotation></semantics></math>. By imposing column-wise structured sparsity, the entire column can be pruned. These pruned columns are excluded from matrix multiplication, resulting in the reduction of FLOPs. Hence, the total loss is defined as,</p>\n\n",
                "matched_terms": [
                    "memory",
                    "flops",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a two-pass pruning algorithm, which is given in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#alg1\" title=\"Algorithm 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The structured <math alttext=\"L_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">L_{2}</annotation></semantics></math> loss regularization drives entire columns to zero, and hence in the first pass (lines 3-7) we prune columns which are approximately sparse. The columns with high density of &#8220;almost zero weights&#8221; are called &#8221;approximate sparse columns&#8221;. A weight, <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mi>w</mi><annotation encoding=\"application/x-tex\">w</annotation></semantics></math> is defined as an &#8220;almost zero weight&#8221; if <math alttext=\"|w|&lt;\\theta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>w</mi><mo stretchy=\"false\">|</mo></mrow><mo>&lt;</mo><msub><mi>&#952;</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">|w|&lt;\\theta_{0}</annotation></semantics></math> for some positive constant <math alttext=\"\\theta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\theta_{0}</annotation></semantics></math>. A column <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> of a weight matrix is defined as &#8220;approximately sparse&#8221; if the approximate sparsity of the column, <math alttext=\"\\hat{S}_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>S</mi><mo>^</mo></mover><mi>c</mi></msub><annotation encoding=\"application/x-tex\">\\hat{S}_{c}</annotation></semantics></math> is greater than a positive threshold <math alttext=\"\\theta_{\\hat{S}_{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><msub><mover accent=\"true\"><mi>S</mi><mo>^</mo></mover><mi>c</mi></msub></msub><annotation encoding=\"application/x-tex\">\\theta_{\\hat{S}_{c}}</annotation></semantics></math>. Approximate sparsity is defined as <math alttext=\"\\hat{S}_{c}(w;\\theta_{0})=\\frac{\\mathbf{1}_{|w|&lt;\\theta_{0}}(C)}{|C|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mover accent=\"true\"><mi>S</mi><mo>^</mo></mover><mi>c</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo>;</mo><msub><mi>&#952;</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mn>&#120783;</mn><mrow><mrow><mo stretchy=\"false\">|</mo><mi>w</mi><mo stretchy=\"false\">|</mo></mrow><mo>&lt;</mo><msub><mi>&#952;</mi><mn>0</mn></msub></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><mi>C</mi><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\hat{S}_{c}(w;\\theta_{0})=\\frac{\\mathbf{1}_{|w|&lt;\\theta_{0}}(C)}{|C|}</annotation></semantics></math> where, <math alttext=\"\\mathbf{1}(.)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><mrow><mn>&#120783;</mn><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0.167em\">.</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{1}(.)</annotation></semantics></math> is an indicator function, and <math alttext=\"|C|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>C</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|C|</annotation></semantics></math> is the length of column <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m11\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>. During the second pass of pruning (lines 8&#8211;12), we prune individual weights that have been driven to zero by the <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m12\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> loss regularization with a threshold <math alttext=\"\\theta_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m13\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{w}</annotation></semantics></math>.\nWhile pruning thresholds are typically tuned by trial and error, we propose to analyze the weight statistics of the model and derive pruning thresholds <math alttext=\"\\theta_{0},\\theta_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m14\" intent=\":literal\"><semantics><mrow><msub><mi>&#952;</mi><mn>0</mn></msub><mo>,</mo><msub><mi>&#952;</mi><mi>c</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\theta_{0},\\theta_{c}</annotation></semantics></math>, and <math alttext=\"\\theta_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m15\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{w}</annotation></semantics></math>. The detailed analysis is provided in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S3\" title=\"3 ANALYSIS OF WEIGHT STATISTICS &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we analyze the weight statistics of three fine-tuned models when regularization is applied on: (a) only fully connected (FC) layers of Whisper-small, (b) all layers of Whisper-small, and (c) only FC layers of Whisper-medium. These models are referred to as Net-1, Net-2, and Net-3, respectively. In all models, the mean <math alttext=\"\\mu\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>&#956;</mi><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math> and standard deviation <math alttext=\"\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>&#963;</mi><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math> of weights are in the order of <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-4}</annotation></semantics></math> and <math alttext=\"10^{-2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>2</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-2}</annotation></semantics></math>, respectively, in all layers (except LayerNorm in Net-2). We observe three main types of weight distributions, namely (A) highly spiked (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf1\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>), (B) narrowly spread (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf2\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>), and (C) widely spread (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf3\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(c)</span></a>) distributions around mean. In highly spiked distributions (type A), the weights between the <math alttext=\"25^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><msup><mn>25</mn><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">25^{th}</annotation></semantics></math> and <math alttext=\"75^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><msup><mn>75</mn><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">75^{th}</annotation></semantics></math> percentiles, denoted by <math alttext=\"Q_{2},Q_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mn>2</mn></msub><mo>,</mo><msub><mi>Q</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">Q_{2},Q_{3}</annotation></semantics></math>, are in the order of <math alttext=\"10^{-11}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m8\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>11</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-11}</annotation></semantics></math>. This implies that <math alttext=\"50\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m9\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">50\\%</annotation></semantics></math> of the weights are very close to zero, and hence, can be pruned immediately. However, we prune all weights between <math alttext=\"\\mu-0.1\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m10\" intent=\":literal\"><semantics><mrow><mi>&#956;</mi><mo>&#8722;</mo><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mu-0.1\\sigma</annotation></semantics></math> and <math alttext=\"\\mu+0.1\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m11\" intent=\":literal\"><semantics><mrow><mi>&#956;</mi><mo>+</mo><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mu+0.1\\sigma</annotation></semantics></math> in order to prune more weights than those in the interquartile range. So, we set the threshold function as <math alttext=\"T_{1}=0.1\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m12\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>=</mo><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">T_{1}=0.1\\sigma</annotation></semantics></math>. In narrowly spread distributions (type B), <math alttext=\"Q_{2},Q_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m13\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mn>2</mn></msub><mo>,</mo><msub><mi>Q</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">Q_{2},Q_{3}</annotation></semantics></math> are at <math alttext=\"\\sim(\\mu\\pm 0.3\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m14\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#956;</mi><mo>&#177;</mo><mrow><mn>0.3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim(\\mu\\pm 0.3\\sigma)</annotation></semantics></math>. To prune slightly more weights than those in the inter-quartile range, the threshold function is set as <math alttext=\"T_{2}=\\max\\{|Q_{2}|,|Q_{3}|\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m15\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>2</mn></msub><mo>=</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>Q</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>Q</mi><mn>3</mn></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">T_{2}=\\max\\{|Q_{2}|,|Q_{3}|\\}</annotation></semantics></math>. The widely spread distributions (type C), have <math alttext=\"Q_{2},Q_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m16\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mn>2</mn></msub><mo>,</mo><msub><mi>Q</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">Q_{2},Q_{3}</annotation></semantics></math> locations at <math alttext=\"\\sim(\\mu\\pm 0.67\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m17\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#956;</mi><mo>&#177;</mo><mrow><mn>0.67</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim(\\mu\\pm 0.67\\sigma)</annotation></semantics></math>. We can not apply threshold to be <math alttext=\"\\max\\{|Q_{2}|,|Q_{3}|\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m18\" intent=\":literal\"><semantics><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>Q</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>Q</mi><mn>3</mn></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\max\\{|Q_{2}|,|Q_{3}|\\}</annotation></semantics></math> as it will prune too many weights. Hence, we use the widely adopted threshold function, <math alttext=\"T_{3}=\\eta W_{max}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m19\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>3</mn></msub><mo>=</mo><mrow><mi>&#951;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>W</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">T_{3}=\\eta W_{max}</annotation></semantics></math> where <math alttext=\"W_{max}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m20\" intent=\":literal\"><semantics><msub><mi>W</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msub><annotation encoding=\"application/x-tex\">W_{max}</annotation></semantics></math> is the maximum absolute weight of matrix <math alttext=\"W\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m21\" intent=\":literal\"><semantics><mi>W</mi><annotation encoding=\"application/x-tex\">W</annotation></semantics></math> and <math alttext=\"\\eta&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m22\" intent=\":literal\"><semantics><mrow><mi>&#951;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\eta&gt;0</annotation></semantics></math> is a hyperparameter <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib13\" title=\"\">13</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "net2",
                    "net1",
                    "net3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze the weight distributions of the columns of weight matrices in different layers. The columns that follow highly spiked distributions (type A), have weights very close to mean (<math alttext=\"\\sim 0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0</annotation></semantics></math>) i.e., almost zero weights. To identify these weights, <math alttext=\"\\theta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\theta_{0}</annotation></semantics></math> is set as <math alttext=\"0.1\\sigma_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#963;</mi><mi>c</mi></msub></mrow><annotation encoding=\"application/x-tex\">0.1\\sigma_{c}</annotation></semantics></math>. To prune columns with 90% approximate sparsity, <math alttext=\"\\theta_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{c}</annotation></semantics></math> is set as <math alttext=\"0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m5\" intent=\":literal\"><semantics><mn>0.9</mn><annotation encoding=\"application/x-tex\">0.9</annotation></semantics></math> (see a sample pruned column weight distribution in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf4\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(d)</span></a>). The weights of the unpruned columns (a sample weight distribution shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf5\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(e)</span></a>) of a layer are further pruned in the second pass of pruning using a threshold <math alttext=\"\\theta_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m6\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{w}</annotation></semantics></math>, which is set as <math alttext=\"T_{1},T_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m7\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">T_{1},T_{2}</annotation></semantics></math> or <math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m8\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math>, depending on whether the layer follows distribution type A, B, or C. We observe the following weight distributions in Net-1, 2, and 3.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Net-1:</span> The first 6 FC layers of the encoder have type A distributions, remaining encoder layers follow type B, while the decoder layers follow type C distributions.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Net-2:</span> There are 3 FC + 7 attention (ATT) + 1 convolution (CONV) layers in the encoder, and 14 ATT + 2 FC layers in the decoder; i.e., a total of 27 layers (denoted by <math alttext=\"\\mathcal{P}_{1})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.p2.m9\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{1})</annotation></semantics></math>, which exhibit distributions of type A. The remaining layers follow distributions of type C. The LayerNorm layers of both the encoder and decoder have significantly larger weights (<math alttext=\"\\mu\\sim 1.0,\\sigma\\sim 10^{-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m10\" intent=\":literal\"><semantics><mrow><mrow><mi>&#956;</mi><mo>&#8764;</mo><mn>1.0</mn></mrow><mo>,</mo><mrow><mi>&#963;</mi><mo>&#8764;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mu\\sim 1.0,\\sigma\\sim 10^{-1}</annotation></semantics></math>) and hence, cannot be pruned.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Net-3:</span> The first 5 layers of the encoder follow type A, while all the remaining layers of the encoder and decoder follow type B distribution. We apply the derived thresholds and provide results in the next section.</p>\n\n",
                "matched_terms": [
                    "net2",
                    "net1",
                    "net3",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method obtains WER of 23.32, 23.94 and 15.86, on Net-1, 2, 3, respectively, with SGL regularization before pruning. We conduct more experiments with various combinations of <math alttext=\"T_{1},T_{2},T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">T_{1},T_{2},T_{3}</annotation></semantics></math> to set <math alttext=\"\\theta_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{w}</annotation></semantics></math> for the Net-1,2, 3 to provide the tradeoff between WER and pruning <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>. A few experimental results are reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S4.T1\" title=\"Table 1 &#8227; 4 RESULTS AND DISCUSSION &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Net-1 gives lowest WER of <math alttext=\"24.84\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\"><semantics><mn>24.84</mn><annotation encoding=\"application/x-tex\">24.84</annotation></semantics></math> with 35.4% reduction in parameters. The &#8221;lowest WER&#8221; refers to the smallest WER obtained among all the experiments conducted for a model. The lowest WER reported for Net-2 is 32.86 with 32.2% reduction in parameters. This indicates that all layers must not be regularized as non-FC layers capture significant information about the ASR learning. Hence, we refrain from performing regularization on all layers of Whisper-medium, but rather only on FC layers. Net-3 gives lowest WER of 19.58 when being pruned by <math alttext=\"31\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\"><semantics><mrow><mn>31</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">31\\%</annotation></semantics></math> of parameters. All our models substantially outperform the baseline IMP-based method both in terms of pruning % and WER. Specifically, Net-1 prunes 18.7% more parameters with a WER reduction of 12.31 when compared to IMP on Whisper-small. Similarly, on Whisper-medium, Net-3 achieves a WER reduction by 10.91 with 10.71% more pruning. Further, in all our three networks, one can achieve a greater pruning <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m6\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> of upto <math alttext=\"44.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m7\" intent=\":literal\"><semantics><mn>44.9</mn><annotation encoding=\"application/x-tex\">44.9</annotation></semantics></math>, <math alttext=\"38.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m8\" intent=\":literal\"><semantics><mn>38.4</mn><annotation encoding=\"application/x-tex\">38.4</annotation></semantics></math>, and <math alttext=\"51.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m9\" intent=\":literal\"><semantics><mn>51.5</mn><annotation encoding=\"application/x-tex\">51.5</annotation></semantics></math> respectively, provided the increase in WER is tolerable for the ASR application considered. In all experiments, WERs shown are computed using our proposed custom Hindi normalizer. However, we also compare with other normalizers in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S4.T3\" title=\"Table 3 &#8227; 4 RESULTS AND DISCUSSION &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "before",
                    "net3",
                    "pruning",
                    "net1",
                    "net2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">VWS, VWM refer to Whisper-small, Whisper-medium, respectively, when fine-tuned without any regularization. Net-3 (15.86) has improved upon the state-of-the-art IndicWhisper (16.64). Note that IndicWhisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib18\" title=\"\">18</a>]</cite> was fine-tuned with 2150 hours of Hindi data, while we use only 13 hours of data. Net-1 maintains its WER (24.84 after pruning), comparable to its full model (23.32), and VWS (22.88). Net-3 (19.58), after pruning, is also fairly competitive with full IndicWhisper (16.64). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S4.T3\" title=\"Table 3 &#8227; 4 RESULTS AND DISCUSSION &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (columns &#8216;Whisper&#8217;, &#8216;Indic&#8217;, &#8216;Ours&#8217;), WERs obtained with our normalizer lie between WERs with Whisper and Indic, showing it balances the two extremes.</p>\n\n",
                "matched_terms": [
                    "model",
                    "net3",
                    "pruning",
                    "net1",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a framework TSPAR to obtain compact fine-tuned versions of Whisper. TSPAR applies SGL loss regularization to induce both column-wise structured sparsity and element-wise unstructured sparsity. Structured sparsity gives us a significant reduction in FLOPs and model weights. TSPAR then performs a two-pass pruning adaptive to weight statistics, exploiting the induced sparsity.\nAs a result, we outperform the state-of-the-art IMP method both in pruning and WER performance, making our models suitable for resource constrained edge device applications.\nWe also design our custom Hindi text normalizer balancing the over-simplification by Whisper and over-preservation rules by Indic normalizers.\nA future direction is to leverage our TSPAR method to design low-complex variants of other state-of-the-art models under resource constraints.\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "flops",
                    "pruning"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models",
        "caption": "Table 3: WER comparison across different text normalizers before pruning",
        "body": "Model\nWhisper\nIndic\nOurs\n\n\n\n\nVWM\n10.95\n19.74\n15.71\n\n\n[18]\n7.99\n23.46\n16.64\n\n\nNet-3\n10.98\n20.03\n15.86\n\n\nVWS\n16.44\n27.5\n22.88\n\n\nNet-1\n16.53\n28.09\n23.32\n\n\nNet-2\n17.04\n28.57\n23.94",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Whisper</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Indic</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Ours</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">VWM</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">10.95</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">19.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">15.71</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib18\" title=\"\">18</a>]</cite></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">7.99</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">23.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16.64</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Net-3</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">10.98</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">15.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">VWS</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">16.44</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">27.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">22.88</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Net-1</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">16.53</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">28.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">23.32</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Net-2</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">17.04</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">28.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">23.94</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "whisper",
            "indic",
            "model",
            "across",
            "before",
            "text",
            "vws",
            "vwm",
            "different",
            "net3",
            "pruning",
            "ours",
            "net1",
            "net2",
            "normalizers",
            "wer",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our method obtains WER of 23.32, 23.94 and 15.86, on Net-1, 2, 3, respectively, with SGL regularization before pruning. We conduct more experiments with various combinations of <math alttext=\"T_{1},T_{2},T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub><mo>,</mo><msub><mi>T</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">T_{1},T_{2},T_{3}</annotation></semantics></math> to set <math alttext=\"\\theta_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{w}</annotation></semantics></math> for the Net-1,2, 3 to provide the tradeoff between WER and pruning <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math>. A few experimental results are reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S4.T1\" title=\"Table 1 &#8227; 4 RESULTS AND DISCUSSION &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. Net-1 gives lowest WER of <math alttext=\"24.84\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m4\" intent=\":literal\"><semantics><mn>24.84</mn><annotation encoding=\"application/x-tex\">24.84</annotation></semantics></math> with 35.4% reduction in parameters. The &#8221;lowest WER&#8221; refers to the smallest WER obtained among all the experiments conducted for a model. The lowest WER reported for Net-2 is 32.86 with 32.2% reduction in parameters. This indicates that all layers must not be regularized as non-FC layers capture significant information about the ASR learning. Hence, we refrain from performing regularization on all layers of Whisper-medium, but rather only on FC layers. Net-3 gives lowest WER of 19.58 when being pruned by <math alttext=\"31\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m5\" intent=\":literal\"><semantics><mrow><mn>31</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">31\\%</annotation></semantics></math> of parameters. All our models substantially outperform the baseline IMP-based method both in terms of pruning % and WER. Specifically, Net-1 prunes 18.7% more parameters with a WER reduction of 12.31 when compared to IMP on Whisper-small. Similarly, on Whisper-medium, Net-3 achieves a WER reduction by 10.91 with 10.71% more pruning. Further, in all our three networks, one can achieve a greater pruning <math alttext=\"\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m6\" intent=\":literal\"><semantics><mo>%</mo><annotation encoding=\"application/x-tex\">\\%</annotation></semantics></math> of upto <math alttext=\"44.9\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m7\" intent=\":literal\"><semantics><mn>44.9</mn><annotation encoding=\"application/x-tex\">44.9</annotation></semantics></math>, <math alttext=\"38.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m8\" intent=\":literal\"><semantics><mn>38.4</mn><annotation encoding=\"application/x-tex\">38.4</annotation></semantics></math>, and <math alttext=\"51.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m9\" intent=\":literal\"><semantics><mn>51.5</mn><annotation encoding=\"application/x-tex\">51.5</annotation></semantics></math> respectively, provided the increase in WER is tolerable for the ASR application considered. In all experiments, WERs shown are computed using our proposed custom Hindi normalizer. However, we also compare with other normalizers in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S4.T3\" title=\"Table 3 &#8227; 4 RESULTS AND DISCUSSION &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">In order to overcome shortcomings in existing Whisper and Indic normalizers, our proposed normalizer: (1) preserves diacritics except nasal marks, expands conjunct characters into constituent consonants without change in meaning; (2) removes punctuation but retains numerals and spacing to align model output and reference text; (3) maps common morphological variants, e.g., btAie, btAao (&#8220;tell&#8221;), to canonical forms; (4) normalizes acoustically similar vowels, e.g., (i (IPA i), I (IPA i:), to a standard representation for robustness to audio distortion.\nIts comparison with existing normalizers on WER evaluations is given in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S4.T3\" title=\"Table 3 &#8227; 4 RESULTS AND DISCUSSION &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">VWS, VWM refer to Whisper-small, Whisper-medium, respectively, when fine-tuned without any regularization. Net-3 (15.86) has improved upon the state-of-the-art IndicWhisper (16.64). Note that IndicWhisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib18\" title=\"\">18</a>]</cite> was fine-tuned with 2150 hours of Hindi data, while we use only 13 hours of data. Net-1 maintains its WER (24.84 after pruning), comparable to its full model (23.32), and VWS (22.88). Net-3 (19.58), after pruning, is also fairly competitive with full IndicWhisper (16.64). In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S4.T3\" title=\"Table 3 &#8227; 4 RESULTS AND DISCUSSION &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> (columns &#8216;Whisper&#8217;, &#8216;Indic&#8217;, &#8216;Ours&#8217;), WERs obtained with our normalizer lie between WERs with Whisper and Indic, showing it balances the two extremes.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Whisper models have achieved remarkable progress in speech recognition; yet their large size remains a bottleneck for deployment on resource-constrained edge devices. This paper proposes a framework to design fine-tuned variants of Whisper which address the above problem. Structured sparsity is enforced via the Sparse Group LASSO penalty as a loss regularizer, to reduce the number of FLOating Point operations (FLOPs). Further, a weight statistics aware pruning algorithm is proposed. We also design our custom text normalizer for WER evaluation. On Common Voice 11.0 Hindi dataset, we obtain, without degrading WER, (a) 35.4% reduction in model parameters, 14.25% lower memory consumption and 18.5% fewer FLOPs on Whisper-small, and (b) 31% reduction in model parameters, 15.29% lower memory consumption and 16.95% fewer FLOPs on Whisper-medium; and, (c) substantially outperform the state-of-the-art Iterative Magnitude Pruning based method by pruning 18.7% more parameters along with a 12.31 reduction in WER.</p>\n\n",
                "matched_terms": [
                    "model",
                    "text",
                    "pruning",
                    "wer",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\nfine-tuning, loss regularization, structured sparsity, pruning, text normalizer</p>\n\n",
                "matched_terms": [
                    "text",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There has been active research on fine-tuning large pre-trained models like Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib1\" title=\"\">1</a>]</cite> for Automatic Speech Recognition (ASR) in different low-resource languages, such as Amharic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib2\" title=\"\">2</a>]</cite>, Nepali <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib3\" title=\"\">3</a>]</cite>, Swiss German <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib4\" title=\"\">4</a>]</cite>, Afrikaans, Belarusian, Icelandic, Kazakh, Marathi, Nepali, and Swahili <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib5\" title=\"\">5</a>]</cite>. The IndicWhisper model in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib6\" title=\"\">6</a>]</cite> fine-tunes Whisper-medium on <math alttext=\"10,700\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>10</mn><mo>,</mo><mn>700</mn></mrow><annotation encoding=\"application/x-tex\">10,700</annotation></semantics></math> hours of data belonging to <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m2\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> Indian languages, including Hindi. In edge applications limited by compute and memory, one would typically like to prune these models.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pruning of neural networks has been studied in literature, starting from the landmark paper<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib7\" title=\"\">7</a>]</cite> which demonstrated pruning without loss of accuracy for a simple neural network based on second order loss derivatives. The authors of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib8\" title=\"\">8</a>]</cite> demonstrated that through iterative magnitude pruning (IMP), one can recover a subnetwork that reaches an accuracy comparable to that of the full network, known as the lottery ticket hypothesis (LTH). Based on this hypothesis, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib9\" title=\"\">9</a>]</cite> pruned a transformer model with IMP. Following this, the authors of <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib10\" title=\"\">10</a>]</cite> applied the pruning methodology of LTH to the Whisper model and obtained results comparable to the unpruned model. However, the existing works have three major limitations: (1) the pruning methods are iterative<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib10\" title=\"\">10</a>]</cite>, (2) they require additional adapters plugged into the model layers, increasing the inference time complexity<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib10\" title=\"\">10</a>]</cite>, and, (3) pruning with IMP does not reduce the number of FLOPs as the pruning is unstructured.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to address these shortcomings, this paper proposes a fine-tuning and pruning method, called TSPAR (fine<span class=\"ltx_text ltx_font_bold\">T</span>uning for <span class=\"ltx_text ltx_font_bold\">S</span>tructured s<span class=\"ltx_text ltx_font_bold\">P</span>arsity and <span class=\"ltx_text ltx_font_bold\">A</span>dapti-ve p<span class=\"ltx_text ltx_font_bold\">R</span>uning), which is applied on the Whisper-small and Whisper-medium models. In pruning literature, loss regularization has traditionally been used to sparsify the network during training <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib13\" title=\"\">13</a>]</cite>. However, imposing a penalty on the weight magnitudes only imposes unstructured sparsity, which does not reduce the number of FLOating Point operations (FLOPs). Using the Sparse Group LASSO (SGL)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib14\" title=\"\">14</a>]</cite> penalty function to regularize loss, one can achieve structured sparsity wherever it exists and unstructured sparsity elsewhere, which saves both FLOPs and memory utilization. We employ SGL regularized fine-tuning on Whisper, and prune according to the weight statistics of the regularized model, achieving comparable results to the full model. Ours is a non-iterative approach, which prunes up to <math alttext=\"51.5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>51.5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">51.5\\%</annotation></semantics></math> parameters in one shot with a considerable reduction in FLOPs, without requiring any additional adapters.</p>\n\n",
                "matched_terms": [
                    "ours",
                    "model",
                    "whisper",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WER metric has traditionally been used to evaluate ASR models. Existing normalizers such as Whisper&#8217;s default &#8220;Non-English&#8221; normalizer<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib1\" title=\"\">1</a>]</cite> or the Hindi normalizer from Indic NLP library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://indic-nlp-library.readthedocs.io/en/latest/</span></span></span> are not ideal: Whisper&#8217;s normalization is overly aggressive and discards meaningful diacritics, while Indic normalization preserves all constructs.\nThis makes WER values across languages unfair; Hindi ends up with a more misleading WER than less phonemically complex languages like English <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib15\" title=\"\">15</a>]</cite>.\nWe propose a more balanced text normalization for Hindi, and adopt it in our WER computation. In summary, our main contributions are threefold: (1) imposing structured sparsity on fine-tuned Whisper models using the SGL penalty function, thereby reducing both memory footprint and FLOPs; (2) proposing a weight-statistics&#8211;driven pruning strategy that adapts dynamically to model parameters; and (3) designing a custom Hindi text normalizer for fair WER evaluation.</p>\n\n",
                "matched_terms": [
                    "indic",
                    "model",
                    "across",
                    "text",
                    "pruning",
                    "normalizers",
                    "wer",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Loss regularization techniques such as weight decay or <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> norm minimization involves adding a penalty term on the magnitude of the weights to regularize the cross entropy function <math alttext=\"\\mathcal{L}_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{CE}</annotation></semantics></math>. This falls in the category of regularized loss minimization, while just using <math alttext=\"\\mathcal{L}_{CE}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{CE}</annotation></semantics></math> is empirical risk minimization. The sparsity induced by the <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> mechanism is unstructured, as the weights being driven to zero are randomly spread across the weight matrices. Pruning these parameters reduces memory requirement, but does not reduce FLOPs.\nIn the context of statistical literature, Sparse Group LASSO (SGL) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib14\" title=\"\">14</a>]</cite> was introduced where, an additional grouped loss regularization imposes structured sparsity on a group of weights as a whole. We exploit this SGL penalty function in our work to drive certain columns of weight matrices to zero.\nLet <math alttext=\"\\mathbf{W}_{jl}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}</annotation></semantics></math> be the <math alttext=\"j^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m6\" intent=\":literal\"><semantics><msup><mi>j</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">j^{th}</annotation></semantics></math> weight matrix of the <math alttext=\"l^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m7\" intent=\":literal\"><semantics><msup><mi>l</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">l^{th}</annotation></semantics></math> layer, with a total of <math alttext=\"J_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>J</mi><mi>l</mi></msub><annotation encoding=\"application/x-tex\">J_{l}</annotation></semantics></math> matrices for <math alttext=\"1\\leq l\\leq L\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8804;</mo><mi>l</mi><mo>&#8804;</mo><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">1\\leq l\\leq L</annotation></semantics></math> layers. Let <math alttext=\"\\mathbf{W}_{jl}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m10\" intent=\":literal\"><semantics><msubsup><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}^{i}</annotation></semantics></math> denote the <math alttext=\"i^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m11\" intent=\":literal\"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">i^{th}</annotation></semantics></math> column of <math alttext=\"\\mathbf{W}_{jl}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m12\" intent=\":literal\"><semantics><msub><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}</annotation></semantics></math> and <math alttext=\"I_{J_{l}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m13\" intent=\":literal\"><semantics><msub><mi>I</mi><msub><mi>J</mi><mi>l</mi></msub></msub><annotation encoding=\"application/x-tex\">I_{J_{l}}</annotation></semantics></math> denote the number of columns of each matrix <math alttext=\"\\mathbf{W}_{jl}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m14\" intent=\":literal\"><semantics><msub><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}</annotation></semantics></math>. Each such column <math alttext=\"\\mathbf{W}_{jl}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m15\" intent=\":literal\"><semantics><msubsup><mi>&#119830;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{W}_{jl}^{i}</annotation></semantics></math> represents the set of outgoing connections from a neuron in layer <math alttext=\"j\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m16\" intent=\":literal\"><semantics><mi>j</mi><annotation encoding=\"application/x-tex\">j</annotation></semantics></math> to every neuron in the next layer <math alttext=\"j+1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m17\" intent=\":literal\"><semantics><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">j+1</annotation></semantics></math>. By imposing column-wise structured sparsity, the entire column can be pruned. These pruned columns are excluded from matrix multiplication, resulting in the reduction of FLOPs. Hence, the total loss is defined as,</p>\n\n",
                "matched_terms": [
                    "pruning",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a two-pass pruning algorithm, which is given in Algorithm <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#alg1\" title=\"Algorithm 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The structured <math alttext=\"L_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">L_{2}</annotation></semantics></math> loss regularization drives entire columns to zero, and hence in the first pass (lines 3-7) we prune columns which are approximately sparse. The columns with high density of &#8220;almost zero weights&#8221; are called &#8221;approximate sparse columns&#8221;. A weight, <math alttext=\"w\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mi>w</mi><annotation encoding=\"application/x-tex\">w</annotation></semantics></math> is defined as an &#8220;almost zero weight&#8221; if <math alttext=\"|w|&lt;\\theta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi>w</mi><mo stretchy=\"false\">|</mo></mrow><mo>&lt;</mo><msub><mi>&#952;</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">|w|&lt;\\theta_{0}</annotation></semantics></math> for some positive constant <math alttext=\"\\theta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\theta_{0}</annotation></semantics></math>. A column <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math> of a weight matrix is defined as &#8220;approximately sparse&#8221; if the approximate sparsity of the column, <math alttext=\"\\hat{S}_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>S</mi><mo>^</mo></mover><mi>c</mi></msub><annotation encoding=\"application/x-tex\">\\hat{S}_{c}</annotation></semantics></math> is greater than a positive threshold <math alttext=\"\\theta_{\\hat{S}_{c}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><msub><mover accent=\"true\"><mi>S</mi><mo>^</mo></mover><mi>c</mi></msub></msub><annotation encoding=\"application/x-tex\">\\theta_{\\hat{S}_{c}}</annotation></semantics></math>. Approximate sparsity is defined as <math alttext=\"\\hat{S}_{c}(w;\\theta_{0})=\\frac{\\mathbf{1}_{|w|&lt;\\theta_{0}}(C)}{|C|}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mover accent=\"true\"><mi>S</mi><mo>^</mo></mover><mi>c</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>w</mi><mo>;</mo><msub><mi>&#952;</mi><mn>0</mn></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msub><mn>&#120783;</mn><mrow><mrow><mo stretchy=\"false\">|</mo><mi>w</mi><mo stretchy=\"false\">|</mo></mrow><mo>&lt;</mo><msub><mi>&#952;</mi><mn>0</mn></msub></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mrow><mo stretchy=\"false\">|</mo><mi>C</mi><mo stretchy=\"false\">|</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\hat{S}_{c}(w;\\theta_{0})=\\frac{\\mathbf{1}_{|w|&lt;\\theta_{0}}(C)}{|C|}</annotation></semantics></math> where, <math alttext=\"\\mathbf{1}(.)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><mrow><mn>&#120783;</mn><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0.167em\">.</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{1}(.)</annotation></semantics></math> is an indicator function, and <math alttext=\"|C|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi>C</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|C|</annotation></semantics></math> is the length of column <math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m11\" intent=\":literal\"><semantics><mi>C</mi><annotation encoding=\"application/x-tex\">C</annotation></semantics></math>. During the second pass of pruning (lines 8&#8211;12), we prune individual weights that have been driven to zero by the <math alttext=\"L_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m12\" intent=\":literal\"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">L_{1}</annotation></semantics></math> loss regularization with a threshold <math alttext=\"\\theta_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m13\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{w}</annotation></semantics></math>.\nWhile pruning thresholds are typically tuned by trial and error, we propose to analyze the weight statistics of the model and derive pruning thresholds <math alttext=\"\\theta_{0},\\theta_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m14\" intent=\":literal\"><semantics><mrow><msub><mi>&#952;</mi><mn>0</mn></msub><mo>,</mo><msub><mi>&#952;</mi><mi>c</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\theta_{0},\\theta_{c}</annotation></semantics></math>, and <math alttext=\"\\theta_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m15\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{w}</annotation></semantics></math>. The detailed analysis is provided in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S3\" title=\"3 ANALYSIS OF WEIGHT STATISTICS &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pruning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we analyze the weight statistics of three fine-tuned models when regularization is applied on: (a) only fully connected (FC) layers of Whisper-small, (b) all layers of Whisper-small, and (c) only FC layers of Whisper-medium. These models are referred to as Net-1, Net-2, and Net-3, respectively. In all models, the mean <math alttext=\"\\mu\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>&#956;</mi><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math> and standard deviation <math alttext=\"\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mi>&#963;</mi><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math> of weights are in the order of <math alttext=\"10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-4}</annotation></semantics></math> and <math alttext=\"10^{-2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>2</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-2}</annotation></semantics></math>, respectively, in all layers (except LayerNorm in Net-2). We observe three main types of weight distributions, namely (A) highly spiked (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf1\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(a)</span></a>), (B) narrowly spread (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf2\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(b)</span></a>), and (C) widely spread (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf3\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(c)</span></a>) distributions around mean. In highly spiked distributions (type A), the weights between the <math alttext=\"25^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m5\" intent=\":literal\"><semantics><msup><mn>25</mn><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">25^{th}</annotation></semantics></math> and <math alttext=\"75^{th}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m6\" intent=\":literal\"><semantics><msup><mn>75</mn><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msup><annotation encoding=\"application/x-tex\">75^{th}</annotation></semantics></math> percentiles, denoted by <math alttext=\"Q_{2},Q_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m7\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mn>2</mn></msub><mo>,</mo><msub><mi>Q</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">Q_{2},Q_{3}</annotation></semantics></math>, are in the order of <math alttext=\"10^{-11}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m8\" intent=\":literal\"><semantics><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>11</mn></mrow></msup><annotation encoding=\"application/x-tex\">10^{-11}</annotation></semantics></math>. This implies that <math alttext=\"50\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m9\" intent=\":literal\"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">50\\%</annotation></semantics></math> of the weights are very close to zero, and hence, can be pruned immediately. However, we prune all weights between <math alttext=\"\\mu-0.1\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m10\" intent=\":literal\"><semantics><mrow><mi>&#956;</mi><mo>&#8722;</mo><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mu-0.1\\sigma</annotation></semantics></math> and <math alttext=\"\\mu+0.1\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m11\" intent=\":literal\"><semantics><mrow><mi>&#956;</mi><mo>+</mo><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\mu+0.1\\sigma</annotation></semantics></math> in order to prune more weights than those in the interquartile range. So, we set the threshold function as <math alttext=\"T_{1}=0.1\\sigma\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m12\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>=</mo><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><annotation encoding=\"application/x-tex\">T_{1}=0.1\\sigma</annotation></semantics></math>. In narrowly spread distributions (type B), <math alttext=\"Q_{2},Q_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m13\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mn>2</mn></msub><mo>,</mo><msub><mi>Q</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">Q_{2},Q_{3}</annotation></semantics></math> are at <math alttext=\"\\sim(\\mu\\pm 0.3\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m14\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#956;</mi><mo>&#177;</mo><mrow><mn>0.3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim(\\mu\\pm 0.3\\sigma)</annotation></semantics></math>. To prune slightly more weights than those in the inter-quartile range, the threshold function is set as <math alttext=\"T_{2}=\\max\\{|Q_{2}|,|Q_{3}|\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m15\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>2</mn></msub><mo>=</mo><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>Q</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>Q</mi><mn>3</mn></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">T_{2}=\\max\\{|Q_{2}|,|Q_{3}|\\}</annotation></semantics></math>. The widely spread distributions (type C), have <math alttext=\"Q_{2},Q_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m16\" intent=\":literal\"><semantics><mrow><msub><mi>Q</mi><mn>2</mn></msub><mo>,</mo><msub><mi>Q</mi><mn>3</mn></msub></mrow><annotation encoding=\"application/x-tex\">Q_{2},Q_{3}</annotation></semantics></math> locations at <math alttext=\"\\sim(\\mu\\pm 0.67\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m17\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#956;</mi><mo>&#177;</mo><mrow><mn>0.67</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#963;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim(\\mu\\pm 0.67\\sigma)</annotation></semantics></math>. We can not apply threshold to be <math alttext=\"\\max\\{|Q_{2}|,|Q_{3}|\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m18\" intent=\":literal\"><semantics><mrow><mi>max</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>Q</mi><mn>2</mn></msub><mo stretchy=\"false\">|</mo></mrow><mo>,</mo><mrow><mo stretchy=\"false\">|</mo><msub><mi>Q</mi><mn>3</mn></msub><mo stretchy=\"false\">|</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\max\\{|Q_{2}|,|Q_{3}|\\}</annotation></semantics></math> as it will prune too many weights. Hence, we use the widely adopted threshold function, <math alttext=\"T_{3}=\\eta W_{max}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m19\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>3</mn></msub><mo>=</mo><mrow><mi>&#951;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>W</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">T_{3}=\\eta W_{max}</annotation></semantics></math> where <math alttext=\"W_{max}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m20\" intent=\":literal\"><semantics><msub><mi>W</mi><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi></mrow></msub><annotation encoding=\"application/x-tex\">W_{max}</annotation></semantics></math> is the maximum absolute weight of matrix <math alttext=\"W\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m21\" intent=\":literal\"><semantics><mi>W</mi><annotation encoding=\"application/x-tex\">W</annotation></semantics></math> and <math alttext=\"\\eta&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m22\" intent=\":literal\"><semantics><mrow><mi>&#951;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\eta&gt;0</annotation></semantics></math> is a hyperparameter <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#bib.bib13\" title=\"\">13</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "net2",
                    "net1",
                    "net3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze the weight distributions of the columns of weight matrices in different layers. The columns that follow highly spiked distributions (type A), have weights very close to mean (<math alttext=\"\\sim 0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0</annotation></semantics></math>) i.e., almost zero weights. To identify these weights, <math alttext=\"\\theta_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\theta_{0}</annotation></semantics></math> is set as <math alttext=\"0.1\\sigma_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m3\" intent=\":literal\"><semantics><mrow><mn>0.1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#963;</mi><mi>c</mi></msub></mrow><annotation encoding=\"application/x-tex\">0.1\\sigma_{c}</annotation></semantics></math>. To prune columns with 90% approximate sparsity, <math alttext=\"\\theta_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>c</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{c}</annotation></semantics></math> is set as <math alttext=\"0.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m5\" intent=\":literal\"><semantics><mn>0.9</mn><annotation encoding=\"application/x-tex\">0.9</annotation></semantics></math> (see a sample pruned column weight distribution in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf4\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(d)</span></a>). The weights of the unpruned columns (a sample weight distribution shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12666v1#S2.F1.sf5\" title=\"In Figure 1 &#8227; 2.1 Regularized training for structured sparsity &#8227; 2 PROPOSED METHOD &#8227; Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models\"><span class=\"ltx_text ltx_ref_tag\">1(e)</span></a>) of a layer are further pruned in the second pass of pruning using a threshold <math alttext=\"\\theta_{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m6\" intent=\":literal\"><semantics><msub><mi>&#952;</mi><mi>w</mi></msub><annotation encoding=\"application/x-tex\">\\theta_{w}</annotation></semantics></math>, which is set as <math alttext=\"T_{1},T_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m7\" intent=\":literal\"><semantics><mrow><msub><mi>T</mi><mn>1</mn></msub><mo>,</mo><msub><mi>T</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">T_{1},T_{2}</annotation></semantics></math> or <math alttext=\"T_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m8\" intent=\":literal\"><semantics><msub><mi>T</mi><mn>3</mn></msub><annotation encoding=\"application/x-tex\">T_{3}</annotation></semantics></math>, depending on whether the layer follows distribution type A, B, or C. We observe the following weight distributions in Net-1, 2, and 3.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Net-1:</span> The first 6 FC layers of the encoder have type A distributions, remaining encoder layers follow type B, while the decoder layers follow type C distributions.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Net-2:</span> There are 3 FC + 7 attention (ATT) + 1 convolution (CONV) layers in the encoder, and 14 ATT + 2 FC layers in the decoder; i.e., a total of 27 layers (denoted by <math alttext=\"\\mathcal{P}_{1})\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.p2.m9\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{1})</annotation></semantics></math>, which exhibit distributions of type A. The remaining layers follow distributions of type C. The LayerNorm layers of both the encoder and decoder have significantly larger weights (<math alttext=\"\\mu\\sim 1.0,\\sigma\\sim 10^{-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m10\" intent=\":literal\"><semantics><mrow><mrow><mi>&#956;</mi><mo>&#8764;</mo><mn>1.0</mn></mrow><mo>,</mo><mrow><mi>&#963;</mi><mo>&#8764;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\mu\\sim 1.0,\\sigma\\sim 10^{-1}</annotation></semantics></math>) and hence, cannot be pruned.\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Net-3:</span> The first 5 layers of the encoder follow type A, while all the remaining layers of the encoder and decoder follow type B distribution. We apply the derived thresholds and provide results in the next section.</p>\n\n",
                "matched_terms": [
                    "different",
                    "net3",
                    "pruning",
                    "net2",
                    "net1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a framework TSPAR to obtain compact fine-tuned versions of Whisper. TSPAR applies SGL loss regularization to induce both column-wise structured sparsity and element-wise unstructured sparsity. Structured sparsity gives us a significant reduction in FLOPs and model weights. TSPAR then performs a two-pass pruning adaptive to weight statistics, exploiting the induced sparsity.\nAs a result, we outperform the state-of-the-art IMP method both in pruning and WER performance, making our models suitable for resource constrained edge device applications.\nWe also design our custom Hindi text normalizer balancing the over-simplification by Whisper and over-preservation rules by Indic normalizers.\nA future direction is to leverage our TSPAR method to design low-complex variants of other state-of-the-art models under resource constraints.\n</p>\n\n",
                "matched_terms": [
                    "indic",
                    "model",
                    "text",
                    "pruning",
                    "normalizers",
                    "wer",
                    "whisper"
                ]
            }
        ]
    }
}