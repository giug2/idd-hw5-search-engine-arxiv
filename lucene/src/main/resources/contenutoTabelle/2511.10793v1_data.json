{
    "S5.T1": {
        "source_file": "Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces",
        "caption": "Table 1: EER (%) comparison of pretrained models (PTMs) under mismatched training-testing conditions. TR-A TE-D: Train on ASVP, Test on DFADD; TR-D TE-A: Train on DFADD, Test on ASVP. Lower EER indicates better performance.",
        "body": "PTMs\nBaseline\nRHYME\n\n\nTR-A TE-D\nTR-D TE-A\nTR-A TE-D\nTR-D TE-A\n\n\nXvector\n33.27\n22.71\n28.09\n16.96\n\n\nWavLM\n29.37\n20.05\n21.87\n13.17\n\n\nHuBERT\n33.52\n25.56\n28.66\n16.63\n\n\nWhisper\n30.36\n24.38\n27.54\n17.45\n\n\nWav2Vec\n28.42\n19.89\n22.81\n12.46\n\n\nPaSST\n27.24\n17.23\n20.38\n11.59\n\n\nUSAD\n20.51\n15.19\n14.12\n10.26",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">PTMs</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Baseline</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">TR-A TE-D</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">TR-D TE-A</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">TR-A TE-D</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">TR-D TE-A</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Xvector</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">22.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">16.96</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">WavLM</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">29.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">20.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#ECF7EE;padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF7EE;\">21.87</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">13.17</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">HuBERT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">33.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">25.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.66</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">16.63</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Whisper</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">30.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">24.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">27.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">17.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Wav2Vec</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#ECF7EE;padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF7EE;\">28.42</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#ECF7EE;padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF7EE;\">19.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">22.81</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#ECF7EE;padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF7EE;\">12.46</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PaSST</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#C8E6D2;padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C8E6D2;\">27.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#C8E6D2;padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C8E6D2;\">17.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#C8E6D2;padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C8E6D2;\">20.38</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#C8E6D2;padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C8E6D2;\">11.59</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">USAD</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#A4D6B9;padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#A4D6B9;\">20.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#A4D6B9;padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#A4D6B9;\">15.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"--ltx-bg-color:#A4D6B9;padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#A4D6B9;\">14.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#A4D6B9;padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#A4D6B9;\">10.26</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "rhyme",
            "ptms",
            "hubert",
            "lower",
            "pretrained",
            "better",
            "mismatched",
            "test",
            "trainingtesting",
            "conditions",
            "baseline",
            "wavlm",
            "usad",
            "whisper",
            "wav2vec",
            "tea",
            "train",
            "trd",
            "performance",
            "indicates",
            "dfadd",
            "ted",
            "passt",
            "asvp",
            "models",
            "under",
            "xvector",
            "eer",
            "comparison"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Benchmark Dataset &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the EER (%) results for each model across two cross-domain setups: training on ASVspoof and testing on DFADD (TR-A &#8594; TE-D), and the reverse (TR-D &#8594; TE-A). Each PTM is assessed in both its original form (Baseline) and its <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span>-enhanced version (Novel). From the results, we observe that across both settings, the RHYME-enhanced models consistently achieve lower EERs than their baseline counterparts. For instance, WavLM benefits from a noticeable EER drop from 29.37% to 21.87% (TR-A &#8594; TE-D), and from 20.05% to 13.17% (TR-D &#8594; TE-A). Similarly, models like HuBERT, Whisper, and Wav2Vec2 also show clear improvements under both configurations. These trends confirm that RHYME effectively helps in aligning diverse cues, leading to better generalization under unseen conditions. A consistent observation is that training on DFADD and testing on ASVspoof (TR-D &#8594; TE-A) results in relatively lower EERs than the reverse direction. This suggests that the diverse spoofing methods and higher-quality generation found in DFADD lead to more transferable representations. Among all models, USAD achieves the strongest results with EERs of 14.12% (TR-A &#8594; TE-D) and 10.26% (TR-D &#8594; TE-A), showcasing RHYME&#8217;s ability to handle significant domain shifts. It is also important to note that the DFADD paper <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024dfadd</span></cite> reports an average 32.44% EER for the state-of-the-art end-to-end model AASIST-L under the TR-A &#8594; TE-D setup. While our framework does not perform full model fine-tuning and instead relies on frozen pretrained embeddings, our framework achieves a substantially lower EER of 14.12%&#8212;underscoring the efficacy of geometry-aware fusion for robust and synthesis-invariant detection. \n<br class=\"ltx_break\"/>In Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Benchmark Dataset &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we evaluate the performance of <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span>, which is specifically designed to assess generalization in truly unseen conditions. In this setup, we first extract embeddings using USAD backbone PTMs from both ASVspoof and DFADD datasets. These embeddings are used to train downstream models using our <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> framework. The testing phase involves audio samples synthesized from various TTS models&#8212;which were not part of the training data. These samples were collected from official demo pages of each model, introducing a strong domain shift due to differences in speaker identity, acoustic environments, and generation fidelity. Since the model was only trained on the curated subsets from DFADD, it did not encounter any of the demo-sourced speech samples during training, making this a challenging and realistic generalization test. Despite the difficulty of this setting, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> achieves strong performance across several domains. Among the generators, CMTTS consistently shows the lowest EERs, including 0% on DFADD, 14.14% on D1, and as low as 0.019% on F1. DiTTo-TTS and ReFlow-TTS also perform well across most domains, indicating RHYME&#8217;s ability to handle both autoregressive and diffusion-based synthesis techniques. On comparatively more variable systems like Diffr and DiPro, we observe a wider spread in EERs across domains; however, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> still maintains stability, with EERs well below 20% in most cases. To further understand our framework robustness, we conduct an additional set of experiments where the model is trained on data generated from a single synthesizer&#8212;such as D1, D2, D3, F1, or F2&#8212;and tested on unseen samples from other models like Voicebox and NaturalSpeech3. This controlled setup isolates how well the model can transfer from one generator&#8217;s characteristics to another. Even with such limited training data, RHYME generalizes effectively in many cases, suggesting that the learned representations are transferable across diverse generation styles. We further investigate the representational quality and decision reliability of RHYME using t-SNE visualizations and calibration plots. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F2\" title=\"Figure 2 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the distribution of embeddings using t-SNE. In subfigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>, we visualize raw USAD embeddings extracted from DFADD and ASVspoof. The overlap between real and fake samples suggests weak discrimination in the original embedding space. While, subfigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> shows RHYME-fused embeddings when trained on DFADD and evaluated on both DFADD and ASVspoof. Here, the real and fake classes form visibly distinct clusters, highlighting RHYME&#8217;s ability to disentangle domain-invariant and spoof-discriminative cues through geometric fusion. To evaluate how well model confidence aligns with actual correctness, we use calibration diagrams shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Subfigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a> depicts the calibration curve when the model is trained on DFADD and tested on both DFADD and ASVspoof. The curve closely follows the diagonal, indicating good calibration between predicted probabilities and true outcome frequencies, more importantly, the reliability curve remains closely aligned for the out-of-domain ASVspoof test split. This demonstrates that the model&#8217;s softmax outputs remain trustworthy even under out-of-domain detection. On the other hand, subfigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, where the model is trained on ASVspoof, shows larger deviations&#8212;highlighting miscalibration. These results reinforce that RHYME not only improves detection accuracy but also produces more reliable probability estimates, a key requirement for deployment in risk-sensitive applications.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenge of generalizable audio deepfake detection (ADD) across diverse speech synthesis paradigms&#8212;including conventional text-to-speech (TTS) systems and modern diffusion or flow-matching (FM) based generators. Prior work has mostly targeted individual synthesis families and often fails to generalize across paradigms due to overfitting to generation-specific artifacts. We hypothesize that synthetic speech, irrespective of its generative origin, leaves behind shared structural distortions in the embedding space that can be aligned through geometry-aware modeling. To this end, we propose <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span>, a unified detection framework that fuses utterance-level embeddings from diverse pretrained speech encoders using non-Euclidean projections. <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> maps representations into hyperbolic and spherical manifolds&#8212;where hyperbolic geometry excels at modeling hierarchical generator families, and spherical projections capture angular, energy-invariant cues such as periodic vocoder artifacts. The fused representation is obtained via Riemannian barycentric averaging, enabling synthesis-invariant alignment. <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> outperforms individual PTMs and homogeneous fusion baselines, achieving top performance and setting new state-of-the-art in cross-paradigm ADD.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "rhyme",
                    "performance",
                    "ptms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The field of speech synthesis has undergone a paradigm shift over the past few years with the emergence of neural text-to-speech (TTS) and diffusion-based models, delivering human-level audio realism that is increasingly difficult to distinguish from genuine speech <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yi2023audio</span></cite>. While these technological advances have enabled beneficial applications in accessibility and entertainment, they have simultaneously paved the way for malicious applications. Recent high-profile cases&#8212;such as voice clones used in emotional blackmail and scam calls&#8212;highlight the urgent need for reliable detection mechanisms, since human listeners are no longer able to consistently identify deepfake audio <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">barrington2025people</span></cite>. In response, audio deepfake detection (ADD) has emerged as a crucial research domain within speech security and digital forensics. Despite notable advances in audio deepfake detection, most existing systems are trained on speech generated by conventional TTS or vocoder-based models and demonstrate strong performance under seen conditions. However, these systems often exhibit degraded performance when exposed to synthetic speech generated using unseen paradigms or under distributional shift. <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2020generalization</span></cite> identified this issue early on, showing that detectors often fail even within the same benchmark when exposed to unfamiliar spoofing techniques. Extending this analysis, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kulkarni2024exploring</span></cite> evaluated a range of self-supervised models across datasets and synthesis types, reporting consistent performance drops in cross-domain settings. These findings highlight a fundamental limitation in existing ADD pipelines and point toward the need for detection strategies that are resilient to synthesis diversity and adaptable to new generation mechanisms. Although neural speech generators differ widely in architecture and training objectives, we hypothesize that synthetic speech&#8212;regardless of whether it is produced by TTS, vocoder, or diffusion-based models&#8212;shares certain latent artifacts that distinguish it from natural speech. These artifacts may not be easily perceptible in the waveform or spectrogram, but they can manifest as distortions in the underlying feature space. By leveraging diverse pre-trained speech encoders, it is possible to capture complementary representations that highlight different aspects of these synthesis-induced irregularities. The central challenge lies in aligning these heterogeneous embeddings in a way that preserves discriminative cues while minimizing overfitting to a specific synthesis family. This motivates our exploration of a shared, synthesis-invariant representation space for generalized detection; in this work, we emphasize acoustic evidence, analyzing signal-level artifacts while remaining independent of language or text.</p>\n\n",
                "matched_terms": [
                    "models",
                    "under",
                    "conditions",
                    "pretrained",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To this end, we propose <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME:</span> <span class=\"ltx_text ltx_font_bold\">R</span>iemannian fusion of <span class=\"ltx_text ltx_font_bold\">HY</span>perbolic and sph<span class=\"ltx_text ltx_font_bold\">E</span>rical embeddings a unified and geometry-aware audio deepfake detection framework. RHYME fuses utterance-level embeddings from multiple frozen speech PTMs and projects them into two complementary non-Euclidean manifolds: hyperbolic space (to capture hierarchical generative traces) and spherical space (to model periodic and spectral anomalies). These projections are then fused using Riemannian barycentric averaging in the Poincar&#233; ball, forming a synthesis-agnostic representation space.\nEach of these geometric components serves a distinct purpose&#8212;hyperbolic projection models generator hierarchies, spherical projection highlights periodic artifacts, and barycentric fusion unifies them while preserving their respective curvatures. Hyperbolic curvature compactly encodes tree-like generator lineages (e.g., vocoder <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> autoregressive TTS (AR-TTS) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> diffusion/flow) with low distortion, placing related synthesizers along nearby geodesics while genuine speech occupies distinct regions. Consequently, the hyperbolic branch separates synthetic from real speech by tracing these geodesic hierarchies (model families/versions). As newer diffusion or flow-based methods still follow a structured progression of architectural improvements, their embeddings naturally fall on new branches of this learned hyperbolic hierarchy. In parallel, the spherical branch captures angular, energy-invariant cues associated with periodicity artifacts, such as those introduced by vocoders or diffusion sampling (See Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S4.SS1\" title=\"4.1 Proposed Framework: RHYME &#8227; 4 Modeling Pipeline &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a> for details). Even though waveform-level characteristics vary, these models still imprint subtle periodic distortions, which manifest as consistent angular deviations on the hypersphere. By fusing these complementary cues via Riemannian averaging, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> ensures that unseen synthetic speech cannot simultaneously satisfy both the hierarchical and periodic constraints of genuine speech&#8212;resulting in robust generalization across synthesis families. <span class=\"ltx_text ltx_font_italic\">We hypothesize that this geometry-aware fusion encourages synthesis-invariant alignment while preserving discriminative cues necessary for deepfake detection.</span> We validate <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> through comprehensive experiments on two widely-used benchmark datasets&#8212;ASVspoof and DFADD&#8212;which include speech synthesized using TTS, vocoder, and diffusion-based models. Our evaluation covers zero-shot, cross-corpus, and unseen-generator scenarios to rigorously test generalization. Across all settings, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> consistently delivers strong performance, even when faced with synthetic speech from generators it has never seen during training. Unlike previous methods that tend to overfit to specific synthesis families, our geometry-aware fusion strategy enables <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> to learn shared structural cues across different types of fake speech. This shows that RHYME handles both familiar and previously unseen generators well, making it a strong candidate for practical deployment in deepfake detection systems. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "ptms",
                    "test",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate that <span class=\"ltx_text ltx_font_bold\">RHYME</span> consistently outperforms individual PTMs and homogeneous fusion baselines, achieving state-of-the-art performance under cross-corpus and unseen-generator conditions.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "ptms",
                    "under",
                    "conditions",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The field of audio deepfake detection (ADD) originated from the vulnerabilities identified in automatic speaker verification (ASV) systems. Early countermeasures utilized handcrafted spectral features such as constant-Q cepstral coefficients (CQCC) and mel-frequency cepstral coefficients (MFCC), often paired with Gaussian mixture models or support vector machines. Early research in this area primarily leveraged handcrafted spectral features such as constant-Q cepstral coefficients (CQCC) and linear frequency cepstral coefficients (LFCC), often in combination with traditional machine learning classifiers for spoofing detection tasks <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">todisco2017constant</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kinnunen2017asvspoof</span></cite>. The advent of the ASVspoof challenge series <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">todisco2019asvspoof</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020asvspoof</span></cite> propelled the field toward deep learning solutions, with convolutional and recurrent neural networks trained on spectrograms or raw waveforms becoming standard for text-to-speech (TTS) and voice conversion (VC) based deepfake detection <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cai2023face</span></cite>. However, most of these methods are tailored to specific synthesis techniques and struggle to generalize to audio produced by newer paradigms such as diffusion and flow-matching models. The emergence of self-supervised learning (SSL) has significantly advanced the capabilities of audio deepfake detection. Models such as wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baevski2020wav2vec</span></cite>, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hsu2021hubert</span></cite> and WavLM <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span></cite> have demonstrated robust performance across various paralinguistic and spoofing benchmarks, including ASVspoof and ADD tasks. These models have since been adapted to a range of spoofing detection tasks. <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tak2022automatic</span></cite> investigated the efficacy of wav2vec 2.0 in detecting synthetic audio under various ASVspoof 2021 conditions, highlighting the potential of SSL models in practical ADD scenarios. Building on this line of work, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024audio</span></cite> proposed a multi-level fusion framework based on WavLM, achieving competitive performance across multiple spoofing benchmarks including ASVspoof 2019 and 2021. Building on this body of work, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kheir2025comprehensive</span></cite> conducted a detailed layer-wise study of multiple SSL models and found that lower transformer layers consistently offer the most discriminative features for deepfake detection across languages and tasks. These insights reinforce the potential of lightweight, generalizable detection pipelines. Nevertheless, most existing methods remain limited to single-model fine-tuning, restricting their adaptability to unseen synthesis techniques&#8212;an issue our work seeks to address through a fusion-driven, synthesis-agnostic approach. Despite progress in ADD, a key limitation persists in generalizing across diverse synthesis paradigms. Most existing detectors are trained primarily on speech generated by conventional text-to-speech (TTS) systems and struggle to maintain performance when exposed to speech produced using newer generative methods such as diffusion or flow-matching (FM) models. To address this shift, recent studies have proposed fusion-based frameworks that leverage multimodal foundation models <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chetia2025towards</span></cite> and paralinguistic speech encoders <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akhtar2025source</span></cite> to improve robustness in source attribution tasks. These approaches have shown the benefit of combining heterogeneous pre-trained models to capture generator-specific characteristics. Motivated by this direction, our work targets the detection problem under similar distribution shifts, specifically focusing on bridging generalization gaps across synthesis families. We aim to learn synthesis-invariant embeddings by fusing multiple speech encoders and projecting their representations into a hyperspherical space. This allows our model to detect synthetic speech reliably across both known and unseen generation mechanisms, including diffusion-based generators.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hubert",
                    "conditions",
                    "under",
                    "lower",
                    "wavlm",
                    "pretrained",
                    "performance",
                    "wav2vec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we describe the speech representation models considered in our study. \n<br class=\"ltx_break\"/>USAD<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/collections/MIT-SLS/usad-models-68491d4c7d0978b85d0c4299\" title=\"\">https://huggingface.co/collections/MIT-SLS/usad-models-68491d4c7d0978b85d0c4299</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chang2025usad</span></cite> is a universal audio representation model trained using multi-teacher distillation across speech, music, and sound domains, and we adopt its Base variant (94M parameters). PaSST<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/kkoutini/PaSST\" title=\"\">https://github.com/kkoutini/PaSST</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koutini2021efficient</span></cite> adapts vision transformers for spectrograms with patch-masking for regularization; we use the PaSST-S model (87M). Whisper<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-base\" title=\"\">https://huggingface.co/openai/whisper-base</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span></cite>, a multilingual ASR model trained on 680k hours of weakly supervised audio-text data, is used in its Base variant (74M), and we extract encoder embeddings. x-vector<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/speechbrain/spkrec-xvect-voxceleb\" title=\"\">https://huggingface.co/speechbrain/spkrec-xvect-voxceleb</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">8461375</span></cite>, a 4.2M parameter TDNN trained for speaker recognition, is included as a lightweight yet strong baseline and has proven effective for deepfake-related tasks. WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/microsoft/wavlm-base\" title=\"\">https://huggingface.co/microsoft/wavlm-base</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span></cite> builds on Wav2Vec 2.0 with masked prediction and mixture modeling and we use its Base version (94M) pretrained on 94K hours of audio. HuBERT<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-base-ls960\" title=\"\">https://huggingface.co/facebook/hubert-base-ls960</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hsu2021hubert</span></cite> is included for its phonetic and prosodic modeling ability, and we adopt the Base version (95M) trained on 960h of LibriSpeech. Lastly, Wav2Vec 2.0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/wav2vec2-base\" title=\"\">https://huggingface.co/facebook/wav2vec2-base</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baevski2020wav2vec</span></cite> is a contrastive SSL model for speech representation learning; we use the Base variant (94M), which provides strong acoustic modeling directly from raw waveforms and serves as a solid backbone for capturing subtle synthetic patterns in speech. \n<br class=\"ltx_break\"/>We extract representations from the last hidden state of the frozen PTMs using average pooling. The resulting representation dimensions are: 768 for USAD and PaSST; 768 for WavLM, Wav2Vec2, and HuBERT; 512 for x-vector; and 768 for Whisper (Base). All audio samples are resampled to 16kHz before feeding into the PTMs.</p>\n\n",
                "matched_terms": [
                    "ptms",
                    "passt",
                    "models",
                    "hubert",
                    "baseline",
                    "wavlm",
                    "pretrained",
                    "xvector",
                    "usad",
                    "whisper",
                    "wav2vec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> for generalizable audio deepfake detection across diverse synthesis models. The architecture is presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. \n<br class=\"ltx_break\"/>We begin by encoding the input waveform. Let <math alttext=\"x(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x(t)</annotation></semantics></math> be the raw audio waveform. We first encode <math alttext=\"x(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x(t)</annotation></semantics></math> using a frozen self-supervised speech-foundation model <math alttext=\"g\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mi>g</mi><annotation encoding=\"application/x-tex\">g</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "models",
                    "rhyme"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Ensuring that <math alttext=\"\\lVert z^{*}\\rVert&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p3.m4\" intent=\":literal\"><semantics><mrow><mrow><mo fence=\"true\" rspace=\"0em\">&#8741;</mo><msup><mi>z</mi><mo>&#8727;</mo></msup><mo fence=\"true\" lspace=\"0em\" rspace=\"0.1389em\">&#8741;</mo></mrow><mo lspace=\"0.1389em\">&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\lVert z^{*}\\rVert&lt;1</annotation></semantics></math> maintains numerical stability within the hyperspherical fusion space. A fully connected classifier with a dense layer is attached to the Euclidean representation <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p3.m5\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>, followed by a softmax output that predicts the probability of the input being real or synthetic. <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> has a parameter footprint ranging from 8 to 14 million, depending on the combination and dimensionality of the pretrained speech encoders used.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "rhyme"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study is grounded in experiments on two datasets: \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">DFADD</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024dfadd</span></cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/isjwdu/DFADD\" title=\"\">https://github.com/isjwdu/DFADD</a></span></span></span>: It introduces a new generation of highly realistic spoofed audio samples generated using diffusion and flow-matching (FM) text-to-speech models. It includes over 163,500 synthetic samples paired with 44,455 bonafide utterances from the VCTK corpus, covering 109 speakers. The spoofed audio is synthesized using Diffusion-based and Flow-matching-based speech generation models. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">ASVSpoof 2019 (ASV)</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020asvspoof</span></cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://datashare.ed.ac.uk/handle/10283/3336\" title=\"\">https://datashare.ed.ac.uk/handle/10283/3336</a></span></span></span>: The dataset is a widely adopted benchmark for evaluating spoof detection systems under both logical access (LA) and physical access (PA) scenarios. In our work, we utilize the LA subset, which contains speech spoofed using traditional TTS and voice conversion (VC) methods&#8212;primarily waveform concatenation, parametric synthesis, and neural vocoders. The subset consists of bonafide utterances from 20 speakers sourced from the VCTK corpus Spoofed audio generated using 17 different TTS and VC systems, including some previously unseen systems in the evaluation set. We adopt the original train/test split of the LA subset as specified in the paper. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Dataset Usage Protocol</span>:\nIn our study, we use these datasets to simulate cross-paradigm generalization. Specifically, we train on traditional TTS data from ASV and evaluate on DFADD to measure forward generalization from older to newer synthesis methods. Conversely, we also train on DFADD and evaluate on ASV to assess backward generalization from modern diffusion/FM generators to legacy TTS systems. This bidirectional setup allows us to rigorously test the synthesis-invariance of our proposed approach. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Training Details</span>:\nAll models are trained using the Adam optimizer with cross-entropy loss. The learning rate is set to 1e-3, with a batch size of 32 for 50 epochs. Dropout and early stopping are applied to prevent overfitting. We use five-fold cross-validation within each dataset and report average metrics. For cross-dataset evaluation, no target data is used during training.</p>\n\n",
                "matched_terms": [
                    "test",
                    "models",
                    "train",
                    "under",
                    "dfadd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the effectiveness and generalization ability of <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> under two settings, we use multiple pretrained models (PTMs). We extract embeddings from these PTMs and train models on dataset (ASVspoof or DFADD), then test on the other. This helps us understand how well each model adapts to changes in data distribution and spoofing style. In the second setup, we test the model on deepfakes generated by a variety of TTS systems not seen during training. These samples are collected from the official demo pages of VoiceBox <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span></cite>, VoiceFlow <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024voiceflow</span></cite>, NaturalSpeech 3 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span></cite>, CMTTS <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024cm</span></cite>, DiffProsody <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">oh2024diffprosody</span></cite>, DiffAR <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2023diffar</span></cite>, DiTTo-TTS <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lee2024ditto</span></cite>, and ReFlow-TTS <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guan2024reflow</span></cite>. VoiceBox and VoiceFlow use flow-matching, while the others are diffusion-based. The model was trained only on DFADD and ASVspoof data, so this setup gives us a clear view of how well it performs in truly unseen scenarios. We also perform a focused experiment where the model is trained on samples (D1, D2, D3, F1, F2) and then tested on other synthesizers. This helps isolate how well proposed framework generalizes across different types of generators.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "ptms",
                    "test",
                    "train",
                    "models",
                    "under",
                    "pretrained",
                    "dfadd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the impact of individual components within the proposed <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> framework, we perform a detailed ablation study along three dimensions: architectural geometry (spherical and hyperbolic branches), fusion strategy (with and without geometry), and integration mechanism (gating). Results are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> under two cross-domain setups: training on ASVspoof and testing on DFADD (TR-A&#160;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#160;TE-D), and vice versa (TR-D&#160;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#160;TE-A). The full <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> configuration achieves the best performance, with EERs of 14.12% and 10.26%, clearly outperforming all ablated variants. When the geometry-aware branches are removed, performance degrades&#8212;removing the spherical branch increases EER to 18.01% (TR-A&#160;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#160;TE-D), and removing the hyperbolic branch results in 17.88%. This confirms that both geometric projections contribute complementary and discriminative information for detecting synthetic speech. Replacing the RHYME fusion mechanism with a naive Euclidean fusion further degrades performance (19.33%, 15.12%), indicating that conventional feature concatenation fails to capture the curved manifold structure inherent in speech embeddings across domains. We also examine the role of the gated fusion mechanism. Disabling it by fixing the gating scalar to <math alttext=\"\\alpha=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.5</annotation></semantics></math> leads to higher EERs (19.94%, 14.78%), suggesting that the learned weighting between the two branches is crucial for adaptively balancing domain cues.\nFinally, using a single PTM (USAD only) without any fusion results in the weakest performance (20.51%, 15.19%), reinforcing the importance of <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> multi-geometry, multi-PTM architecture. Together, these results highlight that each component of <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> contributes meaningfully, and that their combination yields a substantial gain in generalization under mismatched and unseen conditions.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "ted",
                    "tea",
                    "under",
                    "conditions",
                    "eer",
                    "trd",
                    "mismatched",
                    "performance",
                    "usad",
                    "dfadd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span>, a unified and geometry-aware framework for generalizable audio deepfake detection across diverse synthesis paradigms, including TTS, vocoder, and diffusion-based generators. By leveraging hyperbolic and spherical projections to model complementary synthesis artifacts and fusing them via Riemannian barycentric averaging, the proposed method learns a synthesis-invariant embedding space that supports strong generalization. The outcomes confirm that uniting hyperbolic hierarchy modeling with spherical artifact encoding through Riemannian fusion yields a geometry-driven, synthesis-invariant detector that generalizes across unseen generators. Extensive experiments show that it consistently outperforms existing detectors and fusion baselines under cross-corpus, zero-shot, and unseen-generator settings. These findings demonstrate the effectiveness of our approach and highlight its potential as a reliable and scalable solution for audio deepfake detection under diverse and previously unseen conditions. Our work also calls upon researchers to build on our extended benchmarks to further advance performance in generalizable audio deepfake detection.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "performance",
                    "conditions",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work aims to support the growing need for detecting synthetic speech by introducing a generalizable deepfake detection framework. As audio generation models become more realistic, tools like <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> can help protect against misuse in areas such as voice fraud, impersonation, and misinformation. At the same time, we recognize the importance of using such detection systems responsibly. Our current study is limited to English datasets, and we encourage future research to consider more diverse languages and speaker populations. We also acknowledge that deepfake detection technology could be misused, for example, in surveillance or censorship. To reduce such risks, our work is shared for research purposes only and does not involve any sensitive or private data. We believe this study provides a step forward in building safer and more trustworthy speech systems.</p>\n\n",
                "matched_terms": [
                    "models",
                    "rhyme"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces",
        "caption": "Table 2: Performance on RHYME framework using the USAD backbone, evaluated under the proposed unseen-synthesis generalization protocol. Each row corresponds to a different speech synthesizer, while columns represent test domains: ASVspoof, DFADD, and five held-out subsets (D1D3: diffusion-based; F1F2: flow-matching based). All values are reported as Equal Error Rate (EER %), where lower scores indicate better generalization and spoof detection capability.",
        "body": "Synthesizer\nRHYME USAD\n\n\nASVP\nDFADD\nD1\nD2\nD3\nF1\nF2\n\n\n\nVoiceBox le2023voicebox\n\n31.32\n10.38\n48.17\n56.83\n44.91\n24.26\n31.45\n\n\n\nVoiceFlow guo2024voiceflow\n\n24.78\n4.02\n29.62\n27.39\n28.55\n38.04\n10.93\n\n\n\nNaturalSpeech3 ju2024naturalspeech\n\n11.63\n1.79\n27.45\n50.07\n57.89\n10.38\n8.56\n\n\n\nCausal Multi-scale TTS li2024cm\n\n20.92\n0\n14.14\n1.09\n0.69\n0.019\n0.02\n\n\n\nDiffProsody oh2024diffprosody\n\n19.18\n3.46\n53.77\n20.85\n17.66\n14.51\n13.24\n\n\n\nDiffar benita2023diffar\n\n32.47\n0\n67.38\n48.22\n19.84\n13.79\n0.13\n\n\n\nDiTTo-TTS lee2024ditto\n\n10.34\n0.37\n24.51\n23.07\n14.88\n16.92\n5.37\n\n\n\nReFlow-TTS guan2024reflow\n\n13.69\n0\n29.02\n26.85\n16.43\n15.27\n3.08",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">Synthesizer</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"7\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME<span class=\"ltx_text ltx_font_serif\"> USAD</span></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">ASVP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">DFADD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">D1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">D2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">D3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">F1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\"><span class=\"ltx_text ltx_font_bold\">F2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">\n<span class=\"ltx_text ltx_font_bold\">VoiceBox</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">31.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">10.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">48.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">56.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">44.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">24.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">31.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">\n<span class=\"ltx_text ltx_font_bold\">VoiceFlow</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024voiceflow</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">24.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">4.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">29.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">27.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">28.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">38.04</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">10.93</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">\n<span class=\"ltx_text ltx_font_bold\">NaturalSpeech3</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">11.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">1.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">27.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">50.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">57.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">10.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">8.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">\n<span class=\"ltx_text ltx_font_bold\">Causal Multi-scale TTS</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024cm</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">20.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">14.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">1.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.019</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">\n<span class=\"ltx_text ltx_font_bold\">DiffProsody</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">oh2024diffprosody</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">19.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">3.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">53.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">20.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">17.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">14.51</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">13.24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">\n<span class=\"ltx_text ltx_font_bold\">Diffar</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2023diffar</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">32.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">67.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">48.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">19.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">13.79</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.13</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">\n<span class=\"ltx_text ltx_font_bold\">DiTTo-TTS</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lee2024ditto</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">10.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">24.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">23.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">14.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">16.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">5.37</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">\n<span class=\"ltx_text ltx_font_bold\">ReFlow-TTS</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guan2024reflow</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">13.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">29.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">26.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">16.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">15.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:10.5pt;padding-right:10.5pt;\">3.08</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "rhyme",
            "detection",
            "multiscale",
            "guo2024voiceflow",
            "domains",
            "reflowtts",
            "unseensynthesis",
            "d1d3",
            "asvspoof",
            "rate",
            "diffar",
            "equal",
            "proposed",
            "lower",
            "ju2024naturalspeech",
            "columns",
            "better",
            "guan2024reflow",
            "speech",
            "each",
            "li2024cm",
            "evaluated",
            "f1f2",
            "test",
            "voiceflow",
            "all",
            "five",
            "tts",
            "benita2023diffar",
            "where",
            "naturalspeech3",
            "heldout",
            "causal",
            "diffprosody",
            "row",
            "indicate",
            "usad",
            "oh2024diffprosody",
            "backbone",
            "subsets",
            "generalization",
            "capability",
            "dittotts",
            "values",
            "performance",
            "voicebox",
            "diffusionbased",
            "represent",
            "dfadd",
            "corresponds",
            "lee2024ditto",
            "reported",
            "le2023voicebox",
            "asvp",
            "under",
            "different",
            "scores",
            "synthesizer",
            "flowmatching",
            "framework",
            "protocol",
            "while",
            "spoof",
            "eer",
            "error",
            "based"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Benchmark Dataset &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the EER (%) results for each model across two cross-domain setups: training on ASVspoof and testing on DFADD (TR-A &#8594; TE-D), and the reverse (TR-D &#8594; TE-A). Each PTM is assessed in both its original form (Baseline) and its <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span>-enhanced version (Novel). From the results, we observe that across both settings, the RHYME-enhanced models consistently achieve lower EERs than their baseline counterparts. For instance, WavLM benefits from a noticeable EER drop from 29.37% to 21.87% (TR-A &#8594; TE-D), and from 20.05% to 13.17% (TR-D &#8594; TE-A). Similarly, models like HuBERT, Whisper, and Wav2Vec2 also show clear improvements under both configurations. These trends confirm that RHYME effectively helps in aligning diverse cues, leading to better generalization under unseen conditions. A consistent observation is that training on DFADD and testing on ASVspoof (TR-D &#8594; TE-A) results in relatively lower EERs than the reverse direction. This suggests that the diverse spoofing methods and higher-quality generation found in DFADD lead to more transferable representations. Among all models, USAD achieves the strongest results with EERs of 14.12% (TR-A &#8594; TE-D) and 10.26% (TR-D &#8594; TE-A), showcasing RHYME&#8217;s ability to handle significant domain shifts. It is also important to note that the DFADD paper <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024dfadd</span></cite> reports an average 32.44% EER for the state-of-the-art end-to-end model AASIST-L under the TR-A &#8594; TE-D setup. While our framework does not perform full model fine-tuning and instead relies on frozen pretrained embeddings, our framework achieves a substantially lower EER of 14.12%&#8212;underscoring the efficacy of geometry-aware fusion for robust and synthesis-invariant detection. \n<br class=\"ltx_break\"/>In Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Benchmark Dataset &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we evaluate the performance of <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span>, which is specifically designed to assess generalization in truly unseen conditions. In this setup, we first extract embeddings using USAD backbone PTMs from both ASVspoof and DFADD datasets. These embeddings are used to train downstream models using our <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> framework. The testing phase involves audio samples synthesized from various TTS models&#8212;which were not part of the training data. These samples were collected from official demo pages of each model, introducing a strong domain shift due to differences in speaker identity, acoustic environments, and generation fidelity. Since the model was only trained on the curated subsets from DFADD, it did not encounter any of the demo-sourced speech samples during training, making this a challenging and realistic generalization test. Despite the difficulty of this setting, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> achieves strong performance across several domains. Among the generators, CMTTS consistently shows the lowest EERs, including 0% on DFADD, 14.14% on D1, and as low as 0.019% on F1. DiTTo-TTS and ReFlow-TTS also perform well across most domains, indicating RHYME&#8217;s ability to handle both autoregressive and diffusion-based synthesis techniques. On comparatively more variable systems like Diffr and DiPro, we observe a wider spread in EERs across domains; however, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> still maintains stability, with EERs well below 20% in most cases. To further understand our framework robustness, we conduct an additional set of experiments where the model is trained on data generated from a single synthesizer&#8212;such as D1, D2, D3, F1, or F2&#8212;and tested on unseen samples from other models like Voicebox and NaturalSpeech3. This controlled setup isolates how well the model can transfer from one generator&#8217;s characteristics to another. Even with such limited training data, RHYME generalizes effectively in many cases, suggesting that the learned representations are transferable across diverse generation styles. We further investigate the representational quality and decision reliability of RHYME using t-SNE visualizations and calibration plots. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F2\" title=\"Figure 2 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the distribution of embeddings using t-SNE. In subfigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>, we visualize raw USAD embeddings extracted from DFADD and ASVspoof. The overlap between real and fake samples suggests weak discrimination in the original embedding space. While, subfigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> shows RHYME-fused embeddings when trained on DFADD and evaluated on both DFADD and ASVspoof. Here, the real and fake classes form visibly distinct clusters, highlighting RHYME&#8217;s ability to disentangle domain-invariant and spoof-discriminative cues through geometric fusion. To evaluate how well model confidence aligns with actual correctness, we use calibration diagrams shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Subfigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a> depicts the calibration curve when the model is trained on DFADD and tested on both DFADD and ASVspoof. The curve closely follows the diagonal, indicating good calibration between predicted probabilities and true outcome frequencies, more importantly, the reliability curve remains closely aligned for the out-of-domain ASVspoof test split. This demonstrates that the model&#8217;s softmax outputs remain trustworthy even under out-of-domain detection. On the other hand, subfigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, where the model is trained on ASVspoof, shows larger deviations&#8212;highlighting miscalibration. These results reinforce that RHYME not only improves detection accuracy but also produces more reliable probability estimates, a key requirement for deployment in risk-sensitive applications.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenge of generalizable audio deepfake detection (ADD) across diverse speech synthesis paradigms&#8212;including conventional text-to-speech (TTS) systems and modern diffusion or flow-matching (FM) based generators. Prior work has mostly targeted individual synthesis families and often fails to generalize across paradigms due to overfitting to generation-specific artifacts. We hypothesize that synthetic speech, irrespective of its generative origin, leaves behind shared structural distortions in the embedding space that can be aligned through geometry-aware modeling. To this end, we propose <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span>, a unified detection framework that fuses utterance-level embeddings from diverse pretrained speech encoders using non-Euclidean projections. <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> maps representations into hyperbolic and spherical manifolds&#8212;where hyperbolic geometry excels at modeling hierarchical generator families, and spherical projections capture angular, energy-invariant cues such as periodic vocoder artifacts. The fused representation is obtained via Riemannian barycentric averaging, enabling synthesis-invariant alignment. <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> outperforms individual PTMs and homogeneous fusion baselines, achieving top performance and setting new state-of-the-art in cross-paradigm ADD.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "rhyme",
                    "detection",
                    "tts",
                    "flowmatching",
                    "framework",
                    "performance",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\">Curved Worlds, Clear Boundaries:<span class=\"ltx_text ltx_font_upright\"> Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces</span></span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The field of speech synthesis has undergone a paradigm shift over the past few years with the emergence of neural text-to-speech (TTS) and diffusion-based models, delivering human-level audio realism that is increasingly difficult to distinguish from genuine speech <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yi2023audio</span></cite>. While these technological advances have enabled beneficial applications in accessibility and entertainment, they have simultaneously paved the way for malicious applications. Recent high-profile cases&#8212;such as voice clones used in emotional blackmail and scam calls&#8212;highlight the urgent need for reliable detection mechanisms, since human listeners are no longer able to consistently identify deepfake audio <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">barrington2025people</span></cite>. In response, audio deepfake detection (ADD) has emerged as a crucial research domain within speech security and digital forensics. Despite notable advances in audio deepfake detection, most existing systems are trained on speech generated by conventional TTS or vocoder-based models and demonstrate strong performance under seen conditions. However, these systems often exhibit degraded performance when exposed to synthetic speech generated using unseen paradigms or under distributional shift. <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2020generalization</span></cite> identified this issue early on, showing that detectors often fail even within the same benchmark when exposed to unfamiliar spoofing techniques. Extending this analysis, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kulkarni2024exploring</span></cite> evaluated a range of self-supervised models across datasets and synthesis types, reporting consistent performance drops in cross-domain settings. These findings highlight a fundamental limitation in existing ADD pipelines and point toward the need for detection strategies that are resilient to synthesis diversity and adaptable to new generation mechanisms. Although neural speech generators differ widely in architecture and training objectives, we hypothesize that synthetic speech&#8212;regardless of whether it is produced by TTS, vocoder, or diffusion-based models&#8212;shares certain latent artifacts that distinguish it from natural speech. These artifacts may not be easily perceptible in the waveform or spectrogram, but they can manifest as distortions in the underlying feature space. By leveraging diverse pre-trained speech encoders, it is possible to capture complementary representations that highlight different aspects of these synthesis-induced irregularities. The central challenge lies in aligning these heterogeneous embeddings in a way that preserves discriminative cues while minimizing overfitting to a specific synthesis family. This motivates our exploration of a shared, synthesis-invariant representation space for generalized detection; in this work, we emphasize acoustic evidence, analyzing signal-level artifacts while remaining independent of language or text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "detection",
                    "evaluated",
                    "under",
                    "tts",
                    "different",
                    "while",
                    "performance",
                    "diffusionbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To this end, we propose <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME:</span> <span class=\"ltx_text ltx_font_bold\">R</span>iemannian fusion of <span class=\"ltx_text ltx_font_bold\">HY</span>perbolic and sph<span class=\"ltx_text ltx_font_bold\">E</span>rical embeddings a unified and geometry-aware audio deepfake detection framework. RHYME fuses utterance-level embeddings from multiple frozen speech PTMs and projects them into two complementary non-Euclidean manifolds: hyperbolic space (to capture hierarchical generative traces) and spherical space (to model periodic and spectral anomalies). These projections are then fused using Riemannian barycentric averaging in the Poincar&#233; ball, forming a synthesis-agnostic representation space.\nEach of these geometric components serves a distinct purpose&#8212;hyperbolic projection models generator hierarchies, spherical projection highlights periodic artifacts, and barycentric fusion unifies them while preserving their respective curvatures. Hyperbolic curvature compactly encodes tree-like generator lineages (e.g., vocoder <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> autoregressive TTS (AR-TTS) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> diffusion/flow) with low distortion, placing related synthesizers along nearby geodesics while genuine speech occupies distinct regions. Consequently, the hyperbolic branch separates synthetic from real speech by tracing these geodesic hierarchies (model families/versions). As newer diffusion or flow-based methods still follow a structured progression of architectural improvements, their embeddings naturally fall on new branches of this learned hyperbolic hierarchy. In parallel, the spherical branch captures angular, energy-invariant cues associated with periodicity artifacts, such as those introduced by vocoders or diffusion sampling (See Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S4.SS1\" title=\"4.1 Proposed Framework: RHYME &#8227; 4 Modeling Pipeline &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a> for details). Even though waveform-level characteristics vary, these models still imprint subtle periodic distortions, which manifest as consistent angular deviations on the hypersphere. By fusing these complementary cues via Riemannian averaging, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> ensures that unseen synthetic speech cannot simultaneously satisfy both the hierarchical and periodic constraints of genuine speech&#8212;resulting in robust generalization across synthesis families. <span class=\"ltx_text ltx_font_italic\">We hypothesize that this geometry-aware fusion encourages synthesis-invariant alignment while preserving discriminative cues necessary for deepfake detection.</span> We validate <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> through comprehensive experiments on two widely-used benchmark datasets&#8212;ASVspoof and DFADD&#8212;which include speech synthesized using TTS, vocoder, and diffusion-based models. Our evaluation covers zero-shot, cross-corpus, and unseen-generator scenarios to rigorously test generalization. Across all settings, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> consistently delivers strong performance, even when faced with synthetic speech from generators it has never seen during training. Unlike previous methods that tend to overfit to specific synthesis families, our geometry-aware fusion strategy enables <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> to learn shared structural cues across different types of fake speech. This shows that RHYME handles both familiar and previously unseen generators well, making it a strong candidate for practical deployment in deepfake detection systems. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "speech",
                    "rhyme",
                    "detection",
                    "each",
                    "test",
                    "all",
                    "tts",
                    "different",
                    "generalization",
                    "framework",
                    "while",
                    "performance",
                    "diffusionbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">RHYME</span>, a unified and geometry-aware framework for generalizable audio deepfake detection across TTS, vocoder, and diffusion-based speech generators.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "rhyme",
                    "detection",
                    "tts",
                    "framework",
                    "diffusionbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate that <span class=\"ltx_text ltx_font_bold\">RHYME</span> consistently outperforms individual PTMs and homogeneous fusion baselines, achieving state-of-the-art performance under cross-corpus and unseen-generator conditions.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "performance",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We extend the DFADD benchmark by adding two new diffusion-based generators, providing a more rigorous testbed for evaluating out-of-distribution generalization in future work.</p>\n\n",
                "matched_terms": [
                    "diffusionbased",
                    "generalization",
                    "dfadd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The field of audio deepfake detection (ADD) originated from the vulnerabilities identified in automatic speaker verification (ASV) systems. Early countermeasures utilized handcrafted spectral features such as constant-Q cepstral coefficients (CQCC) and mel-frequency cepstral coefficients (MFCC), often paired with Gaussian mixture models or support vector machines. Early research in this area primarily leveraged handcrafted spectral features such as constant-Q cepstral coefficients (CQCC) and linear frequency cepstral coefficients (LFCC), often in combination with traditional machine learning classifiers for spoofing detection tasks <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">todisco2017constant</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kinnunen2017asvspoof</span></cite>. The advent of the ASVspoof challenge series <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">todisco2019asvspoof</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020asvspoof</span></cite> propelled the field toward deep learning solutions, with convolutional and recurrent neural networks trained on spectrograms or raw waveforms becoming standard for text-to-speech (TTS) and voice conversion (VC) based deepfake detection <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cai2023face</span></cite>. However, most of these methods are tailored to specific synthesis techniques and struggle to generalize to audio produced by newer paradigms such as diffusion and flow-matching models. The emergence of self-supervised learning (SSL) has significantly advanced the capabilities of audio deepfake detection. Models such as wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baevski2020wav2vec</span></cite>, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hsu2021hubert</span></cite> and WavLM <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span></cite> have demonstrated robust performance across various paralinguistic and spoofing benchmarks, including ASVspoof and ADD tasks. These models have since been adapted to a range of spoofing detection tasks. <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tak2022automatic</span></cite> investigated the efficacy of wav2vec 2.0 in detecting synthetic audio under various ASVspoof 2021 conditions, highlighting the potential of SSL models in practical ADD scenarios. Building on this line of work, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024audio</span></cite> proposed a multi-level fusion framework based on WavLM, achieving competitive performance across multiple spoofing benchmarks including ASVspoof 2019 and 2021. Building on this body of work, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kheir2025comprehensive</span></cite> conducted a detailed layer-wise study of multiple SSL models and found that lower transformer layers consistently offer the most discriminative features for deepfake detection across languages and tasks. These insights reinforce the potential of lightweight, generalizable detection pipelines. Nevertheless, most existing methods remain limited to single-model fine-tuning, restricting their adaptability to unseen synthesis techniques&#8212;an issue our work seeks to address through a fusion-driven, synthesis-agnostic approach. Despite progress in ADD, a key limitation persists in generalizing across diverse synthesis paradigms. Most existing detectors are trained primarily on speech generated by conventional text-to-speech (TTS) systems and struggle to maintain performance when exposed to speech produced using newer generative methods such as diffusion or flow-matching (FM) models. To address this shift, recent studies have proposed fusion-based frameworks that leverage multimodal foundation models <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chetia2025towards</span></cite> and paralinguistic speech encoders <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akhtar2025source</span></cite> to improve robustness in source attribution tasks. These approaches have shown the benefit of combining heterogeneous pre-trained models to capture generator-specific characteristics. Motivated by this direction, our work targets the detection problem under similar distribution shifts, specifically focusing on bridging generalization gaps across synthesis families. We aim to learn synthesis-invariant embeddings by fusing multiple speech encoders and projecting their representations into a hyperspherical space. This allows our model to detect synthetic speech reliably across both known and unseen generation mechanisms, including diffusion-based generators.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "detection",
                    "under",
                    "tts",
                    "asvspoof",
                    "generalization",
                    "flowmatching",
                    "proposed",
                    "lower",
                    "framework",
                    "performance",
                    "diffusionbased",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we describe the speech representation models considered in our study. \n<br class=\"ltx_break\"/>USAD<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/collections/MIT-SLS/usad-models-68491d4c7d0978b85d0c4299\" title=\"\">https://huggingface.co/collections/MIT-SLS/usad-models-68491d4c7d0978b85d0c4299</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chang2025usad</span></cite> is a universal audio representation model trained using multi-teacher distillation across speech, music, and sound domains, and we adopt its Base variant (94M parameters). PaSST<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/kkoutini/PaSST\" title=\"\">https://github.com/kkoutini/PaSST</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koutini2021efficient</span></cite> adapts vision transformers for spectrograms with patch-masking for regularization; we use the PaSST-S model (87M). Whisper<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-base\" title=\"\">https://huggingface.co/openai/whisper-base</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span></cite>, a multilingual ASR model trained on 680k hours of weakly supervised audio-text data, is used in its Base variant (74M), and we extract encoder embeddings. x-vector<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/speechbrain/spkrec-xvect-voxceleb\" title=\"\">https://huggingface.co/speechbrain/spkrec-xvect-voxceleb</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">8461375</span></cite>, a 4.2M parameter TDNN trained for speaker recognition, is included as a lightweight yet strong baseline and has proven effective for deepfake-related tasks. WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/microsoft/wavlm-base\" title=\"\">https://huggingface.co/microsoft/wavlm-base</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span></cite> builds on Wav2Vec 2.0 with masked prediction and mixture modeling and we use its Base version (94M) pretrained on 94K hours of audio. HuBERT<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-base-ls960\" title=\"\">https://huggingface.co/facebook/hubert-base-ls960</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hsu2021hubert</span></cite> is included for its phonetic and prosodic modeling ability, and we adopt the Base version (95M) trained on 960h of LibriSpeech. Lastly, Wav2Vec 2.0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/wav2vec2-base\" title=\"\">https://huggingface.co/facebook/wav2vec2-base</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baevski2020wav2vec</span></cite> is a contrastive SSL model for speech representation learning; we use the Base variant (94M), which provides strong acoustic modeling directly from raw waveforms and serves as a solid backbone for capturing subtle synthetic patterns in speech. \n<br class=\"ltx_break\"/>We extract representations from the last hidden state of the frozen PTMs using average pooling. The resulting representation dimensions are: 768 for USAD and PaSST; 768 for WavLM, Wav2Vec2, and HuBERT; 512 for x-vector; and 768 for Whisper (Base). All audio samples are resampled to 16kHz before feeding into the PTMs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "domains",
                    "all",
                    "backbone",
                    "usad"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> for generalizable audio deepfake detection across diverse synthesis models. The architecture is presented in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S2.F1\" title=\"Figure 1 &#8227; 2 Related Work &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. \n<br class=\"ltx_break\"/>We begin by encoding the input waveform. Let <math alttext=\"x(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x(t)</annotation></semantics></math> be the raw audio waveform. We first encode <math alttext=\"x(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x(t)</annotation></semantics></math> using a frozen self-supervised speech-foundation model <math alttext=\"g\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mi>g</mi><annotation encoding=\"application/x-tex\">g</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Ensuring that <math alttext=\"\\lVert z^{*}\\rVert&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p3.m4\" intent=\":literal\"><semantics><mrow><mrow><mo fence=\"true\" rspace=\"0em\">&#8741;</mo><msup><mi>z</mi><mo>&#8727;</mo></msup><mo fence=\"true\" lspace=\"0em\" rspace=\"0.1389em\">&#8741;</mo></mrow><mo lspace=\"0.1389em\">&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\lVert z^{*}\\rVert&lt;1</annotation></semantics></math> maintains numerical stability within the hyperspherical fusion space. A fully connected classifier with a dense layer is attached to the Euclidean representation <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p3.m5\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>, followed by a softmax output that predicts the probability of the input being real or synthetic. <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> has a parameter footprint ranging from 8 to 14 million, depending on the combination and dimensionality of the pretrained speech encoders used.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "rhyme"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study is grounded in experiments on two datasets: \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">DFADD</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024dfadd</span></cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/isjwdu/DFADD\" title=\"\">https://github.com/isjwdu/DFADD</a></span></span></span>: It introduces a new generation of highly realistic spoofed audio samples generated using diffusion and flow-matching (FM) text-to-speech models. It includes over 163,500 synthetic samples paired with 44,455 bonafide utterances from the VCTK corpus, covering 109 speakers. The spoofed audio is synthesized using Diffusion-based and Flow-matching-based speech generation models. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">ASVSpoof 2019 (ASV)</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020asvspoof</span></cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://datashare.ed.ac.uk/handle/10283/3336\" title=\"\">https://datashare.ed.ac.uk/handle/10283/3336</a></span></span></span>: The dataset is a widely adopted benchmark for evaluating spoof detection systems under both logical access (LA) and physical access (PA) scenarios. In our work, we utilize the LA subset, which contains speech spoofed using traditional TTS and voice conversion (VC) methods&#8212;primarily waveform concatenation, parametric synthesis, and neural vocoders. The subset consists of bonafide utterances from 20 speakers sourced from the VCTK corpus Spoofed audio generated using 17 different TTS and VC systems, including some previously unseen systems in the evaluation set. We adopt the original train/test split of the LA subset as specified in the paper. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Dataset Usage Protocol</span>:\nIn our study, we use these datasets to simulate cross-paradigm generalization. Specifically, we train on traditional TTS data from ASV and evaluate on DFADD to measure forward generalization from older to newer synthesis methods. Conversely, we also train on DFADD and evaluate on ASV to assess backward generalization from modern diffusion/FM generators to legacy TTS systems. This bidirectional setup allows us to rigorously test the synthesis-invariance of our proposed approach. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Training Details</span>:\nAll models are trained using the Adam optimizer with cross-entropy loss. The learning rate is set to 1e-3, with a batch size of 32 for 50 epochs. Dropout and early stopping are applied to prevent overfitting. We use five-fold cross-validation within each dataset and report average metrics. For cross-dataset evaluation, no target data is used during training.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "detection",
                    "each",
                    "test",
                    "all",
                    "tts",
                    "asvspoof",
                    "rate",
                    "generalization",
                    "under",
                    "different",
                    "flowmatching",
                    "proposed",
                    "protocol",
                    "spoof",
                    "diffusionbased",
                    "dfadd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the effectiveness and generalization ability of <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> under two settings, we use multiple pretrained models (PTMs). We extract embeddings from these PTMs and train models on dataset (ASVspoof or DFADD), then test on the other. This helps us understand how well each model adapts to changes in data distribution and spoofing style. In the second setup, we test the model on deepfakes generated by a variety of TTS systems not seen during training. These samples are collected from the official demo pages of VoiceBox <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span></cite>, VoiceFlow <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024voiceflow</span></cite>, NaturalSpeech 3 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span></cite>, CMTTS <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024cm</span></cite>, DiffProsody <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">oh2024diffprosody</span></cite>, DiffAR <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2023diffar</span></cite>, DiTTo-TTS <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lee2024ditto</span></cite>, and ReFlow-TTS <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guan2024reflow</span></cite>. VoiceBox and VoiceFlow use flow-matching, while the others are diffusion-based. The model was trained only on DFADD and ASVspoof data, so this setup gives us a clear view of how well it performs in truly unseen scenarios. We also perform a focused experiment where the model is trained on samples (D1, D2, D3, F1, F2) and then tested on other synthesizers. This helps isolate how well proposed framework generalizes across different types of generators.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "guo2024voiceflow",
                    "reflowtts",
                    "asvspoof",
                    "diffar",
                    "ju2024naturalspeech",
                    "proposed",
                    "guan2024reflow",
                    "li2024cm",
                    "each",
                    "test",
                    "voiceflow",
                    "tts",
                    "benita2023diffar",
                    "where",
                    "diffprosody",
                    "oh2024diffprosody",
                    "generalization",
                    "dittotts",
                    "voicebox",
                    "diffusionbased",
                    "lee2024ditto",
                    "dfadd",
                    "le2023voicebox",
                    "under",
                    "different",
                    "flowmatching",
                    "framework",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify the impact of individual components within the proposed <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> framework, we perform a detailed ablation study along three dimensions: architectural geometry (spherical and hyperbolic branches), fusion strategy (with and without geometry), and integration mechanism (gating). Results are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> under two cross-domain setups: training on ASVspoof and testing on DFADD (TR-A&#160;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#160;TE-D), and vice versa (TR-D&#160;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#160;TE-A). The full <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> configuration achieves the best performance, with EERs of 14.12% and 10.26%, clearly outperforming all ablated variants. When the geometry-aware branches are removed, performance degrades&#8212;removing the spherical branch increases EER to 18.01% (TR-A&#160;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#160;TE-D), and removing the hyperbolic branch results in 17.88%. This confirms that both geometric projections contribute complementary and discriminative information for detecting synthetic speech. Replacing the RHYME fusion mechanism with a naive Euclidean fusion further degrades performance (19.33%, 15.12%), indicating that conventional feature concatenation fails to capture the curved manifold structure inherent in speech embeddings across domains. We also examine the role of the gated fusion mechanism. Disabling it by fixing the gating scalar to <math alttext=\"\\alpha=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.5</annotation></semantics></math> leads to higher EERs (19.94%, 14.78%), suggesting that the learned weighting between the two branches is crucial for adaptively balancing domain cues.\nFinally, using a single PTM (USAD only) without any fusion results in the weakest performance (20.51%, 15.19%), reinforcing the importance of <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> multi-geometry, multi-PTM architecture. Together, these results highlight that each component of <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> contributes meaningfully, and that their combination yields a substantial gain in generalization under mismatched and unseen conditions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "rhyme",
                    "each",
                    "reported",
                    "domains",
                    "all",
                    "under",
                    "asvspoof",
                    "generalization",
                    "eer",
                    "proposed",
                    "framework",
                    "performance",
                    "usad",
                    "dfadd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The observed non-linear improvement when both geometric branches are combined stems from their complementary roles rather than redundancy.\nThe hyperbolic branch captures hierarchical generator relationships, while the spherical branch encodes periodic and energy-invariant synthesis artifacts; these cues occupy largely orthogonal representational subspaces.\nWhen fused through Riemannian barycentric averaging, the interaction between curvature-aware embeddings yields a super-additive gain&#8212;greater than the sum of their individual effects.\nWe confirmed this consistency across five random seeds, reporting mean&#160;<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>&#160;standard-deviation values in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> to ensure statistical reliability.</p>\n\n",
                "matched_terms": [
                    "values",
                    "five",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span>, a unified and geometry-aware framework for generalizable audio deepfake detection across diverse synthesis paradigms, including TTS, vocoder, and diffusion-based generators. By leveraging hyperbolic and spherical projections to model complementary synthesis artifacts and fusing them via Riemannian barycentric averaging, the proposed method learns a synthesis-invariant embedding space that supports strong generalization. The outcomes confirm that uniting hyperbolic hierarchy modeling with spherical artifact encoding through Riemannian fusion yields a geometry-driven, synthesis-invariant detector that generalizes across unseen generators. Extensive experiments show that it consistently outperforms existing detectors and fusion baselines under cross-corpus, zero-shot, and unseen-generator settings. These findings demonstrate the effectiveness of our approach and highlight its potential as a reliable and scalable solution for audio deepfake detection under diverse and previously unseen conditions. Our work also calls upon researchers to build on our extended benchmarks to further advance performance in generalizable audio deepfake detection.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "detection",
                    "under",
                    "tts",
                    "generalization",
                    "proposed",
                    "framework",
                    "performance",
                    "diffusionbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework achieves strong generalization across diverse synthesis paradigms&#8212;including TTS, vocoder, and modern diffusion or flow-matching generators&#8212;but there are certain limitations to our study. First, our evaluation is restricted to English-language datasets; generalization to multilingual and accented speech, including cross-lingual and code-switched scenarios, remains unexplored.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts",
                    "generalization",
                    "flowmatching",
                    "framework"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work aims to support the growing need for detecting synthetic speech by introducing a generalizable deepfake detection framework. As audio generation models become more realistic, tools like <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> can help protect against misuse in areas such as voice fraud, impersonation, and misinformation. At the same time, we recognize the importance of using such detection systems responsibly. Our current study is limited to English datasets, and we encourage future research to consider more diverse languages and speaker populations. We also acknowledge that deepfake detection technology could be misused, for example, in surveillance or censorship. To reduce such risks, our work is shared for research purposes only and does not involve any sensitive or private data. We believe this study provides a step forward in building safer and more trustworthy speech systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "rhyme",
                    "framework",
                    "detection"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces",
        "caption": "Table 3: Equal Error Rate (EER %) evaluation under distribution shift. We assess the effect of removing RHYME components: gating, spherical and hyperbolic branches, and geometry-aware fusion. Results demonstrate that each module contributes to performance, with the RHYME configuration achieving the lowest EERs.",
        "body": "Configuration\nTR-A \\rightarrow TE-D\nTR-D \\rightarrow TE-A\n\n\n\n\nRHYME\n14.12\n10.26\n\n\nNo Gating (=0.5\\alpha=0.5)\n19.94\n14.78\n\n\nNo Spherical Branch\n18.01\n14.22\n\n\nNo Hyperbolic Branch\n17.88\n13.89\n\n\nEuclidean Fusion (No Geometry)\n19.33\n15.12\n\n\nBaseline (USAD only)\n20.51\n15.19",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Configuration</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">TR-A <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> TE-D</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">TR-D <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> TE-A</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">14.12</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">10.26</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">No Gating (<math alttext=\"\\alpha=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.5</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">19.94</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">14.78</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">No Spherical Branch</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">18.01</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">14.22</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">No Hyperbolic Branch</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">17.88</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">13.89</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Euclidean Fusion (No Geometry)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">19.33</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">15.12</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Baseline (USAD only)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">20.51</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">15.19</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "rhyme",
            "rate",
            "equal",
            "rightarrow",
            "assess",
            "demonstrate",
            "geometry",
            "each",
            "shift",
            "spherical",
            "euclidean",
            "geometryaware",
            "baseline",
            "hyperbolic",
            "usad",
            "results",
            "branches",
            "components",
            "tea",
            "achieving",
            "lowest",
            "distribution",
            "evaluation",
            "module",
            "configuration",
            "05alpha05",
            "removing",
            "fusion",
            "trd",
            "performance",
            "gating",
            "only",
            "effect",
            "ted",
            "under",
            "contributes",
            "eers",
            "branch",
            "eer",
            "error"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To quantify the impact of individual components within the proposed <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> framework, we perform a detailed ablation study along three dimensions: architectural geometry (spherical and hyperbolic branches), fusion strategy (with and without geometry), and integration mechanism (gating). Results are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> under two cross-domain setups: training on ASVspoof and testing on DFADD (TR-A&#160;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#160;TE-D), and vice versa (TR-D&#160;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#160;TE-A). The full <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> configuration achieves the best performance, with EERs of 14.12% and 10.26%, clearly outperforming all ablated variants. When the geometry-aware branches are removed, performance degrades&#8212;removing the spherical branch increases EER to 18.01% (TR-A&#160;<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>&#160;TE-D), and removing the hyperbolic branch results in 17.88%. This confirms that both geometric projections contribute complementary and discriminative information for detecting synthetic speech. Replacing the RHYME fusion mechanism with a naive Euclidean fusion further degrades performance (19.33%, 15.12%), indicating that conventional feature concatenation fails to capture the curved manifold structure inherent in speech embeddings across domains. We also examine the role of the gated fusion mechanism. Disabling it by fixing the gating scalar to <math alttext=\"\\alpha=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.5</annotation></semantics></math> leads to higher EERs (19.94%, 14.78%), suggesting that the learned weighting between the two branches is crucial for adaptively balancing domain cues.\nFinally, using a single PTM (USAD only) without any fusion results in the weakest performance (20.51%, 15.19%), reinforcing the importance of <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> multi-geometry, multi-PTM architecture. Together, these results highlight that each component of <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> contributes meaningfully, and that their combination yields a substantial gain in generalization under mismatched and unseen conditions.</p>\n\n",
            "<p class=\"ltx_p\">The observed non-linear improvement when both geometric branches are combined stems from their complementary roles rather than redundancy.\nThe hyperbolic branch captures hierarchical generator relationships, while the spherical branch encodes periodic and energy-invariant synthesis artifacts; these cues occupy largely orthogonal representational subspaces.\nWhen fused through Riemannian barycentric averaging, the interaction between curvature-aware embeddings yields a super-additive gain&#8212;greater than the sum of their individual effects.\nWe confirmed this consistency across five random seeds, reporting mean&#160;<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>&#160;standard-deviation values in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> to ensure statistical reliability.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenge of generalizable audio deepfake detection (ADD) across diverse speech synthesis paradigms&#8212;including conventional text-to-speech (TTS) systems and modern diffusion or flow-matching (FM) based generators. Prior work has mostly targeted individual synthesis families and often fails to generalize across paradigms due to overfitting to generation-specific artifacts. We hypothesize that synthetic speech, irrespective of its generative origin, leaves behind shared structural distortions in the embedding space that can be aligned through geometry-aware modeling. To this end, we propose <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span>, a unified detection framework that fuses utterance-level embeddings from diverse pretrained speech encoders using non-Euclidean projections. <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> maps representations into hyperbolic and spherical manifolds&#8212;where hyperbolic geometry excels at modeling hierarchical generator families, and spherical projections capture angular, energy-invariant cues such as periodic vocoder artifacts. The fused representation is obtained via Riemannian barycentric averaging, enabling synthesis-invariant alignment. <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> outperforms individual PTMs and homogeneous fusion baselines, achieving top performance and setting new state-of-the-art in cross-paradigm ADD.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "achieving",
                    "spherical",
                    "geometryaware",
                    "hyperbolic",
                    "fusion",
                    "performance",
                    "geometry"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\">Curved Worlds, Clear Boundaries:<span class=\"ltx_text ltx_font_upright\"> Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces</span></span>\n</p>\n\n",
                "matched_terms": [
                    "spherical",
                    "geometry",
                    "hyperbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The field of speech synthesis has undergone a paradigm shift over the past few years with the emergence of neural text-to-speech (TTS) and diffusion-based models, delivering human-level audio realism that is increasingly difficult to distinguish from genuine speech <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">yi2023audio</span></cite>. While these technological advances have enabled beneficial applications in accessibility and entertainment, they have simultaneously paved the way for malicious applications. Recent high-profile cases&#8212;such as voice clones used in emotional blackmail and scam calls&#8212;highlight the urgent need for reliable detection mechanisms, since human listeners are no longer able to consistently identify deepfake audio <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">barrington2025people</span></cite>. In response, audio deepfake detection (ADD) has emerged as a crucial research domain within speech security and digital forensics. Despite notable advances in audio deepfake detection, most existing systems are trained on speech generated by conventional TTS or vocoder-based models and demonstrate strong performance under seen conditions. However, these systems often exhibit degraded performance when exposed to synthetic speech generated using unseen paradigms or under distributional shift. <cite class=\"ltx_cite ltx_citemacro_citet\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2020generalization</span></cite> identified this issue early on, showing that detectors often fail even within the same benchmark when exposed to unfamiliar spoofing techniques. Extending this analysis, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kulkarni2024exploring</span></cite> evaluated a range of self-supervised models across datasets and synthesis types, reporting consistent performance drops in cross-domain settings. These findings highlight a fundamental limitation in existing ADD pipelines and point toward the need for detection strategies that are resilient to synthesis diversity and adaptable to new generation mechanisms. Although neural speech generators differ widely in architecture and training objectives, we hypothesize that synthetic speech&#8212;regardless of whether it is produced by TTS, vocoder, or diffusion-based models&#8212;shares certain latent artifacts that distinguish it from natural speech. These artifacts may not be easily perceptible in the waveform or spectrogram, but they can manifest as distortions in the underlying feature space. By leveraging diverse pre-trained speech encoders, it is possible to capture complementary representations that highlight different aspects of these synthesis-induced irregularities. The central challenge lies in aligning these heterogeneous embeddings in a way that preserves discriminative cues while minimizing overfitting to a specific synthesis family. This motivates our exploration of a shared, synthesis-invariant representation space for generalized detection; in this work, we emphasize acoustic evidence, analyzing signal-level artifacts while remaining independent of language or text.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "demonstrate",
                    "under",
                    "shift"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To this end, we propose <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME:</span> <span class=\"ltx_text ltx_font_bold\">R</span>iemannian fusion of <span class=\"ltx_text ltx_font_bold\">HY</span>perbolic and sph<span class=\"ltx_text ltx_font_bold\">E</span>rical embeddings a unified and geometry-aware audio deepfake detection framework. RHYME fuses utterance-level embeddings from multiple frozen speech PTMs and projects them into two complementary non-Euclidean manifolds: hyperbolic space (to capture hierarchical generative traces) and spherical space (to model periodic and spectral anomalies). These projections are then fused using Riemannian barycentric averaging in the Poincar&#233; ball, forming a synthesis-agnostic representation space.\nEach of these geometric components serves a distinct purpose&#8212;hyperbolic projection models generator hierarchies, spherical projection highlights periodic artifacts, and barycentric fusion unifies them while preserving their respective curvatures. Hyperbolic curvature compactly encodes tree-like generator lineages (e.g., vocoder <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> autoregressive TTS (AR-TTS) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> diffusion/flow) with low distortion, placing related synthesizers along nearby geodesics while genuine speech occupies distinct regions. Consequently, the hyperbolic branch separates synthetic from real speech by tracing these geodesic hierarchies (model families/versions). As newer diffusion or flow-based methods still follow a structured progression of architectural improvements, their embeddings naturally fall on new branches of this learned hyperbolic hierarchy. In parallel, the spherical branch captures angular, energy-invariant cues associated with periodicity artifacts, such as those introduced by vocoders or diffusion sampling (See Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S4.SS1\" title=\"4.1 Proposed Framework: RHYME &#8227; 4 Modeling Pipeline &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a> for details). Even though waveform-level characteristics vary, these models still imprint subtle periodic distortions, which manifest as consistent angular deviations on the hypersphere. By fusing these complementary cues via Riemannian averaging, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> ensures that unseen synthetic speech cannot simultaneously satisfy both the hierarchical and periodic constraints of genuine speech&#8212;resulting in robust generalization across synthesis families. <span class=\"ltx_text ltx_font_italic\">We hypothesize that this geometry-aware fusion encourages synthesis-invariant alignment while preserving discriminative cues necessary for deepfake detection.</span> We validate <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> through comprehensive experiments on two widely-used benchmark datasets&#8212;ASVspoof and DFADD&#8212;which include speech synthesized using TTS, vocoder, and diffusion-based models. Our evaluation covers zero-shot, cross-corpus, and unseen-generator scenarios to rigorously test generalization. Across all settings, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> consistently delivers strong performance, even when faced with synthetic speech from generators it has never seen during training. Unlike previous methods that tend to overfit to specific synthesis families, our geometry-aware fusion strategy enables <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> to learn shared structural cues across different types of fake speech. This shows that RHYME handles both familiar and previously unseen generators well, making it a strong candidate for practical deployment in deepfake detection systems. \n<br class=\"ltx_break\"/></p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "each",
                    "branches",
                    "components",
                    "evaluation",
                    "spherical",
                    "geometryaware",
                    "branch",
                    "hyperbolic",
                    "fusion",
                    "rightarrow",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_bold\">RHYME</span>, a unified and geometry-aware framework for generalizable audio deepfake detection across TTS, vocoder, and diffusion-based speech generators.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "geometryaware"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leverage non-Euclidean projections, where hyperbolic geometry captures hierarchical relationships among generators, and spherical space models angular, periodic artifacts commonly introduced by vocoding and diffusion processes.</p>\n\n",
                "matched_terms": [
                    "spherical",
                    "geometry",
                    "hyperbolic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate that <span class=\"ltx_text ltx_font_bold\">RHYME</span> consistently outperforms individual PTMs and homogeneous fusion baselines, achieving state-of-the-art performance under cross-corpus and unseen-generator conditions.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "achieving",
                    "under",
                    "fusion",
                    "demonstrate",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The field of audio deepfake detection (ADD) originated from the vulnerabilities identified in automatic speaker verification (ASV) systems. Early countermeasures utilized handcrafted spectral features such as constant-Q cepstral coefficients (CQCC) and mel-frequency cepstral coefficients (MFCC), often paired with Gaussian mixture models or support vector machines. Early research in this area primarily leveraged handcrafted spectral features such as constant-Q cepstral coefficients (CQCC) and linear frequency cepstral coefficients (LFCC), often in combination with traditional machine learning classifiers for spoofing detection tasks <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">todisco2017constant</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kinnunen2017asvspoof</span></cite>. The advent of the ASVspoof challenge series <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">todisco2019asvspoof</span>; <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020asvspoof</span></cite> propelled the field toward deep learning solutions, with convolutional and recurrent neural networks trained on spectrograms or raw waveforms becoming standard for text-to-speech (TTS) and voice conversion (VC) based deepfake detection <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">cai2023face</span></cite>. However, most of these methods are tailored to specific synthesis techniques and struggle to generalize to audio produced by newer paradigms such as diffusion and flow-matching models. The emergence of self-supervised learning (SSL) has significantly advanced the capabilities of audio deepfake detection. Models such as wav2vec 2.0 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baevski2020wav2vec</span></cite>, HuBERT <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hsu2021hubert</span></cite> and WavLM <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span></cite> have demonstrated robust performance across various paralinguistic and spoofing benchmarks, including ASVspoof and ADD tasks. These models have since been adapted to a range of spoofing detection tasks. <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">tak2022automatic</span></cite> investigated the efficacy of wav2vec 2.0 in detecting synthetic audio under various ASVspoof 2021 conditions, highlighting the potential of SSL models in practical ADD scenarios. Building on this line of work, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024audio</span></cite> proposed a multi-level fusion framework based on WavLM, achieving competitive performance across multiple spoofing benchmarks including ASVspoof 2019 and 2021. Building on this body of work, <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kheir2025comprehensive</span></cite> conducted a detailed layer-wise study of multiple SSL models and found that lower transformer layers consistently offer the most discriminative features for deepfake detection across languages and tasks. These insights reinforce the potential of lightweight, generalizable detection pipelines. Nevertheless, most existing methods remain limited to single-model fine-tuning, restricting their adaptability to unseen synthesis techniques&#8212;an issue our work seeks to address through a fusion-driven, synthesis-agnostic approach. Despite progress in ADD, a key limitation persists in generalizing across diverse synthesis paradigms. Most existing detectors are trained primarily on speech generated by conventional text-to-speech (TTS) systems and struggle to maintain performance when exposed to speech produced using newer generative methods such as diffusion or flow-matching (FM) models. To address this shift, recent studies have proposed fusion-based frameworks that leverage multimodal foundation models <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chetia2025towards</span></cite> and paralinguistic speech encoders <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">akhtar2025source</span></cite> to improve robustness in source attribution tasks. These approaches have shown the benefit of combining heterogeneous pre-trained models to capture generator-specific characteristics. Motivated by this direction, our work targets the detection problem under similar distribution shifts, specifically focusing on bridging generalization gaps across synthesis families. We aim to learn synthesis-invariant embeddings by fusing multiple speech encoders and projecting their representations into a hyperspherical space. This allows our model to detect synthetic speech reliably across both known and unseen generation mechanisms, including diffusion-based generators.</p>\n\n",
                "matched_terms": [
                    "achieving",
                    "shift",
                    "under",
                    "distribution",
                    "fusion",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we describe the speech representation models considered in our study. \n<br class=\"ltx_break\"/>USAD<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/collections/MIT-SLS/usad-models-68491d4c7d0978b85d0c4299\" title=\"\">https://huggingface.co/collections/MIT-SLS/usad-models-68491d4c7d0978b85d0c4299</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chang2025usad</span></cite> is a universal audio representation model trained using multi-teacher distillation across speech, music, and sound domains, and we adopt its Base variant (94M parameters). PaSST<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/kkoutini/PaSST\" title=\"\">https://github.com/kkoutini/PaSST</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">koutini2021efficient</span></cite> adapts vision transformers for spectrograms with patch-masking for regularization; we use the PaSST-S model (87M). Whisper<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-base\" title=\"\">https://huggingface.co/openai/whisper-base</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span></cite>, a multilingual ASR model trained on 680k hours of weakly supervised audio-text data, is used in its Base variant (74M), and we extract encoder embeddings. x-vector<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/speechbrain/spkrec-xvect-voxceleb\" title=\"\">https://huggingface.co/speechbrain/spkrec-xvect-voxceleb</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">8461375</span></cite>, a 4.2M parameter TDNN trained for speaker recognition, is included as a lightweight yet strong baseline and has proven effective for deepfake-related tasks. WavLM<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/microsoft/wavlm-base\" title=\"\">https://huggingface.co/microsoft/wavlm-base</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chen2022wavlm</span></cite> builds on Wav2Vec 2.0 with masked prediction and mixture modeling and we use its Base version (94M) pretrained on 94K hours of audio. HuBERT<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/hubert-base-ls960\" title=\"\">https://huggingface.co/facebook/hubert-base-ls960</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hsu2021hubert</span></cite> is included for its phonetic and prosodic modeling ability, and we adopt the Base version (95M) trained on 960h of LibriSpeech. Lastly, Wav2Vec 2.0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/wav2vec2-base\" title=\"\">https://huggingface.co/facebook/wav2vec2-base</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">baevski2020wav2vec</span></cite> is a contrastive SSL model for speech representation learning; we use the Base variant (94M), which provides strong acoustic modeling directly from raw waveforms and serves as a solid backbone for capturing subtle synthetic patterns in speech. \n<br class=\"ltx_break\"/>We extract representations from the last hidden state of the frozen PTMs using average pooling. The resulting representation dimensions are: 768 for USAD and PaSST; 768 for WavLM, Wav2Vec2, and HuBERT; 512 for x-vector; and 768 for Whisper (Base). All audio samples are resampled to 16kHz before feeding into the PTMs.</p>\n\n",
                "matched_terms": [
                    "usad",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We learn a gating scalar <math alttext=\"\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mi>&#945;</mi><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math> using a sigmoid activation to adaptively control the separation into geometric branches:</p>\n\n",
                "matched_terms": [
                    "gating",
                    "branches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These gated components are subsequently projected into distinct non-Euclidean manifolds for geometry-aware fusion.</p>\n\n",
                "matched_terms": [
                    "geometryaware",
                    "components",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compute the Riemannian barycenter of the projected embeddings <math alttext=\"x_{h}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>h</mi></msub><annotation encoding=\"application/x-tex\">x_{h}</annotation></semantics></math> and <math alttext=\"y_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">y_{s}</annotation></semantics></math> to perform non-Euclidean fusion in the hyperbolic space:</p>\n\n",
                "matched_terms": [
                    "hyperbolic",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Ensuring that <math alttext=\"\\lVert z^{*}\\rVert&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p3.m4\" intent=\":literal\"><semantics><mrow><mrow><mo fence=\"true\" rspace=\"0em\">&#8741;</mo><msup><mi>z</mi><mo>&#8727;</mo></msup><mo fence=\"true\" lspace=\"0em\" rspace=\"0.1389em\">&#8741;</mo></mrow><mo lspace=\"0.1389em\">&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\lVert z^{*}\\rVert&lt;1</annotation></semantics></math> maintains numerical stability within the hyperspherical fusion space. A fully connected classifier with a dense layer is attached to the Euclidean representation <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px4.p3.m5\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>, followed by a softmax output that predicts the probability of the input being real or synthetic. <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> has a parameter footprint ranging from 8 to 14 million, depending on the combination and dimensionality of the pretrained speech encoders used.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "fusion",
                    "euclidean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our study is grounded in experiments on two datasets: \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">DFADD</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024dfadd</span></cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/isjwdu/DFADD\" title=\"\">https://github.com/isjwdu/DFADD</a></span></span></span>: It introduces a new generation of highly realistic spoofed audio samples generated using diffusion and flow-matching (FM) text-to-speech models. It includes over 163,500 synthetic samples paired with 44,455 bonafide utterances from the VCTK corpus, covering 109 speakers. The spoofed audio is synthesized using Diffusion-based and Flow-matching-based speech generation models. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">ASVSpoof 2019 (ASV)</span> <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">wang2020asvspoof</span></cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://datashare.ed.ac.uk/handle/10283/3336\" title=\"\">https://datashare.ed.ac.uk/handle/10283/3336</a></span></span></span>: The dataset is a widely adopted benchmark for evaluating spoof detection systems under both logical access (LA) and physical access (PA) scenarios. In our work, we utilize the LA subset, which contains speech spoofed using traditional TTS and voice conversion (VC) methods&#8212;primarily waveform concatenation, parametric synthesis, and neural vocoders. The subset consists of bonafide utterances from 20 speakers sourced from the VCTK corpus Spoofed audio generated using 17 different TTS and VC systems, including some previously unseen systems in the evaluation set. We adopt the original train/test split of the LA subset as specified in the paper. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Dataset Usage Protocol</span>:\nIn our study, we use these datasets to simulate cross-paradigm generalization. Specifically, we train on traditional TTS data from ASV and evaluate on DFADD to measure forward generalization from older to newer synthesis methods. Conversely, we also train on DFADD and evaluate on ASV to assess backward generalization from modern diffusion/FM generators to legacy TTS systems. This bidirectional setup allows us to rigorously test the synthesis-invariance of our proposed approach. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Training Details</span>:\nAll models are trained using the Adam optimizer with cross-entropy loss. The learning rate is set to 1e-3, with a batch size of 32 for 50 epochs. Dropout and early stopping are applied to prevent overfitting. We use five-fold cross-validation within each dataset and report average metrics. For cross-dataset evaluation, no target data is used during training.</p>\n\n",
                "matched_terms": [
                    "each",
                    "under",
                    "rate",
                    "evaluation",
                    "assess"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.T1\" title=\"Table 1 &#8227; 5.1 Benchmark Dataset &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the EER (%) results for each model across two cross-domain setups: training on ASVspoof and testing on DFADD (TR-A &#8594; TE-D), and the reverse (TR-D &#8594; TE-A). Each PTM is assessed in both its original form (Baseline) and its <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span>-enhanced version (Novel). From the results, we observe that across both settings, the RHYME-enhanced models consistently achieve lower EERs than their baseline counterparts. For instance, WavLM benefits from a noticeable EER drop from 29.37% to 21.87% (TR-A &#8594; TE-D), and from 20.05% to 13.17% (TR-D &#8594; TE-A). Similarly, models like HuBERT, Whisper, and Wav2Vec2 also show clear improvements under both configurations. These trends confirm that RHYME effectively helps in aligning diverse cues, leading to better generalization under unseen conditions. A consistent observation is that training on DFADD and testing on ASVspoof (TR-D &#8594; TE-A) results in relatively lower EERs than the reverse direction. This suggests that the diverse spoofing methods and higher-quality generation found in DFADD lead to more transferable representations. Among all models, USAD achieves the strongest results with EERs of 14.12% (TR-A &#8594; TE-D) and 10.26% (TR-D &#8594; TE-A), showcasing RHYME&#8217;s ability to handle significant domain shifts. It is also important to note that the DFADD paper <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">du2024dfadd</span></cite> reports an average 32.44% EER for the state-of-the-art end-to-end model AASIST-L under the TR-A &#8594; TE-D setup. While our framework does not perform full model fine-tuning and instead relies on frozen pretrained embeddings, our framework achieves a substantially lower EER of 14.12%&#8212;underscoring the efficacy of geometry-aware fusion for robust and synthesis-invariant detection. \n<br class=\"ltx_break\"/>In Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.T2\" title=\"Table 2 &#8227; 5.1 Benchmark Dataset &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we evaluate the performance of <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span>, which is specifically designed to assess generalization in truly unseen conditions. In this setup, we first extract embeddings using USAD backbone PTMs from both ASVspoof and DFADD datasets. These embeddings are used to train downstream models using our <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> framework. The testing phase involves audio samples synthesized from various TTS models&#8212;which were not part of the training data. These samples were collected from official demo pages of each model, introducing a strong domain shift due to differences in speaker identity, acoustic environments, and generation fidelity. Since the model was only trained on the curated subsets from DFADD, it did not encounter any of the demo-sourced speech samples during training, making this a challenging and realistic generalization test. Despite the difficulty of this setting, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> achieves strong performance across several domains. Among the generators, CMTTS consistently shows the lowest EERs, including 0% on DFADD, 14.14% on D1, and as low as 0.019% on F1. DiTTo-TTS and ReFlow-TTS also perform well across most domains, indicating RHYME&#8217;s ability to handle both autoregressive and diffusion-based synthesis techniques. On comparatively more variable systems like Diffr and DiPro, we observe a wider spread in EERs across domains; however, <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> still maintains stability, with EERs well below 20% in most cases. To further understand our framework robustness, we conduct an additional set of experiments where the model is trained on data generated from a single synthesizer&#8212;such as D1, D2, D3, F1, or F2&#8212;and tested on unseen samples from other models like Voicebox and NaturalSpeech3. This controlled setup isolates how well the model can transfer from one generator&#8217;s characteristics to another. Even with such limited training data, RHYME generalizes effectively in many cases, suggesting that the learned representations are transferable across diverse generation styles. We further investigate the representational quality and decision reliability of RHYME using t-SNE visualizations and calibration plots. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F2\" title=\"Figure 2 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the distribution of embeddings using t-SNE. In subfigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F2.sf1\" title=\"In Figure 2 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>, we visualize raw USAD embeddings extracted from DFADD and ASVspoof. The overlap between real and fake samples suggests weak discrimination in the original embedding space. While, subfigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F2.sf2\" title=\"In Figure 2 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a> shows RHYME-fused embeddings when trained on DFADD and evaluated on both DFADD and ASVspoof. Here, the real and fake classes form visibly distinct clusters, highlighting RHYME&#8217;s ability to disentangle domain-invariant and spoof-discriminative cues through geometric fusion. To evaluate how well model confidence aligns with actual correctness, we use calibration diagrams shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F3\" title=\"Figure 3 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Subfigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F3.sf1\" title=\"In Figure 3 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3(a)</span></a> depicts the calibration curve when the model is trained on DFADD and tested on both DFADD and ASVspoof. The curve closely follows the diagonal, indicating good calibration between predicted probabilities and true outcome frequencies, more importantly, the reliability curve remains closely aligned for the out-of-domain ASVspoof test split. This demonstrates that the model&#8217;s softmax outputs remain trustworthy even under out-of-domain detection. On the other hand, subfigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.10793v1#S5.F3.sf2\" title=\"In Figure 3 &#8227; 5.2 Experimental Results &#8227; 5 Experiments &#8227; Curved Worlds, Clear Boundaries: Generalizing Speech Deepfake Detection using Hyperbolic and Spherical Geometry Spaces\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>, where the model is trained on ASVspoof, shows larger deviations&#8212;highlighting miscalibration. These results reinforce that RHYME not only improves detection accuracy but also produces more reliable probability estimates, a key requirement for deployment in risk-sensitive applications.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "assess",
                    "each",
                    "shift",
                    "geometryaware",
                    "baseline",
                    "usad",
                    "results",
                    "tea",
                    "lowest",
                    "distribution",
                    "fusion",
                    "trd",
                    "only",
                    "performance",
                    "ted",
                    "under",
                    "eers",
                    "eer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the effectiveness and generalization ability of <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> under two settings, we use multiple pretrained models (PTMs). We extract embeddings from these PTMs and train models on dataset (ASVspoof or DFADD), then test on the other. This helps us understand how well each model adapts to changes in data distribution and spoofing style. In the second setup, we test the model on deepfakes generated by a variety of TTS systems not seen during training. These samples are collected from the official demo pages of VoiceBox <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">le2023voicebox</span></cite>, VoiceFlow <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guo2024voiceflow</span></cite>, NaturalSpeech 3 <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ju2024naturalspeech</span></cite>, CMTTS <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">li2024cm</span></cite>, DiffProsody <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">oh2024diffprosody</span></cite>, DiffAR <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">benita2023diffar</span></cite>, DiTTo-TTS <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">lee2024ditto</span></cite>, and ReFlow-TTS <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">guan2024reflow</span></cite>. VoiceBox and VoiceFlow use flow-matching, while the others are diffusion-based. The model was trained only on DFADD and ASVspoof data, so this setup gives us a clear view of how well it performs in truly unseen scenarios. We also perform a focused experiment where the model is trained on samples (D1, D2, D3, F1, F2) and then tested on other synthesizers. This helps isolate how well proposed framework generalizes across different types of generators.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "each",
                    "under",
                    "distribution",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we presented <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span>, a unified and geometry-aware framework for generalizable audio deepfake detection across diverse synthesis paradigms, including TTS, vocoder, and diffusion-based generators. By leveraging hyperbolic and spherical projections to model complementary synthesis artifacts and fusing them via Riemannian barycentric averaging, the proposed method learns a synthesis-invariant embedding space that supports strong generalization. The outcomes confirm that uniting hyperbolic hierarchy modeling with spherical artifact encoding through Riemannian fusion yields a geometry-driven, synthesis-invariant detector that generalizes across unseen generators. Extensive experiments show that it consistently outperforms existing detectors and fusion baselines under cross-corpus, zero-shot, and unseen-generator settings. These findings demonstrate the effectiveness of our approach and highlight its potential as a reliable and scalable solution for audio deepfake detection under diverse and previously unseen conditions. Our work also calls upon researchers to build on our extended benchmarks to further advance performance in generalizable audio deepfake detection.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "performance",
                    "under",
                    "spherical",
                    "geometryaware",
                    "hyperbolic",
                    "fusion",
                    "demonstrate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work aims to support the growing need for detecting synthetic speech by introducing a generalizable deepfake detection framework. As audio generation models become more realistic, tools like <span class=\"ltx_text ltx_font_typewriter ltx_font_bold\">RHYME</span> can help protect against misuse in areas such as voice fraud, impersonation, and misinformation. At the same time, we recognize the importance of using such detection systems responsibly. Our current study is limited to English datasets, and we encourage future research to consider more diverse languages and speaker populations. We also acknowledge that deepfake detection technology could be misused, for example, in surveillance or censorship. To reduce such risks, our work is shared for research purposes only and does not involve any sensitive or private data. We believe this study provides a step forward in building safer and more trustworthy speech systems.</p>\n\n",
                "matched_terms": [
                    "rhyme",
                    "only"
                ]
            }
        ]
    }
}