{
    "S5.T1": {
        "source_file": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio–Language Models",
        "caption": "Table 1: SRoA (%) of defenses against six jailbreak attacks on four ALMs.",
        "body": "Model\nMethod\nAdvWave\n\nAdvWave-P\n\n\nPAIR-Audio\n\nGupta et al.\nICA\n\nPAP-Audio\n\nAverage\n\n\n\n\nQwen2-Audio\nNone\n86.4\n80.8\n45.0\n54.3\n1.2\n47.6\n52.5\n\n\nGaussian Noise\n3.7\n13.6\n40.4\n18.1\n0.8\n50.3\n21.1\n\n\nLocal Smoothing\n7.1\n29.2\n40.6\n46.7\n1.0\n49.7\n29.0\n\n\nDownsampling\n10.6\n34.8\n43.9\n8.6\n1.2\n45.5\n24.1\n\n\nSelf-Reminder\n27.5\n38.3\n25.2\n41.0\n2.1\n31.7\n27.6\n\n\nICD\n64.6\n56.5\n15.4\n53.3\n0.4\n35.2\n37.6\n\n\n\n\\rowcolorgray!20\nALMGuard\n3.1\n11.7\n34.9\n0.5\n0.4\n46.2\n16.1\n\n\nLlama-Omni\nNone\n78.1\n79.0\n40.9\n26.9\n29.0\n35.2\n48.2\n\n\nGaussian Noise\n54.8\n54.6\n36.2\n20.7\n28.5\n34.5\n38.2\n\n\nLocal Smoothing\n56.5\n53.8\n39.7\n22.6\n29.8\n32.4\n39.2\n\n\nDownsampling\n54.2\n53.5\n41.3\n21.2\n28.5\n35.2\n39.0\n\n\nSelf-Reminder\n46.2\n44.8\n30.2\n25.7\n31.9\n31.0\n35.0\n\n\nICD\n21.7\n28.1\n17.9\n13.8\n8.5\n19.3\n18.2\n\n\n\n\\rowcolorgray!20\nALMGuard\n2.9\n3.5\n12.8\n0.2\n2.1\n26.9\n8.1\n\n\nLyra-Base\nNone\n23.1\n83.8\n10.1\n10.0\n42.7\n4.1\n29.0\n\n\nGaussian Noise\n0.2\n16.2\n5.4\n5.5\n55.2\n3.5\n14.3\n\n\nLocal Smoothing\n8.9\n54.6\n10.1\n10.7\n40.0\n4.1\n21.4\n\n\nDownsampling\n12.1\n65.0\n10.3\n13.6\n40.0\n4.1\n24.2\n\n\nSelf-Reminder\n6.9\n55.4\n11.7\n3.1\n1.7\n4.8\n13.9\n\n\nICD\n1.5\n23.8\n18.1\n1.2\n0.2\n9.7\n9.1\n\n\n\n\\rowcolorgray!20\nALMGuard\n10.6\n16.2\n6.4\n6.7\n40.2\n0.7\n13.5\n\n\nQwen2.5-Omni\nNone\n26.4\n30.4\n60.4\n26.0\n0.2\n77.9\n36.9\n\n\nGaussian Noise\n1.4\n5.2\n59.8\n17.4\n0.2\n83.5\n27.9\n\n\nLocal Smoothing\n1.2\n9.8\n46.2\n1.9\n0.2\n84.8\n24.0\n\n\nDownsampling\n9.2\n12.1\n48.2\n1.2\n0.2\n85.5\n26.1\n\n\nSelf-Reminder\n1.9\n8.9\n46.2\n13.8\n0.2\n75.9\n24.5\n\n\nICD\n1.2\n2.5\n47.6\n12.4\n0.0\n65.5\n21.5\n\n\n\n\\rowcolorgray!20\nALMGuard\n1.7\n0.0\n51.2\n0.0\n1.0\n70.3\n20.7\n\n\nAverage\nNone\n53.5\n68.5\n39.1\n29.3\n18.3\n41.2\n41.6\n\n\nGaussian Noise\n15.0\n22.4\n35.5\n15.4\n21.2\n42.9\n25.4\n\n\nLocal Smoothing\n18.4\n36.9\n34.1\n20.5\n17.7\n42.8\n28.4\n\n\nDownsampling\n21.5\n41.3\n35.9\n11.1\n17.5\n42.6\n28.3\n\n\nSelf-Reminder\n20.6\n36.8\n28.3\n20.9\n9.0\n35.9\n25.3\n\n\nICD\n22.3\n27.7\n24.7\n20.2\n2.3\n32.4\n21.6\n\n\n\n\\rowcolorgray!20\nALMGuard\n4.6\n7.8\n26.3\n1.9\n10.9\n36.0\n14.6",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">AdvWave</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">AdvWave</span>-<span class=\"ltx_text ltx_font_bold\">P</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">PAIR</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ICA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">PAP</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Average</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"6\">Qwen2-Audio</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">None</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">86.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">80.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">45.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">54.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">52.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Gaussian Noise</th>\n<td class=\"ltx_td ltx_align_center\">3.7</td>\n<td class=\"ltx_td ltx_align_center\">13.6</td>\n<td class=\"ltx_td ltx_align_center\">40.4</td>\n<td class=\"ltx_td ltx_align_center\">18.1</td>\n<td class=\"ltx_td ltx_align_center\">0.8</td>\n<td class=\"ltx_td ltx_align_center\">50.3</td>\n<td class=\"ltx_td ltx_align_center\">21.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Local Smoothing</th>\n<td class=\"ltx_td ltx_align_center\">7.1</td>\n<td class=\"ltx_td ltx_align_center\">29.2</td>\n<td class=\"ltx_td ltx_align_center\">40.6</td>\n<td class=\"ltx_td ltx_align_center\">46.7</td>\n<td class=\"ltx_td ltx_align_center\">1.0</td>\n<td class=\"ltx_td ltx_align_center\">49.7</td>\n<td class=\"ltx_td ltx_align_center\">29.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Downsampling</th>\n<td class=\"ltx_td ltx_align_center\">10.6</td>\n<td class=\"ltx_td ltx_align_center\">34.8</td>\n<td class=\"ltx_td ltx_align_center\">43.9</td>\n<td class=\"ltx_td ltx_align_center\">8.6</td>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n<td class=\"ltx_td ltx_align_center\">45.5</td>\n<td class=\"ltx_td ltx_align_center\">24.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Self-Reminder</th>\n<td class=\"ltx_td ltx_align_center\">27.5</td>\n<td class=\"ltx_td ltx_align_center\">38.3</td>\n<td class=\"ltx_td ltx_align_center\">25.2</td>\n<td class=\"ltx_td ltx_align_center\">41.0</td>\n<td class=\"ltx_td ltx_align_center\">2.1</td>\n<td class=\"ltx_td ltx_align_center\">31.7</td>\n<td class=\"ltx_td ltx_align_center\">27.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ICD</th>\n<td class=\"ltx_td ltx_align_center\">64.6</td>\n<td class=\"ltx_td ltx_align_center\">56.5</td>\n<td class=\"ltx_td ltx_align_center\">15.4</td>\n<td class=\"ltx_td ltx_align_center\">53.3</td>\n<td class=\"ltx_td ltx_align_center\">0.4</td>\n<td class=\"ltx_td ltx_align_center\">35.2</td>\n<td class=\"ltx_td ltx_align_center\">37.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>gray!20</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ALMGuard</th>\n<td class=\"ltx_td ltx_align_center\">3.1</td>\n<td class=\"ltx_td ltx_align_center\">11.7</td>\n<td class=\"ltx_td ltx_align_center\">34.9</td>\n<td class=\"ltx_td ltx_align_center\">0.5</td>\n<td class=\"ltx_td ltx_align_center\">0.4</td>\n<td class=\"ltx_td ltx_align_center\">46.2</td>\n<td class=\"ltx_td ltx_align_center\">16.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"6\">Llama-Omni</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">None</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">78.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">79.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">48.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Gaussian Noise</th>\n<td class=\"ltx_td ltx_align_center\">54.8</td>\n<td class=\"ltx_td ltx_align_center\">54.6</td>\n<td class=\"ltx_td ltx_align_center\">36.2</td>\n<td class=\"ltx_td ltx_align_center\">20.7</td>\n<td class=\"ltx_td ltx_align_center\">28.5</td>\n<td class=\"ltx_td ltx_align_center\">34.5</td>\n<td class=\"ltx_td ltx_align_center\">38.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Local Smoothing</th>\n<td class=\"ltx_td ltx_align_center\">56.5</td>\n<td class=\"ltx_td ltx_align_center\">53.8</td>\n<td class=\"ltx_td ltx_align_center\">39.7</td>\n<td class=\"ltx_td ltx_align_center\">22.6</td>\n<td class=\"ltx_td ltx_align_center\">29.8</td>\n<td class=\"ltx_td ltx_align_center\">32.4</td>\n<td class=\"ltx_td ltx_align_center\">39.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Downsampling</th>\n<td class=\"ltx_td ltx_align_center\">54.2</td>\n<td class=\"ltx_td ltx_align_center\">53.5</td>\n<td class=\"ltx_td ltx_align_center\">41.3</td>\n<td class=\"ltx_td ltx_align_center\">21.2</td>\n<td class=\"ltx_td ltx_align_center\">28.5</td>\n<td class=\"ltx_td ltx_align_center\">35.2</td>\n<td class=\"ltx_td ltx_align_center\">39.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Self-Reminder</th>\n<td class=\"ltx_td ltx_align_center\">46.2</td>\n<td class=\"ltx_td ltx_align_center\">44.8</td>\n<td class=\"ltx_td ltx_align_center\">30.2</td>\n<td class=\"ltx_td ltx_align_center\">25.7</td>\n<td class=\"ltx_td ltx_align_center\">31.9</td>\n<td class=\"ltx_td ltx_align_center\">31.0</td>\n<td class=\"ltx_td ltx_align_center\">35.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ICD</th>\n<td class=\"ltx_td ltx_align_center\">21.7</td>\n<td class=\"ltx_td ltx_align_center\">28.1</td>\n<td class=\"ltx_td ltx_align_center\">17.9</td>\n<td class=\"ltx_td ltx_align_center\">13.8</td>\n<td class=\"ltx_td ltx_align_center\">8.5</td>\n<td class=\"ltx_td ltx_align_center\">19.3</td>\n<td class=\"ltx_td ltx_align_center\">18.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>gray!20</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ALMGuard</th>\n<td class=\"ltx_td ltx_align_center\">2.9</td>\n<td class=\"ltx_td ltx_align_center\">3.5</td>\n<td class=\"ltx_td ltx_align_center\">12.8</td>\n<td class=\"ltx_td ltx_align_center\">0.2</td>\n<td class=\"ltx_td ltx_align_center\">2.1</td>\n<td class=\"ltx_td ltx_align_center\">26.9</td>\n<td class=\"ltx_td ltx_align_center\">8.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"6\">Lyra-Base</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">None</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">83.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">10.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Gaussian Noise</th>\n<td class=\"ltx_td ltx_align_center\">0.2</td>\n<td class=\"ltx_td ltx_align_center\">16.2</td>\n<td class=\"ltx_td ltx_align_center\">5.4</td>\n<td class=\"ltx_td ltx_align_center\">5.5</td>\n<td class=\"ltx_td ltx_align_center\">55.2</td>\n<td class=\"ltx_td ltx_align_center\">3.5</td>\n<td class=\"ltx_td ltx_align_center\">14.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Local Smoothing</th>\n<td class=\"ltx_td ltx_align_center\">8.9</td>\n<td class=\"ltx_td ltx_align_center\">54.6</td>\n<td class=\"ltx_td ltx_align_center\">10.1</td>\n<td class=\"ltx_td ltx_align_center\">10.7</td>\n<td class=\"ltx_td ltx_align_center\">40.0</td>\n<td class=\"ltx_td ltx_align_center\">4.1</td>\n<td class=\"ltx_td ltx_align_center\">21.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Downsampling</th>\n<td class=\"ltx_td ltx_align_center\">12.1</td>\n<td class=\"ltx_td ltx_align_center\">65.0</td>\n<td class=\"ltx_td ltx_align_center\">10.3</td>\n<td class=\"ltx_td ltx_align_center\">13.6</td>\n<td class=\"ltx_td ltx_align_center\">40.0</td>\n<td class=\"ltx_td ltx_align_center\">4.1</td>\n<td class=\"ltx_td ltx_align_center\">24.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Self-Reminder</th>\n<td class=\"ltx_td ltx_align_center\">6.9</td>\n<td class=\"ltx_td ltx_align_center\">55.4</td>\n<td class=\"ltx_td ltx_align_center\">11.7</td>\n<td class=\"ltx_td ltx_align_center\">3.1</td>\n<td class=\"ltx_td ltx_align_center\">1.7</td>\n<td class=\"ltx_td ltx_align_center\">4.8</td>\n<td class=\"ltx_td ltx_align_center\">13.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ICD</th>\n<td class=\"ltx_td ltx_align_center\">1.5</td>\n<td class=\"ltx_td ltx_align_center\">23.8</td>\n<td class=\"ltx_td ltx_align_center\">18.1</td>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n<td class=\"ltx_td ltx_align_center\">0.2</td>\n<td class=\"ltx_td ltx_align_center\">9.7</td>\n<td class=\"ltx_td ltx_align_center\">9.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>gray!20</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ALMGuard</th>\n<td class=\"ltx_td ltx_align_center\">10.6</td>\n<td class=\"ltx_td ltx_align_center\">16.2</td>\n<td class=\"ltx_td ltx_align_center\">6.4</td>\n<td class=\"ltx_td ltx_align_center\">6.7</td>\n<td class=\"ltx_td ltx_align_center\">40.2</td>\n<td class=\"ltx_td ltx_align_center\">0.7</td>\n<td class=\"ltx_td ltx_align_center\">13.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"6\">Qwen2.5-Omni</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">None</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">30.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">77.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Gaussian Noise</th>\n<td class=\"ltx_td ltx_align_center\">1.4</td>\n<td class=\"ltx_td ltx_align_center\">5.2</td>\n<td class=\"ltx_td ltx_align_center\">59.8</td>\n<td class=\"ltx_td ltx_align_center\">17.4</td>\n<td class=\"ltx_td ltx_align_center\">0.2</td>\n<td class=\"ltx_td ltx_align_center\">83.5</td>\n<td class=\"ltx_td ltx_align_center\">27.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Local Smoothing</th>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n<td class=\"ltx_td ltx_align_center\">9.8</td>\n<td class=\"ltx_td ltx_align_center\">46.2</td>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">0.2</td>\n<td class=\"ltx_td ltx_align_center\">84.8</td>\n<td class=\"ltx_td ltx_align_center\">24.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Downsampling</th>\n<td class=\"ltx_td ltx_align_center\">9.2</td>\n<td class=\"ltx_td ltx_align_center\">12.1</td>\n<td class=\"ltx_td ltx_align_center\">48.2</td>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n<td class=\"ltx_td ltx_align_center\">0.2</td>\n<td class=\"ltx_td ltx_align_center\">85.5</td>\n<td class=\"ltx_td ltx_align_center\">26.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Self-Reminder</th>\n<td class=\"ltx_td ltx_align_center\">1.9</td>\n<td class=\"ltx_td ltx_align_center\">8.9</td>\n<td class=\"ltx_td ltx_align_center\">46.2</td>\n<td class=\"ltx_td ltx_align_center\">13.8</td>\n<td class=\"ltx_td ltx_align_center\">0.2</td>\n<td class=\"ltx_td ltx_align_center\">75.9</td>\n<td class=\"ltx_td ltx_align_center\">24.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ICD</th>\n<td class=\"ltx_td ltx_align_center\">1.2</td>\n<td class=\"ltx_td ltx_align_center\">2.5</td>\n<td class=\"ltx_td ltx_align_center\">47.6</td>\n<td class=\"ltx_td ltx_align_center\">12.4</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">65.5</td>\n<td class=\"ltx_td ltx_align_center\">21.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>gray!20</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ALMGuard</th>\n<td class=\"ltx_td ltx_align_center\">1.7</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">51.2</td>\n<td class=\"ltx_td ltx_align_center\">0.0</td>\n<td class=\"ltx_td ltx_align_center\">1.0</td>\n<td class=\"ltx_td ltx_align_center\">70.3</td>\n<td class=\"ltx_td ltx_align_center\">20.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"6\">Average</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">None</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">53.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">18.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Gaussian Noise</th>\n<td class=\"ltx_td ltx_align_center\">15.0</td>\n<td class=\"ltx_td ltx_align_center\">22.4</td>\n<td class=\"ltx_td ltx_align_center\">35.5</td>\n<td class=\"ltx_td ltx_align_center\">15.4</td>\n<td class=\"ltx_td ltx_align_center\">21.2</td>\n<td class=\"ltx_td ltx_align_center\">42.9</td>\n<td class=\"ltx_td ltx_align_center\">25.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Local Smoothing</th>\n<td class=\"ltx_td ltx_align_center\">18.4</td>\n<td class=\"ltx_td ltx_align_center\">36.9</td>\n<td class=\"ltx_td ltx_align_center\">34.1</td>\n<td class=\"ltx_td ltx_align_center\">20.5</td>\n<td class=\"ltx_td ltx_align_center\">17.7</td>\n<td class=\"ltx_td ltx_align_center\">42.8</td>\n<td class=\"ltx_td ltx_align_center\">28.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Downsampling</th>\n<td class=\"ltx_td ltx_align_center\">21.5</td>\n<td class=\"ltx_td ltx_align_center\">41.3</td>\n<td class=\"ltx_td ltx_align_center\">35.9</td>\n<td class=\"ltx_td ltx_align_center\">11.1</td>\n<td class=\"ltx_td ltx_align_center\">17.5</td>\n<td class=\"ltx_td ltx_align_center\">42.6</td>\n<td class=\"ltx_td ltx_align_center\">28.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Self-Reminder</th>\n<td class=\"ltx_td ltx_align_center\">20.6</td>\n<td class=\"ltx_td ltx_align_center\">36.8</td>\n<td class=\"ltx_td ltx_align_center\">28.3</td>\n<td class=\"ltx_td ltx_align_center\">20.9</td>\n<td class=\"ltx_td ltx_align_center\">9.0</td>\n<td class=\"ltx_td ltx_align_center\">35.9</td>\n<td class=\"ltx_td ltx_align_center\">25.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ICD</th>\n<td class=\"ltx_td ltx_align_center\">22.3</td>\n<td class=\"ltx_td ltx_align_center\">27.7</td>\n<td class=\"ltx_td ltx_align_center\">24.7</td>\n<td class=\"ltx_td ltx_align_center\">20.2</td>\n<td class=\"ltx_td ltx_align_center\">2.3</td>\n<td class=\"ltx_td ltx_align_center\">32.4</td>\n<td class=\"ltx_td ltx_align_center\">21.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>gray!20</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\">ALMGuard</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\">4.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\">7.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\">26.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\">1.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\">10.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\">36.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\">14.6</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "icd",
            "gupta",
            "advwavep",
            "jailbreak",
            "qwen25omni",
            "against",
            "selfreminder",
            "six",
            "gaussian",
            "attacks",
            "lyrabase",
            "smoothing",
            "papaudio",
            "advwave",
            "defenses",
            "sroa",
            "local",
            "qwen2audio",
            "none",
            "noise",
            "model",
            "rowcolorgray20",
            "four",
            "average",
            "downsampling",
            "ica",
            "almguard",
            "method",
            "llamaomni",
            "alms",
            "pairaudio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance on seen attacks.</span>\nWe first evaluate the performance of ALMGuard on three seen attacks used during the optimization of the perturbation: AdvWave, AdvWave-P, and PAIR-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method significantly outperforms all baselines on AdvWave and AdvWave-P, reducing the average SRoA across four ALMs from 53.5% and 68.5% to 4.6% and 7.8%, respectively. This indicates that ALMGuard exhibits strong robustness against acoustic-based attacks. On PAIR-Audio, a representative semantic-based attack, ALMGuard reduces the average SRoA to 26.3%, achieving comparable performance to Self-Reminder and ICD. Notably, ALMGuard consistently achieves a lower SRoA than all baselines against AdvWave-P on every model, and even reduces it to 0 on Qwen2.5-Omni, making the model completely robust - a result that no existing defense has achieved.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferability to unseen attacks.</span>\nWe evaluate the transferability of ALMGuard on three unseen attacks: Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>, ICA, and PAP-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ALMGuard significantly reduces the average SRoA by 27.4%, 7.4%, and 5.2%, respectively. In particular, the average SRoA on Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;is reduced to only 1.9%, which is the lowest among all attacks. Given that AdvWave and Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;represent the current SOTA ALM-specific jailbreak attacks, we believe that ALMGuard achieves strong robustness against this class of threats.\nFor baselines, we observe that Type I defenses tend to perform better against acoustic-based attacks. In contrast, Type II defenses show significantly better performance on semantic-based attacks.\nThis observation suggests that no existing defense can dominate across all types of attacks.\nIn comparison, ALMGuard consistently outperforms Type I defenses across all attack categories. Compared to Type II defenses, ALMGuard achieves significantly better results on acoustic-based attacks.\nOn average, ALMGuard reduces the overall SRoA to 14.6%, which represents the current SOTA.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats.\nTo address this issue, we propose <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, the first defense framework tailored to ALMs.\nBased on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time.\nTo better sift out effective triggers while preserving the model&#8217;s utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding.\nBoth theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, ALMGuard reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/WeifeiJin/ALMGuard\" title=\"\">https://github.com/WeifeiJin/ALMGuard</a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "attacks",
                    "jailbreak",
                    "method",
                    "defenses",
                    "four",
                    "average",
                    "alms",
                    "against",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Why ALM-specific defense?</span>\nRecent research proposing jailbreak attacks tailored to ALMs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite> has substantiated that the integration of audio modality introduces distinct and previously unexplored threats.\nWe observe that existing defense methods transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a></cite> are largely ineffective in mitigating these ALM-specific threats.\nThis can be attributed to a lack of consideration of the behavioral diversity and inherent complexity of ALMs, as well as an insufficient adaptation to the distinct characteristics of the audio modality. A similar limitation also exists on the attack side&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "alms",
                    "jailbreak",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our stance.</span>\nThe limitations of existing defenses against ALM-specific jailbreaks motivate a novel approach. We hypothesize that well-aligned ALMs inherently possess <em class=\"ltx_emph ltx_font_italic\">safety shortcuts</em>, which are latent pathways or input sensitivities that, if correctly triggered, can steer models towards safer behavior and mitigate jailbreaks. These differ from explicit safety alignments, representing intrinsic model properties that can be leveraged for defense.\nThe primary challenge is to activate these safety shortcuts efficiently and harmlessly at inference time. We suggest that this can be achieved by applying a lightweight, universal acoustic perturbation to the input, referred to as the <span class=\"ltx_text ltx_font_bold\">Shortcut Activation Perturbation (SAP)</span>. SAP is designed to engage these safety-conducive pathways without requiring any model retraining. However, to prevent such a perturbation from degrading benign task performance, its application must be precisely targeted.\nTo this end, we introduce the <span class=\"ltx_text ltx_font_bold\">Mel-Gradient Sparse Mask (M-GSM)</span>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, M-GSM identifies a sparse set of Mel-frequency bins that are highly influential for jailbreak mitigation yet largely inconsequential for benign speech understanding, as measured by automatic speech recognition (ASR) tasks. Our framework, <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, then synergistically employs M-GSM to guide the application of the universal SAP only to Mel bins that are security-critical yet benign-insensitive.\nThis targeted strategy allows ALMGuard to effectively activate safety shortcuts for robust defense while preserving model utility (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, performance on benign inputs).</p>\n\n",
                "matched_terms": [
                    "model",
                    "jailbreak",
                    "defenses",
                    "alms",
                    "against",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and results.</span>\nWe evaluate the proposed method across four state-of-the-art (SOTA) ALMs and six representative jailbreak attacks. On Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a></cite>, ALMGuard reduces the success rate of the two most recent ALM-specific attacks, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> and Gupta <span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite>, to 3.1% and 0.5% respectively, outperforming all baselines and establishing itself as the new SOTA defense.\nMoreover, ALMGuard generalizes effectively to unseen attacks. As a lightweight defense framework, it enables near zero-cost deployment with negligible inference overhead. Evaluations on two benign benchmarks further confirm that ALMGuard does not noticeably degrade model utility. In addition, it achieves strong robustness against adaptive attacks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "six",
                    "attacks",
                    "gupta",
                    "advwave",
                    "jailbreak",
                    "method",
                    "four",
                    "alms",
                    "qwen2audio",
                    "against",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce the concept of inherent safety shortcuts in ALMs and propose ALMGuard as the first comprehensive and principled framework to systematically discover and activate these latent pathways for robust and generalizable jailbreak defense.</p>\n\n",
                "matched_terms": [
                    "alms",
                    "jailbreak",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through extensive evaluations, complemented by theoretical analyses, we show that ALMGuard achieves SOTA defense against six jailbreaks on four SOTA ALMs, with strong generalization, high benign-task utility, and negligible inference overhead.</p>\n\n",
                "matched_terms": [
                    "six",
                    "four",
                    "alms",
                    "against",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Shortcut learning.</span>\nDeep neural networks (DNNs) are known to learn &#8220;shortcut&#8221; features from data, which correlate with training labels but may not align with the designer&#8217;s intent or generalize well&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib13\" title=\"\">geirhos2020shortcut </a></cite>. These shortcuts can be harnessed for both beneficial and harmful purposes.\nOn the one hand, they may improve robustness, as demonstrated by unadversarial perturbations in computer vision&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib39\" title=\"\">salman2021unadversarial </a></cite>,\nand can even be deliberately leveraged for defensive purposes, such as constructing safety-aligned or unlearnable features&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib48\" title=\"\">wu2023one </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib44\" title=\"\">wang2025provably </a></cite>.\nOn the other hand, they can be exploited for malicious objectives, such as in backdoor attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib14\" title=\"\">gu2017badnets </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib26\" title=\"\">li2022backdoor </a></cite>,\npoisoning attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib5\" title=\"\">biggio2012poisoning </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib58\" title=\"\">zhao2025data </a></cite>,\nand adversarial examples&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib19\" title=\"\">ilyas2019adversarial </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib41\" title=\"\">szegedy2013intriguing </a></cite>.\nThe core hypothesis of this work is that similar shortcut mechanisms may naturally exist or can be revealed within well-trained ALMs, specifically those that align with safety objectives, which we term safety-aligned shortcuts. Our goal is to identify and leverage these beneficial shortcuts for defensive purposes. To this end, we propose a method to discover such universal safety-aligned shortcuts within the acoustic input space. We then activate these shortcuts at inference time via a carefully crafted audio perturbation, which serves as a lightweight and effective safeguard. Let <math alttext=\"\\mathcal{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><annotation encoding=\"application/x-tex\">\\mathcal{L}</annotation></semantics></math> denote the training loss, our objective is to optimize a shortcut activation perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> that increases the model&#8217;s tendency to produce safe outputs <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math> when given any malicious Mel-spectrogram <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "alms",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Jailbreaks and defenses.</span>\nJailbreaking was initially introduced in the context of text-based LLMs, where attackers craft malicious prompts to bypass the model&#8217;s built-in safety mechanisms and induce it to generate harmful content. Existing jailbreak techniques for LLMs can be broadly categorized into two types: suffix-based and semantic-based. Suffix-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib60\" title=\"\">zou2023universal </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib27\" title=\"\">liao2024amplegcg </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib3\" title=\"\">basani2024gasp </a></cite> append an adversarial suffix after a harmful query, while semantic-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib29\" title=\"\">liu2023autodan </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib6\" title=\"\">chao2023jailbreaking </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib31\" title=\"\">mehrotra2024tree </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib57\" title=\"\">zeng2024johnny </a></cite> manipulate the prompt content using strategies such as persuasion or logical traps to elicit the desired malicious response.\nIn the context of ALMs, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> is a suffix-based attack that appends an adversarial noise segment, while Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite> explore prefix and perturbation formats. Modern foundation models are typically enhanced with preference-based alignment (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, reinforcement learning from human feedback (RLHF)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib7\" title=\"\">christiano2017deep </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib32\" title=\"\">ouyang2022training </a></cite> and direct preference optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib37\" title=\"\">rafailov2023direct </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib2\" title=\"\">amini2024direct </a></cite>) where human judgments guide model fine-tuning. As jailbreak attacks continue to emerge, corresponding defense mechanisms have been proposed, including input-level detection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib1\" title=\"\">alon2023detecting </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib38\" title=\"\">robey2023smoothllm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib20\" title=\"\">inan2023llama </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib34\" title=\"\">phute2023llm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib50\" title=\"\">xie2024gradsafe </a></cite> and mitigation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib21\" title=\"\">jain2023baseline </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib54\" title=\"\">xu2024safedecoding </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, and output-level intervention&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib45\" title=\"\">wang2024selfdefend </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib35\" title=\"\">qian2024hsf </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib46\" title=\"\">wang2025vulnerability </a></cite>.\nHowever, defenses tailored to ALMs remain underexplored. In this paper, we address this gap by proposing a dedicated framework that targets their unique vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "model",
                    "attacks",
                    "gupta",
                    "advwave",
                    "jailbreak",
                    "defenses",
                    "alms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our overall objective is to identify an acoustic signal, SAP, that can effectively activate the model&#8217;s inherent safety shortcuts to mitigate jailbreaks, without significantly degrading the model&#8217;s performance on benign inputs. We formulate this as an optimization problem aimed at making the model output safe responses to jailbreaking audio when subjected to this perturbation.\nSpecifically, given an ALM <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math>, a set of malicious instructions <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>, and a set of jailbreak algorithms <math alttext=\"\\mathcal{A}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{A}^{\\text{jb}}</annotation></semantics></math>, our goal is formulated as:</p>\n\n",
                "matched_terms": [
                    "jailbreak",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate this limitation, we conduct a deeper analysis and observe that only a small subset of frequency bands contribute meaningfully to jailbreak mitigation, while most Mel bins are insensitive to the defense objective, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Motivated by this observation, we propose to apply perturbations only to the most critical frequency bands and filter out the rest. This greatly reduces the perturbation region and thus minimizes its impact on model utility.</p>\n\n",
                "matched_terms": [
                    "jailbreak",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given our goal of activating safety shortcuts that effectively mitigate jailbreaks while minimizing the impact on model performance in benign tasks, we aim to identify frequency bands that are highly sensitive to jailbreak mitigation objectives but relatively insensitive to ASR tasks. Based on this intuition, we define a combined sensitivity score as follows:</p>\n\n",
                "matched_terms": [
                    "jailbreak",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ALMGuard recap.</span> We first construct the training dataset <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> by applying the jailbreak algorithm set <math alttext=\"\\mathcal{A}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{A}^{\\text{jb}}</annotation></semantics></math> to the jailbreak prompt set <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>. We then compute the average gradient-based sensitivity scores over this dataset to obtain the M-GSM <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>. The perturbation is initialized from a normal distribution <math alttext=\"\\mathcal{N}(0,\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>&#963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,\\sigma)</annotation></semantics></math> and optimized iteratively over <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math>. The pseudocode is presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A2\" title=\"Appendix B Pseudocode &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "average",
                    "jailbreak",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we provide theoretical analyses of ALMGuard by examining its generalization from training data and seen attacks to unseen examples and unseen attacks. Specifically, we define empirical and population safety risks, and derive an upper bound on the generalization gap.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The empirical safety risk measures the average failure on the training jailbreak set, while the population risk reflects the expected safety violation over real-world jailbreak examples.</p>\n\n",
                "matched_terms": [
                    "average",
                    "jailbreak"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This theorem implies that as long as the empirical safety risk on the training set is sufficiently low, we can guarantee with high confidence that the population safety risk on unseen attacks and unseen examples is also low. This provides a theoretical justification for the generalization ability of ALMGuard. We provide a complete proof of this result in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A3.SS1\" title=\"C.1 Proof of Safety Risk Generalization Bound &#8227; Appendix C Proofs &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">C.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we theoretically analyze how ALMGuard limits its impact on ALM performance over benign examples, thereby satisfying the constraint Equation&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). We begin by bounding the perturbation-induced change in audio embeddings, and subsequently leverage this bound to derive the performance degradation bound of ALMs on benign tasks.</p>\n\n",
                "matched_terms": [
                    "alms",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This bound shows that the performance impact on benign tasks is proportional to the tunable hyperparameters, <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> and <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (through <math alttext=\"d_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">d_{k}</annotation></semantics></math>), and two stability-related constants, <math alttext=\"L_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>enc</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{enc}}</annotation></semantics></math> and <math alttext=\"G_{\\max}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>max</mi></msub><annotation encoding=\"application/x-tex\">G_{\\max}</annotation></semantics></math>. The design of M-GSM aims to apply perturbations to regions where these stability factors collectively result in minimal adverse impact on benign tasks. This theoretical upper bound supports the claim that ALMGuard can preserve model performance on benign tasks, which aligns with our empirical results shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.SS3\" title=\"5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset and models.</span> In line with AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite>, we adopt AdvBench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib60\" title=\"\">zou2023universal </a></cite>, a benchmark widely used in text-based jailbreak research, which contains a total of 520 prompts. These prompts are converted into audio using OpenAI&#8217;s text-to-speech (TTS) API to construct <span class=\"ltx_text ltx_font_bold\">AdvBench</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span>, comprising 520 audio queries. We evaluate four state-of-the-art audio-language models: <span class=\"ltx_text ltx_font_bold\">Qwen2</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a></cite>, <span class=\"ltx_text ltx_font_bold\">LLaMA</span>-<span class=\"ltx_text ltx_font_bold\">Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib10\" title=\"\">fang2024llama </a></cite>, <span class=\"ltx_text ltx_font_bold\">Lyra</span>-<span class=\"ltx_text ltx_font_bold\">Base</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib59\" title=\"\">zhong2024lyra </a></cite>, and <span class=\"ltx_text ltx_font_bold\">Qwen2.5</span>-<span class=\"ltx_text ltx_font_bold\">Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib53\" title=\"\">xu2025qwen2 </a></cite>. All models are capable of accepting audio as input and producing either textual responses.</p>\n\n",
                "matched_terms": [
                    "lyrabase",
                    "advwave",
                    "jailbreak",
                    "four",
                    "llamaomni",
                    "qwen25omni",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attacks.</span>\nFor the attack methods, we first adopt two SOTA jailbreak approaches specifically designed for ALMs, namely <span class=\"ltx_text ltx_font_bold\">AdvWave</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> and the method proposed by <span class=\"ltx_text ltx_font_bold\">Gupta <span class=\"ltx_text ltx_font_italic\">et al.</span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite>. In addition, we adapt perturbation-based attacks from the traditional domain of audio adversarial attacks into the AdvWave framework, resulting in a variant named <span class=\"ltx_text ltx_font_bold\">AdvWave</span>-<span class=\"ltx_text ltx_font_bold\">P</span>. We further transfer several representative techniques from the text-based jailbreak literature, including <span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Attack (ICA)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, Prompt Automatic Iterative Refinement (PAIR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib6\" title=\"\">chao2023jailbreaking </a></cite>, and Persuasive Adversarial Prompts (PAP)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib57\" title=\"\">zeng2024johnny </a></cite>. For ICA, we prepend malicious textual demonstrations as context to the audio prompts. For PAIR and PAP, we convert their generated jailbreak texts into audio using OpenAI&#8217;s TTS API, thereby forming the <span class=\"ltx_text ltx_font_bold\">PAIR</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span> and <span class=\"ltx_text ltx_font_bold\">PAP</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span> variants. We classify AdvWave, AdvWave-P, and Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;as <span class=\"ltx_text ltx_font_italic\">acoustic-based</span> attacks, and the remaining three as <span class=\"ltx_text ltx_font_italic\">semantic-based</span> attacks. A detailed description of all attack methods is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS2\" title=\"D.2 Attack Setup &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "gupta",
                    "advwavep",
                    "papaudio",
                    "advwave",
                    "jailbreak",
                    "method",
                    "alms",
                    "pairaudio",
                    "ica"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span>\nFor the defense methods, due to the lack of dedicated defenses targeting jailbreaks in ALMs, we explore two directions of transfer. <span class=\"ltx_text ltx_font_bold\">Type I:</span> From the domain of traditional audio adversarial defenses, we consider three widely adopted techniques&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib12\" title=\"\">ge2023advddos </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib11\" title=\"\">fang2024zero </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib23\" title=\"\">jin2025whispering </a></cite>: (<span class=\"ltx_text ltx_font_italic\">i</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Gaussian Noise</span>, (<span class=\"ltx_text ltx_font_italic\">ii</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Local Smoothing</span>, and (<span class=\"ltx_text ltx_font_italic\">iii</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Downsampling</span>. <span class=\"ltx_text ltx_font_bold\">Type II:</span> From the domain of text-based jailbreak defense, we implement two representative methods: <span class=\"ltx_text ltx_font_bold\">Self-Reminder</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a></cite> and <span class=\"ltx_text ltx_font_bold\">In-Context Defense (ICD)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>. Details of these defense techniques are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS3\" title=\"D.3 Baseline Setup &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "selfreminder",
                    "icd",
                    "gaussian",
                    "smoothing",
                    "jailbreak",
                    "defenses",
                    "local",
                    "alms",
                    "downsampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics.</span>\nWe evaluate the effectiveness of the aforementioned jailbreak attacks and defenses using the <span class=\"ltx_text ltx_font_bold\">Success Rate of Attack (SRoA)</span>. Following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite>, we adopt a well-tuned LLM judge model from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib49\" title=\"\">xie2024sorry </a></cite> to determine whether a jailbreak attempt is successful.\nTo assess model utility, we consider two benchmarks. First, we sample 500 audio clips from the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib33\" title=\"\">panayotov2015librispeech </a></cite>, a standard ASR task, to measure the model&#8217;s basic speech understanding capability, using <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span> as the evaluation metric.\nIn addition, we employ 800 speech samples from AIR-Bench-Chat&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib55\" title=\"\">yang2024air </a></cite> to further evaluate the model&#8217;s audio-to-text interaction performance. For this evaluation, each response is assigned a <span class=\"ltx_text ltx_font_bold\">Response Quality Score (RQS)</span> on a 1-10 scale by DeepSeek-V3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib28\" title=\"\">liu2024deepseek </a></cite>, where higher RQS values indicate stronger model performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "attacks",
                    "jailbreak",
                    "sroa",
                    "defenses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ALMGuard&#160;setup.</span>\nDuring the optimization of ALMGuard, we randomly select 50 audio samples from AdvBench-Audio and apply a set of attack algorithms, namely AdvWave, AdvWave-P, and PAIR-Audio, for training-time perturbation optimization. We believe that the selected samples and attack methods are sufficiently representative to enable transferability to unseen examples and attacks. The perturbation duration is set to 30 seconds, consistent with the default input length of Whisper, and the perturbation budget is constrained by <math alttext=\"\\epsilon=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.5</annotation></semantics></math>. We set the value of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> to 48, and provide a detailed analysis in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.SS4\" title=\"5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "advwavep",
                    "advwave",
                    "pairaudio",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T2\" title=\"Table 2 &#8227; 5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the performance of our method on two benign benchmarks. On Qwen2-Audio, ALMGuard causes only a slight degradation in performance, increasing the WER on LibriSpeech by 1.85% and decreasing the AIR-Bench-Chat score by 0.56, which we regard as negligible. In contrast, both Self-Reminder and ICD significantly impair ASR performance, increasing the WER by 26.27% and 8.98% respectively, indicating that both baselines considerably disrupt the model&#8217;s understanding of speech semantics. We hypothesize that this is due to Qwen2-Audio being highly sensitive to system prompts, where the presence of Self-Reminder and ICD causes the model to generate refusal responses even for normal ASR inputs. However, on AIR-Bench-Chat, where task instructions are more diverse, both baselines return to relatively normal performance.\nNotably, ALMGuard even improves model performance on Lyra-Base, reducing WER by 1.16% and increasing the AIR-Bench-Chat RQS by 0.15, outperforming all baselines and even the original model without defense.\nFor completeness, we also report results on LLaMA-Omni and Qwen2.5-Omni in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS5\" title=\"D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.5</span></a>.\nOverall, our method demonstrates minimal impact on model utility, suggesting that it can be reliably deployed in real-world ALM systems and significantly enhances practical usability.</p>\n\n",
                "matched_terms": [
                    "selfreminder",
                    "model",
                    "icd",
                    "lyrabase",
                    "method",
                    "llamaomni",
                    "qwen25omni",
                    "qwen2audio",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution of M-GSM.</span>\nIn ALMGuard, we employ M-GSM to ensure that the model&#8217;s utility is not significantly affected. To validate the effectiveness of this key component, we conduct an ablation study on Qwen2-Audio by testing three jailbreak attacks and two benign benchmarks, both with and without M-GSM. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "qwen2audio",
                    "jailbreak",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In terms of defense effectiveness, the results with and without M-GSM show similar performance, both achieving over 50% reduction in average SRoA. However, regarding model utility, we observe a clear distinction. Without M-GSM, the WER on the ASR task increases by 20%, which substantially degrades the model&#8217;s ability to understand speech. In addition, the RQS drops by 1.17. In contrast, with M-GSM enabled, the fluctuations in WER and RQS are limited to within approximately 2% and 0.5, respectively, indicating a negligible impact on utility.</p>\n\n",
                "matched_terms": [
                    "average",
                    "model",
                    "sroa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hyperparameter analyses.</span>\nAn important hyperparameter in our method is the value of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>, which determines the number of Mel-frequency bins to which perturbations are applied. This parameter controls the trade-off between defense effectiveness and the impact on model utility. To investigate its influence, we conduct experiments with <math alttext=\"k\\in\\{0,16,48,96,128\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>48</mn><mo>,</mo><mn>96</mn><mo>,</mo><mn>128</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k\\in\\{0,16,48,96,128\\}</annotation></semantics></math>, where <math alttext=\"k=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">k=0</annotation></semantics></math> corresponds to the undefended setting with no perturbation applied, and <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math> represents the case without masking (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, full-band perturbation).\nWe evaluate the defense performance on PAIR-Audio and the benign-task performance on LibriSpeech. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F3\" title=\"Figure 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, increasing <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> leads to a monotonic decrease in SRoA and a monotonic increase in WER. This trend indicates that while a larger <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> improves robustness, it also introduces more distortion to benign inputs. Since our goal is to preserve utility while maximizing robustness, we identify <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m7\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> as a balanced configuration, where SRoA is reduced to 34.9% and WER remains low at 8.70%.</p>\n\n",
                "matched_terms": [
                    "method",
                    "model",
                    "pairaudio",
                    "sroa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive attacks.</span>\nTo further demonstrate the superiority of our method, we consider a more practical and challenging setting where the attacker has full knowledge of the defense mechanism, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, a white-box threat model. Under this setting, we evaluate adaptive AdvWave attacks on LLaMA-Omni by optimizing the adversarial suffix in the presence of each of the six defense methods.\nAs an example, when attacking ALMGuard, the attacker adds our well-trained SAP to the input at each iteration during AdvWave optimization.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, our method still achieves the best defense performance among all methods, reducing the SRoA by 11.9% compared to the original attack, even under this strongest threat model. In contrast, all baseline defenses yield an SRoA above 70% under adaptive attacks.\nInterestingly, for traditional audio defenses such as local smoothing, incorporating the corresponding transformations into the optimization process actually increases SRoA. We hypothesize that this is because these operations act similarly to data augmentation, which improves the robustness of the adversarial suffix and thus enhances the attack strength.\nIn summary, our method demonstrates the strongest resistance to adaptive attacks among all evaluated defenses.</p>\n\n",
                "matched_terms": [
                    "model",
                    "six",
                    "attacks",
                    "smoothing",
                    "advwave",
                    "method",
                    "defenses",
                    "sroa",
                    "llamaomni",
                    "local",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce ALMGuard, a novel framework that pioneers activating inherent safety shortcuts in ALMs via universal SAPs. Our M-GSM technique precisely guides these SAPs to critical frequency regions, enabling robust jailbreak mitigation while preserving model utility.\nEvaluations across six attack methods and four SOTA ALMs show that ALMGuard achieves overall better performance compared to existing defenses, while keeping its impact on model utility well-controlled. This offers a new perspective on enhancing robustness for multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "six",
                    "jailbreak",
                    "four",
                    "defenses",
                    "alms",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the pseudocode of ALMGuard in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#algorithm1\" title=\"In Appendix B Pseudocode &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The algorithm begins by constructing the training dataset <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> through the application of a jailbreak algorithm set <math alttext=\"\\mathcal{A}_{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\text{jb}}</annotation></semantics></math> to a curated jailbreak prompt set <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>. Based on this dataset, we compute the average sensitivity scores to derive the M-GSM <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, which identifies Mel-frequency bins that are critical for jailbreak mitigation yet minimally influential on benign performance. The universal perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> is initialized from a Gaussian distribution <math alttext=\"\\mathcal{N}(0,\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m6\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>&#963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,\\sigma)</annotation></semantics></math>, and iteratively optimized over the dataset to minimize the empirical safety risk.</p>\n\n",
                "matched_terms": [
                    "average",
                    "jailbreak",
                    "almguard",
                    "gaussian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rationality analyses of Assumption&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption2\" title=\"Assumption 2 (Local Sensitivity Bound of LLM Backbone). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</span>\nWe focus on how the LLM backbone <math alttext=\"f_{\\mathrm{LLM}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>LLM</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{LLM}}</annotation></semantics></math> responds to the small audio embedding perturbations <math alttext=\"\\Delta\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathbf{E}_{a}</annotation></semantics></math> that have already been &#8220;filtered&#8221; and &#8220;restricted&#8221; by the encoder <math alttext=\"f_{\\mathrm{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>enc</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{enc}}</annotation></semantics></math> and M-GSM. Although the exact global Lipschitz constant of an LLM is infeasible to compute, it is reasonable to assume that around its benign operating points (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, within the local neighborhood of a clean embedding <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m4\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>), the model exhibits relative smoothness and stability.</p>\n\n",
                "matched_terms": [
                    "local",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, LLMs are trained via self&#8208;supervised learning on massive text corpora, acquiring rich semantic representations and fluent generation capabilities. To achieve such generality and maintain good out&#8208;of&#8208;domain performance, the model must tolerate small perturbations in its inputs, including those in <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math> that carry no core semantic content. If the loss function of the LLM reacts with large gradient norms (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, a very large <math alttext=\"G_{\\max}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p6.m2\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>max</mi></msub><annotation encoding=\"application/x-tex\">G_{\\max}</annotation></semantics></math>) to any infinitesimal change in its input embedding, then stable training will become difficult and the model will be overly sensitive to noise, thereby degrading its comprehension and generation quality.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AdvWave.</span>\nA white-box jailbreak attack specifically designed for ALMs. It optimizes an adversarial suffix using cross-entropy loss. We set the suffix length to 44,100 samples (approximately 2.76 seconds) at a sampling rate of 16kHz. Following the original paper, we implement the dynamic target selection mechanism, which constructs a unique jailbreak target for each query. The maximum number of iterations is set to 3000, and the attack is considered successful when the loss falls below 0.1, at which point optimization terminates.</p>\n\n",
                "matched_terms": [
                    "advwave",
                    "alms",
                    "jailbreak"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AdvWave</span>-<span class=\"ltx_text ltx_font_bold\">P.</span>\nA variant of AdvWave is designed to better align with the audio adversarial attack paradigm. The suffix is replaced with an additive perturbation constrained under an <math alttext=\"\\ell_{\\infty}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mi mathvariant=\"normal\">&#8734;</mi></msub><annotation encoding=\"application/x-tex\">\\ell_{\\infty}</annotation></semantics></math> budget of 0.03. All other settings remain unchanged.</p>\n\n",
                "matched_terms": [
                    "advwave",
                    "advwavep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PAIR</span>-<span class=\"ltx_text ltx_font_bold\">Audio.</span>\nA black-box adversarial attack that iteratively queries the target LLM using inputs generated through interaction with an attacker LLM, requiring no human intervention. We follow the original PAIR procedure without modification to its core logic. The only changes are replacing the target LLM with our selected ALMs and converting the attacker LLM&#8217;s dialogue into audio using TTS. We use GPT-3.5-Turbo as the attacker LLM, consistent with the original paper.</p>\n\n",
                "matched_terms": [
                    "alms",
                    "pairaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gupta <span class=\"ltx_text ltx_font_italic\">et al</span>.</span>\nA white-box jailbreak method that evaluates various attack forms and stealth enhancement techniques. We adopt the most effective universal prefix proposed in their work. It is trained on 100 randomly selected samples from AdvBench-Audio and tested on the remaining 420.</p>\n\n",
                "matched_terms": [
                    "jailbreak",
                    "method",
                    "gupta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Attack.</span>\nA black-box jailbreak strategy that prepends a few successful attack demonstrations to the input prompt to induce in-context learning. Following the authors&#8217; recommended configuration, we use 10-shot prompts for LLaMA-Omni, Lyra-Base, and Qwen2.5-Omni. Due to context length limitations, we use 5-shot prompts for Qwen2-Audio.</p>\n\n",
                "matched_terms": [
                    "lyrabase",
                    "jailbreak",
                    "llamaomni",
                    "qwen25omni",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PAP</span>-<span class=\"ltx_text ltx_font_bold\">Audio.</span>\nA black-box jailbreak method inspired by social science-based persuasion strategies. We convert 145 successful jailbreak prompts provided by the authors into audio via TTS for evaluation.</p>\n\n",
                "matched_terms": [
                    "jailbreak",
                    "method",
                    "papaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gaussian Noise.</span>\nWe add Gaussian noise with a standard deviation of 0.01 to the input audio. This simple defense can effectively remove certain brittle adversarial perturbations, particularly suffix-based or prefix-based attacks.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "attacks",
                    "gaussian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Smoothing.</span>\nWe apply a moving average filter to smooth the waveform. Each audio sample is replaced by the average of its neighboring values within a window of size <math alttext=\"h=2\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I2.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">h=2</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "average",
                    "local",
                    "smoothing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Defense.</span>\nProposed in the same work as ICA, this method uses 2-shot in-context learning to guide the model toward safe behavior. We follow the original configuration and apply 2-shot demonstrations in the context prompt.</p>\n\n",
                "matched_terms": [
                    "method",
                    "model",
                    "ica"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate whether incorporating <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide the gradient direction is effective, we conduct a preliminary comparison study. Under the setting of <math alttext=\"k=16\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">k=16</annotation></semantics></math>, we evaluated both model utility and defense performance against AdvWave-P on Qwen2-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T6\" title=\"Table 6 &#8227; D.4 Ablation on &#8466;_&quot;ASR&quot; &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, when the M-GSM is applied, the presence or absence of <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> has negligible effect on both metrics. The WER and SRoA differ by only 0.07% and 1.5%, which can be attributed to random variations. In contrast, when M-GSM is removed, using <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide optimization fails to preserve utility: the WER increases by 17.88% compared to the undefended case.\nIn summary, we conclude that <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> does not provide a meaningful benefit in achieving the goal of activating safety shortcuts to mitigate jailbreaks while preserving model utility. Therefore, we exclude <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> from the final design of ALMGuard.</p>\n\n",
                "matched_terms": [
                    "model",
                    "advwavep",
                    "sroa",
                    "qwen2audio",
                    "against",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the evaluation results on AIR-Bench-Chat for LLaMA-Omni and Qwen2.5-Omni in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T7\" title=\"Table 7 &#8227; D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. On LLaMA-Omni, our method achieves a RQS of 4.68, outperforming most baselines. Notably, while ICD provides the strongest defense among baselines on this model, it also causes the most significant degradation in utility. It is worth noting that for Qwen2.5-Omni, we set <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>, whereas <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> is used for other models. This adjustment is due to our observation that smaller values of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> fail to effectively defend against semantic-based attacks such as PAIR-Audio. We suspect this is primarily because Qwen2.5-Omni is highly sensitive to such attacks, as evidenced by its noticeably lower robustness on these attacks compared to other models. Nevertheless, given that Qwen2.5-Omni is inherently stronger, it still achieves a RQS of 6.02 even with <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>. Considering the trade-off between defense effectiveness and model utility, we believe this result is acceptable. In practice, we recommend users to adjust <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> flexibly based on specific deployment requirements.</p>\n\n",
                "matched_terms": [
                    "model",
                    "icd",
                    "attacks",
                    "method",
                    "llamaomni",
                    "qwen25omni",
                    "against",
                    "pairaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Model-Specific Traits.</span> We observe that the effectiveness of jailbreak attacks and corresponding defenses is closely tied to the intrinsic characteristics of the target model. For instance, as discussed earlier, Qwen2.5-Omni appears particularly vulnerable to semantic-based attacks, with PAP achieves a high SRoA of 77.9% on this model. Similarly, Lyra-Base exhibits strong sensitivity to prompt context, making it especially susceptible to attacks like ICA.\nIn real-world deployment, it is advisable to consider the model-specific traits when designing comprehensive defense strategies, potentially combining multiple techniques for better protection. Nevertheless, disregarding such model-specific factors, our method consistently demonstrates the universal effectiveness to activate safety-aligned shortcuts across all models, leading to substantial reductions in jailbreak success rates.</p>\n\n",
                "matched_terms": [
                    "model",
                    "attacks",
                    "lyrabase",
                    "jailbreak",
                    "method",
                    "defenses",
                    "sroa",
                    "qwen25omni",
                    "ica"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span>\nDespite its strong overall performance, we observe that our perturbation-based defense has room for improvement against semantic-based attacks. In some cases, ALMGuard underperforms compared to the best-performing baselines. We attribute this to the fact that our acoustic perturbation is mainly optimized to activate ALMs&#8217; inherent acoustic-related safety shortcuts to defend against acoustic-based attacks, but does not explicitly target the semantic intent of adversarial prompts. Future improvements may involve integrating semantic-level and intent-aware objectives during optimization. Additionally, given the plug-and-play nature of our method, it could be integrated with complementary defense techniques to form a more comprehensive defense framework.</p>\n\n",
                "matched_terms": [
                    "method",
                    "attacks",
                    "against",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presents heatmaps of M-GSM sensitivity score rankings for each model in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A6.F5\" title=\"Figure 5 &#8227; F.1 Visualization of M-GSM &#8227; Appendix F Visualizations and Examples &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The results show a strong similarity across the M-GSM masks for the four models, with their top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A6.SS1.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> highest-ranked bins showing significant overlap. This commonality suggests, on one hand, that our method is generalizable across different models. On the other hand, it also reveals that the inherent latent safety shortcuts within these diverse ALMs may share considerable similarities, thus allowing them to be activated in a consistent manner, potentially by the same universal perturbations.</p>\n\n",
                "matched_terms": [
                    "method",
                    "alms",
                    "model",
                    "four"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The following cases illustrate successful defenses achieved by ALMGuard.</p>\n\n",
                "matched_terms": [
                    "almguard",
                    "defenses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Justification: Due to the high computational cost of evaluating multiple ALMs across numerous attacks, results are based on single runs.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "alms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "defenses"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio–Language Models",
        "caption": "Table 2: Model utility on benign tasks. Results on LibriSpeech and AIR-Bench-Chat indicate that our method preserves benign-task performance while outperforming most baseline defenses.",
        "body": "Defense\n\nQwen2-Audio\n\n\nLyra-Base\n\n\n\n\nWER ↓\\downarrow\n\n\nRQS ↑\\uparrow\n\n\nWER ↓\\downarrow\n\n\nRQS ↑\\uparrow\n\n\n\n\n\nNone\n6.85%\n6.25\n9.03%\n2.81\n\n\nGaussian Noise\n12.14%\n5.65\n10.99%\n2.86\n\n\nLocal Smoothing\n8.72%\n5.55\n9.23%\n2.81\n\n\nDownsampling\n7.85%\n5.85\n9.10%\n2.83\n\n\nSelf-Reminder\n33.12%\n5.64\n9.23%\n2.91\n\n\nICD\n15.83%\n6.16\n9.18%\n2.82\n\n\n\n\\rowcolorgray!20\nALMGuard\n\n8.70%\n5.69\n7.87%\n2.96",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Defense</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" colspan=\"2\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Qwen2</span><span class=\"ltx_text\" style=\"font-size:80%;\">-</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Audio</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\" colspan=\"2\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Lyra</span><span class=\"ltx_text\" style=\"font-size:80%;\">-</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Base</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">WER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">RQS </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">WER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">RQS </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">None</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">6.85%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">6.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.03%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.81</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">Gaussian Noise</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">12.14%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.65</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.99%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.86</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">Local Smoothing</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.72%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.55</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.23%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.81</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">Downsampling</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.85%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.85</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.10%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.83</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">Self-Reminder</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">33.12%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.64</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.23%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.91</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">ICD</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">15.83%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.18%</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.82</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span><span class=\"ltx_text\" style=\"font-size:80%;\">gray!20\n</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">ALMGuard</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.70%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">7.87%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">2.96</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "airbenchchat",
            "tasks",
            "icd",
            "librispeech",
            "↓downarrow",
            "rqs",
            "benigntask",
            "outperforming",
            "our",
            "benign",
            "selfreminder",
            "utility",
            "most",
            "preserves",
            "lyrabase",
            "gaussian",
            "smoothing",
            "baseline",
            "defenses",
            "indicate",
            "wer",
            "local",
            "qwen2audio",
            "results",
            "none",
            "noise",
            "model",
            "defense",
            "rowcolorgray20",
            "performance",
            "↑uparrow",
            "downsampling",
            "almguard",
            "method",
            "while"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T2\" title=\"Table 2 &#8227; 5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the performance of our method on two benign benchmarks. On Qwen2-Audio, ALMGuard causes only a slight degradation in performance, increasing the WER on LibriSpeech by 1.85% and decreasing the AIR-Bench-Chat score by 0.56, which we regard as negligible. In contrast, both Self-Reminder and ICD significantly impair ASR performance, increasing the WER by 26.27% and 8.98% respectively, indicating that both baselines considerably disrupt the model&#8217;s understanding of speech semantics. We hypothesize that this is due to Qwen2-Audio being highly sensitive to system prompts, where the presence of Self-Reminder and ICD causes the model to generate refusal responses even for normal ASR inputs. However, on AIR-Bench-Chat, where task instructions are more diverse, both baselines return to relatively normal performance.\nNotably, ALMGuard even improves model performance on Lyra-Base, reducing WER by 1.16% and increasing the AIR-Bench-Chat RQS by 0.15, outperforming all baselines and even the original model without defense.\nFor completeness, we also report results on LLaMA-Omni and Qwen2.5-Omni in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS5\" title=\"D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.5</span></a>.\nOverall, our method demonstrates minimal impact on model utility, suggesting that it can be reliably deployed in real-world ALM systems and significantly enhances practical usability.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats.\nTo address this issue, we propose <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, the first defense framework tailored to ALMs.\nBased on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time.\nTo better sift out effective triggers while preserving the model&#8217;s utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding.\nBoth theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, ALMGuard reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/WeifeiJin/ALMGuard\" title=\"\">https://github.com/WeifeiJin/ALMGuard</a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "utility",
                    "tasks",
                    "defense",
                    "method",
                    "almguard",
                    "defenses",
                    "while",
                    "results",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Why ALM-specific defense?</span>\nRecent research proposing jailbreak attacks tailored to ALMs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite> has substantiated that the integration of audio modality introduces distinct and previously unexplored threats.\nWe observe that existing defense methods transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a></cite> are largely ineffective in mitigating these ALM-specific threats.\nThis can be attributed to a lack of consideration of the behavioral diversity and inherent complexity of ALMs, as well as an insufficient adaptation to the distinct characteristics of the audio modality. A similar limitation also exists on the attack side&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "defense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our stance.</span>\nThe limitations of existing defenses against ALM-specific jailbreaks motivate a novel approach. We hypothesize that well-aligned ALMs inherently possess <em class=\"ltx_emph ltx_font_italic\">safety shortcuts</em>, which are latent pathways or input sensitivities that, if correctly triggered, can steer models towards safer behavior and mitigate jailbreaks. These differ from explicit safety alignments, representing intrinsic model properties that can be leveraged for defense.\nThe primary challenge is to activate these safety shortcuts efficiently and harmlessly at inference time. We suggest that this can be achieved by applying a lightweight, universal acoustic perturbation to the input, referred to as the <span class=\"ltx_text ltx_font_bold\">Shortcut Activation Perturbation (SAP)</span>. SAP is designed to engage these safety-conducive pathways without requiring any model retraining. However, to prevent such a perturbation from degrading benign task performance, its application must be precisely targeted.\nTo this end, we introduce the <span class=\"ltx_text ltx_font_bold\">Mel-Gradient Sparse Mask (M-GSM)</span>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, M-GSM identifies a sparse set of Mel-frequency bins that are highly influential for jailbreak mitigation yet largely inconsequential for benign speech understanding, as measured by automatic speech recognition (ASR) tasks. Our framework, <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, then synergistically employs M-GSM to guide the application of the universal SAP only to Mel bins that are security-critical yet benign-insensitive.\nThis targeted strategy allows ALMGuard to effectively activate safety shortcuts for robust defense while preserving model utility (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, performance on benign inputs).</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "utility",
                    "tasks",
                    "defense",
                    "almguard",
                    "defenses",
                    "while",
                    "performance",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and results.</span>\nWe evaluate the proposed method across four state-of-the-art (SOTA) ALMs and six representative jailbreak attacks. On Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a></cite>, ALMGuard reduces the success rate of the two most recent ALM-specific attacks, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> and Gupta <span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite>, to 3.1% and 0.5% respectively, outperforming all baselines and establishing itself as the new SOTA defense.\nMoreover, ALMGuard generalizes effectively to unseen attacks. As a lightweight defense framework, it enables near zero-cost deployment with negligible inference overhead. Evaluations on two benign benchmarks further confirm that ALMGuard does not noticeably degrade model utility. In addition, it achieves strong robustness against adaptive attacks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "most",
                    "defense",
                    "method",
                    "almguard",
                    "outperforming",
                    "qwen2audio",
                    "results",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce the concept of inherent safety shortcuts in ALMs and propose ALMGuard as the first comprehensive and principled framework to systematically discover and activate these latent pathways for robust and generalizable jailbreak defense.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core technical innovation within ALMGuard involves a universal SAP which is precisely guided by our M-GSM to engage these safety shortcuts, targeting sparse acoustic regions for maximal defense effectiveness with minimal utility impact.</p>\n\n",
                "matched_terms": [
                    "almguard",
                    "defense",
                    "utility",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through extensive evaluations, complemented by theoretical analyses, we show that ALMGuard achieves SOTA defense against six jailbreaks on four SOTA ALMs, with strong generalization, high benign-task utility, and negligible inference overhead.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "benigntask",
                    "almguard",
                    "utility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Shortcut learning.</span>\nDeep neural networks (DNNs) are known to learn &#8220;shortcut&#8221; features from data, which correlate with training labels but may not align with the designer&#8217;s intent or generalize well&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib13\" title=\"\">geirhos2020shortcut </a></cite>. These shortcuts can be harnessed for both beneficial and harmful purposes.\nOn the one hand, they may improve robustness, as demonstrated by unadversarial perturbations in computer vision&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib39\" title=\"\">salman2021unadversarial </a></cite>,\nand can even be deliberately leveraged for defensive purposes, such as constructing safety-aligned or unlearnable features&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib48\" title=\"\">wu2023one </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib44\" title=\"\">wang2025provably </a></cite>.\nOn the other hand, they can be exploited for malicious objectives, such as in backdoor attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib14\" title=\"\">gu2017badnets </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib26\" title=\"\">li2022backdoor </a></cite>,\npoisoning attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib5\" title=\"\">biggio2012poisoning </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib58\" title=\"\">zhao2025data </a></cite>,\nand adversarial examples&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib19\" title=\"\">ilyas2019adversarial </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib41\" title=\"\">szegedy2013intriguing </a></cite>.\nThe core hypothesis of this work is that similar shortcut mechanisms may naturally exist or can be revealed within well-trained ALMs, specifically those that align with safety objectives, which we term safety-aligned shortcuts. Our goal is to identify and leverage these beneficial shortcuts for defensive purposes. To this end, we propose a method to discover such universal safety-aligned shortcuts within the acoustic input space. We then activate these shortcuts at inference time via a carefully crafted audio perturbation, which serves as a lightweight and effective safeguard. Let <math alttext=\"\\mathcal{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><annotation encoding=\"application/x-tex\">\\mathcal{L}</annotation></semantics></math> denote the training loss, our objective is to optimize a shortcut activation perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> that increases the model&#8217;s tendency to produce safe outputs <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math> when given any malicious Mel-spectrogram <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "method",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Jailbreaks and defenses.</span>\nJailbreaking was initially introduced in the context of text-based LLMs, where attackers craft malicious prompts to bypass the model&#8217;s built-in safety mechanisms and induce it to generate harmful content. Existing jailbreak techniques for LLMs can be broadly categorized into two types: suffix-based and semantic-based. Suffix-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib60\" title=\"\">zou2023universal </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib27\" title=\"\">liao2024amplegcg </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib3\" title=\"\">basani2024gasp </a></cite> append an adversarial suffix after a harmful query, while semantic-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib29\" title=\"\">liu2023autodan </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib6\" title=\"\">chao2023jailbreaking </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib31\" title=\"\">mehrotra2024tree </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib57\" title=\"\">zeng2024johnny </a></cite> manipulate the prompt content using strategies such as persuasion or logical traps to elicit the desired malicious response.\nIn the context of ALMs, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> is a suffix-based attack that appends an adversarial noise segment, while Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite> explore prefix and perturbation formats. Modern foundation models are typically enhanced with preference-based alignment (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, reinforcement learning from human feedback (RLHF)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib7\" title=\"\">christiano2017deep </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib32\" title=\"\">ouyang2022training </a></cite> and direct preference optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib37\" title=\"\">rafailov2023direct </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib2\" title=\"\">amini2024direct </a></cite>) where human judgments guide model fine-tuning. As jailbreak attacks continue to emerge, corresponding defense mechanisms have been proposed, including input-level detection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib1\" title=\"\">alon2023detecting </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib38\" title=\"\">robey2023smoothllm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib20\" title=\"\">inan2023llama </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib34\" title=\"\">phute2023llm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib50\" title=\"\">xie2024gradsafe </a></cite> and mitigation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib21\" title=\"\">jain2023baseline </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib54\" title=\"\">xu2024safedecoding </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, and output-level intervention&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib45\" title=\"\">wang2024selfdefend </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib35\" title=\"\">qian2024hsf </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib46\" title=\"\">wang2025vulnerability </a></cite>.\nHowever, defenses tailored to ALMs remain underexplored. In this paper, we address this gap by proposing a dedicated framework that targets their unique vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "model",
                    "defense",
                    "while",
                    "defenses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our overall objective is to identify an acoustic signal, SAP, that can effectively activate the model&#8217;s inherent safety shortcuts to mitigate jailbreaks, without significantly degrading the model&#8217;s performance on benign inputs. We formulate this as an optimization problem aimed at making the model output safe responses to jailbreaking audio when subjected to this perturbation.\nSpecifically, given an ALM <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math>, a set of malicious instructions <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>, and a set of jailbreak algorithms <math alttext=\"\\mathcal{A}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{A}^{\\text{jb}}</annotation></semantics></math>, our goal is formulated as:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "benign",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{X}^{\\text{bg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>bg</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{bg}}</annotation></semantics></math> refers to benign task prompts,\n<math alttext=\"M(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M(\\cdot)</annotation></semantics></math> is the Mel-spectrogram transformation, and <math alttext=\"\\text{Err}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mtext>Err</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{Err}(\\cdot,\\cdot)</annotation></semantics></math> measures the prediction difference between the perturbed and original inputs. Note that we choose to apply perturbations on the Mel-spectrogram rather than the raw waveform, because we find that perturbing the Mel-spectrogram leads to less degradation in model utility (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, utility with clean user inputs) under the same optimization settings, with details provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS5\" title=\"D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.5</span></a>. The safety loss <math alttext=\"\\mathcal{L}_{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>safe</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{safe}}</annotation></semantics></math> can be instantiated as the cross-entropy loss between the model output and the safe target sequence <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> denotes the length of the token sequence. Given the consistent refusal behavior observed across different jailbreak prompts, we assign the same <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math> to all inputs. We believe that using a unified target also facilitates the generalizability of the SAP. The constraint bounded by <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m11\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) ensures that the perturbation does not cause significant degradation in the model&#8217;s utility. The <math alttext=\"\\ell_{\\infty}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m12\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mi mathvariant=\"normal\">&#8734;</mi></msub><annotation encoding=\"application/x-tex\">\\ell_{\\infty}</annotation></semantics></math> constraint bounded by <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m13\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> limits the perturbation magnitude to prevent excessive distortion that could disrupt inherent benign acoustic features in the Mel-spectrogram.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the constraint in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), we initially attempt to introduce a loss function based on benign tasks to guide the direction of the perturbation and prevent it from affecting model utility. Specifically, we use Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib36\" title=\"\">radford2023robust </a></cite> to transcribe the input examples and compute the cross-entropy loss between the transcription result and the ground-truth text <math alttext=\"y^{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>ASR</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{ASR}}</annotation></semantics></math>, which we denote as <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "benign",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, experimental results show that this auxiliary loss fails to effectively reduce the impact of perturbations on model utility. Detailed results can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS4\" title=\"D.4 Ablation on &#8466;_&quot;ASR&quot; &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate this limitation, we conduct a deeper analysis and observe that only a small subset of frequency bands contribute meaningfully to jailbreak mitigation, while most Mel bins are insensitive to the defense objective, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Motivated by this observation, we propose to apply perturbations only to the most critical frequency bands and filter out the rest. This greatly reduces the perturbation region and thus minimizes its impact on model utility.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "most",
                    "defense",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given our goal of activating safety shortcuts that effectively mitigate jailbreaks while minimizing the impact on model performance in benign tasks, we aim to identify frequency bands that are highly sensitive to jailbreak mitigation objectives but relatively insensitive to ASR tasks. Based on this intuition, we define a combined sensitivity score as follows:</p>\n\n",
                "matched_terms": [
                    "model",
                    "tasks",
                    "benign",
                    "while",
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Instead of explicitly enforcing the constraint in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), we rely on M-GSM to indirectly preserve the model&#8217;s performance on benign inputs. We optimize the perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> through iterative updates using the Projected Gradient Descent (PGD)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib30\" title=\"\">madry2017towards </a></cite> algorithm:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\eta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#951;</mi><annotation encoding=\"application/x-tex\">\\eta</annotation></semantics></math> is the step size, and <math alttext=\"\\Pi_{|\\cdot|_{\\infty}\\leq\\epsilon}(\\cdot)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">&#928;</mi><mrow><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><msub><mo fence=\"false\" stretchy=\"false\">|</mo><mi mathvariant=\"normal\">&#8734;</mi></msub><mo lspace=\"0.167em\">&#8804;</mo><mi>&#1013;</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\Pi_{|\\cdot|_{\\infty}\\leq\\epsilon}(\\cdot)</annotation></semantics></math> denotes the projection onto the <math alttext=\"\\ell_{\\infty}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mi mathvariant=\"normal\">&#8734;</mi></msub><annotation encoding=\"application/x-tex\">\\ell_{\\infty}</annotation></semantics></math> ball with radius <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m6\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math>. The mask <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m7\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> ensures that the perturbation is only applied to the most sensitive frequency bands as determined by M-GSM. This allows ALMGuard to concentrate its perturbation budget on a small but effective subset of the input space.</p>\n\n",
                "matched_terms": [
                    "almguard",
                    "most"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we theoretically analyze how ALMGuard limits its impact on ALM performance over benign examples, thereby satisfying the constraint Equation&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). We begin by bounding the perturbation-induced change in audio embeddings, and subsequently leverage this bound to derive the performance degradation bound of ALMs on benign tasks.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "almguard",
                    "benign",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Given a benign input <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and its Mel-spectrogram <math alttext=\"M(x)\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M(x)</annotation></semantics></math>, assume that for the perturbation masked by M-GSM, which satisfies <math alttext=\"\\|m\\odot\\delta\\|_{\\infty}\\leq\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8857;</mo><mi>&#948;</mi></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mi mathvariant=\"normal\">&#8734;</mi></msub><mo>&#8804;</mo><mi>&#1013;</mi></mrow><annotation encoding=\"application/x-tex\">\\|m\\odot\\delta\\|_{\\infty}\\leq\\epsilon</annotation></semantics></math>, the encoder <math alttext=\"f_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext class=\"ltx_mathvariant_italic\">enc</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{enc}}</annotation></semantics></math> exhibits bounded local sensitivity. Specifically, there exists a valid local Lipschitz constant <math alttext=\"L_{\\text{enc}}\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mtext class=\"ltx_mathvariant_italic\">enc</mtext></msub><mo>&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">L_{\\text{enc}}\\geq 0</annotation></semantics></math> such that the change in output embedding <math alttext=\"\\Delta\\mathbf{E}_{a}=f_{\\text{enc}}(M(x)+m\\odot\\delta)-f_{\\text{enc}}(M(x))\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow><mo>=</mo><mrow><mrow><msub><mi>f</mi><mtext class=\"ltx_mathvariant_italic\">enc</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8857;</mo><mi>&#948;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><msub><mi>f</mi><mtext class=\"ltx_mathvariant_italic\">enc</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathbf{E}_{a}=f_{\\text{enc}}(M(x)+m\\odot\\delta)-f_{\\text{enc}}(M(x))</annotation></semantics></math> satisfies:</span>\n</p>\n\n",
                "matched_terms": [
                    "local",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This assumption is justified by our perturbation selection mechanism, which ensures the local stability of the encoder around benign samples. A detailed explanation is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A3.SS2\" title=\"C.2 Proof of Benign Task Deviation Bound &#8227; Appendix C Proofs &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">C.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "local",
                    "our",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Given a benign-task embedding <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>, under Assumption&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption1\" title=\"Assumption 1 (Local Sensitivity Bound of Audio Encoder). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> with small perturbation <math alttext=\"\\Delta\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathbf{E}_{a}</annotation></semantics></math>, we assume that the loss function of downstream ALM tasks (e.g., speech instruction following), denoted as <math alttext=\"\\mathcal{L}_{\\text{ALM}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_italic\">ALM</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ALM}}</annotation></semantics></math>, has a bounded gradient norm with respect to <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>. That is, there exists a constant <math alttext=\"G_{\\max}\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>max</mi></msub><mo>&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">G_{\\max}\\geq 0</annotation></semantics></math> such that:</span>\n</p>\n\n",
                "matched_terms": [
                    "benigntask",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Under Assumptions&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption1\" title=\"Assumption 1 (Local Sensitivity Bound of Audio Encoder). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption2\" title=\"Assumption 2 (Local Sensitivity Bound of LLM Backbone). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the impact of ALMGuard on benign ALM tasks (measured by the loss difference <math alttext=\"\\Delta\\mathcal{L}_{\\text{ALM}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmproposition1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_italic\">ALM</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathcal{L}_{\\text{ALM}}</annotation></semantics></math>) is bounded:</span>\n</p>\n\n",
                "matched_terms": [
                    "benign",
                    "almguard",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This bound shows that the performance impact on benign tasks is proportional to the tunable hyperparameters, <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> and <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (through <math alttext=\"d_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">d_{k}</annotation></semantics></math>), and two stability-related constants, <math alttext=\"L_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>enc</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{enc}}</annotation></semantics></math> and <math alttext=\"G_{\\max}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>max</mi></msub><annotation encoding=\"application/x-tex\">G_{\\max}</annotation></semantics></math>. The design of M-GSM aims to apply perturbations to regions where these stability factors collectively result in minimal adverse impact on benign tasks. This theoretical upper bound supports the claim that ALMGuard can preserve model performance on benign tasks, which aligns with our empirical results shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.SS3\" title=\"5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "tasks",
                    "almguard",
                    "performance",
                    "results",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset and models.</span> In line with AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite>, we adopt AdvBench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib60\" title=\"\">zou2023universal </a></cite>, a benchmark widely used in text-based jailbreak research, which contains a total of 520 prompts. These prompts are converted into audio using OpenAI&#8217;s text-to-speech (TTS) API to construct <span class=\"ltx_text ltx_font_bold\">AdvBench</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span>, comprising 520 audio queries. We evaluate four state-of-the-art audio-language models: <span class=\"ltx_text ltx_font_bold\">Qwen2</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a></cite>, <span class=\"ltx_text ltx_font_bold\">LLaMA</span>-<span class=\"ltx_text ltx_font_bold\">Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib10\" title=\"\">fang2024llama </a></cite>, <span class=\"ltx_text ltx_font_bold\">Lyra</span>-<span class=\"ltx_text ltx_font_bold\">Base</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib59\" title=\"\">zhong2024lyra </a></cite>, and <span class=\"ltx_text ltx_font_bold\">Qwen2.5</span>-<span class=\"ltx_text ltx_font_bold\">Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib53\" title=\"\">xu2025qwen2 </a></cite>. All models are capable of accepting audio as input and producing either textual responses.</p>\n\n",
                "matched_terms": [
                    "lyrabase",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span>\nFor the defense methods, due to the lack of dedicated defenses targeting jailbreaks in ALMs, we explore two directions of transfer. <span class=\"ltx_text ltx_font_bold\">Type I:</span> From the domain of traditional audio adversarial defenses, we consider three widely adopted techniques&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib12\" title=\"\">ge2023advddos </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib11\" title=\"\">fang2024zero </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib23\" title=\"\">jin2025whispering </a></cite>: (<span class=\"ltx_text ltx_font_italic\">i</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Gaussian Noise</span>, (<span class=\"ltx_text ltx_font_italic\">ii</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Local Smoothing</span>, and (<span class=\"ltx_text ltx_font_italic\">iii</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Downsampling</span>. <span class=\"ltx_text ltx_font_bold\">Type II:</span> From the domain of text-based jailbreak defense, we implement two representative methods: <span class=\"ltx_text ltx_font_bold\">Self-Reminder</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a></cite> and <span class=\"ltx_text ltx_font_bold\">In-Context Defense (ICD)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>. Details of these defense techniques are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS3\" title=\"D.3 Baseline Setup &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "selfreminder",
                    "icd",
                    "gaussian",
                    "smoothing",
                    "defense",
                    "defenses",
                    "local",
                    "downsampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics.</span>\nWe evaluate the effectiveness of the aforementioned jailbreak attacks and defenses using the <span class=\"ltx_text ltx_font_bold\">Success Rate of Attack (SRoA)</span>. Following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite>, we adopt a well-tuned LLM judge model from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib49\" title=\"\">xie2024sorry </a></cite> to determine whether a jailbreak attempt is successful.\nTo assess model utility, we consider two benchmarks. First, we sample 500 audio clips from the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib33\" title=\"\">panayotov2015librispeech </a></cite>, a standard ASR task, to measure the model&#8217;s basic speech understanding capability, using <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span> as the evaluation metric.\nIn addition, we employ 800 speech samples from AIR-Bench-Chat&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib55\" title=\"\">yang2024air </a></cite> to further evaluate the model&#8217;s audio-to-text interaction performance. For this evaluation, each response is assigned a <span class=\"ltx_text ltx_font_bold\">Response Quality Score (RQS)</span> on a 1-10 scale by DeepSeek-V3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib28\" title=\"\">liu2024deepseek </a></cite>, where higher RQS values indicate stronger model performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "airbenchchat",
                    "librispeech",
                    "wer",
                    "rqs",
                    "defenses",
                    "indicate",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance on seen attacks.</span>\nWe first evaluate the performance of ALMGuard on three seen attacks used during the optimization of the perturbation: AdvWave, AdvWave-P, and PAIR-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method significantly outperforms all baselines on AdvWave and AdvWave-P, reducing the average SRoA across four ALMs from 53.5% and 68.5% to 4.6% and 7.8%, respectively. This indicates that ALMGuard exhibits strong robustness against acoustic-based attacks. On PAIR-Audio, a representative semantic-based attack, ALMGuard reduces the average SRoA to 26.3%, achieving comparable performance to Self-Reminder and ICD. Notably, ALMGuard consistently achieves a lower SRoA than all baselines against AdvWave-P on every model, and even reduces it to 0 on Qwen2.5-Omni, making the model completely robust - a result that no existing defense has achieved.</p>\n\n",
                "matched_terms": [
                    "selfreminder",
                    "model",
                    "icd",
                    "defense",
                    "method",
                    "almguard",
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferability to unseen attacks.</span>\nWe evaluate the transferability of ALMGuard on three unseen attacks: Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>, ICA, and PAP-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ALMGuard significantly reduces the average SRoA by 27.4%, 7.4%, and 5.2%, respectively. In particular, the average SRoA on Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;is reduced to only 1.9%, which is the lowest among all attacks. Given that AdvWave and Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;represent the current SOTA ALM-specific jailbreak attacks, we believe that ALMGuard achieves strong robustness against this class of threats.\nFor baselines, we observe that Type I defenses tend to perform better against acoustic-based attacks. In contrast, Type II defenses show significantly better performance on semantic-based attacks.\nThis observation suggests that no existing defense can dominate across all types of attacks.\nIn comparison, ALMGuard consistently outperforms Type I defenses across all attack categories. Compared to Type II defenses, ALMGuard achieves significantly better results on acoustic-based attacks.\nOn average, ALMGuard reduces the overall SRoA to 14.6%, which represents the current SOTA.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "defenses",
                    "performance",
                    "results",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution of M-GSM.</span>\nIn ALMGuard, we employ M-GSM to ensure that the model&#8217;s utility is not significantly affected. To validate the effectiveness of this key component, we conduct an ablation study on Qwen2-Audio by testing three jailbreak attacks and two benign benchmarks, both with and without M-GSM. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "benign",
                    "qwen2audio",
                    "results",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In terms of defense effectiveness, the results with and without M-GSM show similar performance, both achieving over 50% reduction in average SRoA. However, regarding model utility, we observe a clear distinction. Without M-GSM, the WER on the ASR task increases by 20%, which substantially degrades the model&#8217;s ability to understand speech. In addition, the RQS drops by 1.17. In contrast, with M-GSM enabled, the fluctuations in WER and RQS are limited to within approximately 2% and 0.5, respectively, indicating a negligible impact on utility.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "defense",
                    "rqs",
                    "wer",
                    "performance",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hyperparameter analyses.</span>\nAn important hyperparameter in our method is the value of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>, which determines the number of Mel-frequency bins to which perturbations are applied. This parameter controls the trade-off between defense effectiveness and the impact on model utility. To investigate its influence, we conduct experiments with <math alttext=\"k\\in\\{0,16,48,96,128\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>48</mn><mo>,</mo><mn>96</mn><mo>,</mo><mn>128</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k\\in\\{0,16,48,96,128\\}</annotation></semantics></math>, where <math alttext=\"k=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">k=0</annotation></semantics></math> corresponds to the undefended setting with no perturbation applied, and <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math> represents the case without masking (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, full-band perturbation).\nWe evaluate the defense performance on PAIR-Audio and the benign-task performance on LibriSpeech. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F3\" title=\"Figure 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, increasing <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> leads to a monotonic decrease in SRoA and a monotonic increase in WER. This trend indicates that while a larger <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> improves robustness, it also introduces more distortion to benign inputs. Since our goal is to preserve utility while maximizing robustness, we identify <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m7\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> as a balanced configuration, where SRoA is reduced to 34.9% and WER remains low at 8.70%.</p>\n\n",
                "matched_terms": [
                    "our",
                    "model",
                    "utility",
                    "librispeech",
                    "defense",
                    "method",
                    "benigntask",
                    "while",
                    "wer",
                    "performance",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive attacks.</span>\nTo further demonstrate the superiority of our method, we consider a more practical and challenging setting where the attacker has full knowledge of the defense mechanism, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, a white-box threat model. Under this setting, we evaluate adaptive AdvWave attacks on LLaMA-Omni by optimizing the adversarial suffix in the presence of each of the six defense methods.\nAs an example, when attacking ALMGuard, the attacker adds our well-trained SAP to the input at each iteration during AdvWave optimization.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, our method still achieves the best defense performance among all methods, reducing the SRoA by 11.9% compared to the original attack, even under this strongest threat model. In contrast, all baseline defenses yield an SRoA above 70% under adaptive attacks.\nInterestingly, for traditional audio defenses such as local smoothing, incorporating the corresponding transformations into the optimization process actually increases SRoA. We hypothesize that this is because these operations act similarly to data augmentation, which improves the robustness of the adversarial suffix and thus enhances the attack strength.\nIn summary, our method demonstrates the strongest resistance to adaptive attacks among all evaluated defenses.</p>\n\n",
                "matched_terms": [
                    "model",
                    "smoothing",
                    "defense",
                    "baseline",
                    "almguard",
                    "defenses",
                    "method",
                    "local",
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce ALMGuard, a novel framework that pioneers activating inherent safety shortcuts in ALMs via universal SAPs. Our M-GSM technique precisely guides these SAPs to critical frequency regions, enabling robust jailbreak mitigation while preserving model utility.\nEvaluations across six attack methods and four SOTA ALMs show that ALMGuard achieves overall better performance compared to existing defenses, while keeping its impact on model utility well-controlled. This offers a new perspective on enhancing robustness for multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "almguard",
                    "defenses",
                    "while",
                    "performance",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the pseudocode of ALMGuard in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#algorithm1\" title=\"In Appendix B Pseudocode &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The algorithm begins by constructing the training dataset <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> through the application of a jailbreak algorithm set <math alttext=\"\\mathcal{A}_{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\text{jb}}</annotation></semantics></math> to a curated jailbreak prompt set <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>. Based on this dataset, we compute the average sensitivity scores to derive the M-GSM <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, which identifies Mel-frequency bins that are critical for jailbreak mitigation yet minimally influential on benign performance. The universal perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> is initialized from a Gaussian distribution <math alttext=\"\\mathcal{N}(0,\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m6\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>&#963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,\\sigma)</annotation></semantics></math>, and iteratively optimized over the dataset to minimize the empirical safety risk.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "almguard",
                    "benign",
                    "gaussian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rationality analyses of Assumption&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption1\" title=\"Assumption 1 (Local Sensitivity Bound of Audio Encoder). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</span>\nThe core idea of M-GSM is to identify and select those Mel&#8208;frequency bins <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> for which the gradient of the ASR task loss <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> is small (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, <math alttext=\"g^{a}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>f</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">g^{a}_{f}</annotation></semantics></math> is low), while the gradient of the jailbreak mitigation objective is large (i.e. <math alttext=\"g^{s}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>f</mi><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">g^{s}_{f}</annotation></semantics></math> is high). In other words, the regions where M-GSM applies perturbations are chosen to be those that minimally affect benign speech understanding, as measured by ASR.</p>\n\n",
                "matched_terms": [
                    "while",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rationality analyses of Assumption&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption2\" title=\"Assumption 2 (Local Sensitivity Bound of LLM Backbone). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</span>\nWe focus on how the LLM backbone <math alttext=\"f_{\\mathrm{LLM}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>LLM</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{LLM}}</annotation></semantics></math> responds to the small audio embedding perturbations <math alttext=\"\\Delta\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathbf{E}_{a}</annotation></semantics></math> that have already been &#8220;filtered&#8221; and &#8220;restricted&#8221; by the encoder <math alttext=\"f_{\\mathrm{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>enc</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{enc}}</annotation></semantics></math> and M-GSM. Although the exact global Lipschitz constant of an LLM is infeasible to compute, it is reasonable to assume that around its benign operating points (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, within the local neighborhood of a clean embedding <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m4\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>), the model exhibits relative smoothness and stability.</p>\n\n",
                "matched_terms": [
                    "local",
                    "model",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, LLMs are trained via self&#8208;supervised learning on massive text corpora, acquiring rich semantic representations and fluent generation capabilities. To achieve such generality and maintain good out&#8208;of&#8208;domain performance, the model must tolerate small perturbations in its inputs, including those in <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math> that carry no core semantic content. If the loss function of the LLM reacts with large gradient norms (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, a very large <math alttext=\"G_{\\max}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p6.m2\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>max</mi></msub><annotation encoding=\"application/x-tex\">G_{\\max}</annotation></semantics></math>) to any infinitesimal change in its input embedding, then stable training will become difficult and the model will be overly sensitive to noise, thereby degrading its comprehension and generation quality.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gupta <span class=\"ltx_text ltx_font_italic\">et al</span>.</span>\nA white-box jailbreak method that evaluates various attack forms and stealth enhancement techniques. We adopt the most effective universal prefix proposed in their work. It is trained on 100 randomly selected samples from AdvBench-Audio and tested on the remaining 420.</p>\n\n",
                "matched_terms": [
                    "method",
                    "most"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Attack.</span>\nA black-box jailbreak strategy that prepends a few successful attack demonstrations to the input prompt to induce in-context learning. Following the authors&#8217; recommended configuration, we use 10-shot prompts for LLaMA-Omni, Lyra-Base, and Qwen2.5-Omni. Due to context length limitations, we use 5-shot prompts for Qwen2-Audio.</p>\n\n",
                "matched_terms": [
                    "lyrabase",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gaussian Noise.</span>\nWe add Gaussian noise with a standard deviation of 0.01 to the input audio. This simple defense can effectively remove certain brittle adversarial perturbations, particularly suffix-based or prefix-based attacks.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "defense",
                    "gaussian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Smoothing.</span>\nWe apply a moving average filter to smooth the waveform. Each audio sample is replaced by the average of its neighboring values within a window of size <math alttext=\"h=2\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I2.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">h=2</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "local",
                    "smoothing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Downsampling.</span>\nWe first downsample the original audio from 16kHz to a lower rate (14kHz), and then upsample it back to 16kHz. This operation distorts adversarial patterns while preserving most benign content.</p>\n\n",
                "matched_terms": [
                    "downsampling",
                    "while",
                    "benign",
                    "most"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Defense.</span>\nProposed in the same work as ICA, this method uses 2-shot in-context learning to guide the model toward safe behavior. We follow the original configuration and apply 2-shot demonstrations in the context prompt.</p>\n\n",
                "matched_terms": [
                    "method",
                    "model",
                    "defense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate whether incorporating <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide the gradient direction is effective, we conduct a preliminary comparison study. Under the setting of <math alttext=\"k=16\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">k=16</annotation></semantics></math>, we evaluated both model utility and defense performance against AdvWave-P on Qwen2-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T6\" title=\"Table 6 &#8227; D.4 Ablation on &#8466;_&quot;ASR&quot; &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, when the M-GSM is applied, the presence or absence of <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> has negligible effect on both metrics. The WER and SRoA differ by only 0.07% and 1.5%, which can be attributed to random variations. In contrast, when M-GSM is removed, using <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide optimization fails to preserve utility: the WER increases by 17.88% compared to the undefended case.\nIn summary, we conclude that <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> does not provide a meaningful benefit in achieving the goal of activating safety shortcuts to mitigate jailbreaks while preserving model utility. Therefore, we exclude <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> from the final design of ALMGuard.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "performance",
                    "defense",
                    "while",
                    "wer",
                    "qwen2audio",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the evaluation results on AIR-Bench-Chat for LLaMA-Omni and Qwen2.5-Omni in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T7\" title=\"Table 7 &#8227; D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. On LLaMA-Omni, our method achieves a RQS of 4.68, outperforming most baselines. Notably, while ICD provides the strongest defense among baselines on this model, it also causes the most significant degradation in utility. It is worth noting that for Qwen2.5-Omni, we set <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>, whereas <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> is used for other models. This adjustment is due to our observation that smaller values of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> fail to effectively defend against semantic-based attacks such as PAIR-Audio. We suspect this is primarily because Qwen2.5-Omni is highly sensitive to such attacks, as evidenced by its noticeably lower robustness on these attacks compared to other models. Nevertheless, given that Qwen2.5-Omni is inherently stronger, it still achieves a RQS of 6.02 even with <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>. Considering the trade-off between defense effectiveness and model utility, we believe this result is acceptable. In practice, we recommend users to adjust <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> flexibly based on specific deployment requirements.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "airbenchchat",
                    "icd",
                    "most",
                    "defense",
                    "rqs",
                    "method",
                    "while",
                    "outperforming",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Model-Specific Traits.</span> We observe that the effectiveness of jailbreak attacks and corresponding defenses is closely tied to the intrinsic characteristics of the target model. For instance, as discussed earlier, Qwen2.5-Omni appears particularly vulnerable to semantic-based attacks, with PAP achieves a high SRoA of 77.9% on this model. Similarly, Lyra-Base exhibits strong sensitivity to prompt context, making it especially susceptible to attacks like ICA.\nIn real-world deployment, it is advisable to consider the model-specific traits when designing comprehensive defense strategies, potentially combining multiple techniques for better protection. Nevertheless, disregarding such model-specific factors, our method consistently demonstrates the universal effectiveness to activate safety-aligned shortcuts across all models, leading to substantial reductions in jailbreak success rates.</p>\n\n",
                "matched_terms": [
                    "model",
                    "lyrabase",
                    "defense",
                    "method",
                    "defenses",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span>\nDespite its strong overall performance, we observe that our perturbation-based defense has room for improvement against semantic-based attacks. In some cases, ALMGuard underperforms compared to the best-performing baselines. We attribute this to the fact that our acoustic perturbation is mainly optimized to activate ALMs&#8217; inherent acoustic-related safety shortcuts to defend against acoustic-based attacks, but does not explicitly target the semantic intent of adversarial prompts. Future improvements may involve integrating semantic-level and intent-aware objectives during optimization. Additionally, given the plug-and-play nature of our method, it could be integrated with complementary defense techniques to form a more comprehensive defense framework.</p>\n\n",
                "matched_terms": [
                    "our",
                    "defense",
                    "method",
                    "performance",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presents heatmaps of M-GSM sensitivity score rankings for each model in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A6.F5\" title=\"Figure 5 &#8227; F.1 Visualization of M-GSM &#8227; Appendix F Visualizations and Examples &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The results show a strong similarity across the M-GSM masks for the four models, with their top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A6.SS1.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> highest-ranked bins showing significant overlap. This commonality suggests, on one hand, that our method is generalizable across different models. On the other hand, it also reveals that the inherent latent safety shortcuts within these diverse ALMs may share considerable similarities, thus allowing them to be activated in a consistent manner, potentially by the same universal perturbations.</p>\n\n",
                "matched_terms": [
                    "method",
                    "model",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The following cases illustrate successful defenses achieved by ALMGuard.</p>\n\n",
                "matched_terms": [
                    "almguard",
                    "defenses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.</p>\n\n",
                "matched_terms": [
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Justification: Paper proposes a defense method; no release of new high-risk models or data.</p>\n\n",
                "matched_terms": [
                    "method",
                    "defense"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio–Language Models",
        "caption": "Table 3: Comparison of ALMGuard ’s defense effectiveness and utility with/without M-GSM.",
        "body": "Metric\nNone\nw/o M-GSM\nALMGuard\n\n\nSRoA ↓\\downarrow\nAdvWave\n86.4%\n3.1%\n3.1%\n\n\nAdvWave-P\n80.8%\n12.7%\n11.7%\n\n\nPAIR-Audio\n45.0%\n27.5%\n34.9%\n\n\nAverage\n70.7%\n14.4%\n16.6%\n\n\nBenign\nWER ↓\\downarrow\n\n6.85%\n26.85%\n8.70%\n\n\nRQS ↑\\uparrow\n\n6.25\n5.08\n5.69",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt ltx_border_t\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Metric</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">None</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">w/o M-GSM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ALMGuard</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text ltx_font_bold\">SRoA <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">AdvWave</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">86.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.1%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AdvWave-P</th>\n<td class=\"ltx_td ltx_align_center\">80.8%</td>\n<td class=\"ltx_td ltx_align_center\">12.7%</td>\n<td class=\"ltx_td ltx_align_center\">11.7%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">PAIR-Audio</th>\n<td class=\"ltx_td ltx_align_center\">45.0%</td>\n<td class=\"ltx_td ltx_align_center\">27.5%</td>\n<td class=\"ltx_td ltx_align_center\">34.9%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Average</th>\n<td class=\"ltx_td ltx_align_center\">70.7%</td>\n<td class=\"ltx_td ltx_align_center\">14.4%</td>\n<td class=\"ltx_td ltx_align_center\">16.6%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Benign</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">WER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.85%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.85%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">8.70%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\">RQS <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\">6.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\">5.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\">5.69</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "withwithout",
            "↓downarrow",
            "mgsm",
            "advwavep",
            "rqs",
            "benign",
            "utility",
            "advwave",
            "sroa",
            "wer",
            "none",
            "effectiveness",
            "defense",
            "average",
            "↑uparrow",
            "almguard",
            "metric",
            "comparison",
            "pairaudio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution of M-GSM.</span>\nIn ALMGuard, we employ M-GSM to ensure that the model&#8217;s utility is not significantly affected. To validate the effectiveness of this key component, we conduct an ablation study on Qwen2-Audio by testing three jailbreak attacks and two benign benchmarks, both with and without M-GSM. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats.\nTo address this issue, we propose <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, the first defense framework tailored to ALMs.\nBased on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time.\nTo better sift out effective triggers while preserving the model&#8217;s utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding.\nBoth theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, ALMGuard reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/WeifeiJin/ALMGuard\" title=\"\">https://github.com/WeifeiJin/ALMGuard</a>.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "defense",
                    "mgsm",
                    "almguard",
                    "average",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our stance.</span>\nThe limitations of existing defenses against ALM-specific jailbreaks motivate a novel approach. We hypothesize that well-aligned ALMs inherently possess <em class=\"ltx_emph ltx_font_italic\">safety shortcuts</em>, which are latent pathways or input sensitivities that, if correctly triggered, can steer models towards safer behavior and mitigate jailbreaks. These differ from explicit safety alignments, representing intrinsic model properties that can be leveraged for defense.\nThe primary challenge is to activate these safety shortcuts efficiently and harmlessly at inference time. We suggest that this can be achieved by applying a lightweight, universal acoustic perturbation to the input, referred to as the <span class=\"ltx_text ltx_font_bold\">Shortcut Activation Perturbation (SAP)</span>. SAP is designed to engage these safety-conducive pathways without requiring any model retraining. However, to prevent such a perturbation from degrading benign task performance, its application must be precisely targeted.\nTo this end, we introduce the <span class=\"ltx_text ltx_font_bold\">Mel-Gradient Sparse Mask (M-GSM)</span>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, M-GSM identifies a sparse set of Mel-frequency bins that are highly influential for jailbreak mitigation yet largely inconsequential for benign speech understanding, as measured by automatic speech recognition (ASR) tasks. Our framework, <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, then synergistically employs M-GSM to guide the application of the universal SAP only to Mel bins that are security-critical yet benign-insensitive.\nThis targeted strategy allows ALMGuard to effectively activate safety shortcuts for robust defense while preserving model utility (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, performance on benign inputs).</p>\n\n",
                "matched_terms": [
                    "utility",
                    "benign",
                    "defense",
                    "mgsm",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and results.</span>\nWe evaluate the proposed method across four state-of-the-art (SOTA) ALMs and six representative jailbreak attacks. On Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a></cite>, ALMGuard reduces the success rate of the two most recent ALM-specific attacks, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> and Gupta <span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite>, to 3.1% and 0.5% respectively, outperforming all baselines and establishing itself as the new SOTA defense.\nMoreover, ALMGuard generalizes effectively to unseen attacks. As a lightweight defense framework, it enables near zero-cost deployment with negligible inference overhead. Evaluations on two benign benchmarks further confirm that ALMGuard does not noticeably degrade model utility. In addition, it achieves strong robustness against adaptive attacks.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "benign",
                    "defense",
                    "advwave",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce the concept of inherent safety shortcuts in ALMs and propose ALMGuard as the first comprehensive and principled framework to systematically discover and activate these latent pathways for robust and generalizable jailbreak defense.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core technical innovation within ALMGuard involves a universal SAP which is precisely guided by our M-GSM to engage these safety shortcuts, targeting sparse acoustic regions for maximal defense effectiveness with minimal utility impact.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "effectiveness",
                    "defense",
                    "mgsm",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through extensive evaluations, complemented by theoretical analyses, we show that ALMGuard achieves SOTA defense against six jailbreaks on four SOTA ALMs, with strong generalization, high benign-task utility, and negligible inference overhead.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "utility",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Jailbreaks and defenses.</span>\nJailbreaking was initially introduced in the context of text-based LLMs, where attackers craft malicious prompts to bypass the model&#8217;s built-in safety mechanisms and induce it to generate harmful content. Existing jailbreak techniques for LLMs can be broadly categorized into two types: suffix-based and semantic-based. Suffix-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib60\" title=\"\">zou2023universal </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib27\" title=\"\">liao2024amplegcg </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib3\" title=\"\">basani2024gasp </a></cite> append an adversarial suffix after a harmful query, while semantic-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib29\" title=\"\">liu2023autodan </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib6\" title=\"\">chao2023jailbreaking </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib31\" title=\"\">mehrotra2024tree </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib57\" title=\"\">zeng2024johnny </a></cite> manipulate the prompt content using strategies such as persuasion or logical traps to elicit the desired malicious response.\nIn the context of ALMs, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> is a suffix-based attack that appends an adversarial noise segment, while Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite> explore prefix and perturbation formats. Modern foundation models are typically enhanced with preference-based alignment (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, reinforcement learning from human feedback (RLHF)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib7\" title=\"\">christiano2017deep </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib32\" title=\"\">ouyang2022training </a></cite> and direct preference optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib37\" title=\"\">rafailov2023direct </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib2\" title=\"\">amini2024direct </a></cite>) where human judgments guide model fine-tuning. As jailbreak attacks continue to emerge, corresponding defense mechanisms have been proposed, including input-level detection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib1\" title=\"\">alon2023detecting </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib38\" title=\"\">robey2023smoothllm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib20\" title=\"\">inan2023llama </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib34\" title=\"\">phute2023llm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib50\" title=\"\">xie2024gradsafe </a></cite> and mitigation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib21\" title=\"\">jain2023baseline </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib54\" title=\"\">xu2024safedecoding </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, and output-level intervention&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib45\" title=\"\">wang2024selfdefend </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib35\" title=\"\">qian2024hsf </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib46\" title=\"\">wang2025vulnerability </a></cite>.\nHowever, defenses tailored to ALMs remain underexplored. In this paper, we address this gap by proposing a dedicated framework that targets their unique vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "advwave",
                    "defense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{X}^{\\text{bg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>bg</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{bg}}</annotation></semantics></math> refers to benign task prompts,\n<math alttext=\"M(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M(\\cdot)</annotation></semantics></math> is the Mel-spectrogram transformation, and <math alttext=\"\\text{Err}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mtext>Err</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{Err}(\\cdot,\\cdot)</annotation></semantics></math> measures the prediction difference between the perturbed and original inputs. Note that we choose to apply perturbations on the Mel-spectrogram rather than the raw waveform, because we find that perturbing the Mel-spectrogram leads to less degradation in model utility (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, utility with clean user inputs) under the same optimization settings, with details provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS5\" title=\"D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.5</span></a>. The safety loss <math alttext=\"\\mathcal{L}_{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>safe</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{safe}}</annotation></semantics></math> can be instantiated as the cross-entropy loss between the model output and the safe target sequence <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "utility",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> denotes the length of the token sequence. Given the consistent refusal behavior observed across different jailbreak prompts, we assign the same <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math> to all inputs. We believe that using a unified target also facilitates the generalizability of the SAP. The constraint bounded by <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m11\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) ensures that the perturbation does not cause significant degradation in the model&#8217;s utility. The <math alttext=\"\\ell_{\\infty}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m12\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mi mathvariant=\"normal\">&#8734;</mi></msub><annotation encoding=\"application/x-tex\">\\ell_{\\infty}</annotation></semantics></math> constraint bounded by <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m13\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> limits the perturbation magnitude to prevent excessive distortion that could disrupt inherent benign acoustic features in the Mel-spectrogram.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the constraint in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), we initially attempt to introduce a loss function based on benign tasks to guide the direction of the perturbation and prevent it from affecting model utility. Specifically, we use Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib36\" title=\"\">radford2023robust </a></cite> to transcribe the input examples and compute the cross-entropy loss between the transcription result and the ground-truth text <math alttext=\"y^{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>ASR</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{ASR}}</annotation></semantics></math>, which we denote as <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "utility",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate this limitation, we conduct a deeper analysis and observe that only a small subset of frequency bands contribute meaningfully to jailbreak mitigation, while most Mel bins are insensitive to the defense objective, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Motivated by this observation, we propose to apply perturbations only to the most critical frequency bands and filter out the rest. This greatly reduces the perturbation region and thus minimizes its impact on model utility.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "utility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In practice, we compute the gradients over all examples in the training set <math alttext=\"\\mathcal{X}^{jb}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m9\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{jb}</annotation></semantics></math>, average them, and then calculate <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m10\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and the corresponding binary mask <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m11\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>. The result is illustrated in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A6.SS1\" title=\"F.1 Visualization of M-GSM &#8227; Appendix F Visualizations and Examples &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">F.1</span></a>. This fixed mask is then applied uniformly to all examples. On one hand, we observe that the Mel bins with high sensitivity scores tend to be similar across different examples. On the other hand, averaging the gradients across the dataset better captures the overall sensitivity, which helps ensure that the resulting perturbation exhibits broad effectiveness and strong generalization to unseen examples.</p>\n\n",
                "matched_terms": [
                    "average",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By integrating the M-GSM with the perturbation strategy, we formulate the final unified optimization objective for ALMGuard as follows, where <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> denotes the Hadamard (element-wise) product:</p>\n\n",
                "matched_terms": [
                    "almguard",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Instead of explicitly enforcing the constraint in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), we rely on M-GSM to indirectly preserve the model&#8217;s performance on benign inputs. We optimize the perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> through iterative updates using the Projected Gradient Descent (PGD)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib30\" title=\"\">madry2017towards </a></cite> algorithm:</p>\n\n",
                "matched_terms": [
                    "benign",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\eta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#951;</mi><annotation encoding=\"application/x-tex\">\\eta</annotation></semantics></math> is the step size, and <math alttext=\"\\Pi_{|\\cdot|_{\\infty}\\leq\\epsilon}(\\cdot)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">&#928;</mi><mrow><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><msub><mo fence=\"false\" stretchy=\"false\">|</mo><mi mathvariant=\"normal\">&#8734;</mi></msub><mo lspace=\"0.167em\">&#8804;</mo><mi>&#1013;</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\Pi_{|\\cdot|_{\\infty}\\leq\\epsilon}(\\cdot)</annotation></semantics></math> denotes the projection onto the <math alttext=\"\\ell_{\\infty}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mi mathvariant=\"normal\">&#8734;</mi></msub><annotation encoding=\"application/x-tex\">\\ell_{\\infty}</annotation></semantics></math> ball with radius <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m6\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math>. The mask <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m7\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> ensures that the perturbation is only applied to the most sensitive frequency bands as determined by M-GSM. This allows ALMGuard to concentrate its perturbation budget on a small but effective subset of the input space.</p>\n\n",
                "matched_terms": [
                    "almguard",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ALMGuard recap.</span> We first construct the training dataset <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> by applying the jailbreak algorithm set <math alttext=\"\\mathcal{A}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{A}^{\\text{jb}}</annotation></semantics></math> to the jailbreak prompt set <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>. We then compute the average gradient-based sensitivity scores over this dataset to obtain the M-GSM <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>. The perturbation is initialized from a normal distribution <math alttext=\"\\mathcal{N}(0,\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>&#963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,\\sigma)</annotation></semantics></math> and optimized iteratively over <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math>. The pseudocode is presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A2\" title=\"Appendix B Pseudocode &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "average",
                    "almguard",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we theoretically analyze how ALMGuard limits its impact on ALM performance over benign examples, thereby satisfying the constraint Equation&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). We begin by bounding the perturbation-induced change in audio embeddings, and subsequently leverage this bound to derive the performance degradation bound of ALMs on benign tasks.</p>\n\n",
                "matched_terms": [
                    "almguard",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Given a benign input <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and its Mel-spectrogram <math alttext=\"M(x)\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M(x)</annotation></semantics></math>, assume that for the perturbation masked by M-GSM, which satisfies <math alttext=\"\\|m\\odot\\delta\\|_{\\infty}\\leq\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8857;</mo><mi>&#948;</mi></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mi mathvariant=\"normal\">&#8734;</mi></msub><mo>&#8804;</mo><mi>&#1013;</mi></mrow><annotation encoding=\"application/x-tex\">\\|m\\odot\\delta\\|_{\\infty}\\leq\\epsilon</annotation></semantics></math>, the encoder <math alttext=\"f_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext class=\"ltx_mathvariant_italic\">enc</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{enc}}</annotation></semantics></math> exhibits bounded local sensitivity. Specifically, there exists a valid local Lipschitz constant <math alttext=\"L_{\\text{enc}}\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mtext class=\"ltx_mathvariant_italic\">enc</mtext></msub><mo>&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">L_{\\text{enc}}\\geq 0</annotation></semantics></math> such that the change in output embedding <math alttext=\"\\Delta\\mathbf{E}_{a}=f_{\\text{enc}}(M(x)+m\\odot\\delta)-f_{\\text{enc}}(M(x))\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow><mo>=</mo><mrow><mrow><msub><mi>f</mi><mtext class=\"ltx_mathvariant_italic\">enc</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8857;</mo><mi>&#948;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><msub><mi>f</mi><mtext class=\"ltx_mathvariant_italic\">enc</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathbf{E}_{a}=f_{\\text{enc}}(M(x)+m\\odot\\delta)-f_{\\text{enc}}(M(x))</annotation></semantics></math> satisfies:</span>\n</p>\n\n",
                "matched_terms": [
                    "benign",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Under Assumptions&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption1\" title=\"Assumption 1 (Local Sensitivity Bound of Audio Encoder). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption2\" title=\"Assumption 2 (Local Sensitivity Bound of LLM Backbone). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the impact of ALMGuard on benign ALM tasks (measured by the loss difference <math alttext=\"\\Delta\\mathcal{L}_{\\text{ALM}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmproposition1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_italic\">ALM</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathcal{L}_{\\text{ALM}}</annotation></semantics></math>) is bounded:</span>\n</p>\n\n",
                "matched_terms": [
                    "benign",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This bound shows that the performance impact on benign tasks is proportional to the tunable hyperparameters, <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> and <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (through <math alttext=\"d_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">d_{k}</annotation></semantics></math>), and two stability-related constants, <math alttext=\"L_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>enc</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{enc}}</annotation></semantics></math> and <math alttext=\"G_{\\max}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>max</mi></msub><annotation encoding=\"application/x-tex\">G_{\\max}</annotation></semantics></math>. The design of M-GSM aims to apply perturbations to regions where these stability factors collectively result in minimal adverse impact on benign tasks. This theoretical upper bound supports the claim that ALMGuard can preserve model performance on benign tasks, which aligns with our empirical results shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.SS3\" title=\"5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "almguard",
                    "benign",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attacks.</span>\nFor the attack methods, we first adopt two SOTA jailbreak approaches specifically designed for ALMs, namely <span class=\"ltx_text ltx_font_bold\">AdvWave</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> and the method proposed by <span class=\"ltx_text ltx_font_bold\">Gupta <span class=\"ltx_text ltx_font_italic\">et al.</span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite>. In addition, we adapt perturbation-based attacks from the traditional domain of audio adversarial attacks into the AdvWave framework, resulting in a variant named <span class=\"ltx_text ltx_font_bold\">AdvWave</span>-<span class=\"ltx_text ltx_font_bold\">P</span>. We further transfer several representative techniques from the text-based jailbreak literature, including <span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Attack (ICA)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, Prompt Automatic Iterative Refinement (PAIR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib6\" title=\"\">chao2023jailbreaking </a></cite>, and Persuasive Adversarial Prompts (PAP)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib57\" title=\"\">zeng2024johnny </a></cite>. For ICA, we prepend malicious textual demonstrations as context to the audio prompts. For PAIR and PAP, we convert their generated jailbreak texts into audio using OpenAI&#8217;s TTS API, thereby forming the <span class=\"ltx_text ltx_font_bold\">PAIR</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span> and <span class=\"ltx_text ltx_font_bold\">PAP</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span> variants. We classify AdvWave, AdvWave-P, and Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;as <span class=\"ltx_text ltx_font_italic\">acoustic-based</span> attacks, and the remaining three as <span class=\"ltx_text ltx_font_italic\">semantic-based</span> attacks. A detailed description of all attack methods is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS2\" title=\"D.2 Attack Setup &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "advwave",
                    "pairaudio",
                    "advwavep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics.</span>\nWe evaluate the effectiveness of the aforementioned jailbreak attacks and defenses using the <span class=\"ltx_text ltx_font_bold\">Success Rate of Attack (SRoA)</span>. Following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite>, we adopt a well-tuned LLM judge model from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib49\" title=\"\">xie2024sorry </a></cite> to determine whether a jailbreak attempt is successful.\nTo assess model utility, we consider two benchmarks. First, we sample 500 audio clips from the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib33\" title=\"\">panayotov2015librispeech </a></cite>, a standard ASR task, to measure the model&#8217;s basic speech understanding capability, using <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span> as the evaluation metric.\nIn addition, we employ 800 speech samples from AIR-Bench-Chat&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib55\" title=\"\">yang2024air </a></cite> to further evaluate the model&#8217;s audio-to-text interaction performance. For this evaluation, each response is assigned a <span class=\"ltx_text ltx_font_bold\">Response Quality Score (RQS)</span> on a 1-10 scale by DeepSeek-V3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib28\" title=\"\">liu2024deepseek </a></cite>, where higher RQS values indicate stronger model performance.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "metric",
                    "rqs",
                    "sroa",
                    "wer",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ALMGuard&#160;setup.</span>\nDuring the optimization of ALMGuard, we randomly select 50 audio samples from AdvBench-Audio and apply a set of attack algorithms, namely AdvWave, AdvWave-P, and PAIR-Audio, for training-time perturbation optimization. We believe that the selected samples and attack methods are sufficiently representative to enable transferability to unseen examples and attacks. The perturbation duration is set to 30 seconds, consistent with the default input length of Whisper, and the perturbation budget is constrained by <math alttext=\"\\epsilon=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.5</annotation></semantics></math>. We set the value of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> to 48, and provide a detailed analysis in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.SS4\" title=\"5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "advwave",
                    "pairaudio",
                    "almguard",
                    "advwavep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance on seen attacks.</span>\nWe first evaluate the performance of ALMGuard on three seen attacks used during the optimization of the perturbation: AdvWave, AdvWave-P, and PAIR-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method significantly outperforms all baselines on AdvWave and AdvWave-P, reducing the average SRoA across four ALMs from 53.5% and 68.5% to 4.6% and 7.8%, respectively. This indicates that ALMGuard exhibits strong robustness against acoustic-based attacks. On PAIR-Audio, a representative semantic-based attack, ALMGuard reduces the average SRoA to 26.3%, achieving comparable performance to Self-Reminder and ICD. Notably, ALMGuard consistently achieves a lower SRoA than all baselines against AdvWave-P on every model, and even reduces it to 0 on Qwen2.5-Omni, making the model completely robust - a result that no existing defense has achieved.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "advwavep",
                    "advwave",
                    "sroa",
                    "average",
                    "pairaudio",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferability to unseen attacks.</span>\nWe evaluate the transferability of ALMGuard on three unseen attacks: Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>, ICA, and PAP-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ALMGuard significantly reduces the average SRoA by 27.4%, 7.4%, and 5.2%, respectively. In particular, the average SRoA on Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;is reduced to only 1.9%, which is the lowest among all attacks. Given that AdvWave and Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;represent the current SOTA ALM-specific jailbreak attacks, we believe that ALMGuard achieves strong robustness against this class of threats.\nFor baselines, we observe that Type I defenses tend to perform better against acoustic-based attacks. In contrast, Type II defenses show significantly better performance on semantic-based attacks.\nThis observation suggests that no existing defense can dominate across all types of attacks.\nIn comparison, ALMGuard consistently outperforms Type I defenses across all attack categories. Compared to Type II defenses, ALMGuard achieves significantly better results on acoustic-based attacks.\nOn average, ALMGuard reduces the overall SRoA to 14.6%, which represents the current SOTA.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "advwave",
                    "sroa",
                    "average",
                    "comparison",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T2\" title=\"Table 2 &#8227; 5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the performance of our method on two benign benchmarks. On Qwen2-Audio, ALMGuard causes only a slight degradation in performance, increasing the WER on LibriSpeech by 1.85% and decreasing the AIR-Bench-Chat score by 0.56, which we regard as negligible. In contrast, both Self-Reminder and ICD significantly impair ASR performance, increasing the WER by 26.27% and 8.98% respectively, indicating that both baselines considerably disrupt the model&#8217;s understanding of speech semantics. We hypothesize that this is due to Qwen2-Audio being highly sensitive to system prompts, where the presence of Self-Reminder and ICD causes the model to generate refusal responses even for normal ASR inputs. However, on AIR-Bench-Chat, where task instructions are more diverse, both baselines return to relatively normal performance.\nNotably, ALMGuard even improves model performance on Lyra-Base, reducing WER by 1.16% and increasing the AIR-Bench-Chat RQS by 0.15, outperforming all baselines and even the original model without defense.\nFor completeness, we also report results on LLaMA-Omni and Qwen2.5-Omni in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS5\" title=\"D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.5</span></a>.\nOverall, our method demonstrates minimal impact on model utility, suggesting that it can be reliably deployed in real-world ALM systems and significantly enhances practical usability.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "defense",
                    "rqs",
                    "almguard",
                    "wer",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In terms of defense effectiveness, the results with and without M-GSM show similar performance, both achieving over 50% reduction in average SRoA. However, regarding model utility, we observe a clear distinction. Without M-GSM, the WER on the ASR task increases by 20%, which substantially degrades the model&#8217;s ability to understand speech. In addition, the RQS drops by 1.17. In contrast, with M-GSM enabled, the fluctuations in WER and RQS are limited to within approximately 2% and 0.5, respectively, indicating a negligible impact on utility.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "defense",
                    "rqs",
                    "mgsm",
                    "sroa",
                    "wer",
                    "average",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hyperparameter analyses.</span>\nAn important hyperparameter in our method is the value of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>, which determines the number of Mel-frequency bins to which perturbations are applied. This parameter controls the trade-off between defense effectiveness and the impact on model utility. To investigate its influence, we conduct experiments with <math alttext=\"k\\in\\{0,16,48,96,128\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>48</mn><mo>,</mo><mn>96</mn><mo>,</mo><mn>128</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k\\in\\{0,16,48,96,128\\}</annotation></semantics></math>, where <math alttext=\"k=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">k=0</annotation></semantics></math> corresponds to the undefended setting with no perturbation applied, and <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math> represents the case without masking (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, full-band perturbation).\nWe evaluate the defense performance on PAIR-Audio and the benign-task performance on LibriSpeech. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F3\" title=\"Figure 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, increasing <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> leads to a monotonic decrease in SRoA and a monotonic increase in WER. This trend indicates that while a larger <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> improves robustness, it also introduces more distortion to benign inputs. Since our goal is to preserve utility while maximizing robustness, we identify <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m7\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> as a balanced configuration, where SRoA is reduced to 34.9% and WER remains low at 8.70%.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "benign",
                    "defense",
                    "sroa",
                    "wer",
                    "pairaudio",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive attacks.</span>\nTo further demonstrate the superiority of our method, we consider a more practical and challenging setting where the attacker has full knowledge of the defense mechanism, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, a white-box threat model. Under this setting, we evaluate adaptive AdvWave attacks on LLaMA-Omni by optimizing the adversarial suffix in the presence of each of the six defense methods.\nAs an example, when attacking ALMGuard, the attacker adds our well-trained SAP to the input at each iteration during AdvWave optimization.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, our method still achieves the best defense performance among all methods, reducing the SRoA by 11.9% compared to the original attack, even under this strongest threat model. In contrast, all baseline defenses yield an SRoA above 70% under adaptive attacks.\nInterestingly, for traditional audio defenses such as local smoothing, incorporating the corresponding transformations into the optimization process actually increases SRoA. We hypothesize that this is because these operations act similarly to data augmentation, which improves the robustness of the adversarial suffix and thus enhances the attack strength.\nIn summary, our method demonstrates the strongest resistance to adaptive attacks among all evaluated defenses.</p>\n\n",
                "matched_terms": [
                    "advwave",
                    "almguard",
                    "defense",
                    "sroa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce ALMGuard, a novel framework that pioneers activating inherent safety shortcuts in ALMs via universal SAPs. Our M-GSM technique precisely guides these SAPs to critical frequency regions, enabling robust jailbreak mitigation while preserving model utility.\nEvaluations across six attack methods and four SOTA ALMs show that ALMGuard achieves overall better performance compared to existing defenses, while keeping its impact on model utility well-controlled. This offers a new perspective on enhancing robustness for multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "almguard",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the pseudocode of ALMGuard in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#algorithm1\" title=\"In Appendix B Pseudocode &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The algorithm begins by constructing the training dataset <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> through the application of a jailbreak algorithm set <math alttext=\"\\mathcal{A}_{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\text{jb}}</annotation></semantics></math> to a curated jailbreak prompt set <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>. Based on this dataset, we compute the average sensitivity scores to derive the M-GSM <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, which identifies Mel-frequency bins that are critical for jailbreak mitigation yet minimally influential on benign performance. The universal perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> is initialized from a Gaussian distribution <math alttext=\"\\mathcal{N}(0,\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m6\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>&#963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,\\sigma)</annotation></semantics></math>, and iteratively optimized over the dataset to minimize the empirical safety risk.</p>\n\n",
                "matched_terms": [
                    "average",
                    "almguard",
                    "benign",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rationality analyses of Assumption&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption1\" title=\"Assumption 1 (Local Sensitivity Bound of Audio Encoder). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</span>\nThe core idea of M-GSM is to identify and select those Mel&#8208;frequency bins <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> for which the gradient of the ASR task loss <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> is small (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, <math alttext=\"g^{a}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>f</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">g^{a}_{f}</annotation></semantics></math> is low), while the gradient of the jailbreak mitigation objective is large (i.e. <math alttext=\"g^{s}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>f</mi><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">g^{s}_{f}</annotation></semantics></math> is high). In other words, the regions where M-GSM applies perturbations are chosen to be those that minimally affect benign speech understanding, as measured by ASR.</p>\n\n",
                "matched_terms": [
                    "benign",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rationality analyses of Assumption&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption2\" title=\"Assumption 2 (Local Sensitivity Bound of LLM Backbone). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</span>\nWe focus on how the LLM backbone <math alttext=\"f_{\\mathrm{LLM}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>LLM</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{LLM}}</annotation></semantics></math> responds to the small audio embedding perturbations <math alttext=\"\\Delta\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathbf{E}_{a}</annotation></semantics></math> that have already been &#8220;filtered&#8221; and &#8220;restricted&#8221; by the encoder <math alttext=\"f_{\\mathrm{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>enc</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{enc}}</annotation></semantics></math> and M-GSM. Although the exact global Lipschitz constant of an LLM is infeasible to compute, it is reasonable to assume that around its benign operating points (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, within the local neighborhood of a clean embedding <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m4\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>), the model exhibits relative smoothness and stability.</p>\n\n",
                "matched_terms": [
                    "benign",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AdvWave</span>-<span class=\"ltx_text ltx_font_bold\">P.</span>\nA variant of AdvWave is designed to better align with the audio adversarial attack paradigm. The suffix is replaced with an additive perturbation constrained under an <math alttext=\"\\ell_{\\infty}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mi mathvariant=\"normal\">&#8734;</mi></msub><annotation encoding=\"application/x-tex\">\\ell_{\\infty}</annotation></semantics></math> budget of 0.03. All other settings remain unchanged.</p>\n\n",
                "matched_terms": [
                    "advwave",
                    "advwavep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate whether incorporating <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide the gradient direction is effective, we conduct a preliminary comparison study. Under the setting of <math alttext=\"k=16\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">k=16</annotation></semantics></math>, we evaluated both model utility and defense performance against AdvWave-P on Qwen2-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T6\" title=\"Table 6 &#8227; D.4 Ablation on &#8466;_&quot;ASR&quot; &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, when the M-GSM is applied, the presence or absence of <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> has negligible effect on both metrics. The WER and SRoA differ by only 0.07% and 1.5%, which can be attributed to random variations. In contrast, when M-GSM is removed, using <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide optimization fails to preserve utility: the WER increases by 17.88% compared to the undefended case.\nIn summary, we conclude that <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> does not provide a meaningful benefit in achieving the goal of activating safety shortcuts to mitigate jailbreaks while preserving model utility. Therefore, we exclude <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> from the final design of ALMGuard.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "defense",
                    "advwavep",
                    "mgsm",
                    "sroa",
                    "wer",
                    "comparison",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the evaluation results on AIR-Bench-Chat for LLaMA-Omni and Qwen2.5-Omni in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T7\" title=\"Table 7 &#8227; D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. On LLaMA-Omni, our method achieves a RQS of 4.68, outperforming most baselines. Notably, while ICD provides the strongest defense among baselines on this model, it also causes the most significant degradation in utility. It is worth noting that for Qwen2.5-Omni, we set <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>, whereas <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> is used for other models. This adjustment is due to our observation that smaller values of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> fail to effectively defend against semantic-based attacks such as PAIR-Audio. We suspect this is primarily because Qwen2.5-Omni is highly sensitive to such attacks, as evidenced by its noticeably lower robustness on these attacks compared to other models. Nevertheless, given that Qwen2.5-Omni is inherently stronger, it still achieves a RQS of 6.02 even with <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>. Considering the trade-off between defense effectiveness and model utility, we believe this result is acceptable. In practice, we recommend users to adjust <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> flexibly based on specific deployment requirements.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "defense",
                    "rqs",
                    "pairaudio",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Model-Specific Traits.</span> We observe that the effectiveness of jailbreak attacks and corresponding defenses is closely tied to the intrinsic characteristics of the target model. For instance, as discussed earlier, Qwen2.5-Omni appears particularly vulnerable to semantic-based attacks, with PAP achieves a high SRoA of 77.9% on this model. Similarly, Lyra-Base exhibits strong sensitivity to prompt context, making it especially susceptible to attacks like ICA.\nIn real-world deployment, it is advisable to consider the model-specific traits when designing comprehensive defense strategies, potentially combining multiple techniques for better protection. Nevertheless, disregarding such model-specific factors, our method consistently demonstrates the universal effectiveness to activate safety-aligned shortcuts across all models, leading to substantial reductions in jailbreak success rates.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "defense",
                    "sroa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span>\nDespite its strong overall performance, we observe that our perturbation-based defense has room for improvement against semantic-based attacks. In some cases, ALMGuard underperforms compared to the best-performing baselines. We attribute this to the fact that our acoustic perturbation is mainly optimized to activate ALMs&#8217; inherent acoustic-related safety shortcuts to defend against acoustic-based attacks, but does not explicitly target the semantic intent of adversarial prompts. Future improvements may involve integrating semantic-level and intent-aware objectives during optimization. Additionally, given the plug-and-play nature of our method, it could be integrated with complementary defense techniques to form a more comprehensive defense framework.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "almguard"
                ]
            }
        ]
    },
    "A1.T4": {
        "source_file": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio–Language Models",
        "caption": "Table 4: Glossary of key terms used in this paper.",
        "body": "Term\n\n\n\n\nExplanation\n\n\n\n\n\n\n\n\nSafety Shortcuts\n\n\n\n\nLatent, safety-aligned pathways within ALMs. ALMGuard triggers these via SAPs to induce safe behavior and defend against jailbreaks.\n\n\n\n\n\n\nShortcut Activation Perturbation\n\n\n\n\nA universal acoustic perturbation by ALMGuard that activates safety shortcuts in ALMs at inference time to promote safe outputs.\n\n\n\n\n\n\nMel-Gradient Sparse Mask\n\n\n\n\nALMGuard’s method to find key Mel-frequency bins for applying SAPs, maximizing defense while minimizing impact on benign speech tasks.\n\n\n\n\n\n\nMel-frequency Bin\n\n\n\n\nA specific frequency-time unit in a Mel-spectrogram where ALMGuard analyzes sensitivity and applies targeted perturbations.\n\n\n\n\n\n\nModel utility\n\n\n\n\nThe ALM’s correct performance on benign inputs, which ALMGuard aims to preserve.\n\n\n\n\n\n\nAcoustic-based Attacks\n\n\n\n\nJailbreaks targeting ALMs by directly manipulating audio signal properties (e.g., noise suffix in AdvWave), exploiting acoustic processing.\n\n\n\n\n\n\nSemantic-based Attacks\n\n\n\n\nJailbreaks targeting ALMs by manipulating the meaning or context of prompts (e.g., PAIR-Audio, PAP-Audio), exploiting language understanding.\n\n\n\n\n\n\nUniversal Perturbation\n\n\n\n\nA single, input-agnostic perturbation designed to be effective across diverse inputs. SAPs are an example used in this work.\n\n\n\n\n\n\nMel-Spectrogram\n\n\n\n\nA time-frequency audio representation with Mel-scaled frequencies, used as an input representation for ALMs in this work.",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt ltx_border_t\" style=\"width:128.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Term</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Explanation</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:128.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Safety Shortcuts</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Latent, safety-aligned pathways within ALMs. ALMGuard triggers these via SAPs to induce safe behavior and defend against jailbreaks.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:128.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Shortcut Activation Perturbation</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">A universal acoustic perturbation by ALMGuard that activates safety shortcuts in ALMs at inference time to promote safe outputs.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:128.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Mel-Gradient Sparse Mask</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">ALMGuard&#8217;s method to find key Mel-frequency bins for applying SAPs, maximizing defense while minimizing impact on benign speech tasks.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:128.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Mel-frequency Bin</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">A specific frequency-time unit in a Mel-spectrogram where ALMGuard analyzes sensitivity and applies targeted perturbations.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:128.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model utility</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">The ALM&#8217;s correct performance on benign inputs, which ALMGuard aims to preserve.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:128.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Acoustic-based Attacks</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Jailbreaks targeting ALMs by directly manipulating audio signal properties (</span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">e.g.</span><span class=\"ltx_text\" style=\"font-size:80%;\">,&#160;noise suffix in AdvWave), exploiting acoustic processing.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:128.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Semantic-based Attacks</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Jailbreaks targeting ALMs by manipulating the meaning or context of prompts (</span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">e.g.</span><span class=\"ltx_text\" style=\"font-size:80%;\">,&#160;PAIR-Audio, PAP-Audio), exploiting language understanding.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:128.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Universal Perturbation</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">A single, input-agnostic perturbation designed to be effective across diverse inputs. SAPs are an example used in this work.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_b ltx_border_t\" style=\"width:128.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Mel-Spectrogram</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_b ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">A time-frequency audio representation with Mel-scaled frequencies, used as an input representation for ALMs in this work.</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "triggers",
            "meaning",
            "applies",
            "targeting",
            "diverse",
            "induce",
            "behavior",
            "key",
            "alm’s",
            "shortcuts",
            "glossary",
            "noise",
            "safety",
            "sparse",
            "promote",
            "activation",
            "bin",
            "inputagnostic",
            "frequencies",
            "inference",
            "method",
            "acoustic",
            "jailbreaks",
            "analyzes",
            "safetyaligned",
            "term",
            "benign",
            "designed",
            "across",
            "mask",
            "attacks",
            "sensitivity",
            "targeted",
            "language",
            "directly",
            "melfrequency",
            "exploiting",
            "melspectrogram",
            "specific",
            "safe",
            "prompts",
            "performance",
            "inputs",
            "manipulating",
            "preserve",
            "alms",
            "pairaudio",
            "tasks",
            "timefrequency",
            "via",
            "input",
            "outputs",
            "single",
            "latent",
            "activates",
            "against",
            "speech",
            "which",
            "frequencytime",
            "unit",
            "where",
            "advwave",
            "papaudio",
            "work",
            "defense",
            "context",
            "impact",
            "pathways",
            "time",
            "semanticbased",
            "defend",
            "understanding",
            "representation",
            "applying",
            "melscaled",
            "signal",
            "universal",
            "paper",
            "used",
            "saps",
            "example",
            "utility",
            "within",
            "properties",
            "model",
            "correct",
            "almguard’s",
            "suffix",
            "explanation",
            "almguard",
            "maximizing",
            "terms",
            "shortcut",
            "perturbations",
            "aims",
            "perturbation",
            "melgradient",
            "bins",
            "find",
            "processing",
            "acousticbased",
            "while",
            "minimizing",
            "effective",
            "audio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To improve clarity and facilitate understanding, we provide a glossary of key terms frequently used throughout the paper, as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Glossary &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats.\nTo address this issue, we propose <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, the first defense framework tailored to ALMs.\nBased on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time.\nTo better sift out effective triggers while preserving the model&#8217;s utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding.\nBoth theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, ALMGuard reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/WeifeiJin/ALMGuard\" title=\"\">https://github.com/WeifeiJin/ALMGuard</a>.</p>\n\n",
                "matched_terms": [
                    "triggers",
                    "jailbreaks",
                    "tasks",
                    "safetyaligned",
                    "universal",
                    "against",
                    "benign",
                    "speech",
                    "saps",
                    "which",
                    "across",
                    "utility",
                    "mask",
                    "attacks",
                    "shortcuts",
                    "language",
                    "model",
                    "directly",
                    "melfrequency",
                    "defense",
                    "safety",
                    "sparse",
                    "activation",
                    "almguard",
                    "shortcut",
                    "time",
                    "perturbations",
                    "melgradient",
                    "bins",
                    "inference",
                    "method",
                    "understanding",
                    "while",
                    "effective",
                    "alms",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Audio-Language Models (ALMs)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib18\" title=\"\">huang2024audiogpt </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib52\" title=\"\">xie2024mini </a></cite> are revolutionizing human-computer interaction by integrating speech understanding and generation capabilities, supporting applications from advanced assistants to real-time translation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib40\" title=\"\">su2025audio </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib22\" title=\"\">ji2024wavchat </a></cite>. As these models become integral to mission-critical systems, such as physical-world robotics (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, Google Gemini Robotics&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib43\" title=\"\">team2025gemini </a></cite>), their safety and security become paramount&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib4\" title=\"\">bengio2024managing </a></cite>. However, safeguarding ALMs poses distinct challenges that existing text-focused guardrails cannot adequately address.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alms",
                    "understanding",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Why ALM-specific defense?</span>\nRecent research proposing jailbreak attacks tailored to ALMs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite> has substantiated that the integration of audio modality introduces distinct and previously unexplored threats.\nWe observe that existing defense methods transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a></cite> are largely ineffective in mitigating these ALM-specific threats.\nThis can be attributed to a lack of consideration of the behavioral diversity and inherent complexity of ALMs, as well as an insufficient adaptation to the distinct characteristics of the audio modality. A similar limitation also exists on the attack side&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "jailbreaks",
                    "attacks",
                    "defense",
                    "alms",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our stance.</span>\nThe limitations of existing defenses against ALM-specific jailbreaks motivate a novel approach. We hypothesize that well-aligned ALMs inherently possess <em class=\"ltx_emph ltx_font_italic\">safety shortcuts</em>, which are latent pathways or input sensitivities that, if correctly triggered, can steer models towards safer behavior and mitigate jailbreaks. These differ from explicit safety alignments, representing intrinsic model properties that can be leveraged for defense.\nThe primary challenge is to activate these safety shortcuts efficiently and harmlessly at inference time. We suggest that this can be achieved by applying a lightweight, universal acoustic perturbation to the input, referred to as the <span class=\"ltx_text ltx_font_bold\">Shortcut Activation Perturbation (SAP)</span>. SAP is designed to engage these safety-conducive pathways without requiring any model retraining. However, to prevent such a perturbation from degrading benign task performance, its application must be precisely targeted.\nTo this end, we introduce the <span class=\"ltx_text ltx_font_bold\">Mel-Gradient Sparse Mask (M-GSM)</span>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, M-GSM identifies a sparse set of Mel-frequency bins that are highly influential for jailbreak mitigation yet largely inconsequential for benign speech understanding, as measured by automatic speech recognition (ASR) tasks. Our framework, <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, then synergistically employs M-GSM to guide the application of the universal SAP only to Mel bins that are security-critical yet benign-insensitive.\nThis targeted strategy allows ALMGuard to effectively activate safety shortcuts for robust defense while preserving model utility (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, performance on benign inputs).</p>\n\n",
                "matched_terms": [
                    "applying",
                    "jailbreaks",
                    "tasks",
                    "input",
                    "latent",
                    "universal",
                    "behavior",
                    "against",
                    "benign",
                    "designed",
                    "speech",
                    "which",
                    "utility",
                    "mask",
                    "shortcuts",
                    "properties",
                    "targeted",
                    "model",
                    "melfrequency",
                    "defense",
                    "safety",
                    "sparse",
                    "performance",
                    "activation",
                    "almguard",
                    "shortcut",
                    "pathways",
                    "time",
                    "inputs",
                    "perturbation",
                    "melgradient",
                    "bins",
                    "inference",
                    "understanding",
                    "while",
                    "acoustic",
                    "alms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and results.</span>\nWe evaluate the proposed method across four state-of-the-art (SOTA) ALMs and six representative jailbreak attacks. On Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a></cite>, ALMGuard reduces the success rate of the two most recent ALM-specific attacks, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> and Gupta <span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite>, to 3.1% and 0.5% respectively, outperforming all baselines and establishing itself as the new SOTA defense.\nMoreover, ALMGuard generalizes effectively to unseen attacks. As a lightweight defense framework, it enables near zero-cost deployment with negligible inference overhead. Evaluations on two benign benchmarks further confirm that ALMGuard does not noticeably degrade model utility. In addition, it achieves strong robustness against adaptive attacks.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "across",
                    "attacks",
                    "defense",
                    "advwave",
                    "method",
                    "almguard",
                    "inference",
                    "alms",
                    "against",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce the concept of inherent safety shortcuts in ALMs and propose ALMGuard as the first comprehensive and principled framework to systematically discover and activate these latent pathways for robust and generalizable jailbreak defense.</p>\n\n",
                "matched_terms": [
                    "pathways",
                    "defense",
                    "safety",
                    "latent",
                    "shortcuts",
                    "alms",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core technical innovation within ALMGuard involves a universal SAP which is precisely guided by our M-GSM to engage these safety shortcuts, targeting sparse acoustic regions for maximal defense effectiveness with minimal utility impact.</p>\n\n",
                "matched_terms": [
                    "which",
                    "utility",
                    "acoustic",
                    "within",
                    "targeting",
                    "defense",
                    "safety",
                    "sparse",
                    "universal",
                    "impact",
                    "shortcuts",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through extensive evaluations, complemented by theoretical analyses, we show that ALMGuard achieves SOTA defense against six jailbreaks on four SOTA ALMs, with strong generalization, high benign-task utility, and negligible inference overhead.</p>\n\n",
                "matched_terms": [
                    "jailbreaks",
                    "utility",
                    "defense",
                    "inference",
                    "alms",
                    "against",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Language models.</span>\nA basic ALM <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math> consists of two main components: an audio encoder <math alttext=\"f_{enc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">f_{enc}</annotation></semantics></math> and an LLM backbone <math alttext=\"f_{LLM}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msub><annotation encoding=\"application/x-tex\">f_{LLM}</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib42\" title=\"\">tang2023salmonn </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib16\" title=\"\">hu2024wavllm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib9\" title=\"\">chu2023qwen </a></cite>. ALMs that support audio output typically include an audio decoder to convert output tokens back to speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib56\" title=\"\">zeng2024glm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib25\" title=\"\">li2025baichuan </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib10\" title=\"\">fang2024llama </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib53\" title=\"\">xu2025qwen2 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib17\" title=\"\">huang2025step </a></cite>. For the audio encoder, the mainstream choice is OpenAI Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib36\" title=\"\">radford2023robust </a></cite>, which takes a Mel-spectrogram as input and converts it into high-level speech features denoted as <math alttext=\"\\mathbf{E}_{a}\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#119812;</mi><mi>a</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math>. The LLM transforms input text tokens into text embeddings through its embedding layer, denoted as <math alttext=\"\\mathbf{E}_{t}\\in\\mathbb{R}^{T^{\\prime}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#119812;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{E}_{t}\\in\\mathbb{R}^{T^{\\prime}\\times d}</annotation></semantics></math>. These embeddings are concatenated with the speech embeddings to form <math alttext=\"\\mathbf{Z}=[\\mathbf{E}_{a};\\mathbf{E}_{t}]\\in\\mathbb{R}^{(n+T^{\\prime})\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119833;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119812;</mi><mi>a</mi></msub><mo>;</mo><msub><mi>&#119812;</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Z}=[\\mathbf{E}_{a};\\mathbf{E}_{t}]\\in\\mathbb{R}^{(n+T^{\\prime})\\times d}</annotation></semantics></math>, which is then fed into the LLM backbone for processing. Let <math alttext=\"\\mathcal{P}_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{P}_{\\theta}</annotation></semantics></math> denote the output probability distribution from <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m8\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math>, and <math alttext=\"y_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m9\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">y_{j}</annotation></semantics></math> denotes the next token. The training objective is to maximize the prediction likelihood of the next token, namely <math alttext=\"\\mathcal{P}_{\\theta}(y_{j}\\mid y_{&lt;j},\\mathbf{Z})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m10\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>&#8739;</mo><mrow><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>j</mi></mrow></msub><mo>,</mo><mi>&#119833;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{\\theta}(y_{j}\\mid y_{&lt;j},\\mathbf{Z})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "which",
                    "input",
                    "processing",
                    "melspectrogram",
                    "alms",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Shortcut learning.</span>\nDeep neural networks (DNNs) are known to learn &#8220;shortcut&#8221; features from data, which correlate with training labels but may not align with the designer&#8217;s intent or generalize well&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib13\" title=\"\">geirhos2020shortcut </a></cite>. These shortcuts can be harnessed for both beneficial and harmful purposes.\nOn the one hand, they may improve robustness, as demonstrated by unadversarial perturbations in computer vision&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib39\" title=\"\">salman2021unadversarial </a></cite>,\nand can even be deliberately leveraged for defensive purposes, such as constructing safety-aligned or unlearnable features&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib48\" title=\"\">wu2023one </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib44\" title=\"\">wang2025provably </a></cite>.\nOn the other hand, they can be exploited for malicious objectives, such as in backdoor attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib14\" title=\"\">gu2017badnets </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib26\" title=\"\">li2022backdoor </a></cite>,\npoisoning attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib5\" title=\"\">biggio2012poisoning </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib58\" title=\"\">zhao2025data </a></cite>,\nand adversarial examples&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib19\" title=\"\">ilyas2019adversarial </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib41\" title=\"\">szegedy2013intriguing </a></cite>.\nThe core hypothesis of this work is that similar shortcut mechanisms may naturally exist or can be revealed within well-trained ALMs, specifically those that align with safety objectives, which we term safety-aligned shortcuts. Our goal is to identify and leverage these beneficial shortcuts for defensive purposes. To this end, we propose a method to discover such universal safety-aligned shortcuts within the acoustic input space. We then activate these shortcuts at inference time via a carefully crafted audio perturbation, which serves as a lightweight and effective safeguard. Let <math alttext=\"\\mathcal{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><annotation encoding=\"application/x-tex\">\\mathcal{L}</annotation></semantics></math> denote the training loss, our objective is to optimize a shortcut activation perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> that increases the model&#8217;s tendency to produce safe outputs <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math> when given any malicious Mel-spectrogram <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "via",
                    "input",
                    "outputs",
                    "safetyaligned",
                    "universal",
                    "term",
                    "which",
                    "attacks",
                    "within",
                    "work",
                    "shortcuts",
                    "safety",
                    "melspectrogram",
                    "safe",
                    "activation",
                    "shortcut",
                    "time",
                    "perturbations",
                    "perturbation",
                    "method",
                    "inference",
                    "effective",
                    "acoustic",
                    "alms",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Jailbreaks and defenses.</span>\nJailbreaking was initially introduced in the context of text-based LLMs, where attackers craft malicious prompts to bypass the model&#8217;s built-in safety mechanisms and induce it to generate harmful content. Existing jailbreak techniques for LLMs can be broadly categorized into two types: suffix-based and semantic-based. Suffix-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib60\" title=\"\">zou2023universal </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib27\" title=\"\">liao2024amplegcg </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib3\" title=\"\">basani2024gasp </a></cite> append an adversarial suffix after a harmful query, while semantic-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib29\" title=\"\">liu2023autodan </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib6\" title=\"\">chao2023jailbreaking </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib31\" title=\"\">mehrotra2024tree </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib57\" title=\"\">zeng2024johnny </a></cite> manipulate the prompt content using strategies such as persuasion or logical traps to elicit the desired malicious response.\nIn the context of ALMs, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> is a suffix-based attack that appends an adversarial noise segment, while Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite> explore prefix and perturbation formats. Modern foundation models are typically enhanced with preference-based alignment (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, reinforcement learning from human feedback (RLHF)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib7\" title=\"\">christiano2017deep </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib32\" title=\"\">ouyang2022training </a></cite> and direct preference optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib37\" title=\"\">rafailov2023direct </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib2\" title=\"\">amini2024direct </a></cite>) where human judgments guide model fine-tuning. As jailbreak attacks continue to emerge, corresponding defense mechanisms have been proposed, including input-level detection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib1\" title=\"\">alon2023detecting </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib38\" title=\"\">robey2023smoothllm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib20\" title=\"\">inan2023llama </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib34\" title=\"\">phute2023llm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib50\" title=\"\">xie2024gradsafe </a></cite> and mitigation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib21\" title=\"\">jain2023baseline </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib54\" title=\"\">xu2024safedecoding </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, and output-level intervention&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib45\" title=\"\">wang2024selfdefend </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib35\" title=\"\">qian2024hsf </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib46\" title=\"\">wang2025vulnerability </a></cite>.\nHowever, defenses tailored to ALMs remain underexplored. In this paper, we address this gap by proposing a dedicated framework that targets their unique vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "paper",
                    "noise",
                    "jailbreaks",
                    "model",
                    "attacks",
                    "perturbation",
                    "defense",
                    "semanticbased",
                    "safety",
                    "where",
                    "advwave",
                    "context",
                    "suffix",
                    "while",
                    "induce",
                    "prompts",
                    "alms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our overall objective is to identify an acoustic signal, SAP, that can effectively activate the model&#8217;s inherent safety shortcuts to mitigate jailbreaks, without significantly degrading the model&#8217;s performance on benign inputs. We formulate this as an optimization problem aimed at making the model output safe responses to jailbreaking audio when subjected to this perturbation.\nSpecifically, given an ALM <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math>, a set of malicious instructions <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>, and a set of jailbreak algorithms <math alttext=\"\\mathcal{A}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{A}^{\\text{jb}}</annotation></semantics></math>, our goal is formulated as:</p>\n\n",
                "matched_terms": [
                    "model",
                    "jailbreaks",
                    "acoustic",
                    "inputs",
                    "perturbation",
                    "safety",
                    "signal",
                    "safe",
                    "shortcuts",
                    "performance",
                    "audio",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{X}^{\\text{bg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>bg</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{bg}}</annotation></semantics></math> refers to benign task prompts,\n<math alttext=\"M(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M(\\cdot)</annotation></semantics></math> is the Mel-spectrogram transformation, and <math alttext=\"\\text{Err}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mtext>Err</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{Err}(\\cdot,\\cdot)</annotation></semantics></math> measures the prediction difference between the perturbed and original inputs. Note that we choose to apply perturbations on the Mel-spectrogram rather than the raw waveform, because we find that perturbing the Mel-spectrogram leads to less degradation in model utility (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, utility with clean user inputs) under the same optimization settings, with details provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS5\" title=\"D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.5</span></a>. The safety loss <math alttext=\"\\mathcal{L}_{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>safe</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{safe}}</annotation></semantics></math> can be instantiated as the cross-entropy loss between the model output and the safe target sequence <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "perturbations",
                    "inputs",
                    "safety",
                    "where",
                    "find",
                    "melspectrogram",
                    "safe",
                    "prompts",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> denotes the length of the token sequence. Given the consistent refusal behavior observed across different jailbreak prompts, we assign the same <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math> to all inputs. We believe that using a unified target also facilitates the generalizability of the SAP. The constraint bounded by <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m11\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) ensures that the perturbation does not cause significant degradation in the model&#8217;s utility. The <math alttext=\"\\ell_{\\infty}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m12\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mi mathvariant=\"normal\">&#8734;</mi></msub><annotation encoding=\"application/x-tex\">\\ell_{\\infty}</annotation></semantics></math> constraint bounded by <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m13\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> limits the perturbation magnitude to prevent excessive distortion that could disrupt inherent benign acoustic features in the Mel-spectrogram.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "across",
                    "acoustic",
                    "inputs",
                    "perturbation",
                    "where",
                    "melspectrogram",
                    "behavior",
                    "prompts",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the constraint in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), we initially attempt to introduce a loss function based on benign tasks to guide the direction of the perturbation and prevent it from affecting model utility. Specifically, we use Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib36\" title=\"\">radford2023robust </a></cite> to transcribe the input examples and compute the cross-entropy loss between the transcription result and the ground-truth text <math alttext=\"y^{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>ASR</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{ASR}}</annotation></semantics></math>, which we denote as <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "which",
                    "tasks",
                    "perturbation",
                    "input",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, experimental results show that this auxiliary loss fails to effectively reduce the impact of perturbations on model utility. Detailed results can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS4\" title=\"D.4 Ablation on &#8466;_&quot;ASR&quot; &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "impact",
                    "model",
                    "perturbations",
                    "utility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate this limitation, we conduct a deeper analysis and observe that only a small subset of frequency bands contribute meaningfully to jailbreak mitigation, while most Mel bins are insensitive to the defense objective, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Motivated by this observation, we propose to apply perturbations only to the most critical frequency bands and filter out the rest. This greatly reduces the perturbation region and thus minimizes its impact on model utility.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "perturbations",
                    "perturbation",
                    "defense",
                    "bins",
                    "impact",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To this end, we design the Mel-Gradient Sparse Mask (M-GSM). For each Mel bin <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math>, we compute its gradient sensitivity with respect to both <math alttext=\"\\mathcal{L}_{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>safe</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{safe}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> by averaging over the <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> time frames:</p>\n\n",
                "matched_terms": [
                    "bin",
                    "sensitivity",
                    "time",
                    "mask",
                    "melgradient",
                    "sparse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given our goal of activating safety shortcuts that effectively mitigate jailbreaks while minimizing the impact on model performance in benign tasks, we aim to identify frequency bands that are highly sensitive to jailbreak mitigation objectives but relatively insensitive to ASR tasks. Based on this intuition, we define a combined sensitivity score as follows:</p>\n\n",
                "matched_terms": [
                    "sensitivity",
                    "jailbreaks",
                    "model",
                    "tasks",
                    "safety",
                    "impact",
                    "while",
                    "shortcuts",
                    "minimizing",
                    "performance",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\varepsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><mi>&#949;</mi><annotation encoding=\"application/x-tex\">\\varepsilon</annotation></semantics></math> is a small constant to prevent division by zero. Then we select the top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> Mel bins <math alttext=\"\\{f_{1},\\ldots,f_{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>f</mi><mi>k</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{f_{1},\\ldots,f_{k}\\}</annotation></semantics></math> with the largest <math alttext=\"s_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m8\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>f</mi></msub><annotation encoding=\"application/x-tex\">s_{f}</annotation></semantics></math>, and define a binary mask:</p>\n\n",
                "matched_terms": [
                    "mask",
                    "bins",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In practice, we compute the gradients over all examples in the training set <math alttext=\"\\mathcal{X}^{jb}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m9\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mrow><mi>j</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>b</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{jb}</annotation></semantics></math>, average them, and then calculate <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m10\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and the corresponding binary mask <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m11\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>. The result is illustrated in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A6.SS1\" title=\"F.1 Visualization of M-GSM &#8227; Appendix F Visualizations and Examples &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">F.1</span></a>. This fixed mask is then applied uniformly to all examples. On one hand, we observe that the Mel bins with high sensitivity scores tend to be similar across different examples. On the other hand, averaging the gradients across the dataset better captures the overall sensitivity, which helps ensure that the resulting perturbation exhibits broad effectiveness and strong generalization to unseen examples.</p>\n\n",
                "matched_terms": [
                    "sensitivity",
                    "which",
                    "across",
                    "mask",
                    "perturbation",
                    "bins"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By integrating the M-GSM with the perturbation strategy, we formulate the final unified optimization objective for ALMGuard as follows, where <math alttext=\"\\odot\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8857;</mo><annotation encoding=\"application/x-tex\">\\odot</annotation></semantics></math> denotes the Hadamard (element-wise) product:</p>\n\n",
                "matched_terms": [
                    "perturbation",
                    "almguard",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Instead of explicitly enforcing the constraint in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), we rely on M-GSM to indirectly preserve the model&#8217;s performance on benign inputs. We optimize the perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> through iterative updates using the Projected Gradient Descent (PGD)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib30\" title=\"\">madry2017towards </a></cite> algorithm:</p>\n\n",
                "matched_terms": [
                    "inputs",
                    "perturbation",
                    "preserve",
                    "performance",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\eta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#951;</mi><annotation encoding=\"application/x-tex\">\\eta</annotation></semantics></math> is the step size, and <math alttext=\"\\Pi_{|\\cdot|_{\\infty}\\leq\\epsilon}(\\cdot)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">&#928;</mi><mrow><mo fence=\"false\" stretchy=\"false\">|</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><msub><mo fence=\"false\" stretchy=\"false\">|</mo><mi mathvariant=\"normal\">&#8734;</mi></msub><mo lspace=\"0.167em\">&#8804;</mo><mi>&#1013;</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\Pi_{|\\cdot|_{\\infty}\\leq\\epsilon}(\\cdot)</annotation></semantics></math> denotes the projection onto the <math alttext=\"\\ell_{\\infty}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mi mathvariant=\"normal\">&#8734;</mi></msub><annotation encoding=\"application/x-tex\">\\ell_{\\infty}</annotation></semantics></math> ball with radius <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m6\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math>. The mask <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m7\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> ensures that the perturbation is only applied to the most sensitive frequency bands as determined by M-GSM. This allows ALMGuard to concentrate its perturbation budget on a small but effective subset of the input space.</p>\n\n",
                "matched_terms": [
                    "mask",
                    "perturbation",
                    "input",
                    "where",
                    "effective",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ALMGuard recap.</span> We first construct the training dataset <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> by applying the jailbreak algorithm set <math alttext=\"\\mathcal{A}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{A}^{\\text{jb}}</annotation></semantics></math> to the jailbreak prompt set <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>. We then compute the average gradient-based sensitivity scores over this dataset to obtain the M-GSM <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>. The perturbation is initialized from a normal distribution <math alttext=\"\\mathcal{N}(0,\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>&#963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,\\sigma)</annotation></semantics></math> and optimized iteratively over <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math>. The pseudocode is presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A2\" title=\"Appendix B Pseudocode &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "sensitivity",
                    "applying",
                    "almguard",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we provide theoretical analyses of ALMGuard by examining its generalization from training data and seen attacks to unseen examples and unseen attacks. Specifically, we define empirical and population safety risks, and derive an upper bound on the generalization gap.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "almguard",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Let <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmdefinition1.p1.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext class=\"ltx_mathvariant_italic\">jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> be the training set consisting of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"Thmdefinition1.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> jailbreak examples, and <math alttext=\"\\mathcal{D}^{\\text{real}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmdefinition1.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext class=\"ltx_mathvariant_italic\">real</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{real}}</annotation></semantics></math> be the distribution over potential jailbreak inputs in real-world deployment. For a given perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"Thmdefinition1.p1.m4\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math>, the empirical safety risk <math alttext=\"\\widehat{\\mathcal{R}}(\\delta)\" class=\"ltx_Math\" display=\"inline\" id=\"Thmdefinition1.p1.m5\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mo>^</mo></mover><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#948;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\widehat{\\mathcal{R}}(\\delta)</annotation></semantics></math> and the population safety risk <math alttext=\"\\mathcal{R}(\\delta)\" class=\"ltx_Math\" display=\"inline\" id=\"Thmdefinition1.p1.m6\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#948;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{R}(\\delta)</annotation></semantics></math> are defined as:</span>\n</p>\n\n",
                "matched_terms": [
                    "inputs",
                    "perturbation",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The empirical safety risk measures the average failure on the training jailbreak set, while the population risk reflects the expected safety violation over real-world jailbreak examples.</p>\n\n",
                "matched_terms": [
                    "while",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Assume the training set <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext class=\"ltx_mathvariant_italic\">jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> consists of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> i.i.d. jailbreak examples sampled from the real-world distribution <math alttext=\"\\mathcal{D}^{\\text{real}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext class=\"ltx_mathvariant_italic\">real</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{real}}</annotation></semantics></math>. Suppose the safety loss <math alttext=\"\\mathcal{L}_{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_italic\">safe</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{safe}}</annotation></semantics></math> is bounded in <math alttext=\"[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,1]</annotation></semantics></math>. Then, for any fixed perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m6\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> and any confidence level <math alttext=\"\\alpha\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in(0,1)</annotation></semantics></math>, with probability at least <math alttext=\"1-\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8722;</mo><mi>&#945;</mi></mrow><annotation encoding=\"application/x-tex\">1-\\alpha</annotation></semantics></math>, the following generalization bound holds:</span>\n</p>\n\n",
                "matched_terms": [
                    "perturbation",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This theorem implies that as long as the empirical safety risk on the training set is sufficiently low, we can guarantee with high confidence that the population safety risk on unseen attacks and unseen examples is also low. This provides a theoretical justification for the generalization ability of ALMGuard. We provide a complete proof of this result in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A3.SS1\" title=\"C.1 Proof of Safety Risk Generalization Bound &#8227; Appendix C Proofs &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">C.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "almguard",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this subsection, we theoretically analyze how ALMGuard limits its impact on ALM performance over benign examples, thereby satisfying the constraint Equation&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). We begin by bounding the perturbation-induced change in audio embeddings, and subsequently leverage this bound to derive the performance degradation bound of ALMs on benign tasks.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "benign",
                    "impact",
                    "alms",
                    "performance",
                    "audio",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Given a benign input <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and its Mel-spectrogram <math alttext=\"M(x)\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M(x)</annotation></semantics></math>, assume that for the perturbation masked by M-GSM, which satisfies <math alttext=\"\\|m\\odot\\delta\\|_{\\infty}\\leq\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mrow><mo stretchy=\"false\">&#8214;</mo><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8857;</mo><mi>&#948;</mi></mrow><mo stretchy=\"false\">&#8214;</mo></mrow><mi mathvariant=\"normal\">&#8734;</mi></msub><mo>&#8804;</mo><mi>&#1013;</mi></mrow><annotation encoding=\"application/x-tex\">\\|m\\odot\\delta\\|_{\\infty}\\leq\\epsilon</annotation></semantics></math>, the encoder <math alttext=\"f_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m4\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext class=\"ltx_mathvariant_italic\">enc</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{enc}}</annotation></semantics></math> exhibits bounded local sensitivity. Specifically, there exists a valid local Lipschitz constant <math alttext=\"L_{\\text{enc}}\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>L</mi><mtext class=\"ltx_mathvariant_italic\">enc</mtext></msub><mo>&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">L_{\\text{enc}}\\geq 0</annotation></semantics></math> such that the change in output embedding <math alttext=\"\\Delta\\mathbf{E}_{a}=f_{\\text{enc}}(M(x)+m\\odot\\delta)-f_{\\text{enc}}(M(x))\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption1.p1.m6\" intent=\":literal\"><semantics><mrow><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow><mo>=</mo><mrow><mrow><msub><mi>f</mi><mtext class=\"ltx_mathvariant_italic\">enc</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8857;</mo><mi>&#948;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><msub><mi>f</mi><mtext class=\"ltx_mathvariant_italic\">enc</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathbf{E}_{a}=f_{\\text{enc}}(M(x)+m\\odot\\delta)-f_{\\text{enc}}(M(x))</annotation></semantics></math> satisfies:</span>\n</p>\n\n",
                "matched_terms": [
                    "sensitivity",
                    "which",
                    "perturbation",
                    "input",
                    "melspectrogram",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This assumption is justified by our perturbation selection mechanism, which ensures the local stability of the encoder around benign samples. A detailed explanation is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A3.SS2\" title=\"C.2 Proof of Benign Task Deviation Bound &#8227; Appendix C Proofs &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">C.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "explanation",
                    "perturbation",
                    "which",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Given a benign-task embedding <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>, under Assumption&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption1\" title=\"Assumption 1 (Local Sensitivity Bound of Audio Encoder). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> with small perturbation <math alttext=\"\\Delta\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathbf{E}_{a}</annotation></semantics></math>, we assume that the loss function of downstream ALM tasks (e.g., speech instruction following), denoted as <math alttext=\"\\mathcal{L}_{\\text{ALM}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_italic\">ALM</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ALM}}</annotation></semantics></math>, has a bounded gradient norm with respect to <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>. That is, there exists a constant <math alttext=\"G_{\\max}\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>max</mi></msub><mo>&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">G_{\\max}\\geq 0</annotation></semantics></math> such that:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "perturbation",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Under Assumptions&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption1\" title=\"Assumption 1 (Local Sensitivity Bound of Audio Encoder). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption2\" title=\"Assumption 2 (Local Sensitivity Bound of LLM Backbone). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the impact of ALMGuard on benign ALM tasks (measured by the loss difference <math alttext=\"\\Delta\\mathcal{L}_{\\text{ALM}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmproposition1.p1.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_italic\">ALM</mtext></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathcal{L}_{\\text{ALM}}</annotation></semantics></math>) is bounded:</span>\n</p>\n\n",
                "matched_terms": [
                    "benign",
                    "impact",
                    "almguard",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This bound shows that the performance impact on benign tasks is proportional to the tunable hyperparameters, <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> and <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (through <math alttext=\"d_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">d_{k}</annotation></semantics></math>), and two stability-related constants, <math alttext=\"L_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>enc</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{enc}}</annotation></semantics></math> and <math alttext=\"G_{\\max}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>max</mi></msub><annotation encoding=\"application/x-tex\">G_{\\max}</annotation></semantics></math>. The design of M-GSM aims to apply perturbations to regions where these stability factors collectively result in minimal adverse impact on benign tasks. This theoretical upper bound supports the claim that ALMGuard can preserve model performance on benign tasks, which aligns with our empirical results shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.SS3\" title=\"5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which",
                    "aims",
                    "tasks",
                    "perturbations",
                    "where",
                    "impact",
                    "almguard",
                    "preserve",
                    "performance",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset and models.</span> In line with AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite>, we adopt AdvBench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib60\" title=\"\">zou2023universal </a></cite>, a benchmark widely used in text-based jailbreak research, which contains a total of 520 prompts. These prompts are converted into audio using OpenAI&#8217;s text-to-speech (TTS) API to construct <span class=\"ltx_text ltx_font_bold\">AdvBench</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span>, comprising 520 audio queries. We evaluate four state-of-the-art audio-language models: <span class=\"ltx_text ltx_font_bold\">Qwen2</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a></cite>, <span class=\"ltx_text ltx_font_bold\">LLaMA</span>-<span class=\"ltx_text ltx_font_bold\">Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib10\" title=\"\">fang2024llama </a></cite>, <span class=\"ltx_text ltx_font_bold\">Lyra</span>-<span class=\"ltx_text ltx_font_bold\">Base</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib59\" title=\"\">zhong2024lyra </a></cite>, and <span class=\"ltx_text ltx_font_bold\">Qwen2.5</span>-<span class=\"ltx_text ltx_font_bold\">Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib53\" title=\"\">xu2025qwen2 </a></cite>. All models are capable of accepting audio as input and producing either textual responses.</p>\n\n",
                "matched_terms": [
                    "which",
                    "input",
                    "advwave",
                    "prompts",
                    "used",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attacks.</span>\nFor the attack methods, we first adopt two SOTA jailbreak approaches specifically designed for ALMs, namely <span class=\"ltx_text ltx_font_bold\">AdvWave</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> and the method proposed by <span class=\"ltx_text ltx_font_bold\">Gupta <span class=\"ltx_text ltx_font_italic\">et al.</span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite>. In addition, we adapt perturbation-based attacks from the traditional domain of audio adversarial attacks into the AdvWave framework, resulting in a variant named <span class=\"ltx_text ltx_font_bold\">AdvWave</span>-<span class=\"ltx_text ltx_font_bold\">P</span>. We further transfer several representative techniques from the text-based jailbreak literature, including <span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Attack (ICA)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, Prompt Automatic Iterative Refinement (PAIR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib6\" title=\"\">chao2023jailbreaking </a></cite>, and Persuasive Adversarial Prompts (PAP)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib57\" title=\"\">zeng2024johnny </a></cite>. For ICA, we prepend malicious textual demonstrations as context to the audio prompts. For PAIR and PAP, we convert their generated jailbreak texts into audio using OpenAI&#8217;s TTS API, thereby forming the <span class=\"ltx_text ltx_font_bold\">PAIR</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span> and <span class=\"ltx_text ltx_font_bold\">PAP</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span> variants. We classify AdvWave, AdvWave-P, and Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;as <span class=\"ltx_text ltx_font_italic\">acoustic-based</span> attacks, and the remaining three as <span class=\"ltx_text ltx_font_italic\">semantic-based</span> attacks. A detailed description of all attack methods is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS2\" title=\"D.2 Attack Setup &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "attacks",
                    "semanticbased",
                    "papaudio",
                    "advwave",
                    "context",
                    "acousticbased",
                    "method",
                    "prompts",
                    "alms",
                    "pairaudio",
                    "designed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span>\nFor the defense methods, due to the lack of dedicated defenses targeting jailbreaks in ALMs, we explore two directions of transfer. <span class=\"ltx_text ltx_font_bold\">Type I:</span> From the domain of traditional audio adversarial defenses, we consider three widely adopted techniques&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib12\" title=\"\">ge2023advddos </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib11\" title=\"\">fang2024zero </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib23\" title=\"\">jin2025whispering </a></cite>: (<span class=\"ltx_text ltx_font_italic\">i</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Gaussian Noise</span>, (<span class=\"ltx_text ltx_font_italic\">ii</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Local Smoothing</span>, and (<span class=\"ltx_text ltx_font_italic\">iii</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Downsampling</span>. <span class=\"ltx_text ltx_font_bold\">Type II:</span> From the domain of text-based jailbreak defense, we implement two representative methods: <span class=\"ltx_text ltx_font_bold\">Self-Reminder</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a></cite> and <span class=\"ltx_text ltx_font_bold\">In-Context Defense (ICD)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>. Details of these defense techniques are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS3\" title=\"D.3 Baseline Setup &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "jailbreaks",
                    "targeting",
                    "defense",
                    "alms",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics.</span>\nWe evaluate the effectiveness of the aforementioned jailbreak attacks and defenses using the <span class=\"ltx_text ltx_font_bold\">Success Rate of Attack (SRoA)</span>. Following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite>, we adopt a well-tuned LLM judge model from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib49\" title=\"\">xie2024sorry </a></cite> to determine whether a jailbreak attempt is successful.\nTo assess model utility, we consider two benchmarks. First, we sample 500 audio clips from the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib33\" title=\"\">panayotov2015librispeech </a></cite>, a standard ASR task, to measure the model&#8217;s basic speech understanding capability, using <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span> as the evaluation metric.\nIn addition, we employ 800 speech samples from AIR-Bench-Chat&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib55\" title=\"\">yang2024air </a></cite> to further evaluate the model&#8217;s audio-to-text interaction performance. For this evaluation, each response is assigned a <span class=\"ltx_text ltx_font_bold\">Response Quality Score (RQS)</span> on a 1-10 scale by DeepSeek-V3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib28\" title=\"\">liu2024deepseek </a></cite>, where higher RQS values indicate stronger model performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "utility",
                    "attacks",
                    "where",
                    "work",
                    "understanding",
                    "performance",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ALMGuard&#160;setup.</span>\nDuring the optimization of ALMGuard, we randomly select 50 audio samples from AdvBench-Audio and apply a set of attack algorithms, namely AdvWave, AdvWave-P, and PAIR-Audio, for training-time perturbation optimization. We believe that the selected samples and attack methods are sufficiently representative to enable transferability to unseen examples and attacks. The perturbation duration is set to 30 seconds, consistent with the default input length of Whisper, and the perturbation budget is constrained by <math alttext=\"\\epsilon=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.5</annotation></semantics></math>. We set the value of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> to 48, and provide a detailed analysis in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.SS4\" title=\"5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "attacks",
                    "perturbation",
                    "input",
                    "advwave",
                    "pairaudio",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance on seen attacks.</span>\nWe first evaluate the performance of ALMGuard on three seen attacks used during the optimization of the perturbation: AdvWave, AdvWave-P, and PAIR-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method significantly outperforms all baselines on AdvWave and AdvWave-P, reducing the average SRoA across four ALMs from 53.5% and 68.5% to 4.6% and 7.8%, respectively. This indicates that ALMGuard exhibits strong robustness against acoustic-based attacks. On PAIR-Audio, a representative semantic-based attack, ALMGuard reduces the average SRoA to 26.3%, achieving comparable performance to Self-Reminder and ICD. Notably, ALMGuard consistently achieves a lower SRoA than all baselines against AdvWave-P on every model, and even reduces it to 0 on Qwen2.5-Omni, making the model completely robust - a result that no existing defense has achieved.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "attacks",
                    "perturbation",
                    "defense",
                    "semanticbased",
                    "advwave",
                    "acousticbased",
                    "method",
                    "alms",
                    "performance",
                    "used",
                    "against",
                    "pairaudio",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferability to unseen attacks.</span>\nWe evaluate the transferability of ALMGuard on three unseen attacks: Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>, ICA, and PAP-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ALMGuard significantly reduces the average SRoA by 27.4%, 7.4%, and 5.2%, respectively. In particular, the average SRoA on Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;is reduced to only 1.9%, which is the lowest among all attacks. Given that AdvWave and Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;represent the current SOTA ALM-specific jailbreak attacks, we believe that ALMGuard achieves strong robustness against this class of threats.\nFor baselines, we observe that Type I defenses tend to perform better against acoustic-based attacks. In contrast, Type II defenses show significantly better performance on semantic-based attacks.\nThis observation suggests that no existing defense can dominate across all types of attacks.\nIn comparison, ALMGuard consistently outperforms Type I defenses across all attack categories. Compared to Type II defenses, ALMGuard achieves significantly better results on acoustic-based attacks.\nOn average, ALMGuard reduces the overall SRoA to 14.6%, which represents the current SOTA.</p>\n\n",
                "matched_terms": [
                    "which",
                    "across",
                    "attacks",
                    "defense",
                    "semanticbased",
                    "papaudio",
                    "advwave",
                    "acousticbased",
                    "performance",
                    "against",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T2\" title=\"Table 2 &#8227; 5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the performance of our method on two benign benchmarks. On Qwen2-Audio, ALMGuard causes only a slight degradation in performance, increasing the WER on LibriSpeech by 1.85% and decreasing the AIR-Bench-Chat score by 0.56, which we regard as negligible. In contrast, both Self-Reminder and ICD significantly impair ASR performance, increasing the WER by 26.27% and 8.98% respectively, indicating that both baselines considerably disrupt the model&#8217;s understanding of speech semantics. We hypothesize that this is due to Qwen2-Audio being highly sensitive to system prompts, where the presence of Self-Reminder and ICD causes the model to generate refusal responses even for normal ASR inputs. However, on AIR-Bench-Chat, where task instructions are more diverse, both baselines return to relatively normal performance.\nNotably, ALMGuard even improves model performance on Lyra-Base, reducing WER by 1.16% and increasing the AIR-Bench-Chat RQS by 0.15, outperforming all baselines and even the original model without defense.\nFor completeness, we also report results on LLaMA-Omni and Qwen2.5-Omni in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS5\" title=\"D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.5</span></a>.\nOverall, our method demonstrates minimal impact on model utility, suggesting that it can be reliably deployed in real-world ALM systems and significantly enhances practical usability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "which",
                    "utility",
                    "inputs",
                    "defense",
                    "where",
                    "method",
                    "impact",
                    "understanding",
                    "prompts",
                    "almguard",
                    "performance",
                    "diverse",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution of M-GSM.</span>\nIn ALMGuard, we employ M-GSM to ensure that the model&#8217;s utility is not significantly affected. To validate the effectiveness of this key component, we conduct an ablation study on Qwen2-Audio by testing three jailbreak attacks and two benign benchmarks, both with and without M-GSM. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "key",
                    "attacks",
                    "benign",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In terms of defense effectiveness, the results with and without M-GSM show similar performance, both achieving over 50% reduction in average SRoA. However, regarding model utility, we observe a clear distinction. Without M-GSM, the WER on the ASR task increases by 20%, which substantially degrades the model&#8217;s ability to understand speech. In addition, the RQS drops by 1.17. In contrast, with M-GSM enabled, the fluctuations in WER and RQS are limited to within approximately 2% and 0.5, respectively, indicating a negligible impact on utility.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "terms",
                    "model",
                    "which",
                    "utility",
                    "within",
                    "defense",
                    "impact",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hyperparameter analyses.</span>\nAn important hyperparameter in our method is the value of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>, which determines the number of Mel-frequency bins to which perturbations are applied. This parameter controls the trade-off between defense effectiveness and the impact on model utility. To investigate its influence, we conduct experiments with <math alttext=\"k\\in\\{0,16,48,96,128\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>48</mn><mo>,</mo><mn>96</mn><mo>,</mo><mn>128</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k\\in\\{0,16,48,96,128\\}</annotation></semantics></math>, where <math alttext=\"k=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">k=0</annotation></semantics></math> corresponds to the undefended setting with no perturbation applied, and <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math> represents the case without masking (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, full-band perturbation).\nWe evaluate the defense performance on PAIR-Audio and the benign-task performance on LibriSpeech. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F3\" title=\"Figure 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, increasing <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> leads to a monotonic decrease in SRoA and a monotonic increase in WER. This trend indicates that while a larger <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> improves robustness, it also introduces more distortion to benign inputs. Since our goal is to preserve utility while maximizing robustness, we identify <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m7\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> as a balanced configuration, where SRoA is reduced to 34.9% and WER remains low at 8.70%.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which",
                    "utility",
                    "maximizing",
                    "perturbations",
                    "inputs",
                    "melfrequency",
                    "perturbation",
                    "defense",
                    "bins",
                    "where",
                    "method",
                    "impact",
                    "while",
                    "preserve",
                    "performance",
                    "pairaudio",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive attacks.</span>\nTo further demonstrate the superiority of our method, we consider a more practical and challenging setting where the attacker has full knowledge of the defense mechanism, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, a white-box threat model. Under this setting, we evaluate adaptive AdvWave attacks on LLaMA-Omni by optimizing the adversarial suffix in the presence of each of the six defense methods.\nAs an example, when attacking ALMGuard, the attacker adds our well-trained SAP to the input at each iteration during AdvWave optimization.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, our method still achieves the best defense performance among all methods, reducing the SRoA by 11.9% compared to the original attack, even under this strongest threat model. In contrast, all baseline defenses yield an SRoA above 70% under adaptive attacks.\nInterestingly, for traditional audio defenses such as local smoothing, incorporating the corresponding transformations into the optimization process actually increases SRoA. We hypothesize that this is because these operations act similarly to data augmentation, which improves the robustness of the adversarial suffix and thus enhances the attack strength.\nIn summary, our method demonstrates the strongest resistance to adaptive attacks among all evaluated defenses.</p>\n\n",
                "matched_terms": [
                    "example",
                    "model",
                    "which",
                    "attacks",
                    "input",
                    "defense",
                    "where",
                    "advwave",
                    "method",
                    "suffix",
                    "performance",
                    "audio",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce ALMGuard, a novel framework that pioneers activating inherent safety shortcuts in ALMs via universal SAPs. Our M-GSM technique precisely guides these SAPs to critical frequency regions, enabling robust jailbreak mitigation while preserving model utility.\nEvaluations across six attack methods and four SOTA ALMs show that ALMGuard achieves overall better performance compared to existing defenses, while keeping its impact on model utility well-controlled. This offers a new perspective on enhancing robustness for multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "saps",
                    "model",
                    "utility",
                    "across",
                    "shortcuts",
                    "via",
                    "safety",
                    "universal",
                    "impact",
                    "while",
                    "paper",
                    "alms",
                    "performance",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the pseudocode of ALMGuard in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#algorithm1\" title=\"In Appendix B Pseudocode &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The algorithm begins by constructing the training dataset <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> through the application of a jailbreak algorithm set <math alttext=\"\\mathcal{A}_{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\text{jb}}</annotation></semantics></math> to a curated jailbreak prompt set <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>. Based on this dataset, we compute the average sensitivity scores to derive the M-GSM <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, which identifies Mel-frequency bins that are critical for jailbreak mitigation yet minimally influential on benign performance. The universal perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> is initialized from a Gaussian distribution <math alttext=\"\\mathcal{N}(0,\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m6\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>&#963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,\\sigma)</annotation></semantics></math>, and iteratively optimized over the dataset to minimize the empirical safety risk.</p>\n\n",
                "matched_terms": [
                    "sensitivity",
                    "which",
                    "melfrequency",
                    "perturbation",
                    "bins",
                    "safety",
                    "universal",
                    "almguard",
                    "performance",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rationality analyses of Assumption&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption1\" title=\"Assumption 1 (Local Sensitivity Bound of Audio Encoder). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</span>\nThe core idea of M-GSM is to identify and select those Mel&#8208;frequency bins <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> for which the gradient of the ASR task loss <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> is small (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, <math alttext=\"g^{a}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>f</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">g^{a}_{f}</annotation></semantics></math> is low), while the gradient of the jailbreak mitigation objective is large (i.e. <math alttext=\"g^{s}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>f</mi><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">g^{s}_{f}</annotation></semantics></math> is high). In other words, the regions where M-GSM applies perturbations are chosen to be those that minimally affect benign speech understanding, as measured by ASR.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "perturbations",
                    "which",
                    "applies",
                    "bins",
                    "where",
                    "while",
                    "understanding",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If, on the other hand, the encoder <math alttext=\"f_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>enc</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{enc}}</annotation></semantics></math> exhibits very high local sensitivity in some band (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, a large local Lipschitz constant <math alttext=\"L_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>enc</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{enc}}</annotation></semantics></math> causing a large change in <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>), then that change in <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math> will be further amplified by the decoder and lead to a significant increase in <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p3.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math>. Such a band would therefore be avoided by M-GSM, which prefers regions of low ASR sensitivity (small <math alttext=\"g^{a}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p3.m6\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>f</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">g^{a}_{f}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "sensitivity",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Thus, the M-GSM selection process can be viewed as implicitly locating and exploiting those subspaces of the encoder&#8217;s input where small perturbations induce stable (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, bounded) changes in <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p4.m1\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>. Equivalently, M-GSM directs perturbations into those &#8220;safe&#8221; regions of the encoder input where a finite, moderately sized <math alttext=\"L_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p4.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>enc</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{enc}}</annotation></semantics></math> assumption is reasonable.</p>\n\n",
                "matched_terms": [
                    "perturbations",
                    "input",
                    "where",
                    "exploiting",
                    "induce"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rationality analyses of Assumption&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption2\" title=\"Assumption 2 (Local Sensitivity Bound of LLM Backbone). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</span>\nWe focus on how the LLM backbone <math alttext=\"f_{\\mathrm{LLM}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>LLM</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{LLM}}</annotation></semantics></math> responds to the small audio embedding perturbations <math alttext=\"\\Delta\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathbf{E}_{a}</annotation></semantics></math> that have already been &#8220;filtered&#8221; and &#8220;restricted&#8221; by the encoder <math alttext=\"f_{\\mathrm{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>enc</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{enc}}</annotation></semantics></math> and M-GSM. Although the exact global Lipschitz constant of an LLM is infeasible to compute, it is reasonable to assume that around its benign operating points (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, within the local neighborhood of a clean embedding <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m4\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>), the model exhibits relative smoothness and stability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "perturbations",
                    "within",
                    "audio",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, LLMs are trained via self&#8208;supervised learning on massive text corpora, acquiring rich semantic representations and fluent generation capabilities. To achieve such generality and maintain good out&#8208;of&#8208;domain performance, the model must tolerate small perturbations in its inputs, including those in <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p6.m1\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math> that carry no core semantic content. If the loss function of the LLM reacts with large gradient norms (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, a very large <math alttext=\"G_{\\max}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p6.m2\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>max</mi></msub><annotation encoding=\"application/x-tex\">G_{\\max}</annotation></semantics></math>) to any infinitesimal change in its input embedding, then stable training will become difficult and the model will be overly sensitive to noise, thereby degrading its comprehension and generation quality.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "model",
                    "perturbations",
                    "inputs",
                    "via",
                    "input",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"\\mathbf{E}^{\\prime}_{a}=\\mathbf{E}_{a}+\\Delta\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p8.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119812;</mi><mi>a</mi><mo>&#8242;</mo></msubsup><mo>=</mo><mrow><msub><mi>&#119812;</mi><mi>a</mi></msub><mo>+</mo><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{E}^{\\prime}_{a}=\\mathbf{E}_{a}+\\Delta\\mathbf{E}_{a}</annotation></semantics></math> be the audio embedding after applying the perturbation <math alttext=\"m\\odot\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p8.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8857;</mo><mi>&#948;</mi></mrow><annotation encoding=\"application/x-tex\">m\\odot\\delta</annotation></semantics></math>. Denote by <math alttext=\"\\mathcal{L}_{\\mathrm{ALM}}(\\mathbf{E}_{a},\\mathbf{E}_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p8.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>ALM</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119812;</mi><mi>a</mi></msub><mo>,</mo><msub><mi>&#119812;</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{ALM}}(\\mathbf{E}_{a},\\mathbf{E}_{t})</annotation></semantics></math> the downstream ALM loss given audio embedding <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p8.m4\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math> and text embedding <math alttext=\"\\mathbf{E}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p8.m5\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{t}</annotation></semantics></math>. We focus on the change in loss:</p>\n\n",
                "matched_terms": [
                    "applying",
                    "audio",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select &#8220;<span class=\"ltx_text ltx_font_italic\">I&#8217;m sorry, but I cannot fulfill this request.</span>&#8221; as the target safe response <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m1\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math> for all examples and models during optimization. We use Adam as the optimizer with a learning rate of 0.001. Training is conducted on a single NVIDIA A800 80GB GPU, with a maximum of 3000 iterations per example and up to 10 epochs.</p>\n\n",
                "matched_terms": [
                    "single",
                    "example",
                    "safe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AdvWave.</span>\nA white-box jailbreak attack specifically designed for ALMs. It optimizes an adversarial suffix using cross-entropy loss. We set the suffix length to 44,100 samples (approximately 2.76 seconds) at a sampling rate of 16kHz. Following the original paper, we implement the dynamic target selection mechanism, which constructs a unique jailbreak target for each query. The maximum number of iterations is set to 3000, and the attack is considered successful when the loss falls below 0.1, at which point optimization terminates.</p>\n\n",
                "matched_terms": [
                    "which",
                    "advwave",
                    "suffix",
                    "paper",
                    "alms",
                    "designed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AdvWave</span>-<span class=\"ltx_text ltx_font_bold\">P.</span>\nA variant of AdvWave is designed to better align with the audio adversarial attack paradigm. The suffix is replaced with an additive perturbation constrained under an <math alttext=\"\\ell_{\\infty}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mi mathvariant=\"normal\">&#8734;</mi></msub><annotation encoding=\"application/x-tex\">\\ell_{\\infty}</annotation></semantics></math> budget of 0.03. All other settings remain unchanged.</p>\n\n",
                "matched_terms": [
                    "perturbation",
                    "advwave",
                    "suffix",
                    "audio",
                    "designed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PAIR</span>-<span class=\"ltx_text ltx_font_bold\">Audio.</span>\nA black-box adversarial attack that iteratively queries the target LLM using inputs generated through interaction with an attacker LLM, requiring no human intervention. We follow the original PAIR procedure without modification to its core logic. The only changes are replacing the target LLM with our selected ALMs and converting the attacker LLM&#8217;s dialogue into audio using TTS. We use GPT-3.5-Turbo as the attacker LLM, consistent with the original paper.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "inputs",
                    "paper",
                    "alms",
                    "pairaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gupta <span class=\"ltx_text ltx_font_italic\">et al</span>.</span>\nA white-box jailbreak method that evaluates various attack forms and stealth enhancement techniques. We adopt the most effective universal prefix proposed in their work. It is trained on 100 randomly selected samples from AdvBench-Audio and tested on the remaining 420.</p>\n\n",
                "matched_terms": [
                    "method",
                    "work",
                    "universal",
                    "effective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Attack.</span>\nA black-box jailbreak strategy that prepends a few successful attack demonstrations to the input prompt to induce in-context learning. Following the authors&#8217; recommended configuration, we use 10-shot prompts for LLaMA-Omni, Lyra-Base, and Qwen2.5-Omni. Due to context length limitations, we use 5-shot prompts for Qwen2-Audio.</p>\n\n",
                "matched_terms": [
                    "context",
                    "input",
                    "induce",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PAP</span>-<span class=\"ltx_text ltx_font_bold\">Audio.</span>\nA black-box jailbreak method inspired by social science-based persuasion strategies. We convert 145 successful jailbreak prompts provided by the authors into audio via TTS for evaluation.</p>\n\n",
                "matched_terms": [
                    "via",
                    "papaudio",
                    "method",
                    "prompts",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gaussian Noise.</span>\nWe add Gaussian noise with a standard deviation of 0.01 to the input audio. This simple defense can effectively remove certain brittle adversarial perturbations, particularly suffix-based or prefix-based attacks.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "perturbations",
                    "attacks",
                    "defense",
                    "input",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Smoothing.</span>\nWe apply a moving average filter to smooth the waveform. Each audio sample is replaced by the average of its neighboring values within a window of size <math alttext=\"h=2\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I2.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">h=2</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "within",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Downsampling.</span>\nWe first downsample the original audio from 16kHz to a lower rate (14kHz), and then upsample it back to 16kHz. This operation distorts adversarial patterns while preserving most benign content.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "while",
                    "benign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Defense.</span>\nProposed in the same work as ICA, this method uses 2-shot in-context learning to guide the model toward safe behavior. We follow the original configuration and apply 2-shot demonstrations in the context prompt.</p>\n\n",
                "matched_terms": [
                    "model",
                    "defense",
                    "work",
                    "context",
                    "method",
                    "behavior",
                    "safe"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate whether incorporating <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide the gradient direction is effective, we conduct a preliminary comparison study. Under the setting of <math alttext=\"k=16\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">k=16</annotation></semantics></math>, we evaluated both model utility and defense performance against AdvWave-P on Qwen2-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T6\" title=\"Table 6 &#8227; D.4 Ablation on &#8466;_&quot;ASR&quot; &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, when the M-GSM is applied, the presence or absence of <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> has negligible effect on both metrics. The WER and SRoA differ by only 0.07% and 1.5%, which can be attributed to random variations. In contrast, when M-GSM is removed, using <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide optimization fails to preserve utility: the WER increases by 17.88% compared to the undefended case.\nIn summary, we conclude that <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> does not provide a meaningful benefit in achieving the goal of activating safety shortcuts to mitigate jailbreaks while preserving model utility. Therefore, we exclude <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> from the final design of ALMGuard.</p>\n\n",
                "matched_terms": [
                    "jailbreaks",
                    "which",
                    "utility",
                    "model",
                    "defense",
                    "safety",
                    "while",
                    "effective",
                    "shortcuts",
                    "preserve",
                    "performance",
                    "against",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the evaluation results on AIR-Bench-Chat for LLaMA-Omni and Qwen2.5-Omni in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T7\" title=\"Table 7 &#8227; D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. On LLaMA-Omni, our method achieves a RQS of 4.68, outperforming most baselines. Notably, while ICD provides the strongest defense among baselines on this model, it also causes the most significant degradation in utility. It is worth noting that for Qwen2.5-Omni, we set <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>, whereas <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> is used for other models. This adjustment is due to our observation that smaller values of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> fail to effectively defend against semantic-based attacks such as PAIR-Audio. We suspect this is primarily because Qwen2.5-Omni is highly sensitive to such attacks, as evidenced by its noticeably lower robustness on these attacks compared to other models. Nevertheless, given that Qwen2.5-Omni is inherently stronger, it still achieves a RQS of 6.02 even with <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>. Considering the trade-off between defense effectiveness and model utility, we believe this result is acceptable. In practice, we recommend users to adjust <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> flexibly based on specific deployment requirements.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "attacks",
                    "defense",
                    "semanticbased",
                    "defend",
                    "specific",
                    "method",
                    "while",
                    "used",
                    "against",
                    "pairaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Model-Specific Traits.</span> We observe that the effectiveness of jailbreak attacks and corresponding defenses is closely tied to the intrinsic characteristics of the target model. For instance, as discussed earlier, Qwen2.5-Omni appears particularly vulnerable to semantic-based attacks, with PAP achieves a high SRoA of 77.9% on this model. Similarly, Lyra-Base exhibits strong sensitivity to prompt context, making it especially susceptible to attacks like ICA.\nIn real-world deployment, it is advisable to consider the model-specific traits when designing comprehensive defense strategies, potentially combining multiple techniques for better protection. Nevertheless, disregarding such model-specific factors, our method consistently demonstrates the universal effectiveness to activate safety-aligned shortcuts across all models, leading to substantial reductions in jailbreak success rates.</p>\n\n",
                "matched_terms": [
                    "sensitivity",
                    "model",
                    "across",
                    "attacks",
                    "defense",
                    "semanticbased",
                    "context",
                    "safetyaligned",
                    "universal",
                    "method",
                    "shortcuts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span>\nDespite its strong overall performance, we observe that our perturbation-based defense has room for improvement against semantic-based attacks. In some cases, ALMGuard underperforms compared to the best-performing baselines. We attribute this to the fact that our acoustic perturbation is mainly optimized to activate ALMs&#8217; inherent acoustic-related safety shortcuts to defend against acoustic-based attacks, but does not explicitly target the semantic intent of adversarial prompts. Future improvements may involve integrating semantic-level and intent-aware objectives during optimization. Additionally, given the plug-and-play nature of our method, it could be integrated with complementary defense techniques to form a more comprehensive defense framework.</p>\n\n",
                "matched_terms": [
                    "acoustic",
                    "attacks",
                    "performance",
                    "perturbation",
                    "defense",
                    "semanticbased",
                    "safety",
                    "defend",
                    "acousticbased",
                    "method",
                    "shortcuts",
                    "prompts",
                    "against",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presents heatmaps of M-GSM sensitivity score rankings for each model in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A6.F5\" title=\"Figure 5 &#8227; F.1 Visualization of M-GSM &#8227; Appendix F Visualizations and Examples &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The results show a strong similarity across the M-GSM masks for the four models, with their top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A6.SS1.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> highest-ranked bins showing significant overlap. This commonality suggests, on one hand, that our method is generalizable across different models. On the other hand, it also reveals that the inherent latent safety shortcuts within these diverse ALMs may share considerable similarities, thus allowing them to be activated in a consistent manner, potentially by the same universal perturbations.</p>\n\n",
                "matched_terms": [
                    "sensitivity",
                    "model",
                    "perturbations",
                    "across",
                    "within",
                    "bins",
                    "safety",
                    "latent",
                    "universal",
                    "method",
                    "shortcuts",
                    "alms",
                    "diverse"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we present several examples consisting of user prompts and the assistant&#8217;s corresponding responses. Each prompt is originally an <span class=\"ltx_text ltx_font_bold\">audio input</span>, and we provide its transcription.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "input",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: Does the paper discuss the limitations of the work performed by the authors?</p>\n\n",
                "matched_terms": [
                    "work",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.</p>\n\n",
                "matched_terms": [
                    "while",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.</p>\n\n",
                "matched_terms": [
                    "model",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "example",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren&#8217;t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p>\n\n",
                "matched_terms": [
                    "used",
                    "while",
                    "preserve",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>\n\n",
                "matched_terms": [
                    "correct",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.</p>\n\n",
                "matched_terms": [
                    "language",
                    "example",
                    "model",
                    "via",
                    "specific"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example</p>\n\n",
                "matched_terms": [
                    "while",
                    "example",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.</p>\n\n",
                "matched_terms": [
                    "model",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.</p>\n\n",
                "matched_terms": [
                    "model",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.</p>\n\n",
                "matched_terms": [
                    "method",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</p>\n\n",
                "matched_terms": [
                    "time",
                    "preserve"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Justification: Due to the high computational cost of evaluating multiple ALMs across numerous attacks, results are based on single runs.</p>\n\n",
                "matched_terms": [
                    "single",
                    "attacks",
                    "alms",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>\n\n",
                "matched_terms": [
                    "time",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Justification: Research aims to improve AI safety; no unethical practices identified.</p>\n\n",
                "matched_terms": [
                    "aims",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>\n\n",
                "matched_terms": [
                    "work",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The answer NA means that there is no societal impact of the work performed.</p>\n\n",
                "matched_terms": [
                    "work",
                    "impact"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.</p>\n\n",
                "matched_terms": [
                    "work",
                    "impact",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.</p>\n\n",
                "matched_terms": [
                    "specific",
                    "impact"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.</p>\n\n",
                "matched_terms": [
                    "example",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "time"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>\n\n",
                "matched_terms": [
                    "language",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Justification: Paper proposes a defense method; no release of new high-risk models or data.</p>\n\n",
                "matched_terms": [
                    "method",
                    "defense",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.</p>\n\n",
                "matched_terms": [
                    "example",
                    "model",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</p>\n\n",
                "matched_terms": [
                    "terms",
                    "used",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The authors should state which version of the asset is used and, if possible, include a URL.</p>\n\n",
                "matched_terms": [
                    "used",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The paper should discuss whether and how consent was obtained from people whose asset is used.</p>\n\n",
                "matched_terms": [
                    "used",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.</p>\n\n",
                "matched_terms": [
                    "which",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.</p>\n\n",
                "matched_terms": [
                    "used",
                    "impact",
                    "paper"
                ]
            }
        ]
    },
    "A4.T5": {
        "source_file": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio–Language Models",
        "caption": "Table 5: Summary of evaluated jailbreak attacks.",
        "body": "Attack\nSource\nOriginal Target\nKnowledge\nType\nFormat\n\n\n\n\nAdvWave\n-\nALM\nWhite-box\nAcoustic-based\nSuffix\n\n\nAdvWave-P\nAdvWave\nALM\nWhite-box\nAcoustic-based\nPerturbation\n\n\n\nGupta et al.\n\n-\nALM\nWhite-box\nAcoustic-based\nPrefix\n\n\nPAIR-Audio\nPAIR\nLLM\nBlack-box\nSemantic-based\nTemplate\n\n\nICA\n-\nLLM\nBlack-box\nSemantic-based\nSystem prompt\n\n\nPAP-Audio\nPAP\nLLM\nBlack-box\nSemantic-based\nTemplate",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Attack</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Source</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Original Target</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Knowledge</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Format</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">AdvWave</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">ALM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">White-box</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">Acoustic-based</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">Suffix</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">AdvWave-P</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">AdvWave</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">ALM</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">White-box</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Acoustic-based</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Perturbation</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Gupta </span><span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">et al.</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">ALM</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">White-box</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Acoustic-based</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Prefix</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">PAIR-Audio</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">PAIR</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">LLM</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Black-box</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Semantic-based</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Template</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">ICA</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">LLM</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Black-box</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Semantic-based</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">System prompt</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:80%;\">PAP-Audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:80%;\">PAP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:80%;\">LLM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:80%;\">Black-box</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:80%;\">Semantic-based</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:80%;\">Template</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "alm",
            "llm",
            "pap",
            "type",
            "source",
            "gupta",
            "advwavep",
            "jailbreak",
            "target",
            "format",
            "knowledge",
            "evaluated",
            "attacks",
            "template",
            "advwave",
            "papaudio",
            "original",
            "attack",
            "system",
            "summary",
            "prefix",
            "pair",
            "suffix",
            "blackbox",
            "whitebox",
            "ica",
            "prompt",
            "perturbation",
            "semanticbased",
            "acousticbased",
            "pairaudio"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We summarize the six attack methods evaluated in our experiments in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T5\" title=\"Table 5 &#8227; D.2 Attack Setup &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Below are their detailed descriptions and our specific configurations:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats.\nTo address this issue, we propose <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, the first defense framework tailored to ALMs.\nBased on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time.\nTo better sift out effective triggers while preserving the model&#8217;s utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding.\nBoth theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, ALMGuard reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/WeifeiJin/ALMGuard\" title=\"\">https://github.com/WeifeiJin/ALMGuard</a>.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "attacks",
                    "jailbreak",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Why ALM-specific defense?</span>\nRecent research proposing jailbreak attacks tailored to ALMs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite> has substantiated that the integration of audio modality introduces distinct and previously unexplored threats.\nWe observe that existing defense methods transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a></cite> are largely ineffective in mitigating these ALM-specific threats.\nThis can be attributed to a lack of consideration of the behavioral diversity and inherent complexity of ALMs, as well as an insufficient adaptation to the distinct characteristics of the audio modality. A similar limitation also exists on the attack side&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "attacks",
                    "jailbreak",
                    "attack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our stance.</span>\nThe limitations of existing defenses against ALM-specific jailbreaks motivate a novel approach. We hypothesize that well-aligned ALMs inherently possess <em class=\"ltx_emph ltx_font_italic\">safety shortcuts</em>, which are latent pathways or input sensitivities that, if correctly triggered, can steer models towards safer behavior and mitigate jailbreaks. These differ from explicit safety alignments, representing intrinsic model properties that can be leveraged for defense.\nThe primary challenge is to activate these safety shortcuts efficiently and harmlessly at inference time. We suggest that this can be achieved by applying a lightweight, universal acoustic perturbation to the input, referred to as the <span class=\"ltx_text ltx_font_bold\">Shortcut Activation Perturbation (SAP)</span>. SAP is designed to engage these safety-conducive pathways without requiring any model retraining. However, to prevent such a perturbation from degrading benign task performance, its application must be precisely targeted.\nTo this end, we introduce the <span class=\"ltx_text ltx_font_bold\">Mel-Gradient Sparse Mask (M-GSM)</span>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, M-GSM identifies a sparse set of Mel-frequency bins that are highly influential for jailbreak mitigation yet largely inconsequential for benign speech understanding, as measured by automatic speech recognition (ASR) tasks. Our framework, <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, then synergistically employs M-GSM to guide the application of the universal SAP only to Mel bins that are security-critical yet benign-insensitive.\nThis targeted strategy allows ALMGuard to effectively activate safety shortcuts for robust defense while preserving model utility (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, performance on benign inputs).</p>\n\n",
                "matched_terms": [
                    "jailbreak",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and results.</span>\nWe evaluate the proposed method across four state-of-the-art (SOTA) ALMs and six representative jailbreak attacks. On Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a></cite>, ALMGuard reduces the success rate of the two most recent ALM-specific attacks, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> and Gupta <span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite>, to 3.1% and 0.5% respectively, outperforming all baselines and establishing itself as the new SOTA defense.\nMoreover, ALMGuard generalizes effectively to unseen attacks. As a lightweight defense framework, it enables near zero-cost deployment with negligible inference overhead. Evaluations on two benign benchmarks further confirm that ALMGuard does not noticeably degrade model utility. In addition, it achieves strong robustness against adaptive attacks.</p>\n\n",
                "matched_terms": [
                    "advwave",
                    "attacks",
                    "jailbreak",
                    "gupta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Language models.</span>\nA basic ALM <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math> consists of two main components: an audio encoder <math alttext=\"f_{enc}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m2\" intent=\":literal\"><semantics><msub><mi>f</mi><mrow><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">f_{enc}</annotation></semantics></math> and an LLM backbone <math alttext=\"f_{LLM}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mrow><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>M</mi></mrow></msub><annotation encoding=\"application/x-tex\">f_{LLM}</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib42\" title=\"\">tang2023salmonn </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib16\" title=\"\">hu2024wavllm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib9\" title=\"\">chu2023qwen </a></cite>. ALMs that support audio output typically include an audio decoder to convert output tokens back to speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib56\" title=\"\">zeng2024glm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib25\" title=\"\">li2025baichuan </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib10\" title=\"\">fang2024llama </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib53\" title=\"\">xu2025qwen2 </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib17\" title=\"\">huang2025step </a></cite>. For the audio encoder, the mainstream choice is OpenAI Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib36\" title=\"\">radford2023robust </a></cite>, which takes a Mel-spectrogram as input and converts it into high-level speech features denoted as <math alttext=\"\\mathbf{E}_{a}\\in\\mathbb{R}^{n\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>&#119812;</mi><mi>a</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>n</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}\\in\\mathbb{R}^{n\\times d}</annotation></semantics></math>. The LLM transforms input text tokens into text embeddings through its embedding layer, denoted as <math alttext=\"\\mathbf{E}_{t}\\in\\mathbb{R}^{T^{\\prime}\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>&#119812;</mi><mi>t</mi></msub><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{E}_{t}\\in\\mathbb{R}^{T^{\\prime}\\times d}</annotation></semantics></math>. These embeddings are concatenated with the speech embeddings to form <math alttext=\"\\mathbf{Z}=[\\mathbf{E}_{a};\\mathbf{E}_{t}]\\in\\mathbb{R}^{(n+T^{\\prime})\\times d}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119833;</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119812;</mi><mi>a</mi></msub><mo>;</mo><msub><mi>&#119812;</mi><mi>t</mi></msub><mo stretchy=\"false\">]</mo></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mi>n</mi><mo>+</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mi>d</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Z}=[\\mathbf{E}_{a};\\mathbf{E}_{t}]\\in\\mathbb{R}^{(n+T^{\\prime})\\times d}</annotation></semantics></math>, which is then fed into the LLM backbone for processing. Let <math alttext=\"\\mathcal{P}_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{P}_{\\theta}</annotation></semantics></math> denote the output probability distribution from <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m8\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math>, and <math alttext=\"y_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m9\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">y_{j}</annotation></semantics></math> denotes the next token. The training objective is to maximize the prediction likelihood of the next token, namely <math alttext=\"\\mathcal{P}_{\\theta}(y_{j}\\mid y_{&lt;j},\\mathbf{Z})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p1.m10\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>&#8739;</mo><mrow><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>j</mi></mrow></msub><mo>,</mo><mi>&#119833;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}_{\\theta}(y_{j}\\mid y_{&lt;j},\\mathbf{Z})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "alm",
                    "llm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Shortcut learning.</span>\nDeep neural networks (DNNs) are known to learn &#8220;shortcut&#8221; features from data, which correlate with training labels but may not align with the designer&#8217;s intent or generalize well&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib13\" title=\"\">geirhos2020shortcut </a></cite>. These shortcuts can be harnessed for both beneficial and harmful purposes.\nOn the one hand, they may improve robustness, as demonstrated by unadversarial perturbations in computer vision&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib39\" title=\"\">salman2021unadversarial </a></cite>,\nand can even be deliberately leveraged for defensive purposes, such as constructing safety-aligned or unlearnable features&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib48\" title=\"\">wu2023one </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib44\" title=\"\">wang2025provably </a></cite>.\nOn the other hand, they can be exploited for malicious objectives, such as in backdoor attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib14\" title=\"\">gu2017badnets </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib26\" title=\"\">li2022backdoor </a></cite>,\npoisoning attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib5\" title=\"\">biggio2012poisoning </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib58\" title=\"\">zhao2025data </a></cite>,\nand adversarial examples&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib19\" title=\"\">ilyas2019adversarial </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib41\" title=\"\">szegedy2013intriguing </a></cite>.\nThe core hypothesis of this work is that similar shortcut mechanisms may naturally exist or can be revealed within well-trained ALMs, specifically those that align with safety objectives, which we term safety-aligned shortcuts. Our goal is to identify and leverage these beneficial shortcuts for defensive purposes. To this end, we propose a method to discover such universal safety-aligned shortcuts within the acoustic input space. We then activate these shortcuts at inference time via a carefully crafted audio perturbation, which serves as a lightweight and effective safeguard. Let <math alttext=\"\\mathcal{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><annotation encoding=\"application/x-tex\">\\mathcal{L}</annotation></semantics></math> denote the training loss, our objective is to optimize a shortcut activation perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> that increases the model&#8217;s tendency to produce safe outputs <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m3\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math> when given any malicious Mel-spectrogram <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Jailbreaks and defenses.</span>\nJailbreaking was initially introduced in the context of text-based LLMs, where attackers craft malicious prompts to bypass the model&#8217;s built-in safety mechanisms and induce it to generate harmful content. Existing jailbreak techniques for LLMs can be broadly categorized into two types: suffix-based and semantic-based. Suffix-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib60\" title=\"\">zou2023universal </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib27\" title=\"\">liao2024amplegcg </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib3\" title=\"\">basani2024gasp </a></cite> append an adversarial suffix after a harmful query, while semantic-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib29\" title=\"\">liu2023autodan </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib6\" title=\"\">chao2023jailbreaking </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib31\" title=\"\">mehrotra2024tree </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib57\" title=\"\">zeng2024johnny </a></cite> manipulate the prompt content using strategies such as persuasion or logical traps to elicit the desired malicious response.\nIn the context of ALMs, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> is a suffix-based attack that appends an adversarial noise segment, while Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite> explore prefix and perturbation formats. Modern foundation models are typically enhanced with preference-based alignment (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, reinforcement learning from human feedback (RLHF)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib7\" title=\"\">christiano2017deep </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib32\" title=\"\">ouyang2022training </a></cite> and direct preference optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib37\" title=\"\">rafailov2023direct </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib2\" title=\"\">amini2024direct </a></cite>) where human judgments guide model fine-tuning. As jailbreak attacks continue to emerge, corresponding defense mechanisms have been proposed, including input-level detection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib1\" title=\"\">alon2023detecting </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib38\" title=\"\">robey2023smoothllm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib20\" title=\"\">inan2023llama </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib34\" title=\"\">phute2023llm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib50\" title=\"\">xie2024gradsafe </a></cite> and mitigation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib21\" title=\"\">jain2023baseline </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib54\" title=\"\">xu2024safedecoding </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, and output-level intervention&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib45\" title=\"\">wang2024selfdefend </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib35\" title=\"\">qian2024hsf </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib46\" title=\"\">wang2025vulnerability </a></cite>.\nHowever, defenses tailored to ALMs remain underexplored. In this paper, we address this gap by proposing a dedicated framework that targets their unique vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "prefix",
                    "attacks",
                    "perturbation",
                    "gupta",
                    "semanticbased",
                    "advwave",
                    "jailbreak",
                    "suffix",
                    "attack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our overall objective is to identify an acoustic signal, SAP, that can effectively activate the model&#8217;s inherent safety shortcuts to mitigate jailbreaks, without significantly degrading the model&#8217;s performance on benign inputs. We formulate this as an optimization problem aimed at making the model output safe responses to jailbreaking audio when subjected to this perturbation.\nSpecifically, given an ALM <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math>, a set of malicious instructions <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>, and a set of jailbreak algorithms <math alttext=\"\\mathcal{A}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{A}^{\\text{jb}}</annotation></semantics></math>, our goal is formulated as:</p>\n\n",
                "matched_terms": [
                    "alm",
                    "jailbreak",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{X}^{\\text{bg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>bg</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{bg}}</annotation></semantics></math> refers to benign task prompts,\n<math alttext=\"M(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M(\\cdot)</annotation></semantics></math> is the Mel-spectrogram transformation, and <math alttext=\"\\text{Err}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mtext>Err</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{Err}(\\cdot,\\cdot)</annotation></semantics></math> measures the prediction difference between the perturbed and original inputs. Note that we choose to apply perturbations on the Mel-spectrogram rather than the raw waveform, because we find that perturbing the Mel-spectrogram leads to less degradation in model utility (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, utility with clean user inputs) under the same optimization settings, with details provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS5\" title=\"D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.5</span></a>. The safety loss <math alttext=\"\\mathcal{L}_{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>safe</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{safe}}</annotation></semantics></math> can be instantiated as the cross-entropy loss between the model output and the safe target sequence <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "original",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> denotes the length of the token sequence. Given the consistent refusal behavior observed across different jailbreak prompts, we assign the same <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math> to all inputs. We believe that using a unified target also facilitates the generalizability of the SAP. The constraint bounded by <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m11\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>) ensures that the perturbation does not cause significant degradation in the model&#8217;s utility. The <math alttext=\"\\ell_{\\infty}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m12\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mi mathvariant=\"normal\">&#8734;</mi></msub><annotation encoding=\"application/x-tex\">\\ell_{\\infty}</annotation></semantics></math> constraint bounded by <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m13\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> limits the perturbation magnitude to prevent excessive distortion that could disrupt inherent benign acoustic features in the Mel-spectrogram.</p>\n\n",
                "matched_terms": [
                    "jailbreak",
                    "target",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate this limitation, we conduct a deeper analysis and observe that only a small subset of frequency bands contribute meaningfully to jailbreak mitigation, while most Mel bins are insensitive to the defense objective, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Motivated by this observation, we propose to apply perturbations only to the most critical frequency bands and filter out the rest. This greatly reduces the perturbation region and thus minimizes its impact on model utility.</p>\n\n",
                "matched_terms": [
                    "jailbreak",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ALMGuard recap.</span> We first construct the training dataset <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> by applying the jailbreak algorithm set <math alttext=\"\\mathcal{A}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{A}^{\\text{jb}}</annotation></semantics></math> to the jailbreak prompt set <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>. We then compute the average gradient-based sensitivity scores over this dataset to obtain the M-GSM <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>. The perturbation is initialized from a normal distribution <math alttext=\"\\mathcal{N}(0,\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>&#963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,\\sigma)</annotation></semantics></math> and optimized iteratively over <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m6\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math>. The pseudocode is presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A2\" title=\"Appendix B Pseudocode &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "jailbreak",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Let <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmdefinition1.p1.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext class=\"ltx_mathvariant_italic\">jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> be the training set consisting of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"Thmdefinition1.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> jailbreak examples, and <math alttext=\"\\mathcal{D}^{\\text{real}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmdefinition1.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext class=\"ltx_mathvariant_italic\">real</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{real}}</annotation></semantics></math> be the distribution over potential jailbreak inputs in real-world deployment. For a given perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"Thmdefinition1.p1.m4\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math>, the empirical safety risk <math alttext=\"\\widehat{\\mathcal{R}}(\\delta)\" class=\"ltx_Math\" display=\"inline\" id=\"Thmdefinition1.p1.m5\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mo>^</mo></mover><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#948;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\widehat{\\mathcal{R}}(\\delta)</annotation></semantics></math> and the population safety risk <math alttext=\"\\mathcal{R}(\\delta)\" class=\"ltx_Math\" display=\"inline\" id=\"Thmdefinition1.p1.m6\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#8475;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#948;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{R}(\\delta)</annotation></semantics></math> are defined as:</span>\n</p>\n\n",
                "matched_terms": [
                    "jailbreak",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Assume the training set <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext class=\"ltx_mathvariant_italic\">jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> consists of <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m2\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> i.i.d. jailbreak examples sampled from the real-world distribution <math alttext=\"\\mathcal{D}^{\\text{real}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext class=\"ltx_mathvariant_italic\">real</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{real}}</annotation></semantics></math>. Suppose the safety loss <math alttext=\"\\mathcal{L}_{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_italic\">safe</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{safe}}</annotation></semantics></math> is bounded in <math alttext=\"[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,1]</annotation></semantics></math>. Then, for any fixed perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m6\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> and any confidence level <math alttext=\"\\alpha\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\alpha\\in(0,1)</annotation></semantics></math>, with probability at least <math alttext=\"1-\\alpha\" class=\"ltx_Math\" display=\"inline\" id=\"Thmtheorem1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8722;</mo><mi>&#945;</mi></mrow><annotation encoding=\"application/x-tex\">1-\\alpha</annotation></semantics></math>, the following generalization bound holds:</span>\n</p>\n\n",
                "matched_terms": [
                    "jailbreak",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\">Given a benign-task embedding <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>, under Assumption&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption1\" title=\"Assumption 1 (Local Sensitivity Bound of Audio Encoder). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> with small perturbation <math alttext=\"\\Delta\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathbf{E}_{a}</annotation></semantics></math>, we assume that the loss function of downstream ALM tasks (e.g., speech instruction following), denoted as <math alttext=\"\\mathcal{L}_{\\text{ALM}}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext class=\"ltx_mathvariant_italic\">ALM</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ALM}}</annotation></semantics></math>, has a bounded gradient norm with respect to <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>. That is, there exists a constant <math alttext=\"G_{\\max}\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"Thmassumption2.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>max</mi></msub><mo>&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">G_{\\max}\\geq 0</annotation></semantics></math> such that:</span>\n</p>\n\n",
                "matched_terms": [
                    "alm",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset and models.</span> In line with AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite>, we adopt AdvBench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib60\" title=\"\">zou2023universal </a></cite>, a benchmark widely used in text-based jailbreak research, which contains a total of 520 prompts. These prompts are converted into audio using OpenAI&#8217;s text-to-speech (TTS) API to construct <span class=\"ltx_text ltx_font_bold\">AdvBench</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span>, comprising 520 audio queries. We evaluate four state-of-the-art audio-language models: <span class=\"ltx_text ltx_font_bold\">Qwen2</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a></cite>, <span class=\"ltx_text ltx_font_bold\">LLaMA</span>-<span class=\"ltx_text ltx_font_bold\">Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib10\" title=\"\">fang2024llama </a></cite>, <span class=\"ltx_text ltx_font_bold\">Lyra</span>-<span class=\"ltx_text ltx_font_bold\">Base</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib59\" title=\"\">zhong2024lyra </a></cite>, and <span class=\"ltx_text ltx_font_bold\">Qwen2.5</span>-<span class=\"ltx_text ltx_font_bold\">Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib53\" title=\"\">xu2025qwen2 </a></cite>. All models are capable of accepting audio as input and producing either textual responses.</p>\n\n",
                "matched_terms": [
                    "advwave",
                    "jailbreak"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attacks.</span>\nFor the attack methods, we first adopt two SOTA jailbreak approaches specifically designed for ALMs, namely <span class=\"ltx_text ltx_font_bold\">AdvWave</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> and the method proposed by <span class=\"ltx_text ltx_font_bold\">Gupta <span class=\"ltx_text ltx_font_italic\">et al.</span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite>. In addition, we adapt perturbation-based attacks from the traditional domain of audio adversarial attacks into the AdvWave framework, resulting in a variant named <span class=\"ltx_text ltx_font_bold\">AdvWave</span>-<span class=\"ltx_text ltx_font_bold\">P</span>. We further transfer several representative techniques from the text-based jailbreak literature, including <span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Attack (ICA)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, Prompt Automatic Iterative Refinement (PAIR)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib6\" title=\"\">chao2023jailbreaking </a></cite>, and Persuasive Adversarial Prompts (PAP)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib57\" title=\"\">zeng2024johnny </a></cite>. For ICA, we prepend malicious textual demonstrations as context to the audio prompts. For PAIR and PAP, we convert their generated jailbreak texts into audio using OpenAI&#8217;s TTS API, thereby forming the <span class=\"ltx_text ltx_font_bold\">PAIR</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span> and <span class=\"ltx_text ltx_font_bold\">PAP</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span> variants. We classify AdvWave, AdvWave-P, and Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;as <span class=\"ltx_text ltx_font_italic\">acoustic-based</span> attacks, and the remaining three as <span class=\"ltx_text ltx_font_italic\">semantic-based</span> attacks. A detailed description of all attack methods is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS2\" title=\"D.2 Attack Setup &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "pap",
                    "attacks",
                    "pair",
                    "gupta",
                    "advwavep",
                    "papaudio",
                    "advwave",
                    "semanticbased",
                    "jailbreak",
                    "acousticbased",
                    "attack",
                    "pairaudio",
                    "ica"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span>\nFor the defense methods, due to the lack of dedicated defenses targeting jailbreaks in ALMs, we explore two directions of transfer. <span class=\"ltx_text ltx_font_bold\">Type I:</span> From the domain of traditional audio adversarial defenses, we consider three widely adopted techniques&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib12\" title=\"\">ge2023advddos </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib11\" title=\"\">fang2024zero </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib23\" title=\"\">jin2025whispering </a></cite>: (<span class=\"ltx_text ltx_font_italic\">i</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Gaussian Noise</span>, (<span class=\"ltx_text ltx_font_italic\">ii</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Local Smoothing</span>, and (<span class=\"ltx_text ltx_font_italic\">iii</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Downsampling</span>. <span class=\"ltx_text ltx_font_bold\">Type II:</span> From the domain of text-based jailbreak defense, we implement two representative methods: <span class=\"ltx_text ltx_font_bold\">Self-Reminder</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a></cite> and <span class=\"ltx_text ltx_font_bold\">In-Context Defense (ICD)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>. Details of these defense techniques are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS3\" title=\"D.3 Baseline Setup &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "type",
                    "jailbreak"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics.</span>\nWe evaluate the effectiveness of the aforementioned jailbreak attacks and defenses using the <span class=\"ltx_text ltx_font_bold\">Success Rate of Attack (SRoA)</span>. Following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite>, we adopt a well-tuned LLM judge model from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib49\" title=\"\">xie2024sorry </a></cite> to determine whether a jailbreak attempt is successful.\nTo assess model utility, we consider two benchmarks. First, we sample 500 audio clips from the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib33\" title=\"\">panayotov2015librispeech </a></cite>, a standard ASR task, to measure the model&#8217;s basic speech understanding capability, using <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span> as the evaluation metric.\nIn addition, we employ 800 speech samples from AIR-Bench-Chat&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib55\" title=\"\">yang2024air </a></cite> to further evaluate the model&#8217;s audio-to-text interaction performance. For this evaluation, each response is assigned a <span class=\"ltx_text ltx_font_bold\">Response Quality Score (RQS)</span> on a 1-10 scale by DeepSeek-V3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib28\" title=\"\">liu2024deepseek </a></cite>, where higher RQS values indicate stronger model performance.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "attacks",
                    "jailbreak",
                    "attack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ALMGuard&#160;setup.</span>\nDuring the optimization of ALMGuard, we randomly select 50 audio samples from AdvBench-Audio and apply a set of attack algorithms, namely AdvWave, AdvWave-P, and PAIR-Audio, for training-time perturbation optimization. We believe that the selected samples and attack methods are sufficiently representative to enable transferability to unseen examples and attacks. The perturbation duration is set to 30 seconds, consistent with the default input length of Whisper, and the perturbation budget is constrained by <math alttext=\"\\epsilon=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.5</annotation></semantics></math>. We set the value of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p5.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> to 48, and provide a detailed analysis in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.SS4\" title=\"5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "perturbation",
                    "advwavep",
                    "advwave",
                    "attack",
                    "pairaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance on seen attacks.</span>\nWe first evaluate the performance of ALMGuard on three seen attacks used during the optimization of the perturbation: AdvWave, AdvWave-P, and PAIR-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method significantly outperforms all baselines on AdvWave and AdvWave-P, reducing the average SRoA across four ALMs from 53.5% and 68.5% to 4.6% and 7.8%, respectively. This indicates that ALMGuard exhibits strong robustness against acoustic-based attacks. On PAIR-Audio, a representative semantic-based attack, ALMGuard reduces the average SRoA to 26.3%, achieving comparable performance to Self-Reminder and ICD. Notably, ALMGuard consistently achieves a lower SRoA than all baselines against AdvWave-P on every model, and even reduces it to 0 on Qwen2.5-Omni, making the model completely robust - a result that no existing defense has achieved.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "perturbation",
                    "semanticbased",
                    "advwavep",
                    "advwave",
                    "acousticbased",
                    "attack",
                    "pairaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferability to unseen attacks.</span>\nWe evaluate the transferability of ALMGuard on three unseen attacks: Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>, ICA, and PAP-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ALMGuard significantly reduces the average SRoA by 27.4%, 7.4%, and 5.2%, respectively. In particular, the average SRoA on Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;is reduced to only 1.9%, which is the lowest among all attacks. Given that AdvWave and Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;represent the current SOTA ALM-specific jailbreak attacks, we believe that ALMGuard achieves strong robustness against this class of threats.\nFor baselines, we observe that Type I defenses tend to perform better against acoustic-based attacks. In contrast, Type II defenses show significantly better performance on semantic-based attacks.\nThis observation suggests that no existing defense can dominate across all types of attacks.\nIn comparison, ALMGuard consistently outperforms Type I defenses across all attack categories. Compared to Type II defenses, ALMGuard achieves significantly better results on acoustic-based attacks.\nOn average, ALMGuard reduces the overall SRoA to 14.6%, which represents the current SOTA.</p>\n\n",
                "matched_terms": [
                    "type",
                    "attacks",
                    "gupta",
                    "semanticbased",
                    "papaudio",
                    "advwave",
                    "jailbreak",
                    "acousticbased",
                    "attack",
                    "ica"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T2\" title=\"Table 2 &#8227; 5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the performance of our method on two benign benchmarks. On Qwen2-Audio, ALMGuard causes only a slight degradation in performance, increasing the WER on LibriSpeech by 1.85% and decreasing the AIR-Bench-Chat score by 0.56, which we regard as negligible. In contrast, both Self-Reminder and ICD significantly impair ASR performance, increasing the WER by 26.27% and 8.98% respectively, indicating that both baselines considerably disrupt the model&#8217;s understanding of speech semantics. We hypothesize that this is due to Qwen2-Audio being highly sensitive to system prompts, where the presence of Self-Reminder and ICD causes the model to generate refusal responses even for normal ASR inputs. However, on AIR-Bench-Chat, where task instructions are more diverse, both baselines return to relatively normal performance.\nNotably, ALMGuard even improves model performance on Lyra-Base, reducing WER by 1.16% and increasing the AIR-Bench-Chat RQS by 0.15, outperforming all baselines and even the original model without defense.\nFor completeness, we also report results on LLaMA-Omni and Qwen2.5-Omni in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS5\" title=\"D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.5</span></a>.\nOverall, our method demonstrates minimal impact on model utility, suggesting that it can be reliably deployed in real-world ALM systems and significantly enhances practical usability.</p>\n\n",
                "matched_terms": [
                    "alm",
                    "system",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution of M-GSM.</span>\nIn ALMGuard, we employ M-GSM to ensure that the model&#8217;s utility is not significantly affected. To validate the effectiveness of this key component, we conduct an ablation study on Qwen2-Audio by testing three jailbreak attacks and two benign benchmarks, both with and without M-GSM. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "jailbreak"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hyperparameter analyses.</span>\nAn important hyperparameter in our method is the value of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>, which determines the number of Mel-frequency bins to which perturbations are applied. This parameter controls the trade-off between defense effectiveness and the impact on model utility. To investigate its influence, we conduct experiments with <math alttext=\"k\\in\\{0,16,48,96,128\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>48</mn><mo>,</mo><mn>96</mn><mo>,</mo><mn>128</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k\\in\\{0,16,48,96,128\\}</annotation></semantics></math>, where <math alttext=\"k=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">k=0</annotation></semantics></math> corresponds to the undefended setting with no perturbation applied, and <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math> represents the case without masking (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, full-band perturbation).\nWe evaluate the defense performance on PAIR-Audio and the benign-task performance on LibriSpeech. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F3\" title=\"Figure 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, increasing <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> leads to a monotonic decrease in SRoA and a monotonic increase in WER. This trend indicates that while a larger <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> improves robustness, it also introduces more distortion to benign inputs. Since our goal is to preserve utility while maximizing robustness, we identify <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m7\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> as a balanced configuration, where SRoA is reduced to 34.9% and WER remains low at 8.70%.</p>\n\n",
                "matched_terms": [
                    "perturbation",
                    "pairaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive attacks.</span>\nTo further demonstrate the superiority of our method, we consider a more practical and challenging setting where the attacker has full knowledge of the defense mechanism, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, a white-box threat model. Under this setting, we evaluate adaptive AdvWave attacks on LLaMA-Omni by optimizing the adversarial suffix in the presence of each of the six defense methods.\nAs an example, when attacking ALMGuard, the attacker adds our well-trained SAP to the input at each iteration during AdvWave optimization.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, our method still achieves the best defense performance among all methods, reducing the SRoA by 11.9% compared to the original attack, even under this strongest threat model. In contrast, all baseline defenses yield an SRoA above 70% under adaptive attacks.\nInterestingly, for traditional audio defenses such as local smoothing, incorporating the corresponding transformations into the optimization process actually increases SRoA. We hypothesize that this is because these operations act similarly to data augmentation, which improves the robustness of the adversarial suffix and thus enhances the attack strength.\nIn summary, our method demonstrates the strongest resistance to adaptive attacks among all evaluated defenses.</p>\n\n",
                "matched_terms": [
                    "summary",
                    "evaluated",
                    "attacks",
                    "advwave",
                    "attack",
                    "suffix",
                    "knowledge",
                    "original",
                    "whitebox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce ALMGuard, a novel framework that pioneers activating inherent safety shortcuts in ALMs via universal SAPs. Our M-GSM technique precisely guides these SAPs to critical frequency regions, enabling robust jailbreak mitigation while preserving model utility.\nEvaluations across six attack methods and four SOTA ALMs show that ALMGuard achieves overall better performance compared to existing defenses, while keeping its impact on model utility well-controlled. This offers a new perspective on enhancing robustness for multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "attack",
                    "jailbreak"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the pseudocode of ALMGuard in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#algorithm1\" title=\"In Appendix B Pseudocode &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The algorithm begins by constructing the training dataset <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> through the application of a jailbreak algorithm set <math alttext=\"\\mathcal{A}_{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\text{jb}}</annotation></semantics></math> to a curated jailbreak prompt set <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>. Based on this dataset, we compute the average sensitivity scores to derive the M-GSM <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, which identifies Mel-frequency bins that are critical for jailbreak mitigation yet minimally influential on benign performance. The universal perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> is initialized from a Gaussian distribution <math alttext=\"\\mathcal{N}(0,\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m6\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>&#963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,\\sigma)</annotation></semantics></math>, and iteratively optimized over the dataset to minimize the empirical safety risk.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "jailbreak",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"\\mathbf{E}^{\\prime}_{a}=\\mathbf{E}_{a}+\\Delta\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p8.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119812;</mi><mi>a</mi><mo>&#8242;</mo></msubsup><mo>=</mo><mrow><msub><mi>&#119812;</mi><mi>a</mi></msub><mo>+</mo><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{E}^{\\prime}_{a}=\\mathbf{E}_{a}+\\Delta\\mathbf{E}_{a}</annotation></semantics></math> be the audio embedding after applying the perturbation <math alttext=\"m\\odot\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p8.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8857;</mo><mi>&#948;</mi></mrow><annotation encoding=\"application/x-tex\">m\\odot\\delta</annotation></semantics></math>. Denote by <math alttext=\"\\mathcal{L}_{\\mathrm{ALM}}(\\mathbf{E}_{a},\\mathbf{E}_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p8.m3\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>ALM</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119812;</mi><mi>a</mi></msub><mo>,</mo><msub><mi>&#119812;</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{ALM}}(\\mathbf{E}_{a},\\mathbf{E}_{t})</annotation></semantics></math> the downstream ALM loss given audio embedding <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p8.m4\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math> and text embedding <math alttext=\"\\mathbf{E}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p8.m5\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{t}</annotation></semantics></math>. We focus on the change in loss:</p>\n\n",
                "matched_terms": [
                    "alm",
                    "perturbation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AdvWave.</span>\nA white-box jailbreak attack specifically designed for ALMs. It optimizes an adversarial suffix using cross-entropy loss. We set the suffix length to 44,100 samples (approximately 2.76 seconds) at a sampling rate of 16kHz. Following the original paper, we implement the dynamic target selection mechanism, which constructs a unique jailbreak target for each query. The maximum number of iterations is set to 3000, and the attack is considered successful when the loss falls below 0.1, at which point optimization terminates.</p>\n\n",
                "matched_terms": [
                    "advwave",
                    "jailbreak",
                    "attack",
                    "target",
                    "suffix",
                    "original",
                    "whitebox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AdvWave</span>-<span class=\"ltx_text ltx_font_bold\">P.</span>\nA variant of AdvWave is designed to better align with the audio adversarial attack paradigm. The suffix is replaced with an additive perturbation constrained under an <math alttext=\"\\ell_{\\infty}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I1.i2.p1.m1\" intent=\":literal\"><semantics><msub><mi mathvariant=\"normal\">&#8467;</mi><mi mathvariant=\"normal\">&#8734;</mi></msub><annotation encoding=\"application/x-tex\">\\ell_{\\infty}</annotation></semantics></math> budget of 0.03. All other settings remain unchanged.</p>\n\n",
                "matched_terms": [
                    "perturbation",
                    "advwavep",
                    "advwave",
                    "suffix",
                    "attack"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PAIR</span>-<span class=\"ltx_text ltx_font_bold\">Audio.</span>\nA black-box adversarial attack that iteratively queries the target LLM using inputs generated through interaction with an attacker LLM, requiring no human intervention. We follow the original PAIR procedure without modification to its core logic. The only changes are replacing the target LLM with our selected ALMs and converting the attacker LLM&#8217;s dialogue into audio using TTS. We use GPT-3.5-Turbo as the attacker LLM, consistent with the original paper.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "pair",
                    "attack",
                    "target",
                    "blackbox",
                    "original",
                    "pairaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gupta <span class=\"ltx_text ltx_font_italic\">et al</span>.</span>\nA white-box jailbreak method that evaluates various attack forms and stealth enhancement techniques. We adopt the most effective universal prefix proposed in their work. It is trained on 100 randomly selected samples from AdvBench-Audio and tested on the remaining 420.</p>\n\n",
                "matched_terms": [
                    "prefix",
                    "gupta",
                    "jailbreak",
                    "attack",
                    "whitebox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Attack.</span>\nA black-box jailbreak strategy that prepends a few successful attack demonstrations to the input prompt to induce in-context learning. Following the authors&#8217; recommended configuration, we use 10-shot prompts for LLaMA-Omni, Lyra-Base, and Qwen2.5-Omni. Due to context length limitations, we use 5-shot prompts for Qwen2-Audio.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "attack",
                    "jailbreak",
                    "blackbox"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PAP</span>-<span class=\"ltx_text ltx_font_bold\">Audio.</span>\nA black-box jailbreak method inspired by social science-based persuasion strategies. We convert 145 successful jailbreak prompts provided by the authors into audio via TTS for evaluation.</p>\n\n",
                "matched_terms": [
                    "jailbreak",
                    "blackbox",
                    "papaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Self</span>-<span class=\"ltx_text ltx_font_bold\">Reminder.</span>\nA psychologically inspired mechanism that reinforces the model&#8217;s responsibility awareness by repeating its alignment commitments twice in the system prompt.</p>\n\n",
                "matched_terms": [
                    "system",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Defense.</span>\nProposed in the same work as ICA, this method uses 2-shot in-context learning to guide the model toward safe behavior. We follow the original configuration and apply 2-shot demonstrations in the context prompt.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "original",
                    "ica"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Self-Reminder, we use the following system prompt:</p>\n\n",
                "matched_terms": [
                    "system",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate whether incorporating <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide the gradient direction is effective, we conduct a preliminary comparison study. Under the setting of <math alttext=\"k=16\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">k=16</annotation></semantics></math>, we evaluated both model utility and defense performance against AdvWave-P on Qwen2-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T6\" title=\"Table 6 &#8227; D.4 Ablation on &#8466;_&quot;ASR&quot; &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, when the M-GSM is applied, the presence or absence of <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> has negligible effect on both metrics. The WER and SRoA differ by only 0.07% and 1.5%, which can be attributed to random variations. In contrast, when M-GSM is removed, using <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide optimization fails to preserve utility: the WER increases by 17.88% compared to the undefended case.\nIn summary, we conclude that <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> does not provide a meaningful benefit in achieving the goal of activating safety shortcuts to mitigate jailbreaks while preserving model utility. Therefore, we exclude <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> from the final design of ALMGuard.</p>\n\n",
                "matched_terms": [
                    "summary",
                    "evaluated",
                    "advwavep"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the evaluation results on AIR-Bench-Chat for LLaMA-Omni and Qwen2.5-Omni in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T7\" title=\"Table 7 &#8227; D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. On LLaMA-Omni, our method achieves a RQS of 4.68, outperforming most baselines. Notably, while ICD provides the strongest defense among baselines on this model, it also causes the most significant degradation in utility. It is worth noting that for Qwen2.5-Omni, we set <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>, whereas <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> is used for other models. This adjustment is due to our observation that smaller values of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> fail to effectively defend against semantic-based attacks such as PAIR-Audio. We suspect this is primarily because Qwen2.5-Omni is highly sensitive to such attacks, as evidenced by its noticeably lower robustness on these attacks compared to other models. Nevertheless, given that Qwen2.5-Omni is inherently stronger, it still achieves a RQS of 6.02 even with <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>. Considering the trade-off between defense effectiveness and model utility, we believe this result is acceptable. In practice, we recommend users to adjust <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> flexibly based on specific deployment requirements.</p>\n\n",
                "matched_terms": [
                    "pairaudio",
                    "attacks",
                    "semanticbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Model-Specific Traits.</span> We observe that the effectiveness of jailbreak attacks and corresponding defenses is closely tied to the intrinsic characteristics of the target model. For instance, as discussed earlier, Qwen2.5-Omni appears particularly vulnerable to semantic-based attacks, with PAP achieves a high SRoA of 77.9% on this model. Similarly, Lyra-Base exhibits strong sensitivity to prompt context, making it especially susceptible to attacks like ICA.\nIn real-world deployment, it is advisable to consider the model-specific traits when designing comprehensive defense strategies, potentially combining multiple techniques for better protection. Nevertheless, disregarding such model-specific factors, our method consistently demonstrates the universal effectiveness to activate safety-aligned shortcuts across all models, leading to substantial reductions in jailbreak success rates.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "pap",
                    "attacks",
                    "semanticbased",
                    "jailbreak",
                    "target",
                    "ica"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span>\nDespite its strong overall performance, we observe that our perturbation-based defense has room for improvement against semantic-based attacks. In some cases, ALMGuard underperforms compared to the best-performing baselines. We attribute this to the fact that our acoustic perturbation is mainly optimized to activate ALMs&#8217; inherent acoustic-related safety shortcuts to defend against acoustic-based attacks, but does not explicitly target the semantic intent of adversarial prompts. Future improvements may involve integrating semantic-level and intent-aware objectives during optimization. Additionally, given the plug-and-play nature of our method, it could be integrated with complementary defense techniques to form a more comprehensive defense framework.</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "perturbation",
                    "semanticbased",
                    "acousticbased",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>\n\n",
                "matched_terms": [
                    "attacks",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.</p>\n\n",
                "matched_terms": [
                    "llm",
                    "original"
                ]
            }
        ]
    },
    "A4.T6": {
        "source_file": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio–Language Models",
        "caption": "Table 6: Comparison of model utility and defense effectiveness on Qwen2-Audio with and without M-GSM and ℒASR\\mathcal{L}_{\\text{ASR}}.",
        "body": "Defense\n\nWER (%) ↓\\downarrow\n\n\nSRoA (%) ↓\\downarrow\n\n\n\n\n\nNone\n6.85\n80.8\n\n\n\nw/o M-GSM,  w/o ℒASR\\mathcal{L}_{\\text{ASR}}\n\n26.85\n12.7\n\n\n\nw/o M-GSM,  w/ ℒASR\\mathcal{L}_{\\text{ASR}}\n\n24.73\n10.8\n\n\n\n\\rowcolorgray!20\nw/ M-GSM,  w/o ℒASR\\mathcal{L}_{\\text{ASR}}\n\n7.61\n12.5\n\n\n\nw/ M-GSM,  w/ ℒASR\\mathcal{L}_{\\text{ASR}}\n\n7.68\n11.0",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Defense</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">WER (%)</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">SRoA (%)</span><span class=\"ltx_text\" style=\"font-size:80%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.800em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">None</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">80.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">w/o M-GSM,&#8201; w/o </span><math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.800em\">&#8466;</mi><mtext mathsize=\"0.800em\">ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">26.85</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">12.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">w/o M-GSM,&#8201; w/ </span><math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.800em\">&#8466;</mi><mtext mathsize=\"0.800em\">ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">24.73</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span><span class=\"ltx_text\" style=\"font-size:80%;\">gray!20\nw/ M-GSM,&#8201; w/o </span><math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.800em\">&#8466;</mi><mtext mathsize=\"0.800em\">ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.61</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">12.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">w/ M-GSM,&#8201; w/ </span><math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m8\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.800em\">&#8466;</mi><mtext mathsize=\"0.800em\">ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "model",
            "utility",
            "defense",
            "without",
            "mgsm",
            "ℒasrmathcalltextasr",
            "↓downarrow",
            "rowcolorgray20",
            "sroa",
            "wer",
            "qwen2audio",
            "comparison",
            "none",
            "effectiveness"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To investigate whether incorporating <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide the gradient direction is effective, we conduct a preliminary comparison study. Under the setting of <math alttext=\"k=16\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">k=16</annotation></semantics></math>, we evaluated both model utility and defense performance against AdvWave-P on Qwen2-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T6\" title=\"Table 6 &#8227; D.4 Ablation on &#8466;_&quot;ASR&quot; &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, when the M-GSM is applied, the presence or absence of <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> has negligible effect on both metrics. The WER and SRoA differ by only 0.07% and 1.5%, which can be attributed to random variations. In contrast, when M-GSM is removed, using <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide optimization fails to preserve utility: the WER increases by 17.88% compared to the undefended case.\nIn summary, we conclude that <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> does not provide a meaningful benefit in achieving the goal of activating safety shortcuts to mitigate jailbreaks while preserving model utility. Therefore, we exclude <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> from the final design of ALMGuard.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats.\nTo address this issue, we propose <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, the first defense framework tailored to ALMs.\nBased on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time.\nTo better sift out effective triggers while preserving the model&#8217;s utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding.\nBoth theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, ALMGuard reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/WeifeiJin/ALMGuard\" title=\"\">https://github.com/WeifeiJin/ALMGuard</a>.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "model",
                    "utility",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Why ALM-specific defense?</span>\nRecent research proposing jailbreak attacks tailored to ALMs&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite> has substantiated that the integration of audio modality introduces distinct and previously unexplored threats.\nWe observe that existing defense methods transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a></cite> are largely ineffective in mitigating these ALM-specific threats.\nThis can be attributed to a lack of consideration of the behavioral diversity and inherent complexity of ALMs, as well as an insufficient adaptation to the distinct characteristics of the audio modality. A similar limitation also exists on the attack side&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "defense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our stance.</span>\nThe limitations of existing defenses against ALM-specific jailbreaks motivate a novel approach. We hypothesize that well-aligned ALMs inherently possess <em class=\"ltx_emph ltx_font_italic\">safety shortcuts</em>, which are latent pathways or input sensitivities that, if correctly triggered, can steer models towards safer behavior and mitigate jailbreaks. These differ from explicit safety alignments, representing intrinsic model properties that can be leveraged for defense.\nThe primary challenge is to activate these safety shortcuts efficiently and harmlessly at inference time. We suggest that this can be achieved by applying a lightweight, universal acoustic perturbation to the input, referred to as the <span class=\"ltx_text ltx_font_bold\">Shortcut Activation Perturbation (SAP)</span>. SAP is designed to engage these safety-conducive pathways without requiring any model retraining. However, to prevent such a perturbation from degrading benign task performance, its application must be precisely targeted.\nTo this end, we introduce the <span class=\"ltx_text ltx_font_bold\">Mel-Gradient Sparse Mask (M-GSM)</span>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, M-GSM identifies a sparse set of Mel-frequency bins that are highly influential for jailbreak mitigation yet largely inconsequential for benign speech understanding, as measured by automatic speech recognition (ASR) tasks. Our framework, <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, then synergistically employs M-GSM to guide the application of the universal SAP only to Mel bins that are security-critical yet benign-insensitive.\nThis targeted strategy allows ALMGuard to effectively activate safety shortcuts for robust defense while preserving model utility (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, performance on benign inputs).</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "defense",
                    "without",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and results.</span>\nWe evaluate the proposed method across four state-of-the-art (SOTA) ALMs and six representative jailbreak attacks. On Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a></cite>, ALMGuard reduces the success rate of the two most recent ALM-specific attacks, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> and Gupta <span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite>, to 3.1% and 0.5% respectively, outperforming all baselines and establishing itself as the new SOTA defense.\nMoreover, ALMGuard generalizes effectively to unseen attacks. As a lightweight defense framework, it enables near zero-cost deployment with negligible inference overhead. Evaluations on two benign benchmarks further confirm that ALMGuard does not noticeably degrade model utility. In addition, it achieves strong robustness against adaptive attacks.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "qwen2audio",
                    "model",
                    "utility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core technical innovation within ALMGuard involves a universal SAP which is precisely guided by our M-GSM to engage these safety shortcuts, targeting sparse acoustic regions for maximal defense effectiveness with minimal utility impact.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "utility",
                    "effectiveness",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through extensive evaluations, complemented by theoretical analyses, we show that ALMGuard achieves SOTA defense against six jailbreaks on four SOTA ALMs, with strong generalization, high benign-task utility, and negligible inference overhead.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "utility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Jailbreaks and defenses.</span>\nJailbreaking was initially introduced in the context of text-based LLMs, where attackers craft malicious prompts to bypass the model&#8217;s built-in safety mechanisms and induce it to generate harmful content. Existing jailbreak techniques for LLMs can be broadly categorized into two types: suffix-based and semantic-based. Suffix-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib60\" title=\"\">zou2023universal </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib27\" title=\"\">liao2024amplegcg </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib3\" title=\"\">basani2024gasp </a></cite> append an adversarial suffix after a harmful query, while semantic-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib29\" title=\"\">liu2023autodan </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib6\" title=\"\">chao2023jailbreaking </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib31\" title=\"\">mehrotra2024tree </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib57\" title=\"\">zeng2024johnny </a></cite> manipulate the prompt content using strategies such as persuasion or logical traps to elicit the desired malicious response.\nIn the context of ALMs, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> is a suffix-based attack that appends an adversarial noise segment, while Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite> explore prefix and perturbation formats. Modern foundation models are typically enhanced with preference-based alignment (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, reinforcement learning from human feedback (RLHF)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib7\" title=\"\">christiano2017deep </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib32\" title=\"\">ouyang2022training </a></cite> and direct preference optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib37\" title=\"\">rafailov2023direct </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib2\" title=\"\">amini2024direct </a></cite>) where human judgments guide model fine-tuning. As jailbreak attacks continue to emerge, corresponding defense mechanisms have been proposed, including input-level detection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib1\" title=\"\">alon2023detecting </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib38\" title=\"\">robey2023smoothllm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib20\" title=\"\">inan2023llama </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib34\" title=\"\">phute2023llm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib50\" title=\"\">xie2024gradsafe </a></cite> and mitigation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib21\" title=\"\">jain2023baseline </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib54\" title=\"\">xu2024safedecoding </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, and output-level intervention&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib45\" title=\"\">wang2024selfdefend </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib35\" title=\"\">qian2024hsf </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib46\" title=\"\">wang2025vulnerability </a></cite>.\nHowever, defenses tailored to ALMs remain underexplored. In this paper, we address this gap by proposing a dedicated framework that targets their unique vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "defense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our overall objective is to identify an acoustic signal, SAP, that can effectively activate the model&#8217;s inherent safety shortcuts to mitigate jailbreaks, without significantly degrading the model&#8217;s performance on benign inputs. We formulate this as an optimization problem aimed at making the model output safe responses to jailbreaking audio when subjected to this perturbation.\nSpecifically, given an ALM <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>&#952;</mi></msub><annotation encoding=\"application/x-tex\">f_{\\theta}</annotation></semantics></math>, a set of malicious instructions <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>, and a set of jailbreak algorithms <math alttext=\"\\mathcal{A}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{A}^{\\text{jb}}</annotation></semantics></math>, our goal is formulated as:</p>\n\n",
                "matched_terms": [
                    "model",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathcal{X}^{\\text{bg}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>bg</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{bg}}</annotation></semantics></math> refers to benign task prompts,\n<math alttext=\"M(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">M(\\cdot)</annotation></semantics></math> is the Mel-spectrogram transformation, and <math alttext=\"\\text{Err}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mtext>Err</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{Err}(\\cdot,\\cdot)</annotation></semantics></math> measures the prediction difference between the perturbed and original inputs. Note that we choose to apply perturbations on the Mel-spectrogram rather than the raw waveform, because we find that perturbing the Mel-spectrogram leads to less degradation in model utility (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, utility with clean user inputs) under the same optimization settings, with details provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS5\" title=\"D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.5</span></a>. The safety loss <math alttext=\"\\mathcal{L}_{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>safe</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{safe}}</annotation></semantics></math> can be instantiated as the cross-entropy loss between the model output and the safe target sequence <math alttext=\"y^{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>safe</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{safe}}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the constraint in Equation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S3.E2\" title=\"In 3.1 Problem Formulation &#8227; 3 ALMGuard &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), we initially attempt to introduce a loss function based on benign tasks to guide the direction of the perturbation and prevent it from affecting model utility. Specifically, we use Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib36\" title=\"\">radford2023robust </a></cite> to transcribe the input examples and compute the cross-entropy loss between the transcription result and the ground-truth text <math alttext=\"y^{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msup><mi>y</mi><mtext>ASR</mtext></msup><annotation encoding=\"application/x-tex\">y^{\\text{ASR}}</annotation></semantics></math>, which we denote as <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "ℒasrmathcalltextasr",
                    "model",
                    "utility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, experimental results show that this auxiliary loss fails to effectively reduce the impact of perturbations on model utility. Detailed results can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS4\" title=\"D.4 Ablation on &#8466;_&quot;ASR&quot; &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further investigate this limitation, we conduct a deeper analysis and observe that only a small subset of frequency bands contribute meaningfully to jailbreak mitigation, while most Mel bins are insensitive to the defense objective, as shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Motivated by this observation, we propose to apply perturbations only to the most critical frequency bands and filter out the rest. This greatly reduces the perturbation region and thus minimizes its impact on model utility.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "model",
                    "utility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To this end, we design the Mel-Gradient Sparse Mask (M-GSM). For each Mel bin <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math>, we compute its gradient sensitivity with respect to both <math alttext=\"\\mathcal{L}_{\\text{safe}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>safe</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{safe}}</annotation></semantics></math> and <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> by averaging over the <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> time frames:</p>\n\n",
                "matched_terms": [
                    "ℒasrmathcalltextasr",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This bound shows that the performance impact on benign tasks is proportional to the tunable hyperparameters, <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> and <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (through <math alttext=\"d_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">d_{k}</annotation></semantics></math>), and two stability-related constants, <math alttext=\"L_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>enc</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{enc}}</annotation></semantics></math> and <math alttext=\"G_{\\max}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>max</mi></msub><annotation encoding=\"application/x-tex\">G_{\\max}</annotation></semantics></math>. The design of M-GSM aims to apply perturbations to regions where these stability factors collectively result in minimal adverse impact on benign tasks. This theoretical upper bound supports the claim that ALMGuard can preserve model performance on benign tasks, which aligns with our empirical results shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.SS3\" title=\"5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics.</span>\nWe evaluate the effectiveness of the aforementioned jailbreak attacks and defenses using the <span class=\"ltx_text ltx_font_bold\">Success Rate of Attack (SRoA)</span>. Following prior work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite>, we adopt a well-tuned LLM judge model from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib49\" title=\"\">xie2024sorry </a></cite> to determine whether a jailbreak attempt is successful.\nTo assess model utility, we consider two benchmarks. First, we sample 500 audio clips from the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib33\" title=\"\">panayotov2015librispeech </a></cite>, a standard ASR task, to measure the model&#8217;s basic speech understanding capability, using <span class=\"ltx_text ltx_font_bold\">Word Error Rate (WER)</span> as the evaluation metric.\nIn addition, we employ 800 speech samples from AIR-Bench-Chat&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib55\" title=\"\">yang2024air </a></cite> to further evaluate the model&#8217;s audio-to-text interaction performance. For this evaluation, each response is assigned a <span class=\"ltx_text ltx_font_bold\">Response Quality Score (RQS)</span> on a 1-10 scale by DeepSeek-V3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib28\" title=\"\">liu2024deepseek </a></cite>, where higher RQS values indicate stronger model performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "sroa",
                    "wer",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance on seen attacks.</span>\nWe first evaluate the performance of ALMGuard on three seen attacks used during the optimization of the perturbation: AdvWave, AdvWave-P, and PAIR-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method significantly outperforms all baselines on AdvWave and AdvWave-P, reducing the average SRoA across four ALMs from 53.5% and 68.5% to 4.6% and 7.8%, respectively. This indicates that ALMGuard exhibits strong robustness against acoustic-based attacks. On PAIR-Audio, a representative semantic-based attack, ALMGuard reduces the average SRoA to 26.3%, achieving comparable performance to Self-Reminder and ICD. Notably, ALMGuard consistently achieves a lower SRoA than all baselines against AdvWave-P on every model, and even reduces it to 0 on Qwen2.5-Omni, making the model completely robust - a result that no existing defense has achieved.</p>\n\n",
                "matched_terms": [
                    "model",
                    "defense",
                    "sroa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferability to unseen attacks.</span>\nWe evaluate the transferability of ALMGuard on three unseen attacks: Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>, ICA, and PAP-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ALMGuard significantly reduces the average SRoA by 27.4%, 7.4%, and 5.2%, respectively. In particular, the average SRoA on Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;is reduced to only 1.9%, which is the lowest among all attacks. Given that AdvWave and Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;represent the current SOTA ALM-specific jailbreak attacks, we believe that ALMGuard achieves strong robustness against this class of threats.\nFor baselines, we observe that Type I defenses tend to perform better against acoustic-based attacks. In contrast, Type II defenses show significantly better performance on semantic-based attacks.\nThis observation suggests that no existing defense can dominate across all types of attacks.\nIn comparison, ALMGuard consistently outperforms Type I defenses across all attack categories. Compared to Type II defenses, ALMGuard achieves significantly better results on acoustic-based attacks.\nOn average, ALMGuard reduces the overall SRoA to 14.6%, which represents the current SOTA.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "defense",
                    "sroa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T2\" title=\"Table 2 &#8227; 5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the performance of our method on two benign benchmarks. On Qwen2-Audio, ALMGuard causes only a slight degradation in performance, increasing the WER on LibriSpeech by 1.85% and decreasing the AIR-Bench-Chat score by 0.56, which we regard as negligible. In contrast, both Self-Reminder and ICD significantly impair ASR performance, increasing the WER by 26.27% and 8.98% respectively, indicating that both baselines considerably disrupt the model&#8217;s understanding of speech semantics. We hypothesize that this is due to Qwen2-Audio being highly sensitive to system prompts, where the presence of Self-Reminder and ICD causes the model to generate refusal responses even for normal ASR inputs. However, on AIR-Bench-Chat, where task instructions are more diverse, both baselines return to relatively normal performance.\nNotably, ALMGuard even improves model performance on Lyra-Base, reducing WER by 1.16% and increasing the AIR-Bench-Chat RQS by 0.15, outperforming all baselines and even the original model without defense.\nFor completeness, we also report results on LLaMA-Omni and Qwen2.5-Omni in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS5\" title=\"D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.5</span></a>.\nOverall, our method demonstrates minimal impact on model utility, suggesting that it can be reliably deployed in real-world ALM systems and significantly enhances practical usability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "defense",
                    "without",
                    "wer",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution of M-GSM.</span>\nIn ALMGuard, we employ M-GSM to ensure that the model&#8217;s utility is not significantly affected. To validate the effectiveness of this key component, we conduct an ablation study on Qwen2-Audio by testing three jailbreak attacks and two benign benchmarks, both with and without M-GSM. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "utility",
                    "without",
                    "mgsm",
                    "qwen2audio",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In terms of defense effectiveness, the results with and without M-GSM show similar performance, both achieving over 50% reduction in average SRoA. However, regarding model utility, we observe a clear distinction. Without M-GSM, the WER on the ASR task increases by 20%, which substantially degrades the model&#8217;s ability to understand speech. In addition, the RQS drops by 1.17. In contrast, with M-GSM enabled, the fluctuations in WER and RQS are limited to within approximately 2% and 0.5, respectively, indicating a negligible impact on utility.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "defense",
                    "without",
                    "mgsm",
                    "sroa",
                    "wer",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hyperparameter analyses.</span>\nAn important hyperparameter in our method is the value of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>, which determines the number of Mel-frequency bins to which perturbations are applied. This parameter controls the trade-off between defense effectiveness and the impact on model utility. To investigate its influence, we conduct experiments with <math alttext=\"k\\in\\{0,16,48,96,128\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>0</mn><mo>,</mo><mn>16</mn><mo>,</mo><mn>48</mn><mo>,</mo><mn>96</mn><mo>,</mo><mn>128</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">k\\in\\{0,16,48,96,128\\}</annotation></semantics></math>, where <math alttext=\"k=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m3\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">k=0</annotation></semantics></math> corresponds to the undefended setting with no perturbation applied, and <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math> represents the case without masking (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, full-band perturbation).\nWe evaluate the defense performance on PAIR-Audio and the benign-task performance on LibriSpeech. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F3\" title=\"Figure 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, increasing <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> leads to a monotonic decrease in SRoA and a monotonic increase in WER. This trend indicates that while a larger <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> improves robustness, it also introduces more distortion to benign inputs. Since our goal is to preserve utility while maximizing robustness, we identify <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS4.p5.m7\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> as a balanced configuration, where SRoA is reduced to 34.9% and WER remains low at 8.70%.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "defense",
                    "without",
                    "sroa",
                    "wer",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive attacks.</span>\nTo further demonstrate the superiority of our method, we consider a more practical and challenging setting where the attacker has full knowledge of the defense mechanism, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, a white-box threat model. Under this setting, we evaluate adaptive AdvWave attacks on LLaMA-Omni by optimizing the adversarial suffix in the presence of each of the six defense methods.\nAs an example, when attacking ALMGuard, the attacker adds our well-trained SAP to the input at each iteration during AdvWave optimization.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, our method still achieves the best defense performance among all methods, reducing the SRoA by 11.9% compared to the original attack, even under this strongest threat model. In contrast, all baseline defenses yield an SRoA above 70% under adaptive attacks.\nInterestingly, for traditional audio defenses such as local smoothing, incorporating the corresponding transformations into the optimization process actually increases SRoA. We hypothesize that this is because these operations act similarly to data augmentation, which improves the robustness of the adversarial suffix and thus enhances the attack strength.\nIn summary, our method demonstrates the strongest resistance to adaptive attacks among all evaluated defenses.</p>\n\n",
                "matched_terms": [
                    "model",
                    "defense",
                    "sroa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce ALMGuard, a novel framework that pioneers activating inherent safety shortcuts in ALMs via universal SAPs. Our M-GSM technique precisely guides these SAPs to critical frequency regions, enabling robust jailbreak mitigation while preserving model utility.\nEvaluations across six attack methods and four SOTA ALMs show that ALMGuard achieves overall better performance compared to existing defenses, while keeping its impact on model utility well-controlled. This offers a new perspective on enhancing robustness for multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "utility",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rationality analyses of Assumption&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption1\" title=\"Assumption 1 (Local Sensitivity Bound of Audio Encoder). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</span>\nThe core idea of M-GSM is to identify and select those Mel&#8208;frequency bins <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> for which the gradient of the ASR task loss <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> is small (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, <math alttext=\"g^{a}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>f</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">g^{a}_{f}</annotation></semantics></math> is low), while the gradient of the jailbreak mitigation objective is large (i.e. <math alttext=\"g^{s}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>f</mi><mi>s</mi></msubsup><annotation encoding=\"application/x-tex\">g^{s}_{f}</annotation></semantics></math> is high). In other words, the regions where M-GSM applies perturbations are chosen to be those that minimally affect benign speech understanding, as measured by ASR.</p>\n\n",
                "matched_terms": [
                    "ℒasrmathcalltextasr",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If, on the other hand, the encoder <math alttext=\"f_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mtext>enc</mtext></msub><annotation encoding=\"application/x-tex\">f_{\\text{enc}}</annotation></semantics></math> exhibits very high local sensitivity in some band (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, a large local Lipschitz constant <math alttext=\"L_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>enc</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{enc}}</annotation></semantics></math> causing a large change in <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>), then that change in <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math> will be further amplified by the decoder and lead to a significant increase in <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p3.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math>. Such a band would therefore be avoided by M-GSM, which prefers regions of low ASR sensitivity (small <math alttext=\"g^{a}_{f}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p3.m6\" intent=\":literal\"><semantics><msubsup><mi>g</mi><mi>f</mi><mi>a</mi></msubsup><annotation encoding=\"application/x-tex\">g^{a}_{f}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "ℒasrmathcalltextasr",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rationality analyses of Assumption&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#Thmassumption2\" title=\"Assumption 2 (Local Sensitivity Bound of LLM Backbone). &#8227; 4.2 Bounded Impact on Benign Examples &#8227; 4 Theoretical Analyses &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</span>\nWe focus on how the LLM backbone <math alttext=\"f_{\\mathrm{LLM}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m1\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>LLM</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{LLM}}</annotation></semantics></math> responds to the small audio embedding perturbations <math alttext=\"\\Delta\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#916;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>&#119812;</mi><mi>a</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\Delta\\mathbf{E}_{a}</annotation></semantics></math> that have already been &#8220;filtered&#8221; and &#8220;restricted&#8221; by the encoder <math alttext=\"f_{\\mathrm{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m3\" intent=\":literal\"><semantics><msub><mi>f</mi><mi>enc</mi></msub><annotation encoding=\"application/x-tex\">f_{\\mathrm{enc}}</annotation></semantics></math> and M-GSM. Although the exact global Lipschitz constant of an LLM is infeasible to compute, it is reasonable to assume that around its benign operating points (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, within the local neighborhood of a clean embedding <math alttext=\"\\mathbf{E}_{a}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.SS2.p5.m4\" intent=\":literal\"><semantics><msub><mi>&#119812;</mi><mi>a</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{E}_{a}</annotation></semantics></math>), the model exhibits relative smoothness and stability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mgsm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Defense.</span>\nProposed in the same work as ICA, this method uses 2-shot in-context learning to guide the model toward safe behavior. We follow the original configuration and apply 2-shot demonstrations in the context prompt.</p>\n\n",
                "matched_terms": [
                    "model",
                    "defense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the evaluation results on AIR-Bench-Chat for LLaMA-Omni and Qwen2.5-Omni in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T7\" title=\"Table 7 &#8227; D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. On LLaMA-Omni, our method achieves a RQS of 4.68, outperforming most baselines. Notably, while ICD provides the strongest defense among baselines on this model, it also causes the most significant degradation in utility. It is worth noting that for Qwen2.5-Omni, we set <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>, whereas <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> is used for other models. This adjustment is due to our observation that smaller values of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> fail to effectively defend against semantic-based attacks such as PAIR-Audio. We suspect this is primarily because Qwen2.5-Omni is highly sensitive to such attacks, as evidenced by its noticeably lower robustness on these attacks compared to other models. Nevertheless, given that Qwen2.5-Omni is inherently stronger, it still achieves a RQS of 6.02 even with <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>. Considering the trade-off between defense effectiveness and model utility, we believe this result is acceptable. In practice, we recommend users to adjust <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> flexibly based on specific deployment requirements.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "model",
                    "utility",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Model-Specific Traits.</span> We observe that the effectiveness of jailbreak attacks and corresponding defenses is closely tied to the intrinsic characteristics of the target model. For instance, as discussed earlier, Qwen2.5-Omni appears particularly vulnerable to semantic-based attacks, with PAP achieves a high SRoA of 77.9% on this model. Similarly, Lyra-Base exhibits strong sensitivity to prompt context, making it especially susceptible to attacks like ICA.\nIn real-world deployment, it is advisable to consider the model-specific traits when designing comprehensive defense strategies, potentially combining multiple techniques for better protection. Nevertheless, disregarding such model-specific factors, our method consistently demonstrates the universal effectiveness to activate safety-aligned shortcuts across all models, leading to substantial reductions in jailbreak success rates.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "model",
                    "defense",
                    "sroa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presents heatmaps of M-GSM sensitivity score rankings for each model in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A6.F5\" title=\"Figure 5 &#8227; F.1 Visualization of M-GSM &#8227; Appendix F Visualizations and Examples &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. The results show a strong similarity across the M-GSM masks for the four models, with their top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A6.SS1.p1.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> highest-ranked bins showing significant overlap. This commonality suggests, on one hand, that our method is generalizable across different models. On the other hand, it also reveals that the inherent latent safety shortcuts within these diverse ALMs may share considerable similarities, thus allowing them to be activated in a consistent manner, potentially by the same universal perturbations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mgsm"
                ]
            }
        ]
    },
    "A4.T7": {
        "source_file": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio–Language Models",
        "caption": "Table 7: \nAIR-Bench-Chat results on LLaMA-Omni and Qwen2.5-Omni.",
        "body": "Defense\nLLaMA-Omni\nQwen2.5-Omni\n\n\n\n\nNone\n4.95\n7.26\n\n\nGaussian Noise\n4.67\n7.27\n\n\nLocal Smoothing\n4.87\n7.23\n\n\nDownsampling\n4.93\n7.28\n\n\nSelf-Reminder\n4.50\n7.49\n\n\nICD\n3.41\n7.71\n\n\n\n\\rowcolorgray!20\nALMGuard\n\n4.68\n6.02",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Defense</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">LLaMA-Omni</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Qwen2.5-Omni</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">None</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.26</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">Gaussian Noise</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.67</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.27</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">Local Smoothing</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">Downsampling</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.28</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">Self-Reminder</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.50</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.49</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:80%;\">ICD</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.41</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">7.71</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_b\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span><span class=\"ltx_text\" style=\"font-size:80%;\">gray!20\n</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">ALMGuard</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:80%;\">6.02</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "noise",
            "selfreminder",
            "airbenchchat",
            "gaussian",
            "icd",
            "defense",
            "smoothing",
            "rowcolorgray20",
            "almguard",
            "llamaomni",
            "local",
            "qwen25omni",
            "results",
            "none",
            "downsampling"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We present the evaluation results on AIR-Bench-Chat for LLaMA-Omni and Qwen2.5-Omni in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T7\" title=\"Table 7 &#8227; D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. On LLaMA-Omni, our method achieves a RQS of 4.68, outperforming most baselines. Notably, while ICD provides the strongest defense among baselines on this model, it also causes the most significant degradation in utility. It is worth noting that for Qwen2.5-Omni, we set <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>, whereas <math alttext=\"k=48\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>48</mn></mrow><annotation encoding=\"application/x-tex\">k=48</annotation></semantics></math> is used for other models. This adjustment is due to our observation that smaller values of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> fail to effectively defend against semantic-based attacks such as PAIR-Audio. We suspect this is primarily because Qwen2.5-Omni is highly sensitive to such attacks, as evidenced by its noticeably lower robustness on these attacks compared to other models. Nevertheless, given that Qwen2.5-Omni is inherently stronger, it still achieves a RQS of 6.02 even with <math alttext=\"k=128\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m4\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding=\"application/x-tex\">k=128</annotation></semantics></math>. Considering the trade-off between defense effectiveness and model utility, we believe this result is acceptable. In practice, we recommend users to adjust <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS5.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> flexibly based on specific deployment requirements.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats.\nTo address this issue, we propose <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, the first defense framework tailored to ALMs.\nBased on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time.\nTo better sift out effective triggers while preserving the model&#8217;s utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding.\nBoth theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, ALMGuard reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at&#160;<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/WeifeiJin/ALMGuard\" title=\"\">https://github.com/WeifeiJin/ALMGuard</a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "defense",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our stance.</span>\nThe limitations of existing defenses against ALM-specific jailbreaks motivate a novel approach. We hypothesize that well-aligned ALMs inherently possess <em class=\"ltx_emph ltx_font_italic\">safety shortcuts</em>, which are latent pathways or input sensitivities that, if correctly triggered, can steer models towards safer behavior and mitigate jailbreaks. These differ from explicit safety alignments, representing intrinsic model properties that can be leveraged for defense.\nThe primary challenge is to activate these safety shortcuts efficiently and harmlessly at inference time. We suggest that this can be achieved by applying a lightweight, universal acoustic perturbation to the input, referred to as the <span class=\"ltx_text ltx_font_bold\">Shortcut Activation Perturbation (SAP)</span>. SAP is designed to engage these safety-conducive pathways without requiring any model retraining. However, to prevent such a perturbation from degrading benign task performance, its application must be precisely targeted.\nTo this end, we introduce the <span class=\"ltx_text ltx_font_bold\">Mel-Gradient Sparse Mask (M-GSM)</span>. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, M-GSM identifies a sparse set of Mel-frequency bins that are highly influential for jailbreak mitigation yet largely inconsequential for benign speech understanding, as measured by automatic speech recognition (ASR) tasks. Our framework, <span class=\"ltx_text ltx_font_bold\">ALMGuard</span>, then synergistically employs M-GSM to guide the application of the universal SAP only to Mel bins that are security-critical yet benign-insensitive.\nThis targeted strategy allows ALMGuard to effectively activate safety shortcuts for robust defense while preserving model utility (<span class=\"ltx_text ltx_font_italic\">i.e.</span>, performance on benign inputs).</p>\n\n",
                "matched_terms": [
                    "defense",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation and results.</span>\nWe evaluate the proposed method across four state-of-the-art (SOTA) ALMs and six representative jailbreak attacks. On Qwen2-Audio&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a></cite>, ALMGuard reduces the success rate of the two most recent ALM-specific attacks, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> and Gupta <span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite>, to 3.1% and 0.5% respectively, outperforming all baselines and establishing itself as the new SOTA defense.\nMoreover, ALMGuard generalizes effectively to unseen attacks. As a lightweight defense framework, it enables near zero-cost deployment with negligible inference overhead. Evaluations on two benign benchmarks further confirm that ALMGuard does not noticeably degrade model utility. In addition, it achieves strong robustness against adaptive attacks.</p>\n\n",
                "matched_terms": [
                    "results",
                    "defense",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce the concept of inherent safety shortcuts in ALMs and propose ALMGuard as the first comprehensive and principled framework to systematically discover and activate these latent pathways for robust and generalizable jailbreak defense.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our core technical innovation within ALMGuard involves a universal SAP which is precisely guided by our M-GSM to engage these safety shortcuts, targeting sparse acoustic regions for maximal defense effectiveness with minimal utility impact.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through extensive evaluations, complemented by theoretical analyses, we show that ALMGuard achieves SOTA defense against six jailbreaks on four SOTA ALMs, with strong generalization, high benign-task utility, and negligible inference overhead.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Jailbreaks and defenses.</span>\nJailbreaking was initially introduced in the context of text-based LLMs, where attackers craft malicious prompts to bypass the model&#8217;s built-in safety mechanisms and induce it to generate harmful content. Existing jailbreak techniques for LLMs can be broadly categorized into two types: suffix-based and semantic-based. Suffix-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib60\" title=\"\">zou2023universal </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib27\" title=\"\">liao2024amplegcg </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib3\" title=\"\">basani2024gasp </a></cite> append an adversarial suffix after a harmful query, while semantic-based attacks&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib29\" title=\"\">liu2023autodan </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib6\" title=\"\">chao2023jailbreaking </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib31\" title=\"\">mehrotra2024tree </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib57\" title=\"\">zeng2024johnny </a></cite> manipulate the prompt content using strategies such as persuasion or logical traps to elicit the desired malicious response.\nIn the context of ALMs, AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite> is a suffix-based attack that appends an adversarial noise segment, while Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib15\" title=\"\">gupta2025bad </a></cite> explore prefix and perturbation formats. Modern foundation models are typically enhanced with preference-based alignment (<span class=\"ltx_text ltx_font_italic\">e.g.</span>, reinforcement learning from human feedback (RLHF)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib7\" title=\"\">christiano2017deep </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib32\" title=\"\">ouyang2022training </a></cite> and direct preference optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib37\" title=\"\">rafailov2023direct </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib2\" title=\"\">amini2024direct </a></cite>) where human judgments guide model fine-tuning. As jailbreak attacks continue to emerge, corresponding defense mechanisms have been proposed, including input-level detection&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib1\" title=\"\">alon2023detecting </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib38\" title=\"\">robey2023smoothllm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib20\" title=\"\">inan2023llama </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib34\" title=\"\">phute2023llm </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib50\" title=\"\">xie2024gradsafe </a></cite> and mitigation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib21\" title=\"\">jain2023baseline </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib54\" title=\"\">xu2024safedecoding </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>, and output-level intervention&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib45\" title=\"\">wang2024selfdefend </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib35\" title=\"\">qian2024hsf </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib46\" title=\"\">wang2025vulnerability </a></cite>.\nHowever, defenses tailored to ALMs remain underexplored. In this paper, we address this gap by proposing a dedicated framework that targets their unique vulnerabilities.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "defense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This bound shows that the performance impact on benign tasks is proportional to the tunable hyperparameters, <math alttext=\"\\epsilon\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\"><semantics><mi>&#1013;</mi><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math> and <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (through <math alttext=\"d_{k}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m3\" intent=\":literal\"><semantics><msub><mi>d</mi><mi>k</mi></msub><annotation encoding=\"application/x-tex\">d_{k}</annotation></semantics></math>), and two stability-related constants, <math alttext=\"L_{\\text{enc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>L</mi><mtext>enc</mtext></msub><annotation encoding=\"application/x-tex\">L_{\\text{enc}}</annotation></semantics></math> and <math alttext=\"G_{\\max}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>max</mi></msub><annotation encoding=\"application/x-tex\">G_{\\max}</annotation></semantics></math>. The design of M-GSM aims to apply perturbations to regions where these stability factors collectively result in minimal adverse impact on benign tasks. This theoretical upper bound supports the claim that ALMGuard can preserve model performance on benign tasks, which aligns with our empirical results shown in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.SS3\" title=\"5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset and models.</span> In line with AdvWave&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib24\" title=\"\">kangadvwave </a></cite>, we adopt AdvBench&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib60\" title=\"\">zou2023universal </a></cite>, a benchmark widely used in text-based jailbreak research, which contains a total of 520 prompts. These prompts are converted into audio using OpenAI&#8217;s text-to-speech (TTS) API to construct <span class=\"ltx_text ltx_font_bold\">AdvBench</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span>, comprising 520 audio queries. We evaluate four state-of-the-art audio-language models: <span class=\"ltx_text ltx_font_bold\">Qwen2</span>-<span class=\"ltx_text ltx_font_bold\">Audio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib8\" title=\"\">chu2024qwen2 </a></cite>, <span class=\"ltx_text ltx_font_bold\">LLaMA</span>-<span class=\"ltx_text ltx_font_bold\">Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib10\" title=\"\">fang2024llama </a></cite>, <span class=\"ltx_text ltx_font_bold\">Lyra</span>-<span class=\"ltx_text ltx_font_bold\">Base</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib59\" title=\"\">zhong2024lyra </a></cite>, and <span class=\"ltx_text ltx_font_bold\">Qwen2.5</span>-<span class=\"ltx_text ltx_font_bold\">Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib53\" title=\"\">xu2025qwen2 </a></cite>. All models are capable of accepting audio as input and producing either textual responses.</p>\n\n",
                "matched_terms": [
                    "qwen25omni",
                    "llamaomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baselines.</span>\nFor the defense methods, due to the lack of dedicated defenses targeting jailbreaks in ALMs, we explore two directions of transfer. <span class=\"ltx_text ltx_font_bold\">Type I:</span> From the domain of traditional audio adversarial defenses, we consider three widely adopted techniques&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib12\" title=\"\">ge2023advddos </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib11\" title=\"\">fang2024zero </a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib23\" title=\"\">jin2025whispering </a></cite>: (<span class=\"ltx_text ltx_font_italic\">i</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Gaussian Noise</span>, (<span class=\"ltx_text ltx_font_italic\">ii</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Local Smoothing</span>, and (<span class=\"ltx_text ltx_font_italic\">iii</span>)&#160;<span class=\"ltx_text ltx_font_bold\">Downsampling</span>. <span class=\"ltx_text ltx_font_bold\">Type II:</span> From the domain of text-based jailbreak defense, we implement two representative methods: <span class=\"ltx_text ltx_font_bold\">Self-Reminder</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib51\" title=\"\">xie2023defending </a></cite> and <span class=\"ltx_text ltx_font_bold\">In-Context Defense (ICD)</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#bib.bib47\" title=\"\">wei2023jailbreak </a></cite>. Details of these defense techniques are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS3\" title=\"D.3 Baseline Setup &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "selfreminder",
                    "icd",
                    "gaussian",
                    "defense",
                    "smoothing",
                    "local",
                    "downsampling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Performance on seen attacks.</span>\nWe first evaluate the performance of ALMGuard on three seen attacks used during the optimization of the perturbation: AdvWave, AdvWave-P, and PAIR-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method significantly outperforms all baselines on AdvWave and AdvWave-P, reducing the average SRoA across four ALMs from 53.5% and 68.5% to 4.6% and 7.8%, respectively. This indicates that ALMGuard exhibits strong robustness against acoustic-based attacks. On PAIR-Audio, a representative semantic-based attack, ALMGuard reduces the average SRoA to 26.3%, achieving comparable performance to Self-Reminder and ICD. Notably, ALMGuard consistently achieves a lower SRoA than all baselines against AdvWave-P on every model, and even reduces it to 0 on Qwen2.5-Omni, making the model completely robust - a result that no existing defense has achieved.</p>\n\n",
                "matched_terms": [
                    "selfreminder",
                    "icd",
                    "defense",
                    "qwen25omni",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transferability to unseen attacks.</span>\nWe evaluate the transferability of ALMGuard on three unseen attacks: Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>, ICA, and PAP-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T1\" title=\"Table 1 &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, ALMGuard significantly reduces the average SRoA by 27.4%, 7.4%, and 5.2%, respectively. In particular, the average SRoA on Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;is reduced to only 1.9%, which is the lowest among all attacks. Given that AdvWave and Gupta&#160;<span class=\"ltx_text ltx_font_italic\">et al.</span>&#160;represent the current SOTA ALM-specific jailbreak attacks, we believe that ALMGuard achieves strong robustness against this class of threats.\nFor baselines, we observe that Type I defenses tend to perform better against acoustic-based attacks. In contrast, Type II defenses show significantly better performance on semantic-based attacks.\nThis observation suggests that no existing defense can dominate across all types of attacks.\nIn comparison, ALMGuard consistently outperforms Type I defenses across all attack categories. Compared to Type II defenses, ALMGuard achieves significantly better results on acoustic-based attacks.\nOn average, ALMGuard reduces the overall SRoA to 14.6%, which represents the current SOTA.</p>\n\n",
                "matched_terms": [
                    "results",
                    "defense",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T2\" title=\"Table 2 &#8227; 5.3 Impact on Benign Examples &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the performance of our method on two benign benchmarks. On Qwen2-Audio, ALMGuard causes only a slight degradation in performance, increasing the WER on LibriSpeech by 1.85% and decreasing the AIR-Bench-Chat score by 0.56, which we regard as negligible. In contrast, both Self-Reminder and ICD significantly impair ASR performance, increasing the WER by 26.27% and 8.98% respectively, indicating that both baselines considerably disrupt the model&#8217;s understanding of speech semantics. We hypothesize that this is due to Qwen2-Audio being highly sensitive to system prompts, where the presence of Self-Reminder and ICD causes the model to generate refusal responses even for normal ASR inputs. However, on AIR-Bench-Chat, where task instructions are more diverse, both baselines return to relatively normal performance.\nNotably, ALMGuard even improves model performance on Lyra-Base, reducing WER by 1.16% and increasing the AIR-Bench-Chat RQS by 0.15, outperforming all baselines and even the original model without defense.\nFor completeness, we also report results on LLaMA-Omni and Qwen2.5-Omni in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.SS5\" title=\"D.5 More Results of Model utility Evaluation &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">D.5</span></a>.\nOverall, our method demonstrates minimal impact on model utility, suggesting that it can be reliably deployed in real-world ALM systems and significantly enhances practical usability.</p>\n\n",
                "matched_terms": [
                    "selfreminder",
                    "airbenchchat",
                    "icd",
                    "defense",
                    "llamaomni",
                    "qwen25omni",
                    "results",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contribution of M-GSM.</span>\nIn ALMGuard, we employ M-GSM to ensure that the model&#8217;s utility is not significantly affected. To validate the effectiveness of this key component, we conduct an ablation study on Qwen2-Audio by testing three jailbreak attacks and two benign benchmarks, both with and without M-GSM. The results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In terms of defense effectiveness, the results with and without M-GSM show similar performance, both achieving over 50% reduction in average SRoA. However, regarding model utility, we observe a clear distinction. Without M-GSM, the WER on the ASR task increases by 20%, which substantially degrades the model&#8217;s ability to understand speech. In addition, the RQS drops by 1.17. In contrast, with M-GSM enabled, the fluctuations in WER and RQS are limited to within approximately 2% and 0.5, respectively, indicating a negligible impact on utility.</p>\n\n",
                "matched_terms": [
                    "results",
                    "defense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adaptive attacks.</span>\nTo further demonstrate the superiority of our method, we consider a more practical and challenging setting where the attacker has full knowledge of the defense mechanism, <span class=\"ltx_text ltx_font_italic\">i.e.</span>, a white-box threat model. Under this setting, we evaluate adaptive AdvWave attacks on LLaMA-Omni by optimizing the adversarial suffix in the presence of each of the six defense methods.\nAs an example, when attacking ALMGuard, the attacker adds our well-trained SAP to the input at each iteration during AdvWave optimization.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#S5.F4\" title=\"Figure 4 &#8227; 5.4 Ablation Study &#8227; 5 Experiments &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, our method still achieves the best defense performance among all methods, reducing the SRoA by 11.9% compared to the original attack, even under this strongest threat model. In contrast, all baseline defenses yield an SRoA above 70% under adaptive attacks.\nInterestingly, for traditional audio defenses such as local smoothing, incorporating the corresponding transformations into the optimization process actually increases SRoA. We hypothesize that this is because these operations act similarly to data augmentation, which improves the robustness of the adversarial suffix and thus enhances the attack strength.\nIn summary, our method demonstrates the strongest resistance to adaptive attacks among all evaluated defenses.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "smoothing",
                    "llamaomni",
                    "local",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the pseudocode of ALMGuard in Algorithm&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#algorithm1\" title=\"In Appendix B Pseudocode &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. The algorithm begins by constructing the training dataset <math alttext=\"\\mathcal{D}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{D}^{\\text{jb}}</annotation></semantics></math> through the application of a jailbreak algorithm set <math alttext=\"\\mathcal{A}_{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mtext>jb</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{A}_{\\text{jb}}</annotation></semantics></math> to a curated jailbreak prompt set <math alttext=\"\\mathcal{X}^{\\text{jb}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m3\" intent=\":literal\"><semantics><msup><mi class=\"ltx_font_mathcaligraphic\">&#119987;</mi><mtext>jb</mtext></msup><annotation encoding=\"application/x-tex\">\\mathcal{X}^{\\text{jb}}</annotation></semantics></math>. Based on this dataset, we compute the average sensitivity scores to derive the M-GSM <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, which identifies Mel-frequency bins that are critical for jailbreak mitigation yet minimally influential on benign performance. The universal perturbation <math alttext=\"\\delta\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m5\" intent=\":literal\"><semantics><mi>&#948;</mi><annotation encoding=\"application/x-tex\">\\delta</annotation></semantics></math> is initialized from a Gaussian distribution <math alttext=\"\\mathcal{N}(0,\\sigma)\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m6\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>0</mn><mo>,</mo><mi>&#963;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(0,\\sigma)</annotation></semantics></math>, and iteratively optimized over the dataset to minimize the empirical safety risk.</p>\n\n",
                "matched_terms": [
                    "almguard",
                    "gaussian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">In</span>-<span class=\"ltx_text ltx_font_bold\">Context Attack.</span>\nA black-box jailbreak strategy that prepends a few successful attack demonstrations to the input prompt to induce in-context learning. Following the authors&#8217; recommended configuration, we use 10-shot prompts for LLaMA-Omni, Lyra-Base, and Qwen2.5-Omni. Due to context length limitations, we use 5-shot prompts for Qwen2-Audio.</p>\n\n",
                "matched_terms": [
                    "qwen25omni",
                    "llamaomni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gaussian Noise.</span>\nWe add Gaussian noise with a standard deviation of 0.01 to the input audio. This simple defense can effectively remove certain brittle adversarial perturbations, particularly suffix-based or prefix-based attacks.</p>\n\n",
                "matched_terms": [
                    "noise",
                    "defense",
                    "gaussian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Local Smoothing.</span>\nWe apply a moving average filter to smooth the waveform. Each audio sample is replaced by the average of its neighboring values within a window of size <math alttext=\"h=2\" class=\"ltx_Math\" display=\"inline\" id=\"A4.I2.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">h=2</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "local",
                    "smoothing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate whether incorporating <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide the gradient direction is effective, we conduct a preliminary comparison study. Under the setting of <math alttext=\"k=16\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">k=16</annotation></semantics></math>, we evaluated both model utility and defense performance against AdvWave-P on Qwen2-Audio. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.26096v1#A4.T6\" title=\"Table 6 &#8227; D.4 Ablation on &#8466;_&quot;ASR&quot; &#8227; Appendix D Experimental Details &#8227; ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio&#8211;Language Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, when the M-GSM is applied, the presence or absence of <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> has negligible effect on both metrics. The WER and SRoA differ by only 0.07% and 1.5%, which can be attributed to random variations. In contrast, when M-GSM is removed, using <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> to guide optimization fails to preserve utility: the WER increases by 17.88% compared to the undefended case.\nIn summary, we conclude that <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> does not provide a meaningful benefit in achieving the goal of activating safety shortcuts to mitigate jailbreaks while preserving model utility. Therefore, we exclude <math alttext=\"\\mathcal{L}_{\\text{ASR}}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>ASR</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{ASR}}</annotation></semantics></math> from the final design of ALMGuard.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "almguard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Model-Specific Traits.</span> We observe that the effectiveness of jailbreak attacks and corresponding defenses is closely tied to the intrinsic characteristics of the target model. For instance, as discussed earlier, Qwen2.5-Omni appears particularly vulnerable to semantic-based attacks, with PAP achieves a high SRoA of 77.9% on this model. Similarly, Lyra-Base exhibits strong sensitivity to prompt context, making it especially susceptible to attacks like ICA.\nIn real-world deployment, it is advisable to consider the model-specific traits when designing comprehensive defense strategies, potentially combining multiple techniques for better protection. Nevertheless, disregarding such model-specific factors, our method consistently demonstrates the universal effectiveness to activate safety-aligned shortcuts across all models, leading to substantial reductions in jailbreak success rates.</p>\n\n",
                "matched_terms": [
                    "qwen25omni",
                    "defense"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span>\nDespite its strong overall performance, we observe that our perturbation-based defense has room for improvement against semantic-based attacks. In some cases, ALMGuard underperforms compared to the best-performing baselines. We attribute this to the fact that our acoustic perturbation is mainly optimized to activate ALMs&#8217; inherent acoustic-related safety shortcuts to defend against acoustic-based attacks, but does not explicitly target the semantic intent of adversarial prompts. Future improvements may involve integrating semantic-level and intent-aware objectives during optimization. Additionally, given the plug-and-play nature of our method, it could be integrated with complementary defense techniques to form a more comprehensive defense framework.</p>\n\n",
                "matched_terms": [
                    "defense",
                    "almguard"
                ]
            }
        ]
    }
}