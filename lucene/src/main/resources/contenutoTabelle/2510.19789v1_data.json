{
    "Sx1.T1": {
        "caption": "Table 1: Comparison between OmniMotion-X and existing human motion generation methods. ”GSTC(S)” and ”GSTC(D)” denote global spatial-temporal controllable motion generation, where ”S” and ”D” indicate sparse and dense controlled joints, respectively. “Reference Motion” originates from user-designed or previously generated motion. ”Mixed-condition” refers to the simultaneous occurrence of multiple conditions during training. “Datasets” indicates the total number of datasets used for training, while ”Hours” represents the longest training dataset duration. For methods like MoMask, trained separately on HumanML3D or KIT, the duration of the HumanML3D dataset is considered.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Model Type</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\">Tasks</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Reference</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Mixed-condition</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Whole-Body</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Training Data</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">T2M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">M2D</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">S2G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">GSTC(S)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">GSTC(D)</td>\n<td class=\"ltx_td ltx_align_center\">Motion</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Datasets</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Hours</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">DiT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m2\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m3\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m4\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m5\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m6\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m7\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m8\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DiT</td>\n<td class=\"ltx_td ltx_align_center\">MCM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m9\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m10\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m11\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m12\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m13\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m14\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m15\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m16\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_align_center\">109.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DiT</td>\n<td class=\"ltx_td ltx_align_center\">LMM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m17\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m18\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m19\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m20\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m21\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m22\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m23\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m24\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">16</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">DiT</td>\n<td class=\"ltx_td ltx_align_center\">MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m25\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m26\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m27\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m28\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m29\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m30\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m31\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m32\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_align_center\">48.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MoMask&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m33\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m34\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m35\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m36\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m37\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m38\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m39\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m40\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AR</td>\n<td class=\"ltx_td ltx_align_center\">MotionGPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib30\" title=\"\">2024a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m41\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m42\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m43\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m44\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m45\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m46\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m47\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m48\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">2</td>\n<td class=\"ltx_td ltx_align_center\">28.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AR</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"M^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m49\" intent=\":literal\"><semantics><msup><mi>M</mi><mn>3</mn></msup><annotation encoding=\"application/x-tex\">M^{3}</annotation></semantics></math>GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib54\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m50\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m51\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m52\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m53\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m54\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m55\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m56\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m57\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_align_center\">164</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AR</td>\n<td class=\"ltx_td ltx_align_center\">MotionLLAMA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m58\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m59\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m60\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m61\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m62\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m63\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m64\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m65\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">11</td>\n<td class=\"ltx_td ltx_align_center\">70.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AR-DiT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AMD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m66\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m67\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m68\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m69\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m70\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m71\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m72\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m73\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">85.87</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AR-DiT</td>\n<td class=\"ltx_td ltx_align_center\">DART&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao, Li, and Tang <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib84\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m74\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m75\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m76\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m77\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m78\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m79\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m80\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m81\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">2</td>\n<td class=\"ltx_td ltx_align_center\">43.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">AR-DiT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;(Ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m82\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m83\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m84\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m85\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m86\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m87\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m88\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T1.m89\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">286.2</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "gstcd",
            "originates",
            "momask",
            "2024c",
            "occurrence",
            "separately",
            "dense",
            "zhao",
            "considered",
            "guo",
            "hours",
            "joints",
            "sparse",
            "from",
            "“datasets”",
            "”d”",
            "represents",
            "like",
            "zhang",
            "”gstcd”",
            "where",
            "datasets",
            "ours",
            "existing",
            "tang",
            "duration",
            "method",
            "reference",
            "motion",
            "”s”",
            "type",
            "m3m3gpt",
            "simultaneous",
            "mcm",
            "han",
            "s2g",
            "“reference",
            "generation",
            "used",
            "trained",
            "ling",
            "mdm",
            "×times",
            "userdesigned",
            "indicates",
            "respectively",
            "dit",
            "total",
            "m2d",
            "human",
            "kit",
            "motionllama",
            "bian",
            "wholebody",
            "motiongpt",
            "previously",
            "tasks",
            "methods",
            "between",
            "conditions",
            "omnimotionx",
            "mixedcondition",
            "denote",
            "longest",
            "number",
            "global",
            "motion”",
            "gstcs",
            "”mixedcondition”",
            "motioncraft",
            "model",
            "data",
            "refers",
            "jiang",
            "dart",
            "controlled",
            "spatialtemporal",
            "while",
            "comparison",
            "amd",
            "t2m",
            "controllable",
            "2024a",
            "”gstcs”",
            "training",
            "indicate",
            "luo",
            "multiple",
            "ardit",
            "humanml3d",
            "tevet",
            "during",
            "✓checkmark",
            "generated",
            "lmm",
            "dataset",
            "”hours”"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Although recent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>; Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>)</cite> attempt to unify multitasks in one model, they meet challenges in multimodal control modeling, versatile tasks, and high-quality motion generation (as shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx1.T1\" title=\"Table 1 &#8227; Introduction &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>):\n(1) <span class=\"ltx_text ltx_font_bold\">Independent Model Training.</span> Previous approaches train separate models for each modality, limiting simultaneous control across inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>)</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">Additional Control Branches.</span> Some methods add separate control branches for each condition, limiting interaction between them&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>)</cite>.\n(3) <span class=\"ltx_text ltx_font_bold\">Conflict Granularity Training.</span> Existing methods use mixed training by combining high-level semantic conditions with low-level controls, which hampers effective control at different levels and leads to optimization challenges&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Luo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib54\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>. Similar phenomena have also been observed in video generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib44\" title=\"\">2025</a>; Ao <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib4\" title=\"\">2024</a>)</cite>.\nIn addition to modeling challenges, relevant works also introduce large-scale motion datasets from multiple tasks and modalities&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, they still exhibit the following significant shortcomings (see comparisons in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx1.T2\" title=\"Table 2 &#8227; Introduction &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):\n(1) <span class=\"ltx_text ltx_font_bold\">Low Motion Quality.</span> Datasets expanded with non-mocap motion estimation exhibit &#8220;garbage in, garbage out&#8221; effects, leading to poor-quality motion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>)</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">Text Inconsistency.</span> Inconsistent text annotations or expanded LLM descriptions lead to uneven text quality and hallucination issues&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>)</cite>.\nand (3) <span class=\"ltx_text ltx_font_bold\">limited tasks.</span> They focus on common tasks, limiting applicability in diverse ones like HOI, HSI, and HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite>.\nThese limitations hinder the development of a unified, high-quality dataset for multimodal whole-body human motion generation across diverse scenarios.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper introduces <span class=\"ltx_text ltx_font_bold\">OmniMotion-X</span>, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks&#8212;including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis)&#8212;as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct <span class=\"ltx_text ltx_font_bold\">OmniMoCap-X</span>, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "global",
                    "existing",
                    "wholebody",
                    "spatialtemporal",
                    "controllable",
                    "omnimotionx",
                    "mixedcondition",
                    "generated",
                    "multiple",
                    "generation",
                    "tasks",
                    "human",
                    "training",
                    "methods",
                    "dataset",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_caption\">We present <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified sequence-to-sequence autoregressive motion diffusion transformer designed for flexible and interactive whole-body human motion generation. It supports a variety of tasks, including text-to-motion, music-to-dance, speech-to-gesture, and globally spatial-temporal controllable motion generation, which encompasses motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis. These conditions can be combined in various ways to enable versatile motion generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "motion",
                    "spatialtemporal",
                    "wholebody",
                    "controllable",
                    "omnimotionx",
                    "generation",
                    "tasks",
                    "human",
                    "conditions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal whole-body human motion generation plays critical roles in animation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib13\" title=\"\">2024b</a>)</cite>, gaming&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib43\" title=\"\">2024</a>)</cite>, virtual reality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib24\" title=\"\">2024b</a>)</cite>, and embodied intelligence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib55\" title=\"\">2024</a>)</cite> across diverse input conditions, including text, audio, and trajectory, etc.\nThe limited multimodal data and task-specific model designs prevent existing motion generation methods from supporting multimodal whole-body human motion generation. Due to the high cost of mocap data collection and labeling, most datasets focus on single domains such as Text-to-Motion (T2M)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Plappert, Mandery, and Asfour <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib62\" title=\"\">2016</a>)</cite>, Music-to-Dance (M2D)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, Speech-to-Gesture (S2G)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>)</cite>, Human-Object Interaction (HOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>)</cite>, Human-Scene Interaction (HSI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and Human-Human Interaction (HHI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>)</cite>, each with inconsistent data formats (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, BVH, SMPL-(H/X) and 3D Keypoints) and control conditions (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Text captions, Audio, and Trajectories).\nTo tackle motion generation across diverse scenarios, it is crucial to build a unified framework that utilizes large-scale, diverse data to achieve more generalized representations.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "existing",
                    "wholebody",
                    "s2g",
                    "m2d",
                    "model",
                    "generation",
                    "data",
                    "from",
                    "human",
                    "jiang",
                    "methods",
                    "conditions",
                    "guo",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address existing challenges, we propose <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified framework for multimodal whole-body human motion generation, and <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset.\nSpecifically, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;employs Diffusion Transformers (DiTs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles and Xie <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib59\" title=\"\">2023</a>)</cite>, incorporates multimodal conditions by concatenating condition tokens as prefix context, and adopts a progressive weak-to-strong mixed-condition training strategy to gradually constrain motion from high-level semantics to dense spatial-temporal alignment.\nNotably, unlike previous methods, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;introduces a novel generation paradigm that utilizes reference motion (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, user-provided or model-predicted motion) as a special condition. This significantly enhances generated motion quality and achieves consistency between reference and generated motion, creating an effective clip-by-clip autoregressive motion diffusion.\nThis enables <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;to support autoregressive interactive generation with strong temporal alignment.\nFurthermore, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;unifies various Global Spatial-Temporal Controllable Generation tasks through spatial-temporal masking strategies.\nTo guarantee high-quality motion generation training, we collect high-quality mocap datasets that support diverse motion generation tasks, unify them under the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format with standard world coordinate systems, and automatically generate hierarchical text captions by rendering motions into videos and annotating them with vision language models (VLMs).\nThis dataset contains about <em class=\"ltx_emph ltx_font_italic\"><math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.p3.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours</em> and integrates multimodal control conditions, supporting versatile tasks, including T2M, M2D, S2G, HOI, HSI, and HHI.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "spatialtemporal",
                    "dense",
                    "t2m",
                    "hours",
                    "s2g",
                    "wholebody",
                    "controllable",
                    "generation",
                    "from",
                    "tasks",
                    "training",
                    "between",
                    "methods",
                    "conditions",
                    "omnimotionx",
                    "mixedcondition",
                    "datasets",
                    "global",
                    "existing",
                    "m2d",
                    "generated",
                    "human",
                    "dataset",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a multimodal autoregressive diffusion transformer for versatile whole-body motion generation. By introducing reference motion, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;significantly enhances consistent content, style, and temporal dynamics generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "wholebody",
                    "omnimotionx",
                    "generation",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset with a unified SMPL-X format, and provides consistent, detailed, and structured text captions to serve versatile motion generation tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "dataset",
                    "generation",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments show that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves state-of-the-art performance across various tasks, including T2M, S2G, M2D, and GSTC. These evaluations were conducted on our more challenging test sets comprising <math alttext=\"280\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.I1.i4.p1.m1\" intent=\":literal\"><semantics><mn>280</mn><annotation encoding=\"application/x-tex\">280</annotation></semantics></math> samples uniformly sampled from our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "s2g",
                    "omnimotionx",
                    "m2d",
                    "tasks",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing methods are typically categorized based on input conditions into three main types: single-modal, cross-modal, and multimodal.\nSingle-modal motion generation uses control conditions from the same modality as the motion, including motion prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib73\" title=\"\">2024a</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib12\" title=\"\">2023a</a>)</cite>, in-betweening&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cohan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib15\" title=\"\">2024</a>)</cite>, and joint/trajectory-guided synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>; Dai et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib16\" title=\"\">2024</a>; Zhao, Li, and Tang <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib84\" title=\"\">2024</a>)</cite>.\nCross-modal motion generation uses control conditions from different modalities, including text in T2M&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>; Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib52\" title=\"\">2024a</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib30\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib83\" title=\"\">2024d</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib80\" title=\"\">2023b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib81\" title=\"\">2024b</a>)</cite>, music in M2D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib35\" title=\"\">2023</a>; Siyao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib66\" title=\"\">2022</a>; Tseng, Castellon, and Liu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib68\" title=\"\">2023</a>)</cite>, speech in S2G&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib10\" title=\"\">2024a</a>)</cite>, target object positions in HOI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib50\" title=\"\">2024</a>)</cite>, scene layouts in HSI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kaufmann et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib33\" title=\"\">2023</a>; Huang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib27\" title=\"\">2022</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and partner movements in HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>)</cite>.\nHowever, these approaches typically rely on task-specific architectures, significantly limiting their cross-task generalization capabilities and practical applications.\nRecently, researchers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Luo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib54\" title=\"\">2024</a>)</cite> have begun exploring multimodal motion generation, with approaches falling into three primary categories:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "2024c",
                    "zhao",
                    "bian",
                    "guo",
                    "han",
                    "t2m",
                    "s2g",
                    "2024a",
                    "generation",
                    "from",
                    "methods",
                    "conditions",
                    "ling",
                    "zhang",
                    "luo",
                    "tevet",
                    "existing",
                    "tang",
                    "m2d",
                    "jiang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Separate Model Training.</span> These approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>)</cite> typically train independent models for different conditional modalities, making multimodal motion generation fundamentally unattainable.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "model",
                    "generation",
                    "training",
                    "han"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Additional Control Branch.</span> These approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>)</cite> incorporate separate control branches into the backbone architecture, each dedicated to a specific condition, resulting in limited interaction between different conditions.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "ling",
                    "between",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Multimodal Condition Modeling.</span> These approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite> use a single model to learn mappings from diverse modalities to target motions, enhancing multi-modal adaptability. However, challenges arise from different constraints across modalities: text provides semantic guidance, spatial-temporal controls impose physical constraints, while audio inputs enforce rhythmic alignment. These disparate constraints often result in compromised controllability and optimization difficulties.</p>\n\n",
                "matched_terms": [
                    "ling",
                    "while",
                    "spatialtemporal",
                    "2024c",
                    "zhang",
                    "model",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current human motion generation datasets are predominantly task-specific.\nDifferent tasks rely on separate collections: text-to-motion (T2M) uses datasets with text captions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Plappert, Mandery, and Asfour <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib62\" title=\"\">2016</a>; Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Punnakkal et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib63\" title=\"\">2021</a>; Liao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib43\" title=\"\">2024</a>)</cite>, music-to-dance (M2D) requires music-annotated datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib35\" title=\"\">2023</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib11\" title=\"\">2021</a>)</cite>, and speech-to-gesture (S2G) employs datasets with paired speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib49\" title=\"\">2022</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>)</cite>.\nSimilarly, interaction datasets remain disconnected across human-object&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib31\" title=\"\">2022</a>; Li, Wu, and Liu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib37\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib79\" title=\"\">2024a</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib50\" title=\"\">2024</a>; Zhan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib77\" title=\"\">2024</a>)</cite>, human-scene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and human-human interaction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>)</cite>. This fragmentation constrains multimodal motion generation capabilities.\nIntegrating these datasets presents challenges due to inconsistent motion formats.\nDatasets vary widely in their representations: some use SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, others employ SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>)</cite>, while many rely on BVH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Harvey et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib26\" title=\"\">2020</a>; Mason, Starke, and Komura <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib56\" title=\"\">2022</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib2\" title=\"\">Adobe </a>; Alexanderson et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib3\" title=\"\">2023</a>; Valle-P&#233;rez et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib69\" title=\"\">2021</a>)</cite> or keypoint representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ionescu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib28\" title=\"\">2013</a>; Ji et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib29\" title=\"\">2018</a>)</cite>. This diversity makes format standardization extremely difficult.\nRecent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite> have attempted to integrate multiple datasets, increasing scale and incorporating some multimodal conditions. However, they still exhibit several limitations:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "ling",
                    "while",
                    "s2g",
                    "2024a",
                    "2024c",
                    "zhang",
                    "m2d",
                    "multiple",
                    "generation",
                    "tasks",
                    "human",
                    "jiang",
                    "conditions",
                    "guo",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-quality Motions</span>: Most integrated datasets derive from non-mocap sources with significant estimation errors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, creating a &#8220;garbage in, garbage out&#8221; effect that compromises motion generation quality.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "ling",
                    "2024c",
                    "zhang",
                    "generation",
                    "from",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text Inconsistency</span>: Existing datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite> use inconsistent text annotations (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, action labels versus semantic descriptions) or rely on LLMs for text refinement without visual motion data, causing variable text quality and hallucinations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "ling",
                    "existing",
                    "2024a",
                    "2024c",
                    "zhang",
                    "data",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Generation Scenarios</span>: Most datasets lack support for critical interaction tasks including HOI, HSI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, and HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>)</cite>, severely limiting their applicability to complex scenarios.</p>\n\n",
                "matched_terms": [
                    "ling",
                    "2024a",
                    "2024c",
                    "zhang",
                    "generation",
                    "tasks",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the data challenges&#8212;the scarcity of high-quality multitask motion datasets, inconsistent textual annotations and motion representations&#8212;we implement a three-pronged approach: (1) integrating diverse motion capture datasets with multitask support, (2) establishing a unified motion representation, and (3) developing high-quality motion captions derived from visual-textual annotations.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "data",
                    "from",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While existing approaches prioritize dataset scale over quality by incorporating substantial amounts of non-mocap data&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, leading to degraded motion generation quality (&#8221;garbage in, garbage out&#8221;), our work emphasizes data quality.\nWe systematically curate open-source mocap datasets and classify them into five acquisition categories: Marker with manual correction, Marker (Vicon), IMU, Multi-View RGB, and Single-View RGB (detailed in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.T3\" title=\"Table 3 &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nOur dataset supports diverse motion generation tasks, including text-to-motion, motion-to-dance, speech-to-gesture, and various interactions (human-object, human-scene, and human-human), providing a high-quality multimodal foundation. Comprising <math alttext=\"64.3\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m1\" intent=\":literal\"><semantics><mn>64.3</mn><annotation encoding=\"application/x-tex\">64.3</annotation></semantics></math> million frames and <math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m2\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours of data, our approach achieves comprehensive task coverage while maintaining superior data quality.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "ling",
                    "while",
                    "hours",
                    "existing",
                    "2024c",
                    "zhang",
                    "generation",
                    "data",
                    "tasks",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We standardize diverse motion formats (BVH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lauterbach et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib34\" title=\"\">2009</a>)</cite>, FBX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wikipedia contributors <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib71\" title=\"\">2024</a>)</cite>, and SMPL&#160;(-H)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib51\" title=\"\">2023</a>)</cite>) into the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format to support our unified multimodal model.\nFirst, we convert these formats into the Motion-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>)</cite> SMPL-X format, comprising root orientation, pose parameters (body, hand, and jaw), facial expressions, facial shape, translation, and body shape parameters.\nWe then normalize the translation scale and initial root orientation across datasets to establish a consistent coordinate system. Additionally, we resample all datasets to 30 frames per second (FPS) to enhance the model&#8217;s ability to capture temporal patterns.\nThe specific conversion process and normalization for different datasets are provided in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "model",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance generalization ability, we extend the widely-used body-only representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>)</cite> to a whole-body format.\nSpecifically, the pose of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th frame is a tuple <math alttext=\"\\mathbf{p}_{i}=({\\dot{r}^{a},\\dot{r}^{x},\\dot{r}^{z},r^{y},\\mathbf{j}^{p},\\mathbf{j}^{v},\\mathbf{c}^{f},\\mathbf{f}})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#119849;</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msup><mover accent=\"true\"><mi>r</mi><mo>&#729;</mo></mover><mi>a</mi></msup><mo>,</mo><msup><mover accent=\"true\"><mi>r</mi><mo>&#729;</mo></mover><mi>x</mi></msup><mo>,</mo><msup><mover accent=\"true\"><mi>r</mi><mo>&#729;</mo></mover><mi>z</mi></msup><mo>,</mo><msup><mi>r</mi><mi>y</mi></msup><mo>,</mo><msup><mi>&#119843;</mi><mi>p</mi></msup><mo>,</mo><msup><mi>&#119843;</mi><mi>v</mi></msup><mo>,</mo><msup><mi>&#119836;</mi><mi>f</mi></msup><mo>,</mo><mi>&#119839;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{p}_{i}=({\\dot{r}^{a},\\dot{r}^{x},\\dot{r}^{z},r^{y},\\mathbf{j}^{p},\\mathbf{j}^{v},\\mathbf{c}^{f},\\mathbf{f}})</annotation></semantics></math>, where <math alttext=\"\\dot{r}^{a}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mover accent=\"true\"><mi>r</mi><mo>&#729;</mo></mover><mi>a</mi></msup><mo>&#8712;</mo><mi>&#8477;</mi></mrow><annotation encoding=\"application/x-tex\">\\dot{r}^{a}\\in\\mathbb{R}</annotation></semantics></math> is root angular velocity around the Y-axis; <math alttext=\"\\dot{r}^{x},\\dot{r}^{z}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m4\" intent=\":literal\"><semantics><mrow><mrow><msup><mover accent=\"true\"><mi>r</mi><mo>&#729;</mo></mover><mi>x</mi></msup><mo>,</mo><msup><mover accent=\"true\"><mi>r</mi><mo>&#729;</mo></mover><mi>z</mi></msup></mrow><mo>&#8712;</mo><mi>&#8477;</mi></mrow><annotation encoding=\"application/x-tex\">\\dot{r}^{x},\\dot{r}^{z}\\in\\mathbb{R}</annotation></semantics></math> are root linear velocity in the XZ plane; <math alttext=\"r^{y}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m5\" intent=\":literal\"><semantics><mrow><msup><mi>r</mi><mi>y</mi></msup><mo>&#8712;</mo><mi>&#8477;</mi></mrow><annotation encoding=\"application/x-tex\">r^{y}\\in\\mathbb{R}</annotation></semantics></math> is root height, <math alttext=\"\\mathbf{j}^{p}\\in\\mathbb{R}^{3N-1}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m6\" intent=\":literal\"><semantics><mrow><msup><mi>&#119843;</mi><mi>p</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi></mrow><mo>&#8722;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{j}^{p}\\in\\mathbb{R}^{3N-1}</annotation></semantics></math> are local joint positions; Notably, <math alttext=\"\\mathbf{j}^{r}\\in\\mathbb{R}^{6N^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m7\" intent=\":literal\"><semantics><mrow><msup><mi>&#119843;</mi><mi>r</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>6</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>N</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{j}^{r}\\in\\mathbb{R}^{6N^{\\prime}}</annotation></semantics></math> are joint 6D rotations of SMPL-X; <math alttext=\"\\mathbf{j}^{v}\\in\\mathbb{R}^{3N}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m8\" intent=\":literal\"><semantics><mrow><msup><mi>&#119843;</mi><mi>v</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{j}^{v}\\in\\mathbb{R}^{3N}</annotation></semantics></math> are joint velocities; <math alttext=\"\\mathbf{c}^{f}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m9\" intent=\":literal\"><semantics><msup><mi>&#119836;</mi><mi>f</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{c}^{f}</annotation></semantics></math> is binary features obtained by thresholding the heel and toe joint velocities to emphasize the foot ground contacts.\nHere, <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m10\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> refers to 127 whole-body joints extracted from SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite>, while <math alttext=\"N^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m11\" intent=\":literal\"><semantics><msup><mi>N</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">N^{\\prime}</annotation></semantics></math> represents 53 joints spanning the body, hands, and jaw.\nFor facial representation, we adopt the Flame Format&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib40\" title=\"\">2017</a>)</cite>, encoding facial features as <math alttext=\"\\mathbf{f}\\in\\mathbb{R}^{100}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m12\" intent=\":literal\"><semantics><mrow><mi>&#119839;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mn>100</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{f}\\in\\mathbb{R}^{100}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "while",
                    "wholebody",
                    "represents",
                    "joints",
                    "from",
                    "refers",
                    "guo",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Refinement</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite> employs LLMs like GPT-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib1\" title=\"\">2023</a>)</cite> to enhance textual descriptions. However, since LLMs cannot directly perceive motion data and rely solely on existing text, they frequently introduce hallucinations and fail to capture precise action details.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "like",
                    "data",
                    "existing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Manual Annotation</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Punnakkal et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib63\" title=\"\">2021</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite> effectively eliminates hallucinations but proves costly and difficult to scale, resulting in limited annotations with simple descriptions.</p>\n\n",
                "matched_terms": [
                    "guo",
                    "ling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Motion-to-Text Model-Based Annotation</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delmas et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib17\" title=\"\">2022</a>; Yazdian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib75\" title=\"\">2023</a>)</cite> generates captions directly from raw motion. While these methods effectively capture low-level kinematic details (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, left elbow bends 45 degrees), they lack the understanding of high-level semantics and action context.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "while",
                    "from",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we propose an automated approach that integrates both visual and textual information for comprehensive motion annotation.\nOur method renders motion into videos and combines them with existing textual annotations (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, descriptions, action labels, and task categories) as input to the state-of-the-art vision-language models (VLMs), GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib1\" title=\"\">2023</a>)</cite>.\nThis multimodal approach generates structured, hierarchical, and precise motion captions that ensure both annotation quality and expression richness. Detailed statistics of the texts quality are in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "existing",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.F1\" title=\"Figure 1 &#8227; Unified Motion Representation &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nWe propose a unified multimodal, autoregressive transformer-based diffusion model for whole-body human motion generation across multiple tasks.\nThe model takes textual description <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> for semantic guidance, global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> for spatial-temporal control, and speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> and music conditions <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> to ensure rhythmic and stylistic coherence.\nFurthermore, it incorporates reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> as a motion prior, derived from either previously generated clips or user-designed motion, thus providing fine-grained details unavailable in other conditions.\nIn addition, the mixed-condition setting allows multiple conditions to occur simultaneously during training, enabling the model to handle complex scenarios with diverse inputs.\nIn this section, we present our approach in two parts: a unified framework for multimodal modeling and a progressive training strategy that ensures precise motion control.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "spatialtemporal",
                    "wholebody",
                    "previously",
                    "generation",
                    "from",
                    "tasks",
                    "training",
                    "conditions",
                    "omnimotionx",
                    "mixedcondition",
                    "userdesigned",
                    "multiple",
                    "global",
                    "during",
                    "model",
                    "generated",
                    "human",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We integrate multimodal conditions using Diffusion Transformers (DiTs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles and Xie <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib59\" title=\"\">2023</a>)</cite> by concatenating condition tokens as prefix contexts, allowing the model to process and fuse multimodal information efficiently.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Conditions.</span> Our framework integrates multiple condition modalities: text <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> providing semantic guidance; global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> ensuring spatial-temporal consistency; speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> synchronizing gestures and lip movements with rhythm; music <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> supplying beat and style information for dance; and reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> serving as a motion prior. Notably, this reference motion condition&#8212;overlooked in previous multimodal motion generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>&#8212;enables our model to maintain precise spatial-temporal pattern consistency with reference, substantially enhancing motion quality and coherence.\nTo fully leverage each modality and facilitate interaction between different conditions, we employ modality-specific encoders (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, T5-XXL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib65\" title=\"\">2020</a>)</cite> for text, a wav encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite> for speech, Librosa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McFee et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib57\" title=\"\">2015</a>)</cite> for music and body-wise encoding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> for motion) to extract features from each modality. These features are aligned to match motion embedding dimensions using learnable linear projections, allowing us to concatenate all condition tokens as prefix context with noisy motion tokens during processing.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "global",
                    "ling",
                    "spatialtemporal",
                    "during",
                    "2024c",
                    "zhang",
                    "model",
                    "multiple",
                    "generation",
                    "from",
                    "bian",
                    "conditions",
                    "between",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> and <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m2\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> denote the modality-specific encoder and projection layer, respectively. The concatenated representation <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m3\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is then fed into our DiT backbone as a conditioning prefix context.\nTo constrain the physical properties of motion, we follow previous works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib13\" title=\"\">2024b</a>)</cite> by directly predicting motion <math alttext=\"\\hat{x}_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\hat{x}_{0}</annotation></semantics></math> rather than noise. Consequently, our diffusion objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "respectively",
                    "dit",
                    "denote",
                    "tevet",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"q(x_{0}|c)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m5\" intent=\":literal\"><semantics><mrow><mi>q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mn>0</mn></msub><mo fence=\"false\">|</mo><mi>c</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">q(x_{0}|c)</annotation></semantics></math> represents the data distribution, <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m6\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the maximum diffusion step. <math alttext=\"G\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m7\" intent=\":literal\"><semantics><mi>G</mi><annotation encoding=\"application/x-tex\">G</annotation></semantics></math> denotes the learned denoising function, while <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m8\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math> represents the noisy motion at step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m9\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, expressed as <math alttext=\"x_{t}=[p_{0}^{t},p_{1}^{t},\\dots,\\mathbf{p}_{N}^{t}]\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m10\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>p</mi><mn>0</mn><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>p</mi><mn>1</mn><mi>t</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>&#119849;</mi><mi>N</mi><mi>t</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{t}=[p_{0}^{t},p_{1}^{t},\\dots,\\mathbf{p}_{N}^{t}]</annotation></semantics></math>, where each <math alttext=\"\\mathbf{p}_{i}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m11\" intent=\":literal\"><semantics><msubsup><mi>&#119849;</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{p}_{i}^{t}</annotation></semantics></math> corresponds to the <math alttext=\"i_{th}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m12\" intent=\":literal\"><semantics><msub><mi>i</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">i_{th}</annotation></semantics></math> pose in motion at step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m13\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "while",
                    "data",
                    "represents",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering our diverse multi-granularity conditioning inputs, we empirically observed that employing all conditions within a single-stage training paradigm leads to difficulties in directly learning the correlation between motion and conditions. Moreover, the model tends to overfit strongly constrained low-level control signals such as fine-grained reference motion and spatiotemporal joint control. While these signals appear dominant, they can suppress other modalities such as text, ultimately compromising overall controllability.\nTo address this, we implement weak-to-strong progressive training: as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F2\" title=\"Figure 2 &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> initial text-conditioned learning establishes motion-semantic alignment, followed by progressive integration of finer-grained conditions, including reference motion, global motion, and audio signals.\nThis progressive approach enables the model to effectively adapt to different conditions, ensuring high-quality and flexible motion generation while precisely adhering to multimodal conditions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "global",
                    "while",
                    "model",
                    "generation",
                    "training",
                    "between",
                    "conditions",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method employs a Transformer Encoder architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib70\" title=\"\">2017</a>)</cite> with 8 layers and 8 attention heads, featuring a hidden dimension of <math alttext=\"d_{\\text{model}}=1536~(128\\times 12)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>=</mo><mrow><mn>1536</mn><mo lspace=\"0.330em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>128</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>12</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d_{\\text{model}}=1536~(128\\times 12)</annotation></semantics></math>, where <math alttext=\"128\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m2\" intent=\":literal\"><semantics><mn>128</mn><annotation encoding=\"application/x-tex\">128</annotation></semantics></math> represents the embedding size per each of the <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m3\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> body parts, and a feedforward dimension of <math alttext=\"3072\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m4\" intent=\":literal\"><semantics><mn>3072</mn><annotation encoding=\"application/x-tex\">3072</annotation></semantics></math>. Training is performed on a single H800 GPU through progressive conditioning: starting with text-only training for <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m5\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, then adding reference motion for another <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m6\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, followed by global spatiotemporal control for <math alttext=\"230K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>230</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">230K</annotation></semantics></math> steps, and finally incorporating full audio conditions for <math alttext=\"920K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>920</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">920K</annotation></semantics></math> steps. The corresponding batch sizes are <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m9\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m10\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m11\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, and <math alttext=\"16\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m12\" intent=\":literal\"><semantics><mn>16</mn><annotation encoding=\"application/x-tex\">16</annotation></semantics></math>, respectively. We optimize with AdamW using an initial learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> (reset for new conditions) and a cosine schedule decaying to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m14\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> within the first <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m15\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps. The default length of motion reference and prediction is <math alttext=\"150\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m16\" intent=\":literal\"><semantics><mn>150</mn><annotation encoding=\"application/x-tex\">150</annotation></semantics></math>. More implementation details are provided in supplementary material.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "respectively",
                    "global",
                    "training",
                    "method",
                    "conditions",
                    "represents",
                    "where",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;on four tasks: Text-to-Motion (T2M), Global Spatiotemporal Controllable Generation (GSTC), Music-to-Dance (M2D), and Speech-to-Gesture (S2G).\nFor T2M and GSTC evaluations, test sets are uniformly sampled across all datasets&#160;(<math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx2.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> samples from each).\nThe M2D benchmark integrates test sequences from AIST++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>)</cite>, and PhantomDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib36\" title=\"\">2022</a>)</cite>, with S2G evaluation conducted on BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "global",
                    "controllable",
                    "s2g",
                    "omnimotionx",
                    "m2d",
                    "generation",
                    "tasks",
                    "from",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">T2M.</span>\nTab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.T4\" title=\"Table 4 &#8227; Consistent Visual-textual Motion Captions &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that previous methods, constrained by small-scale datasets, struggle to generalize in generating diverse and complex motions.\nIn contrast, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;demonstrates a significant advantage, outperforming not only baseline methods trained on small datasets (MDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>)</cite>, MLD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>)</cite>, MoMask&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>)</cite>, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>) but also MoMask* and MotionCraft*, which are trained on our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>.\nCompared to MoMask* and MotionCraft*, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;uses stronger text encoder (T5-XXL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib65\" title=\"\">2020</a>)</cite>), which models complex texts more effectively than CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib64\" title=\"\">2021</a>)</cite>.\nIn addition, it adopts a unified multimodal framework with reference motion during training, enabling the model to learn more detailed whole-body motion representations, leading to better generation quality and generalization.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "momask",
                    "bian",
                    "guo",
                    "t2m",
                    "wholebody",
                    "2024a",
                    "generation",
                    "training",
                    "methods",
                    "trained",
                    "mdm",
                    "omnimotionx",
                    "tevet",
                    "datasets",
                    "during",
                    "motioncraft",
                    "model",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GSTC.</span>\nWe adopt the cross-joint setup from OmniControl&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>)</cite>, simulating spatially dense control by controlling all joints.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T5\" title=\"Table 5 &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, due to the small dataset size, OmniControl&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>)</cite> struggles to generalize in GSTC task.\nIn contrast, our method demonstrates clear advantages, effectively following spatially dense control signals.</p>\n\n",
                "matched_terms": [
                    "joints",
                    "from",
                    "method",
                    "dense",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">M2D and S2G.</span>\nWe directly compare our method with the MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> on the S2G and M2D tasks.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T6\" title=\"Table 6 &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves competitive performance. The lower FID is mainly due to the relatively small test sets in S2G and M2D, while <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is trained on the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, leading to some distribution differences. Additionally, our method demonstrates superior diversity, which may also impact the FID.</p>\n\n",
                "matched_terms": [
                    "while",
                    "s2g",
                    "omnimotionx",
                    "motioncraft",
                    "m2d",
                    "tasks",
                    "method",
                    "bian",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T7\" title=\"Table 7 &#8227; Ablation Study &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the ablation results and demonstrates two key findings: (1) Ablating coarse-to-fine training compromises text-conditioned alignment, indicating finer-grained controls may override coarse semantic constraints; (2) Mixed-condition training impairs spatiotemporal control due to physical constraints introducing optimization conflicts, necessitating weak-to-strong conditioning.</p>\n\n",
                "matched_terms": [
                    "mixedcondition",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F3\" title=\"Figure 3 &#8227; Unified Multimodal Modeling &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;handles various multimodal generation, including T2M, M2D, S2G, and GSTC (covering motion prediction, in-betweening, and joint guidance). When combined with reference motion, the model generates conditioned motions that align with the reference. More visualizations are in the supplementary video.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "s2g",
                    "omnimotionx",
                    "m2d",
                    "model",
                    "generation",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces the <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;and the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>.\n<em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is an autoregressive diffusion model that integrates reference motion and multimodal conditions for precise whole-body motion control. It employs a progressive weak-to-strong conditioning strategy to effectively handle multi-granular constraints.\n<em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;is the largest multimodal MoCap dataset, comprising <math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx6.p1.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours from <math alttext=\"28\" class=\"ltx_Math\" display=\"inline\" id=\"Sx6.p1.m2\" intent=\":literal\"><semantics><mn>28</mn><annotation encoding=\"application/x-tex\">28</annotation></semantics></math> motion capture datasets, unified in SMPL-X with structured and consistent captions across 10 tasks.\nExperiments demonstrate that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;outperforms baselines, laying a strong foundation for large-scale multimodal motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "hours",
                    "wholebody",
                    "omnimotionx",
                    "model",
                    "generation",
                    "from",
                    "tasks",
                    "conditions",
                    "dataset",
                    "datasets",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitation and Future Work.</span> While effective, our method currently lacks scene, object, and human interaction constraints, restricting its use in complex real-world applications. Furthermore, the sample-space denoising process results in slower inference speeds. Future work should focus on integrating interaction constraints to be more versatile and developing more efficient motion representations for improved physical consistency and inference speed.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "while",
                    "human",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Supplementary Material for OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</span>\n</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "omnimotionx",
                    "wholebody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx7.T8\" title=\"Table 8 &#8227; Dataset Text Quality &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents a comprehensive analysis of the text quality across our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;collection, which comprises over <math alttext=\"321,000\" class=\"ltx_Math\" display=\"inline\" id=\"Sx7.p1.m1\" intent=\":literal\"><semantics><mrow><mn>321</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">321,000</annotation></semantics></math> textual descriptions spanning various motion-related tasks. This extensive dataset was created by collecting and curating existing motion datasets, rendering motions to videos, and then leveraging vision-language models (VLMs) to generate rich textual descriptions.\nThe linguistic diversity of our dataset is evidenced by the Type-Token Ratio (TTR) ranges, with maximum values reaching 1.0 in multiple subsets (AIOZ, Motorica, IDEA400, etc.), indicating exceptional lexical richness. The average sentence length of 276.78 words across the entire collection demonstrates the descriptive depth of our motion annotations, providing detailed accounts of nuanced movements rather than simplistic action labels.\nParticularly noteworthy is the variety of verbs captured across different motion categories. The dataset exhibits task-specific verb distributions that align with the semantic nature of each motion type: T2M datasets frequently feature locomotion verbs (&#8220;walk,&#8221; &#8220;step&#8221;), M2D collections emphasize rhythmic movements (&#8220;swing,&#8221; &#8220;lift&#8221;), HOI datasets contain manipulation verbs (&#8220;lift,&#8221; &#8220;hold,&#8221; &#8220;grasp&#8221;), and HHI datasets capture interpersonal dynamics (&#8220;stand,&#8221; &#8220;extend,&#8221; &#8220;lean&#8221;). This semantic richness enables models trained on <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;to understand and generate precise motion descriptions across diverse contexts.\nThe comprehensive coverage across six distinct motion-related tasks (T2M, M2D, S2G, HHI, HOI, HSI) makes <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;uniquely positioned to support generalizable motion understanding. By integrating multiple motion paradigms under a unified annotation framework, our dataset transcends the limitations of task-specific collections, allowing for transfer learning across motion domains. This textual quality and diversity significantly enhance the capability of motion generation models to comprehend natural language instructions and produce corresponding human-like movements.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "existing",
                    "type",
                    "s2g",
                    "m2d",
                    "multiple",
                    "generation",
                    "tasks",
                    "dataset",
                    "trained",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Format Conversion.</span>\nSince many datasets are not in the SMPL-series format, we convert all datasets to the SMPL-X format for consistency. For BVH-format datasets such as Mixamo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib2\" title=\"\">Adobe </a>)</cite>, 100Style&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mason, Starke, and Komura <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib56\" title=\"\">2022</a>)</cite>, Motorica&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alexanderson et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib3\" title=\"\">2023</a>)</cite>, and LaFAN1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Harvey et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib26\" title=\"\">2020</a>)</cite>, we first standardize the reference pose of all BVH files to a T-pose to ensure consistent initialization. We then align the root node&#8217;s coordinate system with that of the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> model in Blender&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib9\" title=\"\">Blender Foundation </a>)</cite>, where the negative Y-axis is defined as the forward direction and the Z-axis as the vertical upward direction.\nTo adapt to the SMPL-X topology, we generate a corresponding BVH skeleton based on a predefined SMPL-X T-pose template. We perform skeleton retargeting in MotionBuilder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib6\" title=\"\">Autodesk Inc. </a>)</cite> to map the original animation data onto the SMPL-X hierarchy.\nFinally, we convert the retargeted BVH files from Euler angles to axis-angle representations and apply Gaussian filtering to smooth both joint rotations and translations over time, yielding stable SMPL-X parameters.\nFor the Choreomaster dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib11\" title=\"\">2021</a>)</cite>, originally in FBX format, we first convert it to BVH using Blender, and then process it using the same pipeline.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "model",
                    "data",
                    "from",
                    "dataset",
                    "where",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Normalization.</span>\nTo reduce the difficulty of model training, we perform temporal and spatial normalization on SMPL-X motion sequences. Specifically, we standardize all motion sequences in four steps. First, we align the initial frame of each sequence to face the positive Z-axis. Then, we reposition the starting frame to a common location with the feet just touching the ground. Next, we unify the frame rate across datasets to 30 fps. Finally, we segment each motion sequence into 5-second clips (150 frames).</p>\n\n",
                "matched_terms": [
                    "motion",
                    "model",
                    "datasets",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text and Motion Feature Extractors.</span>\nExisting research typically relies on pre-trained motion and text feature extractors for evaluation. However, due to significant differences in dataset scale, distribution, and motion representation compared to existing works, these pre-trained extractors are not directly applicable. Therefore, following the approach&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib52\" title=\"\">2024a</a>)</cite>, we re-train motion and text feature extractor using a contrastive learning framework tailored to this dataset. The text feature extractor is based on the Transformer encoder architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Petrovich, Black, and Varol <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib61\" title=\"\">2023</a>)</cite>, which encodes raw text into a semantic vector <math alttext=\"s_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">s_{t}</annotation></semantics></math>. The motion feature extractor also uses a Transformer encoder to encode motion sequences of up to 150 frames into a semantic vector <math alttext=\"s_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">s_{m}</annotation></semantics></math>. Both encoders include additional semantic tokens, with a structure similar to the encoder in ACTOR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Petrovich, Black, and Varol <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib60\" title=\"\">2021</a>)</cite>, but without involving probabilistic distribution modeling. In implementation, the text encoder takes as input text features extracted from a pre-trained and frozen DistilBERT network, while the motion encoder directly processes raw motion sequence data. In the contrastive learning framework, we optimize the feature space such that matching text-motion feature pairs <math alttext=\"(s_{t},s_{m})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(s_{t},s_{m})</annotation></semantics></math> are brought closer in the embedding space, while ensuring that non-matching pairs are separated by at least a distance <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>. This optimization objective is achieved through the following contrastive loss function:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "existing",
                    "while",
                    "2024a",
                    "data",
                    "from",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where, <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m5\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> is a binary label: <math alttext=\"y=0\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m6\" intent=\":literal\"><semantics><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">y=0</annotation></semantics></math> if <math alttext=\"s_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m7\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">s_{t}</annotation></semantics></math> and <math alttext=\"s_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m8\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">s_{m}</annotation></semantics></math> come from a matching text-motion pair, and <math alttext=\"y=1\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m9\" intent=\":literal\"><semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">y=1</annotation></semantics></math> otherwise. <math alttext=\"m&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m10\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">m&gt;0</annotation></semantics></math> is the margin for non-matching pairs, which is set to 10 in our experiments.</p>\n\n",
                "matched_terms": [
                    "from",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Mask.</span> Our model leverages four specialized masking mechanisms: (1) Component attention mask, the standard source key padding mask in Transformers&#8212;regulates attention across different conditional inputs. In our DiT-based architecture, input conditions may be partially missing; this mask zeroes out attention weights for missing inputs, allowing the model to focus only on valid ones. (2) Global task-dependent mask, a spatiotemporal dynamic mask tailored to tasks like motion prediction, interpolation, completion, and trajectory-guided generation&#8212;distinguishes between observed and unobserved motion regions based on task requirements. (3) Global motion disentanglement mask transforms dense global motion conditions into a hybrid sparse-dense representation by selecting key joints (e.g., <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p2.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> joints) for global representation optimization, while the remaining <math alttext=\"n-k\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8722;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">n-k</annotation></semantics></math> joints contribute to local motion representation and noise injection. (4) Motion representation reconstruction mask filters out reconstruction errors of joints without ground-truth annotations during training. For instance, it suppresses hand-joint errors in datasets lacking hand annotations, effectively addressing cross-dataset joint count mismatches.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "global",
                    "while",
                    "during",
                    "model",
                    "joints",
                    "tasks",
                    "training",
                    "dense",
                    "between",
                    "conditions",
                    "like",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and accuracy of our caption generation process, we selected GPT-4o, the current state-of-the-art closed-source multimodal model, as our primary captioning tool. To maximize caption accuracy and comprehensiveness, we implemented several key optimizations to the captioning pipeline. First, we significantly increased the number of input frames provided to the model, allowing for more comprehensive temporal understanding of the video content. Second, we enhanced the video rendering quality to ensure that visual details are clearly preserved and accurately conveyed to the model. Third, we conducted extensive prompt engineering to design optimal instructions that guide the model toward generating more precise and detailed captions that capture both visual elements and temporal dynamics (see Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx9.F4\" title=\"Figure 4 &#8227; More Implementation Details &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> for representative examples). These technical improvements collectively resulted in substantially improved caption quality compared to baseline approaches.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset supports various interaction types, including Human-Human Interaction (HHI), Human-Object Interaction (HOI), and Human-Scene Interaction (HSI), as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx9.F5\" title=\"Figure 5 &#8227; More Implementation Details &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Specifically, human-human interactions encompass a rich spectrum of social scenarios such as conversations, handshakes, embraces, and collaborative activities. Human-object interactions include everyday activities like grasping, manipulation, and tool usage. Human-scene interactions capture the dynamic relationships between individuals and their environments, such as sitting on indoor sofas with feet resting on carpets, and other contextual behavioral patterns. Each interaction type is accompanied by detailed spatio-temporal annotations and semantic descriptions, providing comprehensive training samples for multimodal whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "wholebody",
                    "type",
                    "generation",
                    "training",
                    "between",
                    "dataset",
                    "like"
                ]
            }
        ]
    },
    "Sx1.T2": {
        "caption": "Table 2: Comparisons between OmniMoCap-X and existing merged datasets. ”Mocap Source” indicates the proportion of mocap datasets. ”Caption Source” specifies the method for completing missing descriptions: ”-” (no completion), ”V” (visual information), and ”T” (textual information). ”Hierarchical Caption” shows if captions include hierarchical text.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Dataset</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"6\">Tasks</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Whole-Body</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Motion Source</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Caption</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Hierarchical</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Frames</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Hours</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">T2M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">M2D</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">S2G</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">HOI</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">HSI</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">HHI</td>\n<td class=\"ltx_td ltx_align_center\">Mocap / Total</td>\n<td class=\"ltx_td ltx_align_center\">Source</td>\n<td class=\"ltx_td ltx_align_center\">Caption</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Motion-X (2023-07)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m2\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m3\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m4\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m5\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m6\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m7\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2 / 9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">T (9)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m8\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">15.6M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">144.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">OMG (2024-03)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m9\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m10\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m11\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m12\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m13\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m14\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m15\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">9 / 13</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m16\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">22.3M</td>\n<td class=\"ltx_td ltx_align_center\">206.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MotionUnion (2024-11)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m17\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m18\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m19\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m20\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m21\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m22\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m23\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">4 / 15</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m24\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">30M</td>\n<td class=\"ltx_td ltx_align_center\">260</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MotionVerse (2024-04)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m25\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m26\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m27\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m28\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m29\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m30\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m31\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">5 / 16</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m32\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">100M</span></td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MotionHub (2024-11)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m33\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m34\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m35\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m36\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m37\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m38\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m39\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">8 / 19</td>\n<td class=\"ltx_td ltx_align_center\">V (3) / T (7)</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m40\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">70.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;(Ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m41\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m42\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m43\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m44\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m45\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m46\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m47\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">21</span> <math alttext=\"/\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m48\" intent=\":literal\"><semantics><mo>/</mo><annotation encoding=\"application/x-tex\">/</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">28</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">V + T (28)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math alttext=\"\\checkmark\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.T2.m49\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#10003;</mi><annotation encoding=\"application/x-tex\">\\checkmark</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">64.3M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">286.2</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "motion",
            "hoi",
            "2024b",
            "proportion",
            "include",
            "visual",
            "223m",
            "30m",
            "2024c",
            "caption”",
            "source",
            "textual",
            "information",
            "mocap",
            "t2m",
            "liang",
            "caption",
            "frames",
            "”t”",
            "wholebody",
            "”mocap",
            "hours",
            "s2g",
            "2024a",
            "tasks",
            "merged",
            "between",
            "descriptions",
            "text",
            "omnimocapx",
            "lin",
            "ling",
            "156m",
            "”caption",
            "100m",
            "×times",
            "zhang",
            "missing",
            "comparisons",
            "”v”",
            "643m",
            "indicates",
            "motionhub",
            "motionverse",
            "hsi",
            "datasets",
            "hierarchical",
            "ours",
            "existing",
            "”hierarchical",
            "omg",
            "total",
            "✓checkmark",
            "m2d",
            "motionx",
            "hhi",
            "motionunion",
            "shows",
            "source”",
            "method",
            "specifies",
            "dataset",
            "completing",
            "captions",
            "completion"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Although recent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>; Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>)</cite> attempt to unify multitasks in one model, they meet challenges in multimodal control modeling, versatile tasks, and high-quality motion generation (as shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx1.T1\" title=\"Table 1 &#8227; Introduction &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>):\n(1) <span class=\"ltx_text ltx_font_bold\">Independent Model Training.</span> Previous approaches train separate models for each modality, limiting simultaneous control across inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>)</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">Additional Control Branches.</span> Some methods add separate control branches for each condition, limiting interaction between them&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>)</cite>.\n(3) <span class=\"ltx_text ltx_font_bold\">Conflict Granularity Training.</span> Existing methods use mixed training by combining high-level semantic conditions with low-level controls, which hampers effective control at different levels and leads to optimization challenges&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Luo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib54\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>. Similar phenomena have also been observed in video generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib44\" title=\"\">2025</a>; Ao <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib4\" title=\"\">2024</a>)</cite>.\nIn addition to modeling challenges, relevant works also introduce large-scale motion datasets from multiple tasks and modalities&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, they still exhibit the following significant shortcomings (see comparisons in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx1.T2\" title=\"Table 2 &#8227; Introduction &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):\n(1) <span class=\"ltx_text ltx_font_bold\">Low Motion Quality.</span> Datasets expanded with non-mocap motion estimation exhibit &#8220;garbage in, garbage out&#8221; effects, leading to poor-quality motion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>)</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">Text Inconsistency.</span> Inconsistent text annotations or expanded LLM descriptions lead to uneven text quality and hallucination issues&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>)</cite>.\nand (3) <span class=\"ltx_text ltx_font_bold\">limited tasks.</span> They focus on common tasks, limiting applicability in diverse ones like HOI, HSI, and HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite>.\nThese limitations hinder the development of a unified, high-quality dataset for multimodal whole-body human motion generation across diverse scenarios.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper introduces <span class=\"ltx_text ltx_font_bold\">OmniMotion-X</span>, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks&#8212;including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis)&#8212;as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct <span class=\"ltx_text ltx_font_bold\">OmniMoCap-X</span>, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "mocap",
                    "existing",
                    "wholebody",
                    "captions",
                    "tasks",
                    "completion",
                    "dataset",
                    "omnimocapx",
                    "hierarchical"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_caption\">We present <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified sequence-to-sequence autoregressive motion diffusion transformer designed for flexible and interactive whole-body human motion generation. It supports a variety of tasks, including text-to-motion, music-to-dance, speech-to-gesture, and globally spatial-temporal controllable motion generation, which encompasses motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis. These conditions can be combined in various ways to enable versatile motion generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "motion",
                    "tasks",
                    "wholebody",
                    "completion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal whole-body human motion generation plays critical roles in animation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib13\" title=\"\">2024b</a>)</cite>, gaming&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib43\" title=\"\">2024</a>)</cite>, virtual reality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib24\" title=\"\">2024b</a>)</cite>, and embodied intelligence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib55\" title=\"\">2024</a>)</cite> across diverse input conditions, including text, audio, and trajectory, etc.\nThe limited multimodal data and task-specific model designs prevent existing motion generation methods from supporting multimodal whole-body human motion generation. Due to the high cost of mocap data collection and labeling, most datasets focus on single domains such as Text-to-Motion (T2M)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Plappert, Mandery, and Asfour <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib62\" title=\"\">2016</a>)</cite>, Music-to-Dance (M2D)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, Speech-to-Gesture (S2G)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>)</cite>, Human-Object Interaction (HOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>)</cite>, Human-Scene Interaction (HSI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and Human-Human Interaction (HHI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>)</cite>, each with inconsistent data formats (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, BVH, SMPL-(H/X) and 3D Keypoints) and control conditions (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Text captions, Audio, and Trajectories).\nTo tackle motion generation across diverse scenarios, it is crucial to build a unified framework that utilizes large-scale, diverse data to achieve more generalized representations.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "mocap",
                    "liang",
                    "existing",
                    "wholebody",
                    "s2g",
                    "hoi",
                    "captions",
                    "2024b",
                    "m2d",
                    "hsi",
                    "hhi",
                    "text",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address existing challenges, we propose <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified framework for multimodal whole-body human motion generation, and <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset.\nSpecifically, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;employs Diffusion Transformers (DiTs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles and Xie <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib59\" title=\"\">2023</a>)</cite>, incorporates multimodal conditions by concatenating condition tokens as prefix context, and adopts a progressive weak-to-strong mixed-condition training strategy to gradually constrain motion from high-level semantics to dense spatial-temporal alignment.\nNotably, unlike previous methods, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;introduces a novel generation paradigm that utilizes reference motion (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, user-provided or model-predicted motion) as a special condition. This significantly enhances generated motion quality and achieves consistency between reference and generated motion, creating an effective clip-by-clip autoregressive motion diffusion.\nThis enables <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;to support autoregressive interactive generation with strong temporal alignment.\nFurthermore, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;unifies various Global Spatial-Temporal Controllable Generation tasks through spatial-temporal masking strategies.\nTo guarantee high-quality motion generation training, we collect high-quality mocap datasets that support diverse motion generation tasks, unify them under the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format with standard world coordinate systems, and automatically generate hierarchical text captions by rendering motions into videos and annotating them with vision language models (VLMs).\nThis dataset contains about <em class=\"ltx_emph ltx_font_italic\"><math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.p3.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours</em> and integrates multimodal control conditions, supporting versatile tasks, including T2M, M2D, S2G, HOI, HSI, and HHI.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "hoi",
                    "mocap",
                    "t2m",
                    "wholebody",
                    "hours",
                    "s2g",
                    "tasks",
                    "between",
                    "text",
                    "omnimocapx",
                    "hsi",
                    "datasets",
                    "hierarchical",
                    "existing",
                    "m2d",
                    "hhi",
                    "dataset",
                    "captions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a multimodal autoregressive diffusion transformer for versatile whole-body motion generation. By introducing reference motion, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;significantly enhances consistent content, style, and temporal dynamics generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "wholebody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset with a unified SMPL-X format, and provides consistent, detailed, and structured text captions to serve versatile motion generation tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "mocap",
                    "captions",
                    "tasks",
                    "dataset",
                    "text",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments show that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves state-of-the-art performance across various tasks, including T2M, S2G, M2D, and GSTC. These evaluations were conducted on our more challenging test sets comprising <math alttext=\"280\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.I1.i4.p1.m1\" intent=\":literal\"><semantics><mn>280</mn><annotation encoding=\"application/x-tex\">280</annotation></semantics></math> samples uniformly sampled from our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "s2g",
                    "m2d",
                    "tasks",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing methods are typically categorized based on input conditions into three main types: single-modal, cross-modal, and multimodal.\nSingle-modal motion generation uses control conditions from the same modality as the motion, including motion prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib73\" title=\"\">2024a</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib12\" title=\"\">2023a</a>)</cite>, in-betweening&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cohan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib15\" title=\"\">2024</a>)</cite>, and joint/trajectory-guided synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>; Dai et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib16\" title=\"\">2024</a>; Zhao, Li, and Tang <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib84\" title=\"\">2024</a>)</cite>.\nCross-modal motion generation uses control conditions from different modalities, including text in T2M&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>; Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib52\" title=\"\">2024a</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib30\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib83\" title=\"\">2024d</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib80\" title=\"\">2023b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib81\" title=\"\">2024b</a>)</cite>, music in M2D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib35\" title=\"\">2023</a>; Siyao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib66\" title=\"\">2022</a>; Tseng, Castellon, and Liu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib68\" title=\"\">2023</a>)</cite>, speech in S2G&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib10\" title=\"\">2024a</a>)</cite>, target object positions in HOI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib50\" title=\"\">2024</a>)</cite>, scene layouts in HSI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kaufmann et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib33\" title=\"\">2023</a>; Huang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib27\" title=\"\">2022</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and partner movements in HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>)</cite>.\nHowever, these approaches typically rely on task-specific architectures, significantly limiting their cross-task generalization capabilities and practical applications.\nRecently, researchers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Luo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib54\" title=\"\">2024</a>)</cite> have begun exploring multimodal motion generation, with approaches falling into three primary categories:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "liang",
                    "ling",
                    "existing",
                    "s2g",
                    "hoi",
                    "2024a",
                    "2024b",
                    "2024c",
                    "zhang",
                    "m2d",
                    "hsi",
                    "hhi",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Additional Control Branch.</span> These approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>)</cite> incorporate separate control branches into the backbone architecture, each dedicated to a specific condition, resulting in limited interaction between different conditions.</p>\n\n",
                "matched_terms": [
                    "ling",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Multimodal Condition Modeling.</span> These approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite> use a single model to learn mappings from diverse modalities to target motions, enhancing multi-modal adaptability. However, challenges arise from different constraints across modalities: text provides semantic guidance, spatial-temporal controls impose physical constraints, while audio inputs enforce rhythmic alignment. These disparate constraints often result in compromised controllability and optimization difficulties.</p>\n\n",
                "matched_terms": [
                    "text",
                    "ling",
                    "2024c",
                    "zhang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current human motion generation datasets are predominantly task-specific.\nDifferent tasks rely on separate collections: text-to-motion (T2M) uses datasets with text captions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Plappert, Mandery, and Asfour <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib62\" title=\"\">2016</a>; Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Punnakkal et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib63\" title=\"\">2021</a>; Liao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib43\" title=\"\">2024</a>)</cite>, music-to-dance (M2D) requires music-annotated datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib35\" title=\"\">2023</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib11\" title=\"\">2021</a>)</cite>, and speech-to-gesture (S2G) employs datasets with paired speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib49\" title=\"\">2022</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>)</cite>.\nSimilarly, interaction datasets remain disconnected across human-object&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib31\" title=\"\">2022</a>; Li, Wu, and Liu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib37\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib79\" title=\"\">2024a</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib50\" title=\"\">2024</a>; Zhan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib77\" title=\"\">2024</a>)</cite>, human-scene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and human-human interaction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>)</cite>. This fragmentation constrains multimodal motion generation capabilities.\nIntegrating these datasets presents challenges due to inconsistent motion formats.\nDatasets vary widely in their representations: some use SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, others employ SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>)</cite>, while many rely on BVH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Harvey et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib26\" title=\"\">2020</a>; Mason, Starke, and Komura <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib56\" title=\"\">2022</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib2\" title=\"\">Adobe </a>; Alexanderson et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib3\" title=\"\">2023</a>; Valle-P&#233;rez et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib69\" title=\"\">2021</a>)</cite> or keypoint representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ionescu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib28\" title=\"\">2013</a>; Ji et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib29\" title=\"\">2018</a>)</cite>. This diversity makes format standardization extremely difficult.\nRecent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite> have attempted to integrate multiple datasets, increasing scale and incorporating some multimodal conditions. However, they still exhibit several limitations:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "liang",
                    "lin",
                    "ling",
                    "s2g",
                    "2024a",
                    "2024b",
                    "captions",
                    "2024c",
                    "zhang",
                    "m2d",
                    "tasks",
                    "text",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-quality Motions</span>: Most integrated datasets derive from non-mocap sources with significant estimation errors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, creating a &#8220;garbage in, garbage out&#8221; effect that compromises motion generation quality.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "lin",
                    "ling",
                    "2024b",
                    "2024c",
                    "zhang",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text Inconsistency</span>: Existing datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite> use inconsistent text annotations (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, action labels versus semantic descriptions) or rely on LLMs for text refinement without visual motion data, causing variable text quality and hallucinations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "liang",
                    "lin",
                    "ling",
                    "existing",
                    "2024a",
                    "2024b",
                    "2024c",
                    "zhang",
                    "visual",
                    "descriptions",
                    "text",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Generation Scenarios</span>: Most datasets lack support for critical interaction tasks including HOI, HSI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, and HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>)</cite>, severely limiting their applicability to complex scenarios.</p>\n\n",
                "matched_terms": [
                    "liang",
                    "lin",
                    "ling",
                    "hoi",
                    "2024a",
                    "2024b",
                    "2024c",
                    "zhang",
                    "hhi",
                    "tasks",
                    "hsi",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the data challenges&#8212;the scarcity of high-quality multitask motion datasets, inconsistent textual annotations and motion representations&#8212;we implement a three-pronged approach: (1) integrating diverse motion capture datasets with multitask support, (2) establishing a unified motion representation, and (3) developing high-quality motion captions derived from visual-textual annotations.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "captions",
                    "textual",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While existing approaches prioritize dataset scale over quality by incorporating substantial amounts of non-mocap data&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, leading to degraded motion generation quality (&#8221;garbage in, garbage out&#8221;), our work emphasizes data quality.\nWe systematically curate open-source mocap datasets and classify them into five acquisition categories: Marker with manual correction, Marker (Vicon), IMU, Multi-View RGB, and Single-View RGB (detailed in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.T3\" title=\"Table 3 &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nOur dataset supports diverse motion generation tasks, including text-to-motion, motion-to-dance, speech-to-gesture, and various interactions (human-object, human-scene, and human-human), providing a high-quality multimodal foundation. Comprising <math alttext=\"64.3\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m1\" intent=\":literal\"><semantics><mn>64.3</mn><annotation encoding=\"application/x-tex\">64.3</annotation></semantics></math> million frames and <math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m2\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours of data, our approach achieves comprehensive task coverage while maintaining superior data quality.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "mocap",
                    "lin",
                    "frames",
                    "ling",
                    "hours",
                    "existing",
                    "2024b",
                    "2024c",
                    "zhang",
                    "tasks",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We standardize diverse motion formats (BVH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lauterbach et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib34\" title=\"\">2009</a>)</cite>, FBX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wikipedia contributors <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib71\" title=\"\">2024</a>)</cite>, and SMPL&#160;(-H)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib51\" title=\"\">2023</a>)</cite>) into the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format to support our unified multimodal model.\nFirst, we convert these formats into the Motion-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>)</cite> SMPL-X format, comprising root orientation, pose parameters (body, hand, and jaw), facial expressions, facial shape, translation, and body shape parameters.\nWe then normalize the translation scale and initial root orientation across datasets to establish a consistent coordinate system. Additionally, we resample all datasets to 30 frames per second (FPS) to enhance the model&#8217;s ability to capture temporal patterns.\nThe specific conversion process and normalization for different datasets are provided in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "lin",
                    "frames",
                    "motionx",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Refinement</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite> employs LLMs like GPT-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib1\" title=\"\">2023</a>)</cite> to enhance textual descriptions. However, since LLMs cannot directly perceive motion data and rely solely on existing text, they frequently introduce hallucinations and fail to capture precise action details.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "lin",
                    "existing",
                    "descriptions",
                    "2024b",
                    "textual",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Manual Annotation</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Punnakkal et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib63\" title=\"\">2021</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite> effectively eliminates hallucinations but proves costly and difficult to scale, resulting in limited annotations with simple descriptions.</p>\n\n",
                "matched_terms": [
                    "descriptions",
                    "ling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Motion-to-Text Model-Based Annotation</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delmas et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib17\" title=\"\">2022</a>; Yazdian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib75\" title=\"\">2023</a>)</cite> generates captions directly from raw motion. While these methods effectively capture low-level kinematic details (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, left elbow bends 45 degrees), they lack the understanding of high-level semantics and action context.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "captions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we propose an automated approach that integrates both visual and textual information for comprehensive motion annotation.\nOur method renders motion into videos and combines them with existing textual annotations (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, descriptions, action labels, and task categories) as input to the state-of-the-art vision-language models (VLMs), GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib1\" title=\"\">2023</a>)</cite>.\nThis multimodal approach generates structured, hierarchical, and precise motion captions that ensure both annotation quality and expression richness. Detailed statistics of the texts quality are in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "existing",
                    "descriptions",
                    "visual",
                    "method",
                    "hierarchical",
                    "textual",
                    "captions",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.F1\" title=\"Figure 1 &#8227; Unified Motion Representation &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nWe propose a unified multimodal, autoregressive transformer-based diffusion model for whole-body human motion generation across multiple tasks.\nThe model takes textual description <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> for semantic guidance, global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> for spatial-temporal control, and speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> and music conditions <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> to ensure rhythmic and stylistic coherence.\nFurthermore, it incorporates reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> as a motion prior, derived from either previously generated clips or user-designed motion, thus providing fine-grained details unavailable in other conditions.\nIn addition, the mixed-condition setting allows multiple conditions to occur simultaneously during training, enabling the model to handle complex scenarios with diverse inputs.\nIn this section, we present our approach in two parts: a unified framework for multimodal modeling and a progressive training strategy that ensures precise motion control.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "textual",
                    "tasks",
                    "wholebody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Conditions.</span> Our framework integrates multiple condition modalities: text <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> providing semantic guidance; global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> ensuring spatial-temporal consistency; speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> synchronizing gestures and lip movements with rhythm; music <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> supplying beat and style information for dance; and reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> serving as a motion prior. Notably, this reference motion condition&#8212;overlooked in previous multimodal motion generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>&#8212;enables our model to maintain precise spatial-temporal pattern consistency with reference, substantially enhancing motion quality and coherence.\nTo fully leverage each modality and facilitate interaction between different conditions, we employ modality-specific encoders (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, T5-XXL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib65\" title=\"\">2020</a>)</cite> for text, a wav encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite> for speech, Librosa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McFee et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib57\" title=\"\">2015</a>)</cite> for music and body-wise encoding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> for motion) to extract features from each modality. These features are aligned to match motion embedding dimensions using learnable linear projections, allowing us to concatenate all condition tokens as prefix context with noisy motion tokens during processing.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "ling",
                    "2024c",
                    "zhang",
                    "between",
                    "text",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> and <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m2\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> denote the modality-specific encoder and projection layer, respectively. The concatenated representation <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m3\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is then fed into our DiT backbone as a conditioning prefix context.\nTo constrain the physical properties of motion, we follow previous works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib13\" title=\"\">2024b</a>)</cite> by directly predicting motion <math alttext=\"\\hat{x}_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\hat{x}_{0}</annotation></semantics></math> rather than noise. Consequently, our diffusion objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "2024b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering our diverse multi-granularity conditioning inputs, we empirically observed that employing all conditions within a single-stage training paradigm leads to difficulties in directly learning the correlation between motion and conditions. Moreover, the model tends to overfit strongly constrained low-level control signals such as fine-grained reference motion and spatiotemporal joint control. While these signals appear dominant, they can suppress other modalities such as text, ultimately compromising overall controllability.\nTo address this, we implement weak-to-strong progressive training: as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F2\" title=\"Figure 2 &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> initial text-conditioned learning establishes motion-semantic alignment, followed by progressive integration of finer-grained conditions, including reference motion, global motion, and audio signals.\nThis progressive approach enables the model to effectively adapt to different conditions, ensuring high-quality and flexible motion generation while precisely adhering to multimodal conditions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "text",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method employs a Transformer Encoder architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib70\" title=\"\">2017</a>)</cite> with 8 layers and 8 attention heads, featuring a hidden dimension of <math alttext=\"d_{\\text{model}}=1536~(128\\times 12)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>=</mo><mrow><mn>1536</mn><mo lspace=\"0.330em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>128</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>12</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d_{\\text{model}}=1536~(128\\times 12)</annotation></semantics></math>, where <math alttext=\"128\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m2\" intent=\":literal\"><semantics><mn>128</mn><annotation encoding=\"application/x-tex\">128</annotation></semantics></math> represents the embedding size per each of the <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m3\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> body parts, and a feedforward dimension of <math alttext=\"3072\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m4\" intent=\":literal\"><semantics><mn>3072</mn><annotation encoding=\"application/x-tex\">3072</annotation></semantics></math>. Training is performed on a single H800 GPU through progressive conditioning: starting with text-only training for <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m5\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, then adding reference motion for another <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m6\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, followed by global spatiotemporal control for <math alttext=\"230K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>230</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">230K</annotation></semantics></math> steps, and finally incorporating full audio conditions for <math alttext=\"920K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>920</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">920K</annotation></semantics></math> steps. The corresponding batch sizes are <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m9\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m10\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m11\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, and <math alttext=\"16\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m12\" intent=\":literal\"><semantics><mn>16</mn><annotation encoding=\"application/x-tex\">16</annotation></semantics></math>, respectively. We optimize with AdamW using an initial learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> (reset for new conditions) and a cosine schedule decaying to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m14\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> within the first <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m15\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps. The default length of motion reference and prediction is <math alttext=\"150\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m16\" intent=\":literal\"><semantics><mn>150</mn><annotation encoding=\"application/x-tex\">150</annotation></semantics></math>. More implementation details are provided in supplementary material.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;on four tasks: Text-to-Motion (T2M), Global Spatiotemporal Controllable Generation (GSTC), Music-to-Dance (M2D), and Speech-to-Gesture (S2G).\nFor T2M and GSTC evaluations, test sets are uniformly sampled across all datasets&#160;(<math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx2.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> samples from each).\nThe M2D benchmark integrates test sequences from AIST++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>)</cite>, and PhantomDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib36\" title=\"\">2022</a>)</cite>, with S2G evaluation conducted on BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "s2g",
                    "m2d",
                    "tasks",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">T2M.</span>\nTab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.T4\" title=\"Table 4 &#8227; Consistent Visual-textual Motion Captions &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that previous methods, constrained by small-scale datasets, struggle to generalize in generating diverse and complex motions.\nIn contrast, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;demonstrates a significant advantage, outperforming not only baseline methods trained on small datasets (MDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>)</cite>, MLD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>)</cite>, MoMask&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>)</cite>, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>) but also MoMask* and MotionCraft*, which are trained on our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>.\nCompared to MoMask* and MotionCraft*, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;uses stronger text encoder (T5-XXL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib65\" title=\"\">2020</a>)</cite>), which models complex texts more effectively than CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib64\" title=\"\">2021</a>)</cite>.\nIn addition, it adopts a unified multimodal framework with reference motion during training, enabling the model to learn more detailed whole-body motion representations, leading to better generation quality and generalization.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "datasets",
                    "wholebody",
                    "2024a",
                    "shows",
                    "text",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GSTC.</span>\nWe adopt the cross-joint setup from OmniControl&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>)</cite>, simulating spatially dense control by controlling all joints.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T5\" title=\"Table 5 &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, due to the small dataset size, OmniControl&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>)</cite> struggles to generalize in GSTC task.\nIn contrast, our method demonstrates clear advantages, effectively following spatially dense control signals.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">M2D and S2G.</span>\nWe directly compare our method with the MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> on the S2G and M2D tasks.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T6\" title=\"Table 6 &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves competitive performance. The lower FID is mainly due to the relatively small test sets in S2G and M2D, while <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is trained on the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, leading to some distribution differences. Additionally, our method demonstrates superior diversity, which may also impact the FID.</p>\n\n",
                "matched_terms": [
                    "s2g",
                    "m2d",
                    "tasks",
                    "method",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F3\" title=\"Figure 3 &#8227; Unified Multimodal Modeling &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;handles various multimodal generation, including T2M, M2D, S2G, and GSTC (covering motion prediction, in-betweening, and joint guidance). When combined with reference motion, the model generates conditioned motions that align with the reference. More visualizations are in the supplementary video.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "m2d",
                    "s2g"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces the <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;and the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>.\n<em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is an autoregressive diffusion model that integrates reference motion and multimodal conditions for precise whole-body motion control. It employs a progressive weak-to-strong conditioning strategy to effectively handle multi-granular constraints.\n<em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;is the largest multimodal MoCap dataset, comprising <math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx6.p1.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours from <math alttext=\"28\" class=\"ltx_Math\" display=\"inline\" id=\"Sx6.p1.m2\" intent=\":literal\"><semantics><mn>28</mn><annotation encoding=\"application/x-tex\">28</annotation></semantics></math> motion capture datasets, unified in SMPL-X with structured and consistent captions across 10 tasks.\nExperiments demonstrate that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;outperforms baselines, laying a strong foundation for large-scale multimodal motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "mocap",
                    "datasets",
                    "hours",
                    "wholebody",
                    "captions",
                    "tasks",
                    "dataset",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitation and Future Work.</span> While effective, our method currently lacks scene, object, and human interaction constraints, restricting its use in complex real-world applications. Furthermore, the sample-space denoising process results in slower inference speeds. Future work should focus on integrating interaction constraints to be more versatile and developing more efficient motion representations for improved physical consistency and inference speed.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Supplementary Material for OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</span>\n</p>\n\n",
                "matched_terms": [
                    "motion",
                    "wholebody"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx7.T8\" title=\"Table 8 &#8227; Dataset Text Quality &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents a comprehensive analysis of the text quality across our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;collection, which comprises over <math alttext=\"321,000\" class=\"ltx_Math\" display=\"inline\" id=\"Sx7.p1.m1\" intent=\":literal\"><semantics><mrow><mn>321</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">321,000</annotation></semantics></math> textual descriptions spanning various motion-related tasks. This extensive dataset was created by collecting and curating existing motion datasets, rendering motions to videos, and then leveraging vision-language models (VLMs) to generate rich textual descriptions.\nThe linguistic diversity of our dataset is evidenced by the Type-Token Ratio (TTR) ranges, with maximum values reaching 1.0 in multiple subsets (AIOZ, Motorica, IDEA400, etc.), indicating exceptional lexical richness. The average sentence length of 276.78 words across the entire collection demonstrates the descriptive depth of our motion annotations, providing detailed accounts of nuanced movements rather than simplistic action labels.\nParticularly noteworthy is the variety of verbs captured across different motion categories. The dataset exhibits task-specific verb distributions that align with the semantic nature of each motion type: T2M datasets frequently feature locomotion verbs (&#8220;walk,&#8221; &#8220;step&#8221;), M2D collections emphasize rhythmic movements (&#8220;swing,&#8221; &#8220;lift&#8221;), HOI datasets contain manipulation verbs (&#8220;lift,&#8221; &#8220;hold,&#8221; &#8220;grasp&#8221;), and HHI datasets capture interpersonal dynamics (&#8220;stand,&#8221; &#8220;extend,&#8221; &#8220;lean&#8221;). This semantic richness enables models trained on <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;to understand and generate precise motion descriptions across diverse contexts.\nThe comprehensive coverage across six distinct motion-related tasks (T2M, M2D, S2G, HHI, HOI, HSI) makes <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;uniquely positioned to support generalizable motion understanding. By integrating multiple motion paradigms under a unified annotation framework, our dataset transcends the limitations of task-specific collections, allowing for transfer learning across motion domains. This textual quality and diversity significantly enhance the capability of motion generation models to comprehend natural language instructions and produce corresponding human-like movements.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "datasets",
                    "existing",
                    "s2g",
                    "hoi",
                    "descriptions",
                    "m2d",
                    "hsi",
                    "hhi",
                    "tasks",
                    "dataset",
                    "textual",
                    "text",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Format Conversion.</span>\nSince many datasets are not in the SMPL-series format, we convert all datasets to the SMPL-X format for consistency. For BVH-format datasets such as Mixamo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib2\" title=\"\">Adobe </a>)</cite>, 100Style&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mason, Starke, and Komura <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib56\" title=\"\">2022</a>)</cite>, Motorica&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alexanderson et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib3\" title=\"\">2023</a>)</cite>, and LaFAN1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Harvey et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib26\" title=\"\">2020</a>)</cite>, we first standardize the reference pose of all BVH files to a T-pose to ensure consistent initialization. We then align the root node&#8217;s coordinate system with that of the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> model in Blender&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib9\" title=\"\">Blender Foundation </a>)</cite>, where the negative Y-axis is defined as the forward direction and the Z-axis as the vertical upward direction.\nTo adapt to the SMPL-X topology, we generate a corresponding BVH skeleton based on a predefined SMPL-X T-pose template. We perform skeleton retargeting in MotionBuilder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib6\" title=\"\">Autodesk Inc. </a>)</cite> to map the original animation data onto the SMPL-X hierarchy.\nFinally, we convert the retargeted BVH files from Euler angles to axis-angle representations and apply Gaussian filtering to smooth both joint rotations and translations over time, yielding stable SMPL-X parameters.\nFor the Choreomaster dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib11\" title=\"\">2021</a>)</cite>, originally in FBX format, we first convert it to BVH using Blender, and then process it using the same pipeline.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Normalization.</span>\nTo reduce the difficulty of model training, we perform temporal and spatial normalization on SMPL-X motion sequences. Specifically, we standardize all motion sequences in four steps. First, we align the initial frame of each sequence to face the positive Z-axis. Then, we reposition the starting frame to a common location with the feet just touching the ground. Next, we unify the frame rate across datasets to 30 fps. Finally, we segment each motion sequence into 5-second clips (150 frames).</p>\n\n",
                "matched_terms": [
                    "motion",
                    "frames",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text and Motion Feature Extractors.</span>\nExisting research typically relies on pre-trained motion and text feature extractors for evaluation. However, due to significant differences in dataset scale, distribution, and motion representation compared to existing works, these pre-trained extractors are not directly applicable. Therefore, following the approach&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib52\" title=\"\">2024a</a>)</cite>, we re-train motion and text feature extractor using a contrastive learning framework tailored to this dataset. The text feature extractor is based on the Transformer encoder architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Petrovich, Black, and Varol <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib61\" title=\"\">2023</a>)</cite>, which encodes raw text into a semantic vector <math alttext=\"s_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">s_{t}</annotation></semantics></math>. The motion feature extractor also uses a Transformer encoder to encode motion sequences of up to 150 frames into a semantic vector <math alttext=\"s_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">s_{m}</annotation></semantics></math>. Both encoders include additional semantic tokens, with a structure similar to the encoder in ACTOR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Petrovich, Black, and Varol <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib60\" title=\"\">2021</a>)</cite>, but without involving probabilistic distribution modeling. In implementation, the text encoder takes as input text features extracted from a pre-trained and frozen DistilBERT network, while the motion encoder directly processes raw motion sequence data. In the contrastive learning framework, we optimize the feature space such that matching text-motion feature pairs <math alttext=\"(s_{t},s_{m})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(s_{t},s_{m})</annotation></semantics></math> are brought closer in the embedding space, while ensuring that non-matching pairs are separated by at least a distance <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>. This optimization objective is achieved through the following contrastive loss function:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "frames",
                    "existing",
                    "2024a",
                    "include",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Mask.</span> Our model leverages four specialized masking mechanisms: (1) Component attention mask, the standard source key padding mask in Transformers&#8212;regulates attention across different conditional inputs. In our DiT-based architecture, input conditions may be partially missing; this mask zeroes out attention weights for missing inputs, allowing the model to focus only on valid ones. (2) Global task-dependent mask, a spatiotemporal dynamic mask tailored to tasks like motion prediction, interpolation, completion, and trajectory-guided generation&#8212;distinguishes between observed and unobserved motion regions based on task requirements. (3) Global motion disentanglement mask transforms dense global motion conditions into a hybrid sparse-dense representation by selecting key joints (e.g., <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p2.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> joints) for global representation optimization, while the remaining <math alttext=\"n-k\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8722;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">n-k</annotation></semantics></math> joints contribute to local motion representation and noise injection. (4) Motion representation reconstruction mask filters out reconstruction errors of joints without ground-truth annotations during training. For instance, it suppresses hand-joint errors in datasets lacking hand annotations, effectively addressing cross-dataset joint count mismatches.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "missing",
                    "source",
                    "tasks",
                    "between",
                    "datasets",
                    "completion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and accuracy of our caption generation process, we selected GPT-4o, the current state-of-the-art closed-source multimodal model, as our primary captioning tool. To maximize caption accuracy and comprehensiveness, we implemented several key optimizations to the captioning pipeline. First, we significantly increased the number of input frames provided to the model, allowing for more comprehensive temporal understanding of the video content. Second, we enhanced the video rendering quality to ensure that visual details are clearly preserved and accurately conveyed to the model. Third, we conducted extensive prompt engineering to design optimal instructions that guide the model toward generating more precise and detailed captions that capture both visual elements and temporal dynamics (see Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx9.F4\" title=\"Figure 4 &#8227; More Implementation Details &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> for representative examples). These technical improvements collectively resulted in substantially improved caption quality compared to baseline approaches.</p>\n\n",
                "matched_terms": [
                    "captions",
                    "caption",
                    "frames",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset supports various interaction types, including Human-Human Interaction (HHI), Human-Object Interaction (HOI), and Human-Scene Interaction (HSI), as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx9.F5\" title=\"Figure 5 &#8227; More Implementation Details &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Specifically, human-human interactions encompass a rich spectrum of social scenarios such as conversations, handshakes, embraces, and collaborative activities. Human-object interactions include everyday activities like grasping, manipulation, and tool usage. Human-scene interactions capture the dynamic relationships between individuals and their environments, such as sitting on indoor sofas with feet resting on carpets, and other contextual behavioral patterns. Each interaction type is accompanied by detailed spatio-temporal annotations and semantic descriptions, providing comprehensive training samples for multimodal whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "wholebody",
                    "hoi",
                    "include",
                    "hhi",
                    "between",
                    "dataset",
                    "descriptions",
                    "hsi"
                ]
            }
        ]
    },
    "Sx3.T3": {
        "caption": "Table 3: \nComposition of OmniMoCap-X dataset, unifying motion formats into SMPL-X with captions. We select 2828 publicly available high-quality datasets across various tasks. Frames and Hours are computed based on raw dataset FPS. MoCap represents data capture methods, ranked by quality: Marker with manual correction (Marker-M), Vicon Marker (Marker-V), IMU, Multi-View RGB (MV-RGB), and Single-View RGB (SV-RGB). Format specifies the original motion format.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.5pt 1.0pt;\">HSI</td>\n</tr>\n</table> \n",
        "informative_terms_identified": [
            "motion",
            "format",
            "original",
            "smplx",
            "various",
            "raw",
            "mocap",
            "frames",
            "hours",
            "based",
            "markerv",
            "correction",
            "markerm",
            "computed",
            "tasks",
            "multiview",
            "select",
            "composition",
            "highquality",
            "methods",
            "marker",
            "represents",
            "rgb",
            "omnimocapx",
            "capture",
            "across",
            "unifying",
            "into",
            "manual",
            "mvrgb",
            "available",
            "svrgb",
            "hsi",
            "datasets",
            "fps",
            "imu",
            "formats",
            "vicon",
            "data",
            "ranked",
            "singleview",
            "specifies",
            "dataset",
            "publicly",
            "captions",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">While existing approaches prioritize dataset scale over quality by incorporating substantial amounts of non-mocap data&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, leading to degraded motion generation quality (&#8221;garbage in, garbage out&#8221;), our work emphasizes data quality.\nWe systematically curate open-source mocap datasets and classify them into five acquisition categories: Marker with manual correction, Marker (Vicon), IMU, Multi-View RGB, and Single-View RGB (detailed in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.T3\" title=\"Table 3 &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nOur dataset supports diverse motion generation tasks, including text-to-motion, motion-to-dance, speech-to-gesture, and various interactions (human-object, human-scene, and human-human), providing a high-quality multimodal foundation. Comprising <math alttext=\"64.3\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m1\" intent=\":literal\"><semantics><mn>64.3</mn><annotation encoding=\"application/x-tex\">64.3</annotation></semantics></math> million frames and <math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m2\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours of data, our approach achieves comprehensive task coverage while maintaining superior data quality.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper introduces <span class=\"ltx_text ltx_font_bold\">OmniMotion-X</span>, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks&#8212;including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis)&#8212;as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct <span class=\"ltx_text ltx_font_bold\">OmniMoCap-X</span>, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "mocap",
                    "format",
                    "across",
                    "fps",
                    "into",
                    "captions",
                    "smplx",
                    "dataset",
                    "tasks",
                    "highquality",
                    "methods",
                    "available",
                    "publicly",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_caption\">We present <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified sequence-to-sequence autoregressive motion diffusion transformer designed for flexible and interactive whole-body human motion generation. It supports a variety of tasks, including text-to-motion, music-to-dance, speech-to-gesture, and globally spatial-temporal controllable motion generation, which encompasses motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis. These conditions can be combined in various ways to enable versatile motion generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "motion",
                    "various",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal whole-body human motion generation plays critical roles in animation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib13\" title=\"\">2024b</a>)</cite>, gaming&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib43\" title=\"\">2024</a>)</cite>, virtual reality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib24\" title=\"\">2024b</a>)</cite>, and embodied intelligence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib55\" title=\"\">2024</a>)</cite> across diverse input conditions, including text, audio, and trajectory, etc.\nThe limited multimodal data and task-specific model designs prevent existing motion generation methods from supporting multimodal whole-body human motion generation. Due to the high cost of mocap data collection and labeling, most datasets focus on single domains such as Text-to-Motion (T2M)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Plappert, Mandery, and Asfour <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib62\" title=\"\">2016</a>)</cite>, Music-to-Dance (M2D)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, Speech-to-Gesture (S2G)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>)</cite>, Human-Object Interaction (HOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>)</cite>, Human-Scene Interaction (HSI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and Human-Human Interaction (HHI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>)</cite>, each with inconsistent data formats (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, BVH, SMPL-(H/X) and 3D Keypoints) and control conditions (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Text captions, Audio, and Trajectories).\nTo tackle motion generation across diverse scenarios, it is crucial to build a unified framework that utilizes large-scale, diverse data to achieve more generalized representations.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "mocap",
                    "across",
                    "formats",
                    "captions",
                    "data",
                    "methods",
                    "hsi",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although recent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>; Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>)</cite> attempt to unify multitasks in one model, they meet challenges in multimodal control modeling, versatile tasks, and high-quality motion generation (as shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx1.T1\" title=\"Table 1 &#8227; Introduction &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>):\n(1) <span class=\"ltx_text ltx_font_bold\">Independent Model Training.</span> Previous approaches train separate models for each modality, limiting simultaneous control across inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>)</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">Additional Control Branches.</span> Some methods add separate control branches for each condition, limiting interaction between them&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>)</cite>.\n(3) <span class=\"ltx_text ltx_font_bold\">Conflict Granularity Training.</span> Existing methods use mixed training by combining high-level semantic conditions with low-level controls, which hampers effective control at different levels and leads to optimization challenges&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Luo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib54\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>. Similar phenomena have also been observed in video generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib44\" title=\"\">2025</a>; Ao <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib4\" title=\"\">2024</a>)</cite>.\nIn addition to modeling challenges, relevant works also introduce large-scale motion datasets from multiple tasks and modalities&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, they still exhibit the following significant shortcomings (see comparisons in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx1.T2\" title=\"Table 2 &#8227; Introduction &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):\n(1) <span class=\"ltx_text ltx_font_bold\">Low Motion Quality.</span> Datasets expanded with non-mocap motion estimation exhibit &#8220;garbage in, garbage out&#8221; effects, leading to poor-quality motion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>)</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">Text Inconsistency.</span> Inconsistent text annotations or expanded LLM descriptions lead to uneven text quality and hallucination issues&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>)</cite>.\nand (3) <span class=\"ltx_text ltx_font_bold\">limited tasks.</span> They focus on common tasks, limiting applicability in diverse ones like HOI, HSI, and HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite>.\nThese limitations hinder the development of a unified, high-quality dataset for multimodal whole-body human motion generation across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "across",
                    "tasks",
                    "highquality",
                    "methods",
                    "dataset",
                    "hsi",
                    "datasets",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address existing challenges, we propose <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified framework for multimodal whole-body human motion generation, and <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset.\nSpecifically, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;employs Diffusion Transformers (DiTs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles and Xie <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib59\" title=\"\">2023</a>)</cite>, incorporates multimodal conditions by concatenating condition tokens as prefix context, and adopts a progressive weak-to-strong mixed-condition training strategy to gradually constrain motion from high-level semantics to dense spatial-temporal alignment.\nNotably, unlike previous methods, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;introduces a novel generation paradigm that utilizes reference motion (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, user-provided or model-predicted motion) as a special condition. This significantly enhances generated motion quality and achieves consistency between reference and generated motion, creating an effective clip-by-clip autoregressive motion diffusion.\nThis enables <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;to support autoregressive interactive generation with strong temporal alignment.\nFurthermore, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;unifies various Global Spatial-Temporal Controllable Generation tasks through spatial-temporal masking strategies.\nTo guarantee high-quality motion generation training, we collect high-quality mocap datasets that support diverse motion generation tasks, unify them under the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format with standard world coordinate systems, and automatically generate hierarchical text captions by rendering motions into videos and annotating them with vision language models (VLMs).\nThis dataset contains about <em class=\"ltx_emph ltx_font_italic\"><math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.p3.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours</em> and integrates multimodal control conditions, supporting versatile tasks, including T2M, M2D, S2G, HOI, HSI, and HHI.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "mocap",
                    "format",
                    "datasets",
                    "hours",
                    "into",
                    "captions",
                    "smplx",
                    "hsi",
                    "tasks",
                    "highquality",
                    "methods",
                    "dataset",
                    "various",
                    "omnimocapx",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset with a unified SMPL-X format, and provides consistent, detailed, and structured text captions to serve versatile motion generation tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "mocap",
                    "format",
                    "captions",
                    "smplx",
                    "tasks",
                    "dataset",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments show that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves state-of-the-art performance across various tasks, including T2M, S2G, M2D, and GSTC. These evaluations were conducted on our more challenging test sets comprising <math alttext=\"280\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.I1.i4.p1.m1\" intent=\":literal\"><semantics><mn>280</mn><annotation encoding=\"application/x-tex\">280</annotation></semantics></math> samples uniformly sampled from our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "across",
                    "various",
                    "omnimocapx",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing methods are typically categorized based on input conditions into three main types: single-modal, cross-modal, and multimodal.\nSingle-modal motion generation uses control conditions from the same modality as the motion, including motion prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib73\" title=\"\">2024a</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib12\" title=\"\">2023a</a>)</cite>, in-betweening&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cohan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib15\" title=\"\">2024</a>)</cite>, and joint/trajectory-guided synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>; Dai et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib16\" title=\"\">2024</a>; Zhao, Li, and Tang <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib84\" title=\"\">2024</a>)</cite>.\nCross-modal motion generation uses control conditions from different modalities, including text in T2M&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>; Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib52\" title=\"\">2024a</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib30\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib83\" title=\"\">2024d</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib80\" title=\"\">2023b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib81\" title=\"\">2024b</a>)</cite>, music in M2D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib35\" title=\"\">2023</a>; Siyao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib66\" title=\"\">2022</a>; Tseng, Castellon, and Liu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib68\" title=\"\">2023</a>)</cite>, speech in S2G&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib10\" title=\"\">2024a</a>)</cite>, target object positions in HOI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib50\" title=\"\">2024</a>)</cite>, scene layouts in HSI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kaufmann et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib33\" title=\"\">2023</a>; Huang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib27\" title=\"\">2022</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and partner movements in HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>)</cite>.\nHowever, these approaches typically rely on task-specific architectures, significantly limiting their cross-task generalization capabilities and practical applications.\nRecently, researchers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Luo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib54\" title=\"\">2024</a>)</cite> have begun exploring multimodal motion generation, with approaches falling into three primary categories:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "into",
                    "based",
                    "methods",
                    "hsi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current human motion generation datasets are predominantly task-specific.\nDifferent tasks rely on separate collections: text-to-motion (T2M) uses datasets with text captions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Plappert, Mandery, and Asfour <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib62\" title=\"\">2016</a>; Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Punnakkal et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib63\" title=\"\">2021</a>; Liao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib43\" title=\"\">2024</a>)</cite>, music-to-dance (M2D) requires music-annotated datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib35\" title=\"\">2023</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib11\" title=\"\">2021</a>)</cite>, and speech-to-gesture (S2G) employs datasets with paired speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib49\" title=\"\">2022</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>)</cite>.\nSimilarly, interaction datasets remain disconnected across human-object&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib31\" title=\"\">2022</a>; Li, Wu, and Liu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib37\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib79\" title=\"\">2024a</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib50\" title=\"\">2024</a>; Zhan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib77\" title=\"\">2024</a>)</cite>, human-scene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and human-human interaction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>)</cite>. This fragmentation constrains multimodal motion generation capabilities.\nIntegrating these datasets presents challenges due to inconsistent motion formats.\nDatasets vary widely in their representations: some use SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, others employ SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>)</cite>, while many rely on BVH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Harvey et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib26\" title=\"\">2020</a>; Mason, Starke, and Komura <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib56\" title=\"\">2022</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib2\" title=\"\">Adobe </a>; Alexanderson et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib3\" title=\"\">2023</a>; Valle-P&#233;rez et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib69\" title=\"\">2021</a>)</cite> or keypoint representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ionescu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib28\" title=\"\">2013</a>; Ji et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib29\" title=\"\">2018</a>)</cite>. This diversity makes format standardization extremely difficult.\nRecent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite> have attempted to integrate multiple datasets, increasing scale and incorporating some multimodal conditions. However, they still exhibit several limitations:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "across",
                    "format",
                    "formats",
                    "captions",
                    "smplx",
                    "tasks",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-quality Motions</span>: Most integrated datasets derive from non-mocap sources with significant estimation errors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, creating a &#8220;garbage in, garbage out&#8221; effect that compromises motion generation quality.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "datasets",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text Inconsistency</span>: Existing datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite> use inconsistent text annotations (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, action labels versus semantic descriptions) or rely on LLMs for text refinement without visual motion data, causing variable text quality and hallucinations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "data",
                    "datasets",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Generation Scenarios</span>: Most datasets lack support for critical interaction tasks including HOI, HSI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, and HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>)</cite>, severely limiting their applicability to complex scenarios.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "datasets",
                    "hsi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the data challenges&#8212;the scarcity of high-quality multitask motion datasets, inconsistent textual annotations and motion representations&#8212;we implement a three-pronged approach: (1) integrating diverse motion capture datasets with multitask support, (2) establishing a unified motion representation, and (3) developing high-quality motion captions derived from visual-textual annotations.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "capture",
                    "captions",
                    "data",
                    "highquality",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We standardize diverse motion formats (BVH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lauterbach et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib34\" title=\"\">2009</a>)</cite>, FBX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wikipedia contributors <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib71\" title=\"\">2024</a>)</cite>, and SMPL&#160;(-H)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib51\" title=\"\">2023</a>)</cite>) into the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format to support our unified multimodal model.\nFirst, we convert these formats into the Motion-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>)</cite> SMPL-X format, comprising root orientation, pose parameters (body, hand, and jaw), facial expressions, facial shape, translation, and body shape parameters.\nWe then normalize the translation scale and initial root orientation across datasets to establish a consistent coordinate system. Additionally, we resample all datasets to 30 frames per second (FPS) to enhance the model&#8217;s ability to capture temporal patterns.\nThe specific conversion process and normalization for different datasets are provided in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "capture",
                    "format",
                    "frames",
                    "across",
                    "into",
                    "fps",
                    "formats",
                    "smplx",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enhance generalization ability, we extend the widely-used body-only representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>)</cite> to a whole-body format.\nSpecifically, the pose of the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m1\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th frame is a tuple <math alttext=\"\\mathbf{p}_{i}=({\\dot{r}^{a},\\dot{r}^{x},\\dot{r}^{z},r^{y},\\mathbf{j}^{p},\\mathbf{j}^{v},\\mathbf{c}^{f},\\mathbf{f}})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>&#119849;</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msup><mover accent=\"true\"><mi>r</mi><mo>&#729;</mo></mover><mi>a</mi></msup><mo>,</mo><msup><mover accent=\"true\"><mi>r</mi><mo>&#729;</mo></mover><mi>x</mi></msup><mo>,</mo><msup><mover accent=\"true\"><mi>r</mi><mo>&#729;</mo></mover><mi>z</mi></msup><mo>,</mo><msup><mi>r</mi><mi>y</mi></msup><mo>,</mo><msup><mi>&#119843;</mi><mi>p</mi></msup><mo>,</mo><msup><mi>&#119843;</mi><mi>v</mi></msup><mo>,</mo><msup><mi>&#119836;</mi><mi>f</mi></msup><mo>,</mo><mi>&#119839;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{p}_{i}=({\\dot{r}^{a},\\dot{r}^{x},\\dot{r}^{z},r^{y},\\mathbf{j}^{p},\\mathbf{j}^{v},\\mathbf{c}^{f},\\mathbf{f}})</annotation></semantics></math>, where <math alttext=\"\\dot{r}^{a}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m3\" intent=\":literal\"><semantics><mrow><msup><mover accent=\"true\"><mi>r</mi><mo>&#729;</mo></mover><mi>a</mi></msup><mo>&#8712;</mo><mi>&#8477;</mi></mrow><annotation encoding=\"application/x-tex\">\\dot{r}^{a}\\in\\mathbb{R}</annotation></semantics></math> is root angular velocity around the Y-axis; <math alttext=\"\\dot{r}^{x},\\dot{r}^{z}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m4\" intent=\":literal\"><semantics><mrow><mrow><msup><mover accent=\"true\"><mi>r</mi><mo>&#729;</mo></mover><mi>x</mi></msup><mo>,</mo><msup><mover accent=\"true\"><mi>r</mi><mo>&#729;</mo></mover><mi>z</mi></msup></mrow><mo>&#8712;</mo><mi>&#8477;</mi></mrow><annotation encoding=\"application/x-tex\">\\dot{r}^{x},\\dot{r}^{z}\\in\\mathbb{R}</annotation></semantics></math> are root linear velocity in the XZ plane; <math alttext=\"r^{y}\\in\\mathbb{R}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m5\" intent=\":literal\"><semantics><mrow><msup><mi>r</mi><mi>y</mi></msup><mo>&#8712;</mo><mi>&#8477;</mi></mrow><annotation encoding=\"application/x-tex\">r^{y}\\in\\mathbb{R}</annotation></semantics></math> is root height, <math alttext=\"\\mathbf{j}^{p}\\in\\mathbb{R}^{3N-1}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m6\" intent=\":literal\"><semantics><mrow><msup><mi>&#119843;</mi><mi>p</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi></mrow><mo>&#8722;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{j}^{p}\\in\\mathbb{R}^{3N-1}</annotation></semantics></math> are local joint positions; Notably, <math alttext=\"\\mathbf{j}^{r}\\in\\mathbb{R}^{6N^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m7\" intent=\":literal\"><semantics><mrow><msup><mi>&#119843;</mi><mi>r</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>6</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>N</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{j}^{r}\\in\\mathbb{R}^{6N^{\\prime}}</annotation></semantics></math> are joint 6D rotations of SMPL-X; <math alttext=\"\\mathbf{j}^{v}\\in\\mathbb{R}^{3N}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m8\" intent=\":literal\"><semantics><mrow><msup><mi>&#119843;</mi><mi>v</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mn>3</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{j}^{v}\\in\\mathbb{R}^{3N}</annotation></semantics></math> are joint velocities; <math alttext=\"\\mathbf{c}^{f}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m9\" intent=\":literal\"><semantics><msup><mi>&#119836;</mi><mi>f</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{c}^{f}</annotation></semantics></math> is binary features obtained by thresholding the heel and toe joint velocities to emphasize the foot ground contacts.\nHere, <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m10\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> refers to 127 whole-body joints extracted from SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite>, while <math alttext=\"N^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m11\" intent=\":literal\"><semantics><msup><mi>N</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">N^{\\prime}</annotation></semantics></math> represents 53 joints spanning the body, hands, and jaw.\nFor facial representation, we adopt the Flame Format&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib40\" title=\"\">2017</a>)</cite>, encoding facial features as <math alttext=\"\\mathbf{f}\\in\\mathbb{R}^{100}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx2.p2.m12\" intent=\":literal\"><semantics><mrow><mi>&#119839;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mn>100</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{f}\\in\\mathbb{R}^{100}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "format",
                    "represents",
                    "smplx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Refinement</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite> employs LLMs like GPT-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib1\" title=\"\">2023</a>)</cite> to enhance textual descriptions. However, since LLMs cannot directly perceive motion data and rely solely on existing text, they frequently introduce hallucinations and fail to capture precise action details.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "capture",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Motion-to-Text Model-Based Annotation</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Delmas et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib17\" title=\"\">2022</a>; Yazdian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib75\" title=\"\">2023</a>)</cite> generates captions directly from raw motion. While these methods effectively capture low-level kinematic details (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, left elbow bends 45 degrees), they lack the understanding of high-level semantics and action context.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "capture",
                    "raw",
                    "methods",
                    "captions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we propose an automated approach that integrates both visual and textual information for comprehensive motion annotation.\nOur method renders motion into videos and combines them with existing textual annotations (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, descriptions, action labels, and task categories) as input to the state-of-the-art vision-language models (VLMs), GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib1\" title=\"\">2023</a>)</cite>.\nThis multimodal approach generates structured, hierarchical, and precise motion captions that ensure both annotation quality and expression richness. Detailed statistics of the texts quality are in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "captions",
                    "into",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.F1\" title=\"Figure 1 &#8227; Unified Motion Representation &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nWe propose a unified multimodal, autoregressive transformer-based diffusion model for whole-body human motion generation across multiple tasks.\nThe model takes textual description <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> for semantic guidance, global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> for spatial-temporal control, and speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> and music conditions <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> to ensure rhythmic and stylistic coherence.\nFurthermore, it incorporates reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> as a motion prior, derived from either previously generated clips or user-designed motion, thus providing fine-grained details unavailable in other conditions.\nIn addition, the mixed-condition setting allows multiple conditions to occur simultaneously during training, enabling the model to handle complex scenarios with diverse inputs.\nIn this section, we present our approach in two parts: a unified framework for multimodal modeling and a progressive training strategy that ensures precise motion control.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "across",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Conditions.</span> Our framework integrates multiple condition modalities: text <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> providing semantic guidance; global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> ensuring spatial-temporal consistency; speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> synchronizing gestures and lip movements with rhythm; music <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> supplying beat and style information for dance; and reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> serving as a motion prior. Notably, this reference motion condition&#8212;overlooked in previous multimodal motion generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>&#8212;enables our model to maintain precise spatial-temporal pattern consistency with reference, substantially enhancing motion quality and coherence.\nTo fully leverage each modality and facilitate interaction between different conditions, we employ modality-specific encoders (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, T5-XXL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib65\" title=\"\">2020</a>)</cite> for text, a wav encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite> for speech, Librosa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McFee et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib57\" title=\"\">2015</a>)</cite> for music and body-wise encoding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> for motion) to extract features from each modality. These features are aligned to match motion embedding dimensions using learnable linear projections, allowing us to concatenate all condition tokens as prefix context with noisy motion tokens during processing.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> and <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m2\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> denote the modality-specific encoder and projection layer, respectively. The concatenated representation <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m3\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is then fed into our DiT backbone as a conditioning prefix context.\nTo constrain the physical properties of motion, we follow previous works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib13\" title=\"\">2024b</a>)</cite> by directly predicting motion <math alttext=\"\\hat{x}_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\hat{x}_{0}</annotation></semantics></math> rather than noise. Consequently, our diffusion objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "into"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"q(x_{0}|c)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m5\" intent=\":literal\"><semantics><mrow><mi>q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mn>0</mn></msub><mo fence=\"false\">|</mo><mi>c</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">q(x_{0}|c)</annotation></semantics></math> represents the data distribution, <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m6\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the maximum diffusion step. <math alttext=\"G\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m7\" intent=\":literal\"><semantics><mi>G</mi><annotation encoding=\"application/x-tex\">G</annotation></semantics></math> denotes the learned denoising function, while <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m8\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math> represents the noisy motion at step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m9\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, expressed as <math alttext=\"x_{t}=[p_{0}^{t},p_{1}^{t},\\dots,\\mathbf{p}_{N}^{t}]\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m10\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>p</mi><mn>0</mn><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>p</mi><mn>1</mn><mi>t</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>&#119849;</mi><mi>N</mi><mi>t</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{t}=[p_{0}^{t},p_{1}^{t},\\dots,\\mathbf{p}_{N}^{t}]</annotation></semantics></math>, where each <math alttext=\"\\mathbf{p}_{i}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m11\" intent=\":literal\"><semantics><msubsup><mi>&#119849;</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{p}_{i}^{t}</annotation></semantics></math> corresponds to the <math alttext=\"i_{th}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m12\" intent=\":literal\"><semantics><msub><mi>i</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">i_{th}</annotation></semantics></math> pose in motion at step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m13\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "data",
                    "represents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering our diverse multi-granularity conditioning inputs, we empirically observed that employing all conditions within a single-stage training paradigm leads to difficulties in directly learning the correlation between motion and conditions. Moreover, the model tends to overfit strongly constrained low-level control signals such as fine-grained reference motion and spatiotemporal joint control. While these signals appear dominant, they can suppress other modalities such as text, ultimately compromising overall controllability.\nTo address this, we implement weak-to-strong progressive training: as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F2\" title=\"Figure 2 &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> initial text-conditioned learning establishes motion-semantic alignment, followed by progressive integration of finer-grained conditions, including reference motion, global motion, and audio signals.\nThis progressive approach enables the model to effectively adapt to different conditions, ensuring high-quality and flexible motion generation while precisely adhering to multimodal conditions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "highquality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method employs a Transformer Encoder architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib70\" title=\"\">2017</a>)</cite> with 8 layers and 8 attention heads, featuring a hidden dimension of <math alttext=\"d_{\\text{model}}=1536~(128\\times 12)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>=</mo><mrow><mn>1536</mn><mo lspace=\"0.330em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>128</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>12</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d_{\\text{model}}=1536~(128\\times 12)</annotation></semantics></math>, where <math alttext=\"128\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m2\" intent=\":literal\"><semantics><mn>128</mn><annotation encoding=\"application/x-tex\">128</annotation></semantics></math> represents the embedding size per each of the <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m3\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> body parts, and a feedforward dimension of <math alttext=\"3072\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m4\" intent=\":literal\"><semantics><mn>3072</mn><annotation encoding=\"application/x-tex\">3072</annotation></semantics></math>. Training is performed on a single H800 GPU through progressive conditioning: starting with text-only training for <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m5\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, then adding reference motion for another <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m6\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, followed by global spatiotemporal control for <math alttext=\"230K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>230</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">230K</annotation></semantics></math> steps, and finally incorporating full audio conditions for <math alttext=\"920K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>920</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">920K</annotation></semantics></math> steps. The corresponding batch sizes are <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m9\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m10\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m11\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, and <math alttext=\"16\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m12\" intent=\":literal\"><semantics><mn>16</mn><annotation encoding=\"application/x-tex\">16</annotation></semantics></math>, respectively. We optimize with AdamW using an initial learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> (reset for new conditions) and a cosine schedule decaying to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m14\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> within the first <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m15\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps. The default length of motion reference and prediction is <math alttext=\"150\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m16\" intent=\":literal\"><semantics><mn>150</mn><annotation encoding=\"application/x-tex\">150</annotation></semantics></math>. More implementation details are provided in supplementary material.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "represents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;on four tasks: Text-to-Motion (T2M), Global Spatiotemporal Controllable Generation (GSTC), Music-to-Dance (M2D), and Speech-to-Gesture (S2G).\nFor T2M and GSTC evaluations, test sets are uniformly sampled across all datasets&#160;(<math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx2.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> samples from each).\nThe M2D benchmark integrates test sequences from AIST++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>)</cite>, and PhantomDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib36\" title=\"\">2022</a>)</cite>, with S2G evaluation conducted on BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "tasks",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">T2M.</span>\nTab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.T4\" title=\"Table 4 &#8227; Consistent Visual-textual Motion Captions &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that previous methods, constrained by small-scale datasets, struggle to generalize in generating diverse and complex motions.\nIn contrast, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;demonstrates a significant advantage, outperforming not only baseline methods trained on small datasets (MDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>)</cite>, MLD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>)</cite>, MoMask&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>)</cite>, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>) but also MoMask* and MotionCraft*, which are trained on our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>.\nCompared to MoMask* and MotionCraft*, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;uses stronger text encoder (T5-XXL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib65\" title=\"\">2020</a>)</cite>), which models complex texts more effectively than CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib64\" title=\"\">2021</a>)</cite>.\nIn addition, it adopts a unified multimodal framework with reference motion during training, enabling the model to learn more detailed whole-body motion representations, leading to better generation quality and generalization.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "omnimocapx",
                    "methods",
                    "datasets",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">M2D and S2G.</span>\nWe directly compare our method with the MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> on the S2G and M2D tasks.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T6\" title=\"Table 6 &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves competitive performance. The lower FID is mainly due to the relatively small test sets in S2G and M2D, while <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is trained on the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, leading to some distribution differences. Additionally, our method demonstrates superior diversity, which may also impact the FID.</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F3\" title=\"Figure 3 &#8227; Unified Multimodal Modeling &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;handles various multimodal generation, including T2M, M2D, S2G, and GSTC (covering motion prediction, in-betweening, and joint guidance). When combined with reference motion, the model generates conditioned motions that align with the reference. More visualizations are in the supplementary video.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "various"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces the <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;and the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>.\n<em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is an autoregressive diffusion model that integrates reference motion and multimodal conditions for precise whole-body motion control. It employs a progressive weak-to-strong conditioning strategy to effectively handle multi-granular constraints.\n<em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;is the largest multimodal MoCap dataset, comprising <math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx6.p1.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours from <math alttext=\"28\" class=\"ltx_Math\" display=\"inline\" id=\"Sx6.p1.m2\" intent=\":literal\"><semantics><mn>28</mn><annotation encoding=\"application/x-tex\">28</annotation></semantics></math> motion capture datasets, unified in SMPL-X with structured and consistent captions across 10 tasks.\nExperiments demonstrate that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;outperforms baselines, laying a strong foundation for large-scale multimodal motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "mocap",
                    "capture",
                    "across",
                    "datasets",
                    "hours",
                    "captions",
                    "smplx",
                    "tasks",
                    "dataset",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx7.T8\" title=\"Table 8 &#8227; Dataset Text Quality &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents a comprehensive analysis of the text quality across our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;collection, which comprises over <math alttext=\"321,000\" class=\"ltx_Math\" display=\"inline\" id=\"Sx7.p1.m1\" intent=\":literal\"><semantics><mrow><mn>321</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">321,000</annotation></semantics></math> textual descriptions spanning various motion-related tasks. This extensive dataset was created by collecting and curating existing motion datasets, rendering motions to videos, and then leveraging vision-language models (VLMs) to generate rich textual descriptions.\nThe linguistic diversity of our dataset is evidenced by the Type-Token Ratio (TTR) ranges, with maximum values reaching 1.0 in multiple subsets (AIOZ, Motorica, IDEA400, etc.), indicating exceptional lexical richness. The average sentence length of 276.78 words across the entire collection demonstrates the descriptive depth of our motion annotations, providing detailed accounts of nuanced movements rather than simplistic action labels.\nParticularly noteworthy is the variety of verbs captured across different motion categories. The dataset exhibits task-specific verb distributions that align with the semantic nature of each motion type: T2M datasets frequently feature locomotion verbs (&#8220;walk,&#8221; &#8220;step&#8221;), M2D collections emphasize rhythmic movements (&#8220;swing,&#8221; &#8220;lift&#8221;), HOI datasets contain manipulation verbs (&#8220;lift,&#8221; &#8220;hold,&#8221; &#8220;grasp&#8221;), and HHI datasets capture interpersonal dynamics (&#8220;stand,&#8221; &#8220;extend,&#8221; &#8220;lean&#8221;). This semantic richness enables models trained on <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;to understand and generate precise motion descriptions across diverse contexts.\nThe comprehensive coverage across six distinct motion-related tasks (T2M, M2D, S2G, HHI, HOI, HSI) makes <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;uniquely positioned to support generalizable motion understanding. By integrating multiple motion paradigms under a unified annotation framework, our dataset transcends the limitations of task-specific collections, allowing for transfer learning across motion domains. This textual quality and diversity significantly enhance the capability of motion generation models to comprehend natural language instructions and produce corresponding human-like movements.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "capture",
                    "across",
                    "datasets",
                    "hsi",
                    "tasks",
                    "dataset",
                    "various",
                    "omnimocapx",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Format Conversion.</span>\nSince many datasets are not in the SMPL-series format, we convert all datasets to the SMPL-X format for consistency. For BVH-format datasets such as Mixamo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib2\" title=\"\">Adobe </a>)</cite>, 100Style&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mason, Starke, and Komura <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib56\" title=\"\">2022</a>)</cite>, Motorica&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alexanderson et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib3\" title=\"\">2023</a>)</cite>, and LaFAN1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Harvey et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib26\" title=\"\">2020</a>)</cite>, we first standardize the reference pose of all BVH files to a T-pose to ensure consistent initialization. We then align the root node&#8217;s coordinate system with that of the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> model in Blender&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib9\" title=\"\">Blender Foundation </a>)</cite>, where the negative Y-axis is defined as the forward direction and the Z-axis as the vertical upward direction.\nTo adapt to the SMPL-X topology, we generate a corresponding BVH skeleton based on a predefined SMPL-X T-pose template. We perform skeleton retargeting in MotionBuilder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib6\" title=\"\">Autodesk Inc. </a>)</cite> to map the original animation data onto the SMPL-X hierarchy.\nFinally, we convert the retargeted BVH files from Euler angles to axis-angle representations and apply Gaussian filtering to smooth both joint rotations and translations over time, yielding stable SMPL-X parameters.\nFor the Choreomaster dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib11\" title=\"\">2021</a>)</cite>, originally in FBX format, we first convert it to BVH using Blender, and then process it using the same pipeline.</p>\n\n",
                "matched_terms": [
                    "format",
                    "original",
                    "based",
                    "smplx",
                    "data",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Normalization.</span>\nTo reduce the difficulty of model training, we perform temporal and spatial normalization on SMPL-X motion sequences. Specifically, we standardize all motion sequences in four steps. First, we align the initial frame of each sequence to face the positive Z-axis. Then, we reposition the starting frame to a common location with the feet just touching the ground. Next, we unify the frame rate across datasets to 30 fps. Finally, we segment each motion sequence into 5-second clips (150 frames).</p>\n\n",
                "matched_terms": [
                    "motion",
                    "across",
                    "frames",
                    "fps",
                    "into",
                    "smplx",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text and Motion Feature Extractors.</span>\nExisting research typically relies on pre-trained motion and text feature extractors for evaluation. However, due to significant differences in dataset scale, distribution, and motion representation compared to existing works, these pre-trained extractors are not directly applicable. Therefore, following the approach&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib52\" title=\"\">2024a</a>)</cite>, we re-train motion and text feature extractor using a contrastive learning framework tailored to this dataset. The text feature extractor is based on the Transformer encoder architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Petrovich, Black, and Varol <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib61\" title=\"\">2023</a>)</cite>, which encodes raw text into a semantic vector <math alttext=\"s_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">s_{t}</annotation></semantics></math>. The motion feature extractor also uses a Transformer encoder to encode motion sequences of up to 150 frames into a semantic vector <math alttext=\"s_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">s_{m}</annotation></semantics></math>. Both encoders include additional semantic tokens, with a structure similar to the encoder in ACTOR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Petrovich, Black, and Varol <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib60\" title=\"\">2021</a>)</cite>, but without involving probabilistic distribution modeling. In implementation, the text encoder takes as input text features extracted from a pre-trained and frozen DistilBERT network, while the motion encoder directly processes raw motion sequence data. In the contrastive learning framework, we optimize the feature space such that matching text-motion feature pairs <math alttext=\"(s_{t},s_{m})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(s_{t},s_{m})</annotation></semantics></math> are brought closer in the embedding space, while ensuring that non-matching pairs are separated by at least a distance <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>. This optimization objective is achieved through the following contrastive loss function:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "frames",
                    "into",
                    "based",
                    "data",
                    "dataset",
                    "raw"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Mask.</span> Our model leverages four specialized masking mechanisms: (1) Component attention mask, the standard source key padding mask in Transformers&#8212;regulates attention across different conditional inputs. In our DiT-based architecture, input conditions may be partially missing; this mask zeroes out attention weights for missing inputs, allowing the model to focus only on valid ones. (2) Global task-dependent mask, a spatiotemporal dynamic mask tailored to tasks like motion prediction, interpolation, completion, and trajectory-guided generation&#8212;distinguishes between observed and unobserved motion regions based on task requirements. (3) Global motion disentanglement mask transforms dense global motion conditions into a hybrid sparse-dense representation by selecting key joints (e.g., <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p2.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> joints) for global representation optimization, while the remaining <math alttext=\"n-k\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8722;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">n-k</annotation></semantics></math> joints contribute to local motion representation and noise injection. (4) Motion representation reconstruction mask filters out reconstruction errors of joints without ground-truth annotations during training. For instance, it suppresses hand-joint errors in datasets lacking hand annotations, effectively addressing cross-dataset joint count mismatches.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "across",
                    "based",
                    "into",
                    "tasks",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and accuracy of our caption generation process, we selected GPT-4o, the current state-of-the-art closed-source multimodal model, as our primary captioning tool. To maximize caption accuracy and comprehensiveness, we implemented several key optimizations to the captioning pipeline. First, we significantly increased the number of input frames provided to the model, allowing for more comprehensive temporal understanding of the video content. Second, we enhanced the video rendering quality to ensure that visual details are clearly preserved and accurately conveyed to the model. Third, we conducted extensive prompt engineering to design optimal instructions that guide the model toward generating more precise and detailed captions that capture both visual elements and temporal dynamics (see Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx9.F4\" title=\"Figure 4 &#8227; More Implementation Details &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> for representative examples). These technical improvements collectively resulted in substantially improved caption quality compared to baseline approaches.</p>\n\n",
                "matched_terms": [
                    "capture",
                    "frames",
                    "captions",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset supports various interaction types, including Human-Human Interaction (HHI), Human-Object Interaction (HOI), and Human-Scene Interaction (HSI), as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx9.F5\" title=\"Figure 5 &#8227; More Implementation Details &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Specifically, human-human interactions encompass a rich spectrum of social scenarios such as conversations, handshakes, embraces, and collaborative activities. Human-object interactions include everyday activities like grasping, manipulation, and tool usage. Human-scene interactions capture the dynamic relationships between individuals and their environments, such as sitting on indoor sofas with feet resting on carpets, and other contextual behavioral patterns. Each interaction type is accompanied by detailed spatio-temporal annotations and semantic descriptions, providing comprehensive training samples for multimodal whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "capture",
                    "hsi",
                    "dataset",
                    "various"
                ]
            }
        ]
    },
    "Sx3.T4": {
        "caption": "Table 4: Quantitative results of text-to-motion on the OmniMoCap-X test set. ↑\\uparrow (↓\\downarrow) indicates that a larger (smaller) value is better. →\\rightarrow indicates that a value closer to the GT is better. Red and Blue colors indicate the best and second-best results respectively. All evaluations are repeated 20 times, reporting the mean and 95% confidence interval.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">R Precision<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">FID<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Multimodal Dist.<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Diversity<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">MultiModality<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Top-1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Top-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Top-3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">GT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.535^{\\pm 0.009}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m6\" intent=\":literal\"><semantics><msup><mn>0.535</mn><mrow><mo>&#177;</mo><mn>0.009</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.535^{\\pm 0.009}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.725^{\\pm 0.009}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m7\" intent=\":literal\"><semantics><msup><mn>0.725</mn><mrow><mo>&#177;</mo><mn>0.009</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.725^{\\pm 0.009}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.821^{\\pm 0.008}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m8\" intent=\":literal\"><semantics><msup><mn>0.821</mn><mrow><mo>&#177;</mo><mn>0.008</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.821^{\\pm 0.008}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.013^{\\pm 0.005}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m9\" intent=\":literal\"><semantics><msup><mn>0.013</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.013^{\\pm 0.005}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.493^{\\pm 0.011}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m10\" intent=\":literal\"><semantics><msup><mn>2.493</mn><mrow><mo>&#177;</mo><mn>0.011</mn></mrow></msup><annotation encoding=\"application/x-tex\">2.493^{\\pm 0.011}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"9.194^{\\pm 0.093}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m11\" intent=\":literal\"><semantics><msup><mn>9.194</mn><mrow><mo>&#177;</mo><mn>0.093</mn></mrow></msup><annotation encoding=\"application/x-tex\">9.194^{\\pm 0.093}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.063^{\\pm 0.004}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m12\" intent=\":literal\"><semantics><msup><mn>0.063</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.063^{\\pm 0.004}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.121^{\\pm 0.003}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m13\" intent=\":literal\"><semantics><msup><mn>0.121</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.121^{\\pm 0.003}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.169^{\\pm 0.003}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m14\" intent=\":literal\"><semantics><msup><mn>0.169</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.169^{\\pm 0.003}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"72.928^{\\pm 0.361}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m15\" intent=\":literal\"><semantics><msup><mn>72.928</mn><mrow><mo>&#177;</mo><mn>0.361</mn></mrow></msup><annotation encoding=\"application/x-tex\">72.928^{\\pm 0.361}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"8.376^{\\pm 0.018}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m16\" intent=\":literal\"><semantics><msup><mn>8.376</mn><mrow><mo>&#177;</mo><mn>0.018</mn></mrow></msup><annotation encoding=\"application/x-tex\">8.376^{\\pm 0.018}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1.981^{\\pm 0.003}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m17\" intent=\":literal\"><semantics><msup><mn>1.981</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">1.981^{\\pm 0.003}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.394^{\\pm 0.010}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m18\" intent=\":literal\"><semantics><msup><mn>0.394</mn><mrow><mo>&#177;</mo><mn>0.010</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.394^{\\pm 0.010}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MLD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.084^{\\pm 0.002}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m19\" intent=\":literal\"><semantics><msup><mn>0.084</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.084^{\\pm 0.002}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.152^{\\pm 0.002}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m20\" intent=\":literal\"><semantics><msup><mn>0.152</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.152^{\\pm 0.002}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.209^{\\pm 0.003}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m21\" intent=\":literal\"><semantics><msup><mn>0.209</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.209^{\\pm 0.003}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"70.082^{\\pm 0.468}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m22\" intent=\":literal\"><semantics><msup><mn>70.082</mn><mrow><mo>&#177;</mo><mn>0.468</mn></mrow></msup><annotation encoding=\"application/x-tex\">70.082^{\\pm 0.468}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"8.281^{\\pm 0.025}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m23\" intent=\":literal\"><semantics><msup><mn>8.281</mn><mrow><mo>&#177;</mo><mn>0.025</mn></mrow></msup><annotation encoding=\"application/x-tex\">8.281^{\\pm 0.025}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.998^{\\pm 0.003}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m24\" intent=\":literal\"><semantics><msup><mn>1.998</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">1.998^{\\pm 0.003}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.439}^{\\pm 0.022}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m25\" intent=\":literal\"><semantics><msup><mn>0.439</mn><mrow><mo>&#177;</mo><mn>0.022</mn></mrow></msup><annotation encoding=\"application/x-tex\">{0.439}^{\\pm 0.022}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MoMask&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.104^{\\pm 0.003}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m26\" intent=\":literal\"><semantics><msup><mn>0.104</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.104^{\\pm 0.003}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.163^{\\pm 0.003}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m27\" intent=\":literal\"><semantics><msup><mn>0.163</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.163^{\\pm 0.003}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.199^{\\pm 0.005}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m28\" intent=\":literal\"><semantics><msup><mn>0.199</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.199^{\\pm 0.005}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"69.361^{\\pm 0.365}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m29\" intent=\":literal\"><semantics><msup><mn>69.361</mn><mrow><mo>&#177;</mo><mn>0.365</mn></mrow></msup><annotation encoding=\"application/x-tex\">69.361^{\\pm 0.365}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"8.341^{\\pm 0.022}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m30\" intent=\":literal\"><semantics><msup><mn>8.341</mn><mrow><mo>&#177;</mo><mn>0.022</mn></mrow></msup><annotation encoding=\"application/x-tex\">8.341^{\\pm 0.022}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.123^{\\pm 0.002}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m31\" intent=\":literal\"><semantics><msup><mn>2.123</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">2.123^{\\pm 0.002}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.482^{\\pm 0.008}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m32\" intent=\":literal\"><semantics><msup><mn>0.482</mn><mrow><mo>&#177;</mo><mn>0.008</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.482^{\\pm 0.008}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MoMask*&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.267^{\\pm 0.004}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m33\" intent=\":literal\"><semantics><msup><mn>0.267</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.267^{\\pm 0.004}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.414}^{\\pm 0.004}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m34\" intent=\":literal\"><semantics><msup><mn>0.414</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">{0.414}^{\\pm 0.004}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.530}^{\\pm 0.004}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m35\" intent=\":literal\"><semantics><msup><mn>0.530</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">{0.530}^{\\pm 0.004}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{17.428}^{\\pm 0.400}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m36\" intent=\":literal\"><semantics><msup><mn>17.428</mn><mrow><mo>&#177;</mo><mn>0.400</mn></mrow></msup><annotation encoding=\"application/x-tex\">{17.428}^{\\pm 0.400}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{5.661}^{\\pm 0.024}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m37\" intent=\":literal\"><semantics><msup><mn>5.661</mn><mrow><mo>&#177;</mo><mn>0.024</mn></mrow></msup><annotation encoding=\"application/x-tex\">{5.661}^{\\pm 0.024}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{6.772}^{\\pm 0.013}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m38\" intent=\":literal\"><semantics><msup><mn>6.772</mn><mrow><mo>&#177;</mo><mn>0.013</mn></mrow></msup><annotation encoding=\"application/x-tex\">{6.772}^{\\pm 0.013}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.811^{\\pm 0.067}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m39\" intent=\":literal\"><semantics><msup><mn>0.811</mn><mrow><mo>&#177;</mo><mn>0.067</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.811^{\\pm 0.067}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.176}^{\\pm 0.004}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m40\" intent=\":literal\"><semantics><msup><mn>0.176</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">{0.176}^{\\pm 0.004}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.259^{\\pm 0.003}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m41\" intent=\":literal\"><semantics><msup><mn>0.259</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.259^{\\pm 0.003}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.319^{\\pm 0.004}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m42\" intent=\":literal\"><semantics><msup><mn>0.319</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.319^{\\pm 0.004}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"63.049^{\\pm 0.470}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m43\" intent=\":literal\"><semantics><msup><mn>63.049</mn><mrow><mo>&#177;</mo><mn>0.470</mn></mrow></msup><annotation encoding=\"application/x-tex\">63.049^{\\pm 0.470}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"7.936^{\\pm 0.021}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m44\" intent=\":literal\"><semantics><msup><mn>7.936</mn><mrow><mo>&#177;</mo><mn>0.021</mn></mrow></msup><annotation encoding=\"application/x-tex\">7.936^{\\pm 0.021}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.325^{\\pm 0.013}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m45\" intent=\":literal\"><semantics><msup><mn>2.325</mn><mrow><mo>&#177;</mo><mn>0.013</mn></mrow></msup><annotation encoding=\"application/x-tex\">2.325^{\\pm 0.013}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.557^{\\pm 0.039}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m46\" intent=\":literal\"><semantics><msup><mn>0.557</mn><mrow><mo>&#177;</mo><mn>0.039</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.557^{\\pm 0.039}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MotionCraft*&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.236^{\\pm 0.004}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m47\" intent=\":literal\"><semantics><msup><mn>0.236</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.236^{\\pm 0.004}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.370}^{\\pm 0.004}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m48\" intent=\":literal\"><semantics><msup><mn>0.370</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">{0.370}^{\\pm 0.004}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{0.489}^{\\pm 0.004}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m49\" intent=\":literal\"><semantics><msup><mn>0.489</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">{0.489}^{\\pm 0.004}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{47.428}^{\\pm 0.400}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m50\" intent=\":literal\"><semantics><msup><mn>47.428</mn><mrow><mo>&#177;</mo><mn>0.400</mn></mrow></msup><annotation encoding=\"application/x-tex\">{47.428}^{\\pm 0.400}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{7.424}^{\\pm 0.024}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m51\" intent=\":literal\"><semantics><msup><mn>7.424</mn><mrow><mo>&#177;</mo><mn>0.024</mn></mrow></msup><annotation encoding=\"application/x-tex\">{7.424}^{\\pm 0.024}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"{2.820}^{\\pm 0.013}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m52\" intent=\":literal\"><semantics><msup><mn>2.820</mn><mrow><mo>&#177;</mo><mn>0.013</mn></mrow></msup><annotation encoding=\"application/x-tex\">{2.820}^{\\pm 0.013}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.863^{\\pm 0.067}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m53\" intent=\":literal\"><semantics><msup><mn>0.863</mn><mrow><mo>&#177;</mo><mn>0.067</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.863^{\\pm 0.067}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;(Ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\textbf{0.303}^{\\pm 0.004}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m54\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">0.303</mtext><mrow><mo mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">&#177;</mo><mn mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{0.303}^{\\pm 0.004}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\textbf{0.464}^{\\pm 0.006}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m55\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">0.464</mtext><mrow><mo mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">&#177;</mo><mn mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">0.006</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{0.464}^{\\pm 0.006}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\textbf{0.571}^{\\pm 0.005}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m56\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">0.571</mtext><mrow><mo mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">&#177;</mo><mn mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{0.571}^{\\pm 0.005}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\textbf{5.040}^{\\pm 0.293}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m57\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">5.040</mtext><mrow><mo mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">&#177;</mo><mn mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">0.293</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{5.040}^{\\pm 0.293}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\textbf{4.678}^{\\pm 0.019}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m58\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">4.678</mtext><mrow><mo mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">&#177;</mo><mn mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">0.019</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{4.678}^{\\pm 0.019}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\textbf{8.650}^{\\pm 0.095}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m59\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">8.650</mtext><mrow><mo mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">&#177;</mo><mn mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">0.095</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{8.650}^{\\pm 0.095}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\textbf{1.696}^{\\pm 0.366}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m60\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">1.696</mtext><mrow><mo mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">&#177;</mo><mn mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">0.366</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{1.696}^{\\pm 0.366}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;(Ours + RM)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\textbf{0.346}^{\\pm 0.005}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m61\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">0.346</mtext><mrow><mo mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">&#177;</mo><mn mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{0.346}^{\\pm 0.005}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\textbf{0.511}^{\\pm 0.007}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m62\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">0.511</mtext><mrow><mo mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">&#177;</mo><mn mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">0.007</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{0.511}^{\\pm 0.007}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\textbf{0.629}^{\\pm 0.006}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m63\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">0.629</mtext><mrow><mo mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">&#177;</mo><mn mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">0.006</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{0.629}^{\\pm 0.006}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\textbf{3.199}^{\\pm 0.293}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m64\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">3.199</mtext><mrow><mo mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">&#177;</mo><mn mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">0.293</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{3.199}^{\\pm 0.293}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\textbf{4.106}^{\\pm 0.019}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m65\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">4.106</mtext><mrow><mo mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">&#177;</mo><mn mathcolor=\"#C00000\" style=\"--ltx-fg-color:#C00000;\">0.019</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{4.106}^{\\pm 0.019}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\textbf{8.009}^{\\pm 0.095}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m66\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">8.009</mtext><mrow><mo mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">&#177;</mo><mn mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">0.095</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{8.009}^{\\pm 0.095}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\textbf{1.143}^{\\pm 0.366}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.T4.m67\" intent=\":literal\"><semantics><msup><mtext class=\"ltx_mathvariant_bold\" mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">1.143</mtext><mrow><mo mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">&#177;</mo><mn mathcolor=\"#0C72BA\" style=\"--ltx-fg-color:#0C72BA;\">0.366</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\textbf{1.143}^{\\pm 0.366}</annotation></semantics></math></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "1143±0366textbf1143pm",
            "0176±00040176pm",
            "momask",
            "0346±0005textbf0346pm",
            "blue",
            "smaller",
            "guo",
            "0725±00090725pm",
            "5040±0293textbf5040pm",
            "fid↓downarrow",
            "repeated",
            "top2",
            "1981±00031981pm",
            "reporting",
            "results",
            "0152±00020152pm",
            "confidence",
            "ours",
            "texttomotion",
            "7424±00247424pm",
            "method",
            "times",
            "0535±00090535pm",
            "dist↓downarrow",
            "multimodality↑uparrow",
            "0511±0007textbf0511pm",
            "1696±0366textbf1696pm",
            "0439±00220439pm",
            "2493±00112493pm",
            "interval",
            "0013±00050013pm",
            "→rightarrow",
            "69361±036569361pm",
            "6772±00136772pm",
            "omnimocapx",
            "mdm",
            "0482±00080482pm",
            "0571±0005textbf0571pm",
            "8281±00258281pm",
            "mean",
            "2820±00132820pm",
            "indicates",
            "7936±00217936pm",
            "multimodal",
            "respectively",
            "0236±00040236pm",
            "0863±00670863pm",
            "0063±00040063pm",
            "better",
            "2325±00132325pm",
            "17428±040017428pm",
            "0394±00100394pm",
            "0121±00030121pm",
            "2123±00022123pm",
            "8009±0095textbf8009pm",
            "top3",
            "5661±00245661pm",
            "47428±040047428pm",
            "bian",
            "↓downarrow",
            "diversity→rightarrow",
            "0084±00020084pm",
            "closer",
            "test",
            "0199±00050199pm",
            "4678±0019textbf4678pm",
            "value",
            "0209±00030209pm",
            "8341±00228341pm",
            "omnimotionx",
            "8376±00188376pm",
            "0267±00040267pm",
            "2023b",
            "1998±00031998pm",
            "set",
            "↑uparrow",
            "red",
            "motioncraft",
            "colors",
            "0163±00030163pm",
            "quantitative",
            "all",
            "0414±00040414pm",
            "0489±00040489pm",
            "0319±00040319pm",
            "0370±00040370pm",
            "0557±00390557pm",
            "top1",
            "72928±036172928pm",
            "mld",
            "70082±046870082pm",
            "63049±047063049pm",
            "0811±00670811pm",
            "9194±00939194pm",
            "larger",
            "0464±0006textbf0464pm",
            "4106±0019textbf4106pm",
            "3199±0293textbf3199pm",
            "evaluations",
            "2024a",
            "0259±00030259pm",
            "0530±00040530pm",
            "precision↑uparrow",
            "0169±00030169pm",
            "8650±0095textbf8650pm",
            "indicate",
            "0104±00030104pm",
            "tevet",
            "0629±0006textbf0629pm",
            "secondbest",
            "0821±00080821pm",
            "chen",
            "best",
            "0303±0004textbf0303pm"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">T2M.</span>\nTab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.T4\" title=\"Table 4 &#8227; Consistent Visual-textual Motion Captions &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that previous methods, constrained by small-scale datasets, struggle to generalize in generating diverse and complex motions.\nIn contrast, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;demonstrates a significant advantage, outperforming not only baseline methods trained on small datasets (MDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>)</cite>, MLD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>)</cite>, MoMask&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>)</cite>, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>) but also MoMask* and MotionCraft*, which are trained on our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>.\nCompared to MoMask* and MotionCraft*, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;uses stronger text encoder (T5-XXL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib65\" title=\"\">2020</a>)</cite>), which models complex texts more effectively than CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib64\" title=\"\">2021</a>)</cite>.\nIn addition, it adopts a unified multimodal framework with reference motion during training, enabling the model to learn more detailed whole-body motion representations, leading to better generation quality and generalization.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper introduces <span class=\"ltx_text ltx_font_bold\">OmniMotion-X</span>, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks&#8212;including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis)&#8212;as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct <span class=\"ltx_text ltx_font_bold\">OmniMoCap-X</span>, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "texttomotion",
                    "omnimotionx",
                    "evaluations",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_caption\">We present <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified sequence-to-sequence autoregressive motion diffusion transformer designed for flexible and interactive whole-body human motion generation. It supports a variety of tasks, including text-to-motion, music-to-dance, speech-to-gesture, and globally spatial-temporal controllable motion generation, which encompasses motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis. These conditions can be combined in various ways to enable versatile motion generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "texttomotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal whole-body human motion generation plays critical roles in animation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib13\" title=\"\">2024b</a>)</cite>, gaming&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib43\" title=\"\">2024</a>)</cite>, virtual reality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib24\" title=\"\">2024b</a>)</cite>, and embodied intelligence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib55\" title=\"\">2024</a>)</cite> across diverse input conditions, including text, audio, and trajectory, etc.\nThe limited multimodal data and task-specific model designs prevent existing motion generation methods from supporting multimodal whole-body human motion generation. Due to the high cost of mocap data collection and labeling, most datasets focus on single domains such as Text-to-Motion (T2M)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Plappert, Mandery, and Asfour <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib62\" title=\"\">2016</a>)</cite>, Music-to-Dance (M2D)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, Speech-to-Gesture (S2G)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>)</cite>, Human-Object Interaction (HOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>)</cite>, Human-Scene Interaction (HSI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and Human-Human Interaction (HHI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>)</cite>, each with inconsistent data formats (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, BVH, SMPL-(H/X) and 3D Keypoints) and control conditions (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Text captions, Audio, and Trajectories).\nTo tackle motion generation across diverse scenarios, it is crucial to build a unified framework that utilizes large-scale, diverse data to achieve more generalized representations.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "chen",
                    "guo",
                    "texttomotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although recent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>; Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>)</cite> attempt to unify multitasks in one model, they meet challenges in multimodal control modeling, versatile tasks, and high-quality motion generation (as shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx1.T1\" title=\"Table 1 &#8227; Introduction &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>):\n(1) <span class=\"ltx_text ltx_font_bold\">Independent Model Training.</span> Previous approaches train separate models for each modality, limiting simultaneous control across inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>)</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">Additional Control Branches.</span> Some methods add separate control branches for each condition, limiting interaction between them&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>)</cite>.\n(3) <span class=\"ltx_text ltx_font_bold\">Conflict Granularity Training.</span> Existing methods use mixed training by combining high-level semantic conditions with low-level controls, which hampers effective control at different levels and leads to optimization challenges&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Luo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib54\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>. Similar phenomena have also been observed in video generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib44\" title=\"\">2025</a>; Ao <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib4\" title=\"\">2024</a>)</cite>.\nIn addition to modeling challenges, relevant works also introduce large-scale motion datasets from multiple tasks and modalities&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, they still exhibit the following significant shortcomings (see comparisons in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx1.T2\" title=\"Table 2 &#8227; Introduction &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):\n(1) <span class=\"ltx_text ltx_font_bold\">Low Motion Quality.</span> Datasets expanded with non-mocap motion estimation exhibit &#8220;garbage in, garbage out&#8221; effects, leading to poor-quality motion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>)</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">Text Inconsistency.</span> Inconsistent text annotations or expanded LLM descriptions lead to uneven text quality and hallucination issues&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>)</cite>.\nand (3) <span class=\"ltx_text ltx_font_bold\">limited tasks.</span> They focus on common tasks, limiting applicability in diverse ones like HOI, HSI, and HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite>.\nThese limitations hinder the development of a unified, high-quality dataset for multimodal whole-body human motion generation across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "2024a",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address existing challenges, we propose <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified framework for multimodal whole-body human motion generation, and <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset.\nSpecifically, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;employs Diffusion Transformers (DiTs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles and Xie <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib59\" title=\"\">2023</a>)</cite>, incorporates multimodal conditions by concatenating condition tokens as prefix context, and adopts a progressive weak-to-strong mixed-condition training strategy to gradually constrain motion from high-level semantics to dense spatial-temporal alignment.\nNotably, unlike previous methods, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;introduces a novel generation paradigm that utilizes reference motion (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, user-provided or model-predicted motion) as a special condition. This significantly enhances generated motion quality and achieves consistency between reference and generated motion, creating an effective clip-by-clip autoregressive motion diffusion.\nThis enables <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;to support autoregressive interactive generation with strong temporal alignment.\nFurthermore, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;unifies various Global Spatial-Temporal Controllable Generation tasks through spatial-temporal masking strategies.\nTo guarantee high-quality motion generation training, we collect high-quality mocap datasets that support diverse motion generation tasks, unify them under the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format with standard world coordinate systems, and automatically generate hierarchical text captions by rendering motions into videos and annotating them with vision language models (VLMs).\nThis dataset contains about <em class=\"ltx_emph ltx_font_italic\"><math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.p3.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours</em> and integrates multimodal control conditions, supporting versatile tasks, including T2M, M2D, S2G, HOI, HSI, and HHI.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "omnimotionx",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a multimodal autoregressive diffusion transformer for versatile whole-body motion generation. By introducing reference motion, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;significantly enhances consistent content, style, and temporal dynamics generation.</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset with a unified SMPL-X format, and provides consistent, detailed, and structured text captions to serve versatile motion generation tasks.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments show that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves state-of-the-art performance across various tasks, including T2M, S2G, M2D, and GSTC. These evaluations were conducted on our more challenging test sets comprising <math alttext=\"280\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.I1.i4.p1.m1\" intent=\":literal\"><semantics><mn>280</mn><annotation encoding=\"application/x-tex\">280</annotation></semantics></math> samples uniformly sampled from our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "evaluations",
                    "omnimocapx",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing methods are typically categorized based on input conditions into three main types: single-modal, cross-modal, and multimodal.\nSingle-modal motion generation uses control conditions from the same modality as the motion, including motion prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib73\" title=\"\">2024a</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib12\" title=\"\">2023a</a>)</cite>, in-betweening&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cohan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib15\" title=\"\">2024</a>)</cite>, and joint/trajectory-guided synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>; Dai et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib16\" title=\"\">2024</a>; Zhao, Li, and Tang <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib84\" title=\"\">2024</a>)</cite>.\nCross-modal motion generation uses control conditions from different modalities, including text in T2M&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>; Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib52\" title=\"\">2024a</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib30\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib83\" title=\"\">2024d</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib80\" title=\"\">2023b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib81\" title=\"\">2024b</a>)</cite>, music in M2D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib35\" title=\"\">2023</a>; Siyao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib66\" title=\"\">2022</a>; Tseng, Castellon, and Liu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib68\" title=\"\">2023</a>)</cite>, speech in S2G&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib10\" title=\"\">2024a</a>)</cite>, target object positions in HOI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib50\" title=\"\">2024</a>)</cite>, scene layouts in HSI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kaufmann et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib33\" title=\"\">2023</a>; Huang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib27\" title=\"\">2022</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and partner movements in HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>)</cite>.\nHowever, these approaches typically rely on task-specific architectures, significantly limiting their cross-task generalization capabilities and practical applications.\nRecently, researchers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Luo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib54\" title=\"\">2024</a>)</cite> have begun exploring multimodal motion generation, with approaches falling into three primary categories:</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "2024a",
                    "chen",
                    "2023b",
                    "bian",
                    "guo",
                    "tevet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current human motion generation datasets are predominantly task-specific.\nDifferent tasks rely on separate collections: text-to-motion (T2M) uses datasets with text captions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Plappert, Mandery, and Asfour <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib62\" title=\"\">2016</a>; Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Punnakkal et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib63\" title=\"\">2021</a>; Liao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib43\" title=\"\">2024</a>)</cite>, music-to-dance (M2D) requires music-annotated datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib35\" title=\"\">2023</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib11\" title=\"\">2021</a>)</cite>, and speech-to-gesture (S2G) employs datasets with paired speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib49\" title=\"\">2022</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>)</cite>.\nSimilarly, interaction datasets remain disconnected across human-object&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib31\" title=\"\">2022</a>; Li, Wu, and Liu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib37\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib79\" title=\"\">2024a</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib50\" title=\"\">2024</a>; Zhan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib77\" title=\"\">2024</a>)</cite>, human-scene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and human-human interaction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>)</cite>. This fragmentation constrains multimodal motion generation capabilities.\nIntegrating these datasets presents challenges due to inconsistent motion formats.\nDatasets vary widely in their representations: some use SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, others employ SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>)</cite>, while many rely on BVH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Harvey et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib26\" title=\"\">2020</a>; Mason, Starke, and Komura <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib56\" title=\"\">2022</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib2\" title=\"\">Adobe </a>; Alexanderson et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib3\" title=\"\">2023</a>; Valle-P&#233;rez et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib69\" title=\"\">2021</a>)</cite> or keypoint representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ionescu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib28\" title=\"\">2013</a>; Ji et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib29\" title=\"\">2018</a>)</cite>. This diversity makes format standardization extremely difficult.\nRecent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite> have attempted to integrate multiple datasets, increasing scale and incorporating some multimodal conditions. However, they still exhibit several limitations:</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "texttomotion",
                    "2024a",
                    "chen",
                    "guo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While existing approaches prioritize dataset scale over quality by incorporating substantial amounts of non-mocap data&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, leading to degraded motion generation quality (&#8221;garbage in, garbage out&#8221;), our work emphasizes data quality.\nWe systematically curate open-source mocap datasets and classify them into five acquisition categories: Marker with manual correction, Marker (Vicon), IMU, Multi-View RGB, and Single-View RGB (detailed in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.T3\" title=\"Table 3 &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nOur dataset supports diverse motion generation tasks, including text-to-motion, motion-to-dance, speech-to-gesture, and various interactions (human-object, human-scene, and human-human), providing a high-quality multimodal foundation. Comprising <math alttext=\"64.3\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m1\" intent=\":literal\"><semantics><mn>64.3</mn><annotation encoding=\"application/x-tex\">64.3</annotation></semantics></math> million frames and <math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m2\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours of data, our approach achieves comprehensive task coverage while maintaining superior data quality.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "texttomotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We standardize diverse motion formats (BVH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lauterbach et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib34\" title=\"\">2009</a>)</cite>, FBX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wikipedia contributors <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib71\" title=\"\">2024</a>)</cite>, and SMPL&#160;(-H)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib51\" title=\"\">2023</a>)</cite>) into the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format to support our unified multimodal model.\nFirst, we convert these formats into the Motion-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>)</cite> SMPL-X format, comprising root orientation, pose parameters (body, hand, and jaw), facial expressions, facial shape, translation, and body shape parameters.\nWe then normalize the translation scale and initial root orientation across datasets to establish a consistent coordinate system. Additionally, we resample all datasets to 30 frames per second (FPS) to enhance the model&#8217;s ability to capture temporal patterns.\nThe specific conversion process and normalization for different datasets are provided in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we propose an automated approach that integrates both visual and textual information for comprehensive motion annotation.\nOur method renders motion into videos and combines them with existing textual annotations (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, descriptions, action labels, and task categories) as input to the state-of-the-art vision-language models (VLMs), GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib1\" title=\"\">2023</a>)</cite>.\nThis multimodal approach generates structured, hierarchical, and precise motion captions that ensure both annotation quality and expression richness. Detailed statistics of the texts quality are in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.F1\" title=\"Figure 1 &#8227; Unified Motion Representation &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nWe propose a unified multimodal, autoregressive transformer-based diffusion model for whole-body human motion generation across multiple tasks.\nThe model takes textual description <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> for semantic guidance, global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> for spatial-temporal control, and speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> and music conditions <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> to ensure rhythmic and stylistic coherence.\nFurthermore, it incorporates reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> as a motion prior, derived from either previously generated clips or user-designed motion, thus providing fine-grained details unavailable in other conditions.\nIn addition, the mixed-condition setting allows multiple conditions to occur simultaneously during training, enabling the model to handle complex scenarios with diverse inputs.\nIn this section, we present our approach in two parts: a unified framework for multimodal modeling and a progressive training strategy that ensures precise motion control.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "omnimotionx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Conditions.</span> Our framework integrates multiple condition modalities: text <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> providing semantic guidance; global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> ensuring spatial-temporal consistency; speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> synchronizing gestures and lip movements with rhythm; music <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> supplying beat and style information for dance; and reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> serving as a motion prior. Notably, this reference motion condition&#8212;overlooked in previous multimodal motion generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>&#8212;enables our model to maintain precise spatial-temporal pattern consistency with reference, substantially enhancing motion quality and coherence.\nTo fully leverage each modality and facilitate interaction between different conditions, we employ modality-specific encoders (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, T5-XXL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib65\" title=\"\">2020</a>)</cite> for text, a wav encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite> for speech, Librosa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McFee et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib57\" title=\"\">2015</a>)</cite> for music and body-wise encoding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> for motion) to extract features from each modality. These features are aligned to match motion embedding dimensions using learnable linear projections, allowing us to concatenate all condition tokens as prefix context with noisy motion tokens during processing.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "all",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m1\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> and <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m2\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math> denote the modality-specific encoder and projection layer, respectively. The concatenated representation <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m3\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math> is then fed into our DiT backbone as a conditioning prefix context.\nTo constrain the physical properties of motion, we follow previous works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib13\" title=\"\">2024b</a>)</cite> by directly predicting motion <math alttext=\"\\hat{x}_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m4\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\hat{x}_{0}</annotation></semantics></math> rather than noise. Consequently, our diffusion objective is defined as follows:</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "chen",
                    "tevet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering our diverse multi-granularity conditioning inputs, we empirically observed that employing all conditions within a single-stage training paradigm leads to difficulties in directly learning the correlation between motion and conditions. Moreover, the model tends to overfit strongly constrained low-level control signals such as fine-grained reference motion and spatiotemporal joint control. While these signals appear dominant, they can suppress other modalities such as text, ultimately compromising overall controllability.\nTo address this, we implement weak-to-strong progressive training: as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F2\" title=\"Figure 2 &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> initial text-conditioned learning establishes motion-semantic alignment, followed by progressive integration of finer-grained conditions, including reference motion, global motion, and audio signals.\nThis progressive approach enables the model to effectively adapt to different conditions, ensuring high-quality and flexible motion generation while precisely adhering to multimodal conditions.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method employs a Transformer Encoder architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib70\" title=\"\">2017</a>)</cite> with 8 layers and 8 attention heads, featuring a hidden dimension of <math alttext=\"d_{\\text{model}}=1536~(128\\times 12)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>=</mo><mrow><mn>1536</mn><mo lspace=\"0.330em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>128</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>12</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d_{\\text{model}}=1536~(128\\times 12)</annotation></semantics></math>, where <math alttext=\"128\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m2\" intent=\":literal\"><semantics><mn>128</mn><annotation encoding=\"application/x-tex\">128</annotation></semantics></math> represents the embedding size per each of the <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m3\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> body parts, and a feedforward dimension of <math alttext=\"3072\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m4\" intent=\":literal\"><semantics><mn>3072</mn><annotation encoding=\"application/x-tex\">3072</annotation></semantics></math>. Training is performed on a single H800 GPU through progressive conditioning: starting with text-only training for <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m5\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, then adding reference motion for another <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m6\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, followed by global spatiotemporal control for <math alttext=\"230K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>230</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">230K</annotation></semantics></math> steps, and finally incorporating full audio conditions for <math alttext=\"920K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>920</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">920K</annotation></semantics></math> steps. The corresponding batch sizes are <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m9\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m10\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m11\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, and <math alttext=\"16\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m12\" intent=\":literal\"><semantics><mn>16</mn><annotation encoding=\"application/x-tex\">16</annotation></semantics></math>, respectively. We optimize with AdamW using an initial learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> (reset for new conditions) and a cosine schedule decaying to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m14\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> within the first <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m15\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps. The default length of motion reference and prediction is <math alttext=\"150\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m16\" intent=\":literal\"><semantics><mn>150</mn><annotation encoding=\"application/x-tex\">150</annotation></semantics></math>. More implementation details are provided in supplementary material.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;on four tasks: Text-to-Motion (T2M), Global Spatiotemporal Controllable Generation (GSTC), Music-to-Dance (M2D), and Speech-to-Gesture (S2G).\nFor T2M and GSTC evaluations, test sets are uniformly sampled across all datasets&#160;(<math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx2.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> samples from each).\nThe M2D benchmark integrates test sequences from AIST++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>)</cite>, and PhantomDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib36\" title=\"\">2022</a>)</cite>, with S2G evaluation conducted on BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "texttomotion",
                    "omnimotionx",
                    "evaluations",
                    "all",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GSTC.</span>\nWe adopt the cross-joint setup from OmniControl&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>)</cite>, simulating spatially dense control by controlling all joints.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T5\" title=\"Table 5 &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, due to the small dataset size, OmniControl&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>)</cite> struggles to generalize in GSTC task.\nIn contrast, our method demonstrates clear advantages, effectively following spatially dense control signals.</p>\n\n",
                "matched_terms": [
                    "all",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">M2D and S2G.</span>\nWe directly compare our method with the MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> on the S2G and M2D tasks.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T6\" title=\"Table 6 &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves competitive performance. The lower FID is mainly due to the relatively small test sets in S2G and M2D, while <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is trained on the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, leading to some distribution differences. Additionally, our method demonstrates superior diversity, which may also impact the FID.</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "motioncraft",
                    "method",
                    "test",
                    "bian",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F3\" title=\"Figure 3 &#8227; Unified Multimodal Modeling &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;handles various multimodal generation, including T2M, M2D, S2G, and GSTC (covering motion prediction, in-betweening, and joint guidance). When combined with reference motion, the model generates conditioned motions that align with the reference. More visualizations are in the supplementary video.</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces the <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;and the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>.\n<em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is an autoregressive diffusion model that integrates reference motion and multimodal conditions for precise whole-body motion control. It employs a progressive weak-to-strong conditioning strategy to effectively handle multi-granular constraints.\n<em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;is the largest multimodal MoCap dataset, comprising <math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx6.p1.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours from <math alttext=\"28\" class=\"ltx_Math\" display=\"inline\" id=\"Sx6.p1.m2\" intent=\":literal\"><semantics><mn>28</mn><annotation encoding=\"application/x-tex\">28</annotation></semantics></math> motion capture datasets, unified in SMPL-X with structured and consistent captions across 10 tasks.\nExperiments demonstrate that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;outperforms baselines, laying a strong foundation for large-scale multimodal motion generation.</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "multimodal",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitation and Future Work.</span> While effective, our method currently lacks scene, object, and human interaction constraints, restricting its use in complex real-world applications. Furthermore, the sample-space denoising process results in slower inference speeds. Future work should focus on integrating interaction constraints to be more versatile and developing more efficient motion representations for improved physical consistency and inference speed.</p>\n\n",
                "matched_terms": [
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Supplementary Material for OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</span>\n</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "omnimotionx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Format Conversion.</span>\nSince many datasets are not in the SMPL-series format, we convert all datasets to the SMPL-X format for consistency. For BVH-format datasets such as Mixamo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib2\" title=\"\">Adobe </a>)</cite>, 100Style&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mason, Starke, and Komura <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib56\" title=\"\">2022</a>)</cite>, Motorica&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alexanderson et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib3\" title=\"\">2023</a>)</cite>, and LaFAN1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Harvey et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib26\" title=\"\">2020</a>)</cite>, we first standardize the reference pose of all BVH files to a T-pose to ensure consistent initialization. We then align the root node&#8217;s coordinate system with that of the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> model in Blender&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib9\" title=\"\">Blender Foundation </a>)</cite>, where the negative Y-axis is defined as the forward direction and the Z-axis as the vertical upward direction.\nTo adapt to the SMPL-X topology, we generate a corresponding BVH skeleton based on a predefined SMPL-X T-pose template. We perform skeleton retargeting in MotionBuilder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib6\" title=\"\">Autodesk Inc. </a>)</cite> to map the original animation data onto the SMPL-X hierarchy.\nFinally, we convert the retargeted BVH files from Euler angles to axis-angle representations and apply Gaussian filtering to smooth both joint rotations and translations over time, yielding stable SMPL-X parameters.\nFor the Choreomaster dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib11\" title=\"\">2021</a>)</cite>, originally in FBX format, we first convert it to BVH using Blender, and then process it using the same pipeline.</p>\n\n",
                "matched_terms": [
                    "all",
                    "chen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text and Motion Feature Extractors.</span>\nExisting research typically relies on pre-trained motion and text feature extractors for evaluation. However, due to significant differences in dataset scale, distribution, and motion representation compared to existing works, these pre-trained extractors are not directly applicable. Therefore, following the approach&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib52\" title=\"\">2024a</a>)</cite>, we re-train motion and text feature extractor using a contrastive learning framework tailored to this dataset. The text feature extractor is based on the Transformer encoder architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Petrovich, Black, and Varol <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib61\" title=\"\">2023</a>)</cite>, which encodes raw text into a semantic vector <math alttext=\"s_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">s_{t}</annotation></semantics></math>. The motion feature extractor also uses a Transformer encoder to encode motion sequences of up to 150 frames into a semantic vector <math alttext=\"s_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">s_{m}</annotation></semantics></math>. Both encoders include additional semantic tokens, with a structure similar to the encoder in ACTOR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Petrovich, Black, and Varol <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib60\" title=\"\">2021</a>)</cite>, but without involving probabilistic distribution modeling. In implementation, the text encoder takes as input text features extracted from a pre-trained and frozen DistilBERT network, while the motion encoder directly processes raw motion sequence data. In the contrastive learning framework, we optimize the feature space such that matching text-motion feature pairs <math alttext=\"(s_{t},s_{m})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(s_{t},s_{m})</annotation></semantics></math> are brought closer in the embedding space, while ensuring that non-matching pairs are separated by at least a distance <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>. This optimization objective is achieved through the following contrastive loss function:</p>\n\n",
                "matched_terms": [
                    "2024a",
                    "closer"
                ]
            }
        ]
    },
    "Sx5.T5": {
        "caption": "Table 5: Quantitative results of global spatiotemporal controllable generation on the OmniMoCap-X test set.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">FID<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">R Precision<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Multimodal<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.T5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Top-3</td>\n<td class=\"ltx_td ltx_align_center\">Distance</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">GT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.821</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.493</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">OmniControl</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0C72BA;\">63.725</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0C72BA;\">0.392</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0C72BA;\">8.011</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#C00000;\">4.224</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#C00000;\">0.682</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#C00000;\">4.377</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "set",
            "global",
            "omnicontrol",
            "controllable",
            "omnimotionx",
            "multimodal↓downarrow",
            "spatiotemporal",
            "top3",
            "distance",
            "quantitative",
            "generation",
            "precision↑uparrow",
            "results",
            "test",
            "method",
            "fid↓downarrow",
            "omnimocapx"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GSTC.</span>\nWe adopt the cross-joint setup from OmniControl&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>)</cite>, simulating spatially dense control by controlling all joints.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T5\" title=\"Table 5 &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, due to the small dataset size, OmniControl&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>)</cite> struggles to generalize in GSTC task.\nIn contrast, our method demonstrates clear advantages, effectively following spatially dense control signals.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper introduces <span class=\"ltx_text ltx_font_bold\">OmniMotion-X</span>, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks&#8212;including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis)&#8212;as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct <span class=\"ltx_text ltx_font_bold\">OmniMoCap-X</span>, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.</p>\n\n",
                "matched_terms": [
                    "global",
                    "controllable",
                    "omnimotionx",
                    "generation",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_caption\">We present <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified sequence-to-sequence autoregressive motion diffusion transformer designed for flexible and interactive whole-body human motion generation. It supports a variety of tasks, including text-to-motion, music-to-dance, speech-to-gesture, and globally spatial-temporal controllable motion generation, which encompasses motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis. These conditions can be combined in various ways to enable versatile motion generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "generation",
                    "controllable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address existing challenges, we propose <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified framework for multimodal whole-body human motion generation, and <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset.\nSpecifically, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;employs Diffusion Transformers (DiTs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles and Xie <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib59\" title=\"\">2023</a>)</cite>, incorporates multimodal conditions by concatenating condition tokens as prefix context, and adopts a progressive weak-to-strong mixed-condition training strategy to gradually constrain motion from high-level semantics to dense spatial-temporal alignment.\nNotably, unlike previous methods, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;introduces a novel generation paradigm that utilizes reference motion (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, user-provided or model-predicted motion) as a special condition. This significantly enhances generated motion quality and achieves consistency between reference and generated motion, creating an effective clip-by-clip autoregressive motion diffusion.\nThis enables <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;to support autoregressive interactive generation with strong temporal alignment.\nFurthermore, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;unifies various Global Spatial-Temporal Controllable Generation tasks through spatial-temporal masking strategies.\nTo guarantee high-quality motion generation training, we collect high-quality mocap datasets that support diverse motion generation tasks, unify them under the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format with standard world coordinate systems, and automatically generate hierarchical text captions by rendering motions into videos and annotating them with vision language models (VLMs).\nThis dataset contains about <em class=\"ltx_emph ltx_font_italic\"><math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.p3.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours</em> and integrates multimodal control conditions, supporting versatile tasks, including T2M, M2D, S2G, HOI, HSI, and HHI.</p>\n\n",
                "matched_terms": [
                    "global",
                    "controllable",
                    "omnimotionx",
                    "generation",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a multimodal autoregressive diffusion transformer for versatile whole-body motion generation. By introducing reference motion, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;significantly enhances consistent content, style, and temporal dynamics generation.</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset with a unified SMPL-X format, and provides consistent, detailed, and structured text captions to serve versatile motion generation tasks.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments show that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves state-of-the-art performance across various tasks, including T2M, S2G, M2D, and GSTC. These evaluations were conducted on our more challenging test sets comprising <math alttext=\"280\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.I1.i4.p1.m1\" intent=\":literal\"><semantics><mn>280</mn><annotation encoding=\"application/x-tex\">280</annotation></semantics></math> samples uniformly sampled from our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "omnimocapx",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.F1\" title=\"Figure 1 &#8227; Unified Motion Representation &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nWe propose a unified multimodal, autoregressive transformer-based diffusion model for whole-body human motion generation across multiple tasks.\nThe model takes textual description <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> for semantic guidance, global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> for spatial-temporal control, and speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> and music conditions <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> to ensure rhythmic and stylistic coherence.\nFurthermore, it incorporates reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> as a motion prior, derived from either previously generated clips or user-designed motion, thus providing fine-grained details unavailable in other conditions.\nIn addition, the mixed-condition setting allows multiple conditions to occur simultaneously during training, enabling the model to handle complex scenarios with diverse inputs.\nIn this section, we present our approach in two parts: a unified framework for multimodal modeling and a progressive training strategy that ensures precise motion control.</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "global",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Conditions.</span> Our framework integrates multiple condition modalities: text <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> providing semantic guidance; global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> ensuring spatial-temporal consistency; speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> synchronizing gestures and lip movements with rhythm; music <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> supplying beat and style information for dance; and reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> serving as a motion prior. Notably, this reference motion condition&#8212;overlooked in previous multimodal motion generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>&#8212;enables our model to maintain precise spatial-temporal pattern consistency with reference, substantially enhancing motion quality and coherence.\nTo fully leverage each modality and facilitate interaction between different conditions, we employ modality-specific encoders (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, T5-XXL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib65\" title=\"\">2020</a>)</cite> for text, a wav encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite> for speech, Librosa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McFee et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib57\" title=\"\">2015</a>)</cite> for music and body-wise encoding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> for motion) to extract features from each modality. These features are aligned to match motion embedding dimensions using learnable linear projections, allowing us to concatenate all condition tokens as prefix context with noisy motion tokens during processing.</p>\n\n",
                "matched_terms": [
                    "global",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering our diverse multi-granularity conditioning inputs, we empirically observed that employing all conditions within a single-stage training paradigm leads to difficulties in directly learning the correlation between motion and conditions. Moreover, the model tends to overfit strongly constrained low-level control signals such as fine-grained reference motion and spatiotemporal joint control. While these signals appear dominant, they can suppress other modalities such as text, ultimately compromising overall controllability.\nTo address this, we implement weak-to-strong progressive training: as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F2\" title=\"Figure 2 &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> initial text-conditioned learning establishes motion-semantic alignment, followed by progressive integration of finer-grained conditions, including reference motion, global motion, and audio signals.\nThis progressive approach enables the model to effectively adapt to different conditions, ensuring high-quality and flexible motion generation while precisely adhering to multimodal conditions.</p>\n\n",
                "matched_terms": [
                    "global",
                    "generation",
                    "spatiotemporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method employs a Transformer Encoder architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib70\" title=\"\">2017</a>)</cite> with 8 layers and 8 attention heads, featuring a hidden dimension of <math alttext=\"d_{\\text{model}}=1536~(128\\times 12)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>=</mo><mrow><mn>1536</mn><mo lspace=\"0.330em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>128</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>12</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d_{\\text{model}}=1536~(128\\times 12)</annotation></semantics></math>, where <math alttext=\"128\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m2\" intent=\":literal\"><semantics><mn>128</mn><annotation encoding=\"application/x-tex\">128</annotation></semantics></math> represents the embedding size per each of the <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m3\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> body parts, and a feedforward dimension of <math alttext=\"3072\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m4\" intent=\":literal\"><semantics><mn>3072</mn><annotation encoding=\"application/x-tex\">3072</annotation></semantics></math>. Training is performed on a single H800 GPU through progressive conditioning: starting with text-only training for <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m5\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, then adding reference motion for another <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m6\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, followed by global spatiotemporal control for <math alttext=\"230K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>230</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">230K</annotation></semantics></math> steps, and finally incorporating full audio conditions for <math alttext=\"920K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>920</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">920K</annotation></semantics></math> steps. The corresponding batch sizes are <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m9\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m10\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m11\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, and <math alttext=\"16\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m12\" intent=\":literal\"><semantics><mn>16</mn><annotation encoding=\"application/x-tex\">16</annotation></semantics></math>, respectively. We optimize with AdamW using an initial learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> (reset for new conditions) and a cosine schedule decaying to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m14\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> within the first <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m15\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps. The default length of motion reference and prediction is <math alttext=\"150\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m16\" intent=\":literal\"><semantics><mn>150</mn><annotation encoding=\"application/x-tex\">150</annotation></semantics></math>. More implementation details are provided in supplementary material.</p>\n\n",
                "matched_terms": [
                    "global",
                    "spatiotemporal",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;on four tasks: Text-to-Motion (T2M), Global Spatiotemporal Controllable Generation (GSTC), Music-to-Dance (M2D), and Speech-to-Gesture (S2G).\nFor T2M and GSTC evaluations, test sets are uniformly sampled across all datasets&#160;(<math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx2.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> samples from each).\nThe M2D benchmark integrates test sequences from AIST++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>)</cite>, and PhantomDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib36\" title=\"\">2022</a>)</cite>, with S2G evaluation conducted on BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "controllable",
                    "omnimotionx",
                    "spatiotemporal",
                    "generation",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">T2M.</span>\nTab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.T4\" title=\"Table 4 &#8227; Consistent Visual-textual Motion Captions &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that previous methods, constrained by small-scale datasets, struggle to generalize in generating diverse and complex motions.\nIn contrast, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;demonstrates a significant advantage, outperforming not only baseline methods trained on small datasets (MDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>)</cite>, MLD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>)</cite>, MoMask&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>)</cite>, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>) but also MoMask* and MotionCraft*, which are trained on our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>.\nCompared to MoMask* and MotionCraft*, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;uses stronger text encoder (T5-XXL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib65\" title=\"\">2020</a>)</cite>), which models complex texts more effectively than CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib64\" title=\"\">2021</a>)</cite>.\nIn addition, it adopts a unified multimodal framework with reference motion during training, enabling the model to learn more detailed whole-body motion representations, leading to better generation quality and generalization.</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "generation",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">M2D and S2G.</span>\nWe directly compare our method with the MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> on the S2G and M2D tasks.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T6\" title=\"Table 6 &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves competitive performance. The lower FID is mainly due to the relatively small test sets in S2G and M2D, while <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is trained on the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, leading to some distribution differences. Additionally, our method demonstrates superior diversity, which may also impact the FID.</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "omnimocapx",
                    "method",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T7\" title=\"Table 7 &#8227; Ablation Study &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the ablation results and demonstrates two key findings: (1) Ablating coarse-to-fine training compromises text-conditioned alignment, indicating finer-grained controls may override coarse semantic constraints; (2) Mixed-condition training impairs spatiotemporal control due to physical constraints introducing optimization conflicts, necessitating weak-to-strong conditioning.</p>\n\n",
                "matched_terms": [
                    "spatiotemporal",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F3\" title=\"Figure 3 &#8227; Unified Multimodal Modeling &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;handles various multimodal generation, including T2M, M2D, S2G, and GSTC (covering motion prediction, in-betweening, and joint guidance). When combined with reference motion, the model generates conditioned motions that align with the reference. More visualizations are in the supplementary video.</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces the <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;and the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>.\n<em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is an autoregressive diffusion model that integrates reference motion and multimodal conditions for precise whole-body motion control. It employs a progressive weak-to-strong conditioning strategy to effectively handle multi-granular constraints.\n<em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;is the largest multimodal MoCap dataset, comprising <math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx6.p1.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours from <math alttext=\"28\" class=\"ltx_Math\" display=\"inline\" id=\"Sx6.p1.m2\" intent=\":literal\"><semantics><mn>28</mn><annotation encoding=\"application/x-tex\">28</annotation></semantics></math> motion capture datasets, unified in SMPL-X with structured and consistent captions across 10 tasks.\nExperiments demonstrate that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;outperforms baselines, laying a strong foundation for large-scale multimodal motion generation.</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "generation",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitation and Future Work.</span> While effective, our method currently lacks scene, object, and human interaction constraints, restricting its use in complex real-world applications. Furthermore, the sample-space denoising process results in slower inference speeds. Future work should focus on integrating interaction constraints to be more versatile and developing more efficient motion representations for improved physical consistency and inference speed.</p>\n\n",
                "matched_terms": [
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Supplementary Material for OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</span>\n</p>\n\n",
                "matched_terms": [
                    "omnimotionx",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx7.T8\" title=\"Table 8 &#8227; Dataset Text Quality &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents a comprehensive analysis of the text quality across our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;collection, which comprises over <math alttext=\"321,000\" class=\"ltx_Math\" display=\"inline\" id=\"Sx7.p1.m1\" intent=\":literal\"><semantics><mrow><mn>321</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">321,000</annotation></semantics></math> textual descriptions spanning various motion-related tasks. This extensive dataset was created by collecting and curating existing motion datasets, rendering motions to videos, and then leveraging vision-language models (VLMs) to generate rich textual descriptions.\nThe linguistic diversity of our dataset is evidenced by the Type-Token Ratio (TTR) ranges, with maximum values reaching 1.0 in multiple subsets (AIOZ, Motorica, IDEA400, etc.), indicating exceptional lexical richness. The average sentence length of 276.78 words across the entire collection demonstrates the descriptive depth of our motion annotations, providing detailed accounts of nuanced movements rather than simplistic action labels.\nParticularly noteworthy is the variety of verbs captured across different motion categories. The dataset exhibits task-specific verb distributions that align with the semantic nature of each motion type: T2M datasets frequently feature locomotion verbs (&#8220;walk,&#8221; &#8220;step&#8221;), M2D collections emphasize rhythmic movements (&#8220;swing,&#8221; &#8220;lift&#8221;), HOI datasets contain manipulation verbs (&#8220;lift,&#8221; &#8220;hold,&#8221; &#8220;grasp&#8221;), and HHI datasets capture interpersonal dynamics (&#8220;stand,&#8221; &#8220;extend,&#8221; &#8220;lean&#8221;). This semantic richness enables models trained on <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;to understand and generate precise motion descriptions across diverse contexts.\nThe comprehensive coverage across six distinct motion-related tasks (T2M, M2D, S2G, HHI, HOI, HSI) makes <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;uniquely positioned to support generalizable motion understanding. By integrating multiple motion paradigms under a unified annotation framework, our dataset transcends the limitations of task-specific collections, allowing for transfer learning across motion domains. This textual quality and diversity significantly enhance the capability of motion generation models to comprehend natural language instructions and produce corresponding human-like movements.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "omnimocapx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Mask.</span> Our model leverages four specialized masking mechanisms: (1) Component attention mask, the standard source key padding mask in Transformers&#8212;regulates attention across different conditional inputs. In our DiT-based architecture, input conditions may be partially missing; this mask zeroes out attention weights for missing inputs, allowing the model to focus only on valid ones. (2) Global task-dependent mask, a spatiotemporal dynamic mask tailored to tasks like motion prediction, interpolation, completion, and trajectory-guided generation&#8212;distinguishes between observed and unobserved motion regions based on task requirements. (3) Global motion disentanglement mask transforms dense global motion conditions into a hybrid sparse-dense representation by selecting key joints (e.g., <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p2.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> joints) for global representation optimization, while the remaining <math alttext=\"n-k\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8722;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">n-k</annotation></semantics></math> joints contribute to local motion representation and noise injection. (4) Motion representation reconstruction mask filters out reconstruction errors of joints without ground-truth annotations during training. For instance, it suppresses hand-joint errors in datasets lacking hand annotations, effectively addressing cross-dataset joint count mismatches.</p>\n\n",
                "matched_terms": [
                    "global",
                    "spatiotemporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset supports various interaction types, including Human-Human Interaction (HHI), Human-Object Interaction (HOI), and Human-Scene Interaction (HSI), as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx9.F5\" title=\"Figure 5 &#8227; More Implementation Details &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Specifically, human-human interactions encompass a rich spectrum of social scenarios such as conversations, handshakes, embraces, and collaborative activities. Human-object interactions include everyday activities like grasping, manipulation, and tool usage. Human-scene interactions capture the dynamic relationships between individuals and their environments, such as sitting on indoor sofas with feet resting on carpets, and other contextual behavioral patterns. Each interaction type is accompanied by detailed spatio-temporal annotations and semantic descriptions, providing comprehensive training samples for multimodal whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "spatiotemporal"
                ]
            }
        ]
    },
    "Sx5.T6": {
        "caption": "Table 6: Results of S2G in BEAT2 and M2D in AIST++, FineDance and Phantomdance We respectively evaluate the F​I​DW​h​o​l​e​B​o​d​yFID_{WholeBody}, F​I​DH​a​n​d​sFID_{Hands}, F​a​c​eM​S​EFace_{MSE}, and diversity for S2G and the F​I​DW​h​o​l​e​B​o​d​yFID_{WholeBody}, F​I​DH​a​n​d​sFID_{Hands}, and diversity for M2D.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\">Speech-to-Gesture</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">Music-to-Dance</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FID (Whole-Body)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.T6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FID (Hands)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.T6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"Face_{MSE}\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.T6.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>F</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>e</mi><mrow><mi>M</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>E</mi></mrow></msub></mrow><mo stretchy=\"false\">&#8595;</mo><mi/></mrow><annotation encoding=\"application/x-tex\">Face_{MSE}\\downarrow</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Diversity<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.T6.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FID (Whole Body)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.T6.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">FID (Hands)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.T6.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Diversity<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.T6.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">MotionCraft</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0C72BA;\">3.422</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#C00000;\">5.370</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0C72BA;\">0.182</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0C72BA;\">1.003</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#C00000;\">9.875</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0C72BA;\">7.099</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0C72BA;\">3.798</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>(Ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#C00000;\">2.641</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0C72BA;\">9.095</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#C00000;\">0.045</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#C00000;\">1.664</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#0C72BA;\">16.209</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#C00000;\">5.827</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#C00000;\">4.716</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "omnimotionxours",
            "f​a​c​em​s​e↓facemsedownarrow",
            "finedance",
            "whole",
            "musictodance",
            "aist",
            "s2g",
            "diversity",
            "wholebody↓downarrow",
            "beat2",
            "phantomdance",
            "diversity↑uparrow",
            "body↓downarrow",
            "f​a​c​em​s​efacemse",
            "f​i​dh​a​n​d​sfidhands",
            "results",
            "fid",
            "respectively",
            "evaluate",
            "speechtogesture",
            "f​i​dw​h​o​l​e​b​o​d​yfidwholebody",
            "hands↓downarrow",
            "motioncraft",
            "m2d",
            "method"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">M2D and S2G.</span>\nWe directly compare our method with the MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> on the S2G and M2D tasks.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T6\" title=\"Table 6 &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves competitive performance. The lower FID is mainly due to the relatively small test sets in S2G and M2D, while <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is trained on the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, leading to some distribution differences. Additionally, our method demonstrates superior diversity, which may also impact the FID.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper introduces <span class=\"ltx_text ltx_font_bold\">OmniMotion-X</span>, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks&#8212;including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis)&#8212;as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct <span class=\"ltx_text ltx_font_bold\">OmniMoCap-X</span>, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.</p>\n\n",
                "matched_terms": [
                    "musictodance",
                    "speechtogesture"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_caption\">We present <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified sequence-to-sequence autoregressive motion diffusion transformer designed for flexible and interactive whole-body human motion generation. It supports a variety of tasks, including text-to-motion, music-to-dance, speech-to-gesture, and globally spatial-temporal controllable motion generation, which encompasses motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis. These conditions can be combined in various ways to enable versatile motion generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "musictodance",
                    "speechtogesture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal whole-body human motion generation plays critical roles in animation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib13\" title=\"\">2024b</a>)</cite>, gaming&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib43\" title=\"\">2024</a>)</cite>, virtual reality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib24\" title=\"\">2024b</a>)</cite>, and embodied intelligence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib55\" title=\"\">2024</a>)</cite> across diverse input conditions, including text, audio, and trajectory, etc.\nThe limited multimodal data and task-specific model designs prevent existing motion generation methods from supporting multimodal whole-body human motion generation. Due to the high cost of mocap data collection and labeling, most datasets focus on single domains such as Text-to-Motion (T2M)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Plappert, Mandery, and Asfour <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib62\" title=\"\">2016</a>)</cite>, Music-to-Dance (M2D)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, Speech-to-Gesture (S2G)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>)</cite>, Human-Object Interaction (HOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>)</cite>, Human-Scene Interaction (HSI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and Human-Human Interaction (HHI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>)</cite>, each with inconsistent data formats (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, BVH, SMPL-(H/X) and 3D Keypoints) and control conditions (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Text captions, Audio, and Trajectories).\nTo tackle motion generation across diverse scenarios, it is crucial to build a unified framework that utilizes large-scale, diverse data to achieve more generalized representations.</p>\n\n",
                "matched_terms": [
                    "musictodance",
                    "m2d",
                    "speechtogesture",
                    "s2g"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address existing challenges, we propose <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified framework for multimodal whole-body human motion generation, and <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset.\nSpecifically, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;employs Diffusion Transformers (DiTs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles and Xie <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib59\" title=\"\">2023</a>)</cite>, incorporates multimodal conditions by concatenating condition tokens as prefix context, and adopts a progressive weak-to-strong mixed-condition training strategy to gradually constrain motion from high-level semantics to dense spatial-temporal alignment.\nNotably, unlike previous methods, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;introduces a novel generation paradigm that utilizes reference motion (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, user-provided or model-predicted motion) as a special condition. This significantly enhances generated motion quality and achieves consistency between reference and generated motion, creating an effective clip-by-clip autoregressive motion diffusion.\nThis enables <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;to support autoregressive interactive generation with strong temporal alignment.\nFurthermore, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;unifies various Global Spatial-Temporal Controllable Generation tasks through spatial-temporal masking strategies.\nTo guarantee high-quality motion generation training, we collect high-quality mocap datasets that support diverse motion generation tasks, unify them under the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format with standard world coordinate systems, and automatically generate hierarchical text captions by rendering motions into videos and annotating them with vision language models (VLMs).\nThis dataset contains about <em class=\"ltx_emph ltx_font_italic\"><math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.p3.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours</em> and integrates multimodal control conditions, supporting versatile tasks, including T2M, M2D, S2G, HOI, HSI, and HHI.</p>\n\n",
                "matched_terms": [
                    "m2d",
                    "s2g"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments show that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves state-of-the-art performance across various tasks, including T2M, S2G, M2D, and GSTC. These evaluations were conducted on our more challenging test sets comprising <math alttext=\"280\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.I1.i4.p1.m1\" intent=\":literal\"><semantics><mn>280</mn><annotation encoding=\"application/x-tex\">280</annotation></semantics></math> samples uniformly sampled from our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "m2d",
                    "s2g"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing methods are typically categorized based on input conditions into three main types: single-modal, cross-modal, and multimodal.\nSingle-modal motion generation uses control conditions from the same modality as the motion, including motion prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib73\" title=\"\">2024a</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib12\" title=\"\">2023a</a>)</cite>, in-betweening&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cohan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib15\" title=\"\">2024</a>)</cite>, and joint/trajectory-guided synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>; Dai et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib16\" title=\"\">2024</a>; Zhao, Li, and Tang <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib84\" title=\"\">2024</a>)</cite>.\nCross-modal motion generation uses control conditions from different modalities, including text in T2M&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>; Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib52\" title=\"\">2024a</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib30\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib83\" title=\"\">2024d</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib80\" title=\"\">2023b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib81\" title=\"\">2024b</a>)</cite>, music in M2D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib35\" title=\"\">2023</a>; Siyao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib66\" title=\"\">2022</a>; Tseng, Castellon, and Liu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib68\" title=\"\">2023</a>)</cite>, speech in S2G&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib10\" title=\"\">2024a</a>)</cite>, target object positions in HOI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib50\" title=\"\">2024</a>)</cite>, scene layouts in HSI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kaufmann et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib33\" title=\"\">2023</a>; Huang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib27\" title=\"\">2022</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and partner movements in HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>)</cite>.\nHowever, these approaches typically rely on task-specific architectures, significantly limiting their cross-task generalization capabilities and practical applications.\nRecently, researchers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Luo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib54\" title=\"\">2024</a>)</cite> have begun exploring multimodal motion generation, with approaches falling into three primary categories:</p>\n\n",
                "matched_terms": [
                    "m2d",
                    "s2g"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current human motion generation datasets are predominantly task-specific.\nDifferent tasks rely on separate collections: text-to-motion (T2M) uses datasets with text captions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Plappert, Mandery, and Asfour <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib62\" title=\"\">2016</a>; Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Punnakkal et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib63\" title=\"\">2021</a>; Liao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib43\" title=\"\">2024</a>)</cite>, music-to-dance (M2D) requires music-annotated datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib35\" title=\"\">2023</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib11\" title=\"\">2021</a>)</cite>, and speech-to-gesture (S2G) employs datasets with paired speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib49\" title=\"\">2022</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>)</cite>.\nSimilarly, interaction datasets remain disconnected across human-object&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib31\" title=\"\">2022</a>; Li, Wu, and Liu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib37\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib79\" title=\"\">2024a</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib50\" title=\"\">2024</a>; Zhan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib77\" title=\"\">2024</a>)</cite>, human-scene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and human-human interaction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>)</cite>. This fragmentation constrains multimodal motion generation capabilities.\nIntegrating these datasets presents challenges due to inconsistent motion formats.\nDatasets vary widely in their representations: some use SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, others employ SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>)</cite>, while many rely on BVH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Harvey et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib26\" title=\"\">2020</a>; Mason, Starke, and Komura <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib56\" title=\"\">2022</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib2\" title=\"\">Adobe </a>; Alexanderson et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib3\" title=\"\">2023</a>; Valle-P&#233;rez et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib69\" title=\"\">2021</a>)</cite> or keypoint representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ionescu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib28\" title=\"\">2013</a>; Ji et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib29\" title=\"\">2018</a>)</cite>. This diversity makes format standardization extremely difficult.\nRecent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite> have attempted to integrate multiple datasets, increasing scale and incorporating some multimodal conditions. However, they still exhibit several limitations:</p>\n\n",
                "matched_terms": [
                    "musictodance",
                    "speechtogesture",
                    "s2g",
                    "m2d",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method employs a Transformer Encoder architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib70\" title=\"\">2017</a>)</cite> with 8 layers and 8 attention heads, featuring a hidden dimension of <math alttext=\"d_{\\text{model}}=1536~(128\\times 12)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>=</mo><mrow><mn>1536</mn><mo lspace=\"0.330em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>128</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>12</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d_{\\text{model}}=1536~(128\\times 12)</annotation></semantics></math>, where <math alttext=\"128\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m2\" intent=\":literal\"><semantics><mn>128</mn><annotation encoding=\"application/x-tex\">128</annotation></semantics></math> represents the embedding size per each of the <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m3\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> body parts, and a feedforward dimension of <math alttext=\"3072\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m4\" intent=\":literal\"><semantics><mn>3072</mn><annotation encoding=\"application/x-tex\">3072</annotation></semantics></math>. Training is performed on a single H800 GPU through progressive conditioning: starting with text-only training for <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m5\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, then adding reference motion for another <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m6\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, followed by global spatiotemporal control for <math alttext=\"230K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>230</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">230K</annotation></semantics></math> steps, and finally incorporating full audio conditions for <math alttext=\"920K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>920</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">920K</annotation></semantics></math> steps. The corresponding batch sizes are <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m9\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m10\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m11\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, and <math alttext=\"16\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m12\" intent=\":literal\"><semantics><mn>16</mn><annotation encoding=\"application/x-tex\">16</annotation></semantics></math>, respectively. We optimize with AdamW using an initial learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> (reset for new conditions) and a cosine schedule decaying to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m14\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> within the first <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m15\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps. The default length of motion reference and prediction is <math alttext=\"150\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m16\" intent=\":literal\"><semantics><mn>150</mn><annotation encoding=\"application/x-tex\">150</annotation></semantics></math>. More implementation details are provided in supplementary material.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;on four tasks: Text-to-Motion (T2M), Global Spatiotemporal Controllable Generation (GSTC), Music-to-Dance (M2D), and Speech-to-Gesture (S2G).\nFor T2M and GSTC evaluations, test sets are uniformly sampled across all datasets&#160;(<math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx2.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> samples from each).\nThe M2D benchmark integrates test sequences from AIST++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>)</cite>, and PhantomDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib36\" title=\"\">2022</a>)</cite>, with S2G evaluation conducted on BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "phantomdance",
                    "musictodance",
                    "aist",
                    "evaluate",
                    "s2g",
                    "speechtogesture",
                    "m2d",
                    "finedance",
                    "beat2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F3\" title=\"Figure 3 &#8227; Unified Multimodal Modeling &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;handles various multimodal generation, including T2M, M2D, S2G, and GSTC (covering motion prediction, in-betweening, and joint guidance). When combined with reference motion, the model generates conditioned motions that align with the reference. More visualizations are in the supplementary video.</p>\n\n",
                "matched_terms": [
                    "m2d",
                    "s2g"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitation and Future Work.</span> While effective, our method currently lacks scene, object, and human interaction constraints, restricting its use in complex real-world applications. Furthermore, the sample-space denoising process results in slower inference speeds. Future work should focus on integrating interaction constraints to be more versatile and developing more efficient motion representations for improved physical consistency and inference speed.</p>\n\n",
                "matched_terms": [
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx7.T8\" title=\"Table 8 &#8227; Dataset Text Quality &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents a comprehensive analysis of the text quality across our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;collection, which comprises over <math alttext=\"321,000\" class=\"ltx_Math\" display=\"inline\" id=\"Sx7.p1.m1\" intent=\":literal\"><semantics><mrow><mn>321</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">321,000</annotation></semantics></math> textual descriptions spanning various motion-related tasks. This extensive dataset was created by collecting and curating existing motion datasets, rendering motions to videos, and then leveraging vision-language models (VLMs) to generate rich textual descriptions.\nThe linguistic diversity of our dataset is evidenced by the Type-Token Ratio (TTR) ranges, with maximum values reaching 1.0 in multiple subsets (AIOZ, Motorica, IDEA400, etc.), indicating exceptional lexical richness. The average sentence length of 276.78 words across the entire collection demonstrates the descriptive depth of our motion annotations, providing detailed accounts of nuanced movements rather than simplistic action labels.\nParticularly noteworthy is the variety of verbs captured across different motion categories. The dataset exhibits task-specific verb distributions that align with the semantic nature of each motion type: T2M datasets frequently feature locomotion verbs (&#8220;walk,&#8221; &#8220;step&#8221;), M2D collections emphasize rhythmic movements (&#8220;swing,&#8221; &#8220;lift&#8221;), HOI datasets contain manipulation verbs (&#8220;lift,&#8221; &#8220;hold,&#8221; &#8220;grasp&#8221;), and HHI datasets capture interpersonal dynamics (&#8220;stand,&#8221; &#8220;extend,&#8221; &#8220;lean&#8221;). This semantic richness enables models trained on <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;to understand and generate precise motion descriptions across diverse contexts.\nThe comprehensive coverage across six distinct motion-related tasks (T2M, M2D, S2G, HHI, HOI, HSI) makes <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;uniquely positioned to support generalizable motion understanding. By integrating multiple motion paradigms under a unified annotation framework, our dataset transcends the limitations of task-specific collections, allowing for transfer learning across motion domains. This textual quality and diversity significantly enhance the capability of motion generation models to comprehend natural language instructions and produce corresponding human-like movements.</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "m2d",
                    "s2g"
                ]
            }
        ]
    },
    "Sx5.T7": {
        "caption": "Table 7: Ablation Study on the Training Strategy.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:1.0pt;padding-right:1.0pt;\">GSTC</td>\n</tr>\n</table> \n",
        "informative_terms_identified": [
            "study",
            "strategy",
            "gstc",
            "ablation",
            "training"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T7\" title=\"Table 7 &#8227; Ablation Study &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the ablation results and demonstrates two key findings: (1) Ablating coarse-to-fine training compromises text-conditioned alignment, indicating finer-grained controls may override coarse semantic constraints; (2) Mixed-condition training impairs spatiotemporal control due to physical constraints introducing optimization conflicts, necessitating weak-to-strong conditioning.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper introduces <span class=\"ltx_text ltx_font_bold\">OmniMotion-X</span>, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks&#8212;including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis)&#8212;as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct <span class=\"ltx_text ltx_font_bold\">OmniMoCap-X</span>, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.</p>\n\n",
                "matched_terms": [
                    "strategy",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address existing challenges, we propose <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified framework for multimodal whole-body human motion generation, and <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset.\nSpecifically, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;employs Diffusion Transformers (DiTs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles and Xie <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib59\" title=\"\">2023</a>)</cite>, incorporates multimodal conditions by concatenating condition tokens as prefix context, and adopts a progressive weak-to-strong mixed-condition training strategy to gradually constrain motion from high-level semantics to dense spatial-temporal alignment.\nNotably, unlike previous methods, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;introduces a novel generation paradigm that utilizes reference motion (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, user-provided or model-predicted motion) as a special condition. This significantly enhances generated motion quality and achieves consistency between reference and generated motion, creating an effective clip-by-clip autoregressive motion diffusion.\nThis enables <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;to support autoregressive interactive generation with strong temporal alignment.\nFurthermore, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;unifies various Global Spatial-Temporal Controllable Generation tasks through spatial-temporal masking strategies.\nTo guarantee high-quality motion generation training, we collect high-quality mocap datasets that support diverse motion generation tasks, unify them under the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format with standard world coordinate systems, and automatically generate hierarchical text captions by rendering motions into videos and annotating them with vision language models (VLMs).\nThis dataset contains about <em class=\"ltx_emph ltx_font_italic\"><math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.p3.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours</em> and integrates multimodal control conditions, supporting versatile tasks, including T2M, M2D, S2G, HOI, HSI, and HHI.</p>\n\n",
                "matched_terms": [
                    "strategy",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.F1\" title=\"Figure 1 &#8227; Unified Motion Representation &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nWe propose a unified multimodal, autoregressive transformer-based diffusion model for whole-body human motion generation across multiple tasks.\nThe model takes textual description <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> for semantic guidance, global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> for spatial-temporal control, and speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> and music conditions <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> to ensure rhythmic and stylistic coherence.\nFurthermore, it incorporates reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> as a motion prior, derived from either previously generated clips or user-designed motion, thus providing fine-grained details unavailable in other conditions.\nIn addition, the mixed-condition setting allows multiple conditions to occur simultaneously during training, enabling the model to handle complex scenarios with diverse inputs.\nIn this section, we present our approach in two parts: a unified framework for multimodal modeling and a progressive training strategy that ensures precise motion control.</p>\n\n",
                "matched_terms": [
                    "strategy",
                    "training"
                ]
            }
        ]
    },
    "Sx7.T8": {
        "caption": "Table 8: Text quality statistics across motion datasets. The third column shows the number of text samples per dataset. The fourth column displays the average sentence length (in word count). The fifth column presents the Type-Token Ratio (TTR) range, indicating lexical diversity (higher values represent richer vocabulary). The sixth column lists the ten most frequent verbs in each dataset, reflecting the predominant actions described in the motion text. The datasets are grouped by their primary task: Text-to-Motion (T2M), Music-to-Dance (M2D), Speech-to-Gesture (S2G), Human-Human Interaction (HHI), Human-Object Interaction (HOI), and Human-Scene Interaction (HSI).",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding:-0.5pt 1.0pt;\">HSI</td>\n</tr>\n</table> \n",
        "informative_terms_identified": [
            "motion",
            "humanhuman",
            "column",
            "word",
            "hoi",
            "their",
            "length",
            "lexical",
            "each",
            "actions",
            "lists",
            "fourth",
            "t2m",
            "musictodance",
            "s2g",
            "most",
            "average",
            "frequent",
            "statistics",
            "primary",
            "third",
            "diversity",
            "displays",
            "described",
            "range",
            "sentence",
            "text",
            "ratio",
            "predominant",
            "humanobject",
            "across",
            "count",
            "presents",
            "ttr",
            "reflecting",
            "verbs",
            "samples",
            "grouped",
            "interaction",
            "sixth",
            "humanscene",
            "higher",
            "indicating",
            "typetoken",
            "vocabulary",
            "hsi",
            "number",
            "datasets",
            "values",
            "texttomotion",
            "speechtogesture",
            "ten",
            "task",
            "m2d",
            "represent",
            "hhi",
            "shows",
            "fifth",
            "dataset",
            "richer",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx7.T8\" title=\"Table 8 &#8227; Dataset Text Quality &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents a comprehensive analysis of the text quality across our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;collection, which comprises over <math alttext=\"321,000\" class=\"ltx_Math\" display=\"inline\" id=\"Sx7.p1.m1\" intent=\":literal\"><semantics><mrow><mn>321</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">321,000</annotation></semantics></math> textual descriptions spanning various motion-related tasks. This extensive dataset was created by collecting and curating existing motion datasets, rendering motions to videos, and then leveraging vision-language models (VLMs) to generate rich textual descriptions.\nThe linguistic diversity of our dataset is evidenced by the Type-Token Ratio (TTR) ranges, with maximum values reaching 1.0 in multiple subsets (AIOZ, Motorica, IDEA400, etc.), indicating exceptional lexical richness. The average sentence length of 276.78 words across the entire collection demonstrates the descriptive depth of our motion annotations, providing detailed accounts of nuanced movements rather than simplistic action labels.\nParticularly noteworthy is the variety of verbs captured across different motion categories. The dataset exhibits task-specific verb distributions that align with the semantic nature of each motion type: T2M datasets frequently feature locomotion verbs (&#8220;walk,&#8221; &#8220;step&#8221;), M2D collections emphasize rhythmic movements (&#8220;swing,&#8221; &#8220;lift&#8221;), HOI datasets contain manipulation verbs (&#8220;lift,&#8221; &#8220;hold,&#8221; &#8220;grasp&#8221;), and HHI datasets capture interpersonal dynamics (&#8220;stand,&#8221; &#8220;extend,&#8221; &#8220;lean&#8221;). This semantic richness enables models trained on <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;to understand and generate precise motion descriptions across diverse contexts.\nThe comprehensive coverage across six distinct motion-related tasks (T2M, M2D, S2G, HHI, HOI, HSI) makes <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;uniquely positioned to support generalizable motion understanding. By integrating multiple motion paradigms under a unified annotation framework, our dataset transcends the limitations of task-specific collections, allowing for transfer learning across motion domains. This textual quality and diversity significantly enhance the capability of motion generation models to comprehend natural language instructions and produce corresponding human-like movements.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This paper introduces <span class=\"ltx_text ltx_font_bold\">OmniMotion-X</span>, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks&#8212;including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis)&#8212;as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct <span class=\"ltx_text ltx_font_bold\">OmniMoCap-X</span>, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "across",
                    "musictodance",
                    "texttomotion",
                    "speechtogesture",
                    "actions",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_caption\">We present <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified sequence-to-sequence autoregressive motion diffusion transformer designed for flexible and interactive whole-body human motion generation. It supports a variety of tasks, including text-to-motion, music-to-dance, speech-to-gesture, and globally spatial-temporal controllable motion generation, which encompasses motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis. These conditions can be combined in various ways to enable versatile motion generation.</span>\n</p>\n\n",
                "matched_terms": [
                    "motion",
                    "musictodance",
                    "texttomotion",
                    "speechtogesture"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal whole-body human motion generation plays critical roles in animation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib13\" title=\"\">2024b</a>)</cite>, gaming&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib43\" title=\"\">2024</a>)</cite>, virtual reality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib24\" title=\"\">2024b</a>)</cite>, and embodied intelligence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib55\" title=\"\">2024</a>)</cite> across diverse input conditions, including text, audio, and trajectory, etc.\nThe limited multimodal data and task-specific model designs prevent existing motion generation methods from supporting multimodal whole-body human motion generation. Due to the high cost of mocap data collection and labeling, most datasets focus on single domains such as Text-to-Motion (T2M)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Plappert, Mandery, and Asfour <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib62\" title=\"\">2016</a>)</cite>, Music-to-Dance (M2D)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, Speech-to-Gesture (S2G)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>)</cite>, Human-Object Interaction (HOI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>)</cite>, Human-Scene Interaction (HSI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and Human-Human Interaction (HHI)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>)</cite>, each with inconsistent data formats (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, BVH, SMPL-(H/X) and 3D Keypoints) and control conditions (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, Text captions, Audio, and Trajectories).\nTo tackle motion generation across diverse scenarios, it is crucial to build a unified framework that utilizes large-scale, diverse data to achieve more generalized representations.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "humanhuman",
                    "hoi",
                    "each",
                    "t2m",
                    "musictodance",
                    "s2g",
                    "most",
                    "text",
                    "humanobject",
                    "across",
                    "interaction",
                    "humanscene",
                    "hsi",
                    "datasets",
                    "texttomotion",
                    "speechtogesture",
                    "m2d",
                    "hhi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although recent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>; Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>)</cite> attempt to unify multitasks in one model, they meet challenges in multimodal control modeling, versatile tasks, and high-quality motion generation (as shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx1.T1\" title=\"Table 1 &#8227; Introduction &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>):\n(1) <span class=\"ltx_text ltx_font_bold\">Independent Model Training.</span> Previous approaches train separate models for each modality, limiting simultaneous control across inputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>)</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">Additional Control Branches.</span> Some methods add separate control branches for each condition, limiting interaction between them&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>)</cite>.\n(3) <span class=\"ltx_text ltx_font_bold\">Conflict Granularity Training.</span> Existing methods use mixed training by combining high-level semantic conditions with low-level controls, which hampers effective control at different levels and leads to optimization challenges&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Luo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib54\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>. Similar phenomena have also been observed in video generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib44\" title=\"\">2025</a>; Ao <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib4\" title=\"\">2024</a>)</cite>.\nIn addition to modeling challenges, relevant works also introduce large-scale motion datasets from multiple tasks and modalities&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, they still exhibit the following significant shortcomings (see comparisons in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx1.T2\" title=\"Table 2 &#8227; Introduction &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>):\n(1) <span class=\"ltx_text ltx_font_bold\">Low Motion Quality.</span> Datasets expanded with non-mocap motion estimation exhibit &#8220;garbage in, garbage out&#8221; effects, leading to poor-quality motion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>)</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">Text Inconsistency.</span> Inconsistent text annotations or expanded LLM descriptions lead to uneven text quality and hallucination issues&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>)</cite>.\nand (3) <span class=\"ltx_text ltx_font_bold\">limited tasks.</span> They focus on common tasks, limiting applicability in diverse ones like HOI, HSI, and HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite>.\nThese limitations hinder the development of a unified, high-quality dataset for multimodal whole-body human motion generation across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "across",
                    "hoi",
                    "interaction",
                    "hsi",
                    "hhi",
                    "each",
                    "dataset",
                    "text",
                    "datasets",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address existing challenges, we propose <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>, a unified framework for multimodal whole-body human motion generation, and <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset.\nSpecifically, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;employs Diffusion Transformers (DiTs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peebles and Xie <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib59\" title=\"\">2023</a>)</cite>, incorporates multimodal conditions by concatenating condition tokens as prefix context, and adopts a progressive weak-to-strong mixed-condition training strategy to gradually constrain motion from high-level semantics to dense spatial-temporal alignment.\nNotably, unlike previous methods, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;introduces a novel generation paradigm that utilizes reference motion (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, user-provided or model-predicted motion) as a special condition. This significantly enhances generated motion quality and achieves consistency between reference and generated motion, creating an effective clip-by-clip autoregressive motion diffusion.\nThis enables <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;to support autoregressive interactive generation with strong temporal alignment.\nFurthermore, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;unifies various Global Spatial-Temporal Controllable Generation tasks through spatial-temporal masking strategies.\nTo guarantee high-quality motion generation training, we collect high-quality mocap datasets that support diverse motion generation tasks, unify them under the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format with standard world coordinate systems, and automatically generate hierarchical text captions by rendering motions into videos and annotating them with vision language models (VLMs).\nThis dataset contains about <em class=\"ltx_emph ltx_font_italic\"><math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.p3.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours</em> and integrates multimodal control conditions, supporting versatile tasks, including T2M, M2D, S2G, HOI, HSI, and HHI.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "s2g",
                    "hoi",
                    "m2d",
                    "hsi",
                    "hhi",
                    "dataset",
                    "text",
                    "datasets",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, the largest unified multimodal mocap motion dataset with a unified SMPL-X format, and provides consistent, detailed, and structured text captions to serve versatile motion generation tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extensive experiments show that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves state-of-the-art performance across various tasks, including T2M, S2G, M2D, and GSTC. These evaluations were conducted on our more challenging test sets comprising <math alttext=\"280\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.I1.i4.p1.m1\" intent=\":literal\"><semantics><mn>280</mn><annotation encoding=\"application/x-tex\">280</annotation></semantics></math> samples uniformly sampled from our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "across",
                    "s2g",
                    "samples",
                    "m2d"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Existing methods are typically categorized based on input conditions into three main types: single-modal, cross-modal, and multimodal.\nSingle-modal motion generation uses control conditions from the same modality as the motion, including motion prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib73\" title=\"\">2024a</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib12\" title=\"\">2023a</a>)</cite>, in-betweening&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cohan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib15\" title=\"\">2024</a>)</cite>, and joint/trajectory-guided synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>; Dai et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib16\" title=\"\">2024</a>; Zhao, Li, and Tang <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib84\" title=\"\">2024</a>)</cite>.\nCross-modal motion generation uses control conditions from different modalities, including text in T2M&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>; Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib52\" title=\"\">2024a</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib30\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib83\" title=\"\">2024d</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib80\" title=\"\">2023b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib81\" title=\"\">2024b</a>)</cite>, music in M2D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib35\" title=\"\">2023</a>; Siyao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib66\" title=\"\">2022</a>; Tseng, Castellon, and Liu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib68\" title=\"\">2023</a>)</cite>, speech in S2G&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib10\" title=\"\">2024a</a>)</cite>, target object positions in HOI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib50\" title=\"\">2024</a>)</cite>, scene layouts in HSI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kaufmann et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib33\" title=\"\">2023</a>; Huang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib27\" title=\"\">2022</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and partner movements in HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>)</cite>.\nHowever, these approaches typically rely on task-specific architectures, significantly limiting their cross-task generalization capabilities and practical applications.\nRecently, researchers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Han et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib25\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Luo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib54\" title=\"\">2024</a>)</cite> have begun exploring multimodal motion generation, with approaches falling into three primary categories:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "s2g",
                    "hoi",
                    "their",
                    "m2d",
                    "primary",
                    "hsi",
                    "hhi",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Additional Control Branch.</span> These approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>)</cite> incorporate separate control branches into the backbone architecture, each dedicated to a specific condition, resulting in limited interaction between different conditions.</p>\n\n",
                "matched_terms": [
                    "interaction",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Unified Multimodal Condition Modeling.</span> These approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite> use a single model to learn mappings from diverse modalities to target motions, enhancing multi-modal adaptability. However, challenges arise from different constraints across modalities: text provides semantic guidance, spatial-temporal controls impose physical constraints, while audio inputs enforce rhythmic alignment. These disparate constraints often result in compromised controllability and optimization difficulties.</p>\n\n",
                "matched_terms": [
                    "across",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current human motion generation datasets are predominantly task-specific.\nDifferent tasks rely on separate collections: text-to-motion (T2M) uses datasets with text captions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib23\" title=\"\">2022</a>; Plappert, Mandery, and Asfour <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib62\" title=\"\">2016</a>; Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Punnakkal et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib63\" title=\"\">2021</a>; Liao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib43\" title=\"\">2024</a>)</cite>, music-to-dance (M2D) requires music-annotated datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>; Le et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib35\" title=\"\">2023</a>; Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib11\" title=\"\">2021</a>)</cite>, and speech-to-gesture (S2G) employs datasets with paired speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib49\" title=\"\">2022</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>)</cite>.\nSimilarly, interaction datasets remain disconnected across human-object&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Fan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib18\" title=\"\">2023</a>; Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib31\" title=\"\">2022</a>; Li, Wu, and Liu <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib37\" title=\"\">2023</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib79\" title=\"\">2024a</a>; Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib50\" title=\"\">2024</a>; Zhan et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib77\" title=\"\">2024</a>)</cite>, human-scene&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib32\" title=\"\">2024b</a>)</cite>, and human-human interaction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Xu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib74\" title=\"\">2024b</a>)</cite>. This fragmentation constrains multimodal motion generation capabilities.\nIntegrating these datasets presents challenges due to inconsistent motion formats.\nDatasets vary widely in their representations: some use SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bhatnagar et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib7\" title=\"\">2022</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib42\" title=\"\">2024b</a>; Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, others employ SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>; Yi et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib76\" title=\"\">2023</a>; Feng, Shin, and Yoon <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib19\" title=\"\">2022</a>)</cite>, while many rely on BVH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Harvey et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib26\" title=\"\">2020</a>; Mason, Starke, and Komura <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib56\" title=\"\">2022</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib2\" title=\"\">Adobe </a>; Alexanderson et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib3\" title=\"\">2023</a>; Valle-P&#233;rez et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib69\" title=\"\">2021</a>)</cite> or keypoint representations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ionescu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib28\" title=\"\">2013</a>; Ji et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib29\" title=\"\">2018</a>)</cite>. This diversity makes format standardization extremely difficult.\nRecent works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite> have attempted to integrate multiple datasets, increasing scale and incorporating some multimodal conditions. However, they still exhibit several limitations:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "humanhuman",
                    "musictodance",
                    "across",
                    "datasets",
                    "s2g",
                    "presents",
                    "texttomotion",
                    "speechtogesture",
                    "their",
                    "interaction",
                    "m2d",
                    "humanscene",
                    "diversity",
                    "text",
                    "humanobject"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-quality Motions</span>: Most integrated datasets derive from non-mocap sources with significant estimation errors&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, creating a &#8220;garbage in, garbage out&#8221; effect that compromises motion generation quality.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "most",
                    "datasets",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text Inconsistency</span>: Existing datasets&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite> use inconsistent text annotations (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, action labels versus semantic descriptions) or rely on LLMs for text refinement without visual motion data, causing variable text quality and hallucinations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "text",
                    "datasets",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited Generation Scenarios</span>: Most datasets lack support for critical interaction tasks including HOI, HSI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Liang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib41\" title=\"\">2024a</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, and HHI&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>)</cite>, severely limiting their applicability to complex scenarios.</p>\n\n",
                "matched_terms": [
                    "hoi",
                    "their",
                    "most",
                    "interaction",
                    "hhi",
                    "hsi",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the data challenges&#8212;the scarcity of high-quality multitask motion datasets, inconsistent textual annotations and motion representations&#8212;we implement a three-pronged approach: (1) integrating diverse motion capture datasets with multitask support, (2) establishing a unified motion representation, and (3) developing high-quality motion captions derived from visual-textual annotations.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While existing approaches prioritize dataset scale over quality by incorporating substantial amounts of non-mocap data&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>; Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib46\" title=\"\">2024</a>)</cite>, leading to degraded motion generation quality (&#8221;garbage in, garbage out&#8221;), our work emphasizes data quality.\nWe systematically curate open-source mocap datasets and classify them into five acquisition categories: Marker with manual correction, Marker (Vicon), IMU, Multi-View RGB, and Single-View RGB (detailed in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.T3\" title=\"Table 3 &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nOur dataset supports diverse motion generation tasks, including text-to-motion, motion-to-dance, speech-to-gesture, and various interactions (human-object, human-scene, and human-human), providing a high-quality multimodal foundation. Comprising <math alttext=\"64.3\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m1\" intent=\":literal\"><semantics><mn>64.3</mn><annotation encoding=\"application/x-tex\">64.3</annotation></semantics></math> million frames and <math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx3.SSx1.p1.m2\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours of data, our approach achieves comprehensive task coverage while maintaining superior data quality.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "humanhuman",
                    "texttomotion",
                    "speechtogesture",
                    "task",
                    "humanscene",
                    "humanobject",
                    "dataset",
                    "datasets",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We standardize diverse motion formats (BVH&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lauterbach et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib34\" title=\"\">2009</a>)</cite>, FBX&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wikipedia contributors <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib71\" title=\"\">2024</a>)</cite>, and SMPL&#160;(-H)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib51\" title=\"\">2023</a>)</cite>) into the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> format to support our unified multimodal model.\nFirst, we convert these formats into the Motion-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>)</cite> SMPL-X format, comprising root orientation, pose parameters (body, hand, and jaw), facial expressions, facial shape, translation, and body shape parameters.\nWe then normalize the translation scale and initial root orientation across datasets to establish a consistent coordinate system. Additionally, we resample all datasets to 30 frames per second (FPS) to enhance the model&#8217;s ability to capture temporal patterns.\nThe specific conversion process and normalization for different datasets are provided in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "across",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Refinement</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib45\" title=\"\">2024</a>; Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib53\" title=\"\">2024b</a>)</cite> employs LLMs like GPT-4&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib1\" title=\"\">2023</a>)</cite> to enhance textual descriptions. However, since LLMs cannot directly perceive motion data and rely solely on existing text, they frequently introduce hallucinations and fail to capture precise action details.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we propose an automated approach that integrates both visual and textual information for comprehensive motion annotation.\nOur method renders motion into videos and combines them with existing textual annotations (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, descriptions, action labels, and task categories) as input to the state-of-the-art vision-language models (VLMs), GPT-4o&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib1\" title=\"\">2023</a>)</cite>.\nThis multimodal approach generates structured, hierarchical, and precise motion captions that ensure both annotation quality and expression richness. Detailed statistics of the texts quality are in the supplementary material.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "task",
                    "statistics",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.F1\" title=\"Figure 1 &#8227; Unified Motion Representation &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nWe propose a unified multimodal, autoregressive transformer-based diffusion model for whole-body human motion generation across multiple tasks.\nThe model takes textual description <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> for semantic guidance, global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> for spatial-temporal control, and speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> and music conditions <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> to ensure rhythmic and stylistic coherence.\nFurthermore, it incorporates reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> as a motion prior, derived from either previously generated clips or user-designed motion, thus providing fine-grained details unavailable in other conditions.\nIn addition, the mixed-condition setting allows multiple conditions to occur simultaneously during training, enabling the model to handle complex scenarios with diverse inputs.\nIn this section, we present our approach in two parts: a unified framework for multimodal modeling and a progressive training strategy that ensures precise motion control.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Conditions.</span> Our framework integrates multiple condition modalities: text <math alttext=\"\\mathbf{c}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{t}</annotation></semantics></math> providing semantic guidance; global motion <math alttext=\"\\mathbf{c}_{g}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>g</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{g}</annotation></semantics></math> ensuring spatial-temporal consistency; speech <math alttext=\"\\mathbf{c}_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{s}</annotation></semantics></math> synchronizing gestures and lip movements with rhythm; music <math alttext=\"\\mathbf{c}_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{m}</annotation></semantics></math> supplying beat and style information for dance; and reference motion <math alttext=\"\\mathbf{c}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p2.m5\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{r}</annotation></semantics></math> serving as a motion prior. Notably, this reference motion condition&#8212;overlooked in previous multimodal motion generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib82\" title=\"\">2024c</a>; Ling et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib47\" title=\"\">2023</a>; Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>&#8212;enables our model to maintain precise spatial-temporal pattern consistency with reference, substantially enhancing motion quality and coherence.\nTo fully leverage each modality and facilitate interaction between different conditions, we employ modality-specific encoders (<em class=\"ltx_emph ltx_font_italic\">e.g.</em>, T5-XXL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib65\" title=\"\">2020</a>)</cite> for text, a wav encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite> for speech, Librosa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(McFee et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib57\" title=\"\">2015</a>)</cite> for music and body-wise encoding&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> for motion) to extract features from each modality. These features are aligned to match motion embedding dimensions using learnable linear projections, allowing us to concatenate all condition tokens as prefix context with noisy motion tokens during processing.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "interaction",
                    "each",
                    "text",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"q(x_{0}|c)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m5\" intent=\":literal\"><semantics><mrow><mi>q</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mn>0</mn></msub><mo fence=\"false\">|</mo><mi>c</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">q(x_{0}|c)</annotation></semantics></math> represents the data distribution, <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m6\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the maximum diffusion step. <math alttext=\"G\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m7\" intent=\":literal\"><semantics><mi>G</mi><annotation encoding=\"application/x-tex\">G</annotation></semantics></math> denotes the learned denoising function, while <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m8\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math> represents the noisy motion at step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m9\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, expressed as <math alttext=\"x_{t}=[p_{0}^{t},p_{1}^{t},\\dots,\\mathbf{p}_{N}^{t}]\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m10\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>p</mi><mn>0</mn><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>p</mi><mn>1</mn><mi>t</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>&#119849;</mi><mi>N</mi><mi>t</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{t}=[p_{0}^{t},p_{1}^{t},\\dots,\\mathbf{p}_{N}^{t}]</annotation></semantics></math>, where each <math alttext=\"\\mathbf{p}_{i}^{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m11\" intent=\":literal\"><semantics><msubsup><mi>&#119849;</mi><mi>i</mi><mi>t</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{p}_{i}^{t}</annotation></semantics></math> corresponds to the <math alttext=\"i_{th}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m12\" intent=\":literal\"><semantics><msub><mi>i</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi></mrow></msub><annotation encoding=\"application/x-tex\">i_{th}</annotation></semantics></math> pose in motion at step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"Sx4.SSx1.p3.m13\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Considering our diverse multi-granularity conditioning inputs, we empirically observed that employing all conditions within a single-stage training paradigm leads to difficulties in directly learning the correlation between motion and conditions. Moreover, the model tends to overfit strongly constrained low-level control signals such as fine-grained reference motion and spatiotemporal joint control. While these signals appear dominant, they can suppress other modalities such as text, ultimately compromising overall controllability.\nTo address this, we implement weak-to-strong progressive training: as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F2\" title=\"Figure 2 &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> initial text-conditioned learning establishes motion-semantic alignment, followed by progressive integration of finer-grained conditions, including reference motion, global motion, and audio signals.\nThis progressive approach enables the model to effectively adapt to different conditions, ensuring high-quality and flexible motion generation while precisely adhering to multimodal conditions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method employs a Transformer Encoder architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib70\" title=\"\">2017</a>)</cite> with 8 layers and 8 attention heads, featuring a hidden dimension of <math alttext=\"d_{\\text{model}}=1536~(128\\times 12)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>=</mo><mrow><mn>1536</mn><mo lspace=\"0.330em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>128</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>12</mn></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">d_{\\text{model}}=1536~(128\\times 12)</annotation></semantics></math>, where <math alttext=\"128\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m2\" intent=\":literal\"><semantics><mn>128</mn><annotation encoding=\"application/x-tex\">128</annotation></semantics></math> represents the embedding size per each of the <math alttext=\"12\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m3\" intent=\":literal\"><semantics><mn>12</mn><annotation encoding=\"application/x-tex\">12</annotation></semantics></math> body parts, and a feedforward dimension of <math alttext=\"3072\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m4\" intent=\":literal\"><semantics><mn>3072</mn><annotation encoding=\"application/x-tex\">3072</annotation></semantics></math>. Training is performed on a single H800 GPU through progressive conditioning: starting with text-only training for <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m5\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, then adding reference motion for another <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m6\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps, followed by global spatiotemporal control for <math alttext=\"230K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>230</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">230K</annotation></semantics></math> steps, and finally incorporating full audio conditions for <math alttext=\"920K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>920</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">920K</annotation></semantics></math> steps. The corresponding batch sizes are <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m9\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m10\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, <math alttext=\"48\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m11\" intent=\":literal\"><semantics><mn>48</mn><annotation encoding=\"application/x-tex\">48</annotation></semantics></math>, and <math alttext=\"16\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m12\" intent=\":literal\"><semantics><mn>16</mn><annotation encoding=\"application/x-tex\">16</annotation></semantics></math>, respectively. We optimize with AdamW using an initial learning rate of <math alttext=\"1\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m13\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-4}</annotation></semantics></math> (reset for new conditions) and a cosine schedule decaying to <math alttext=\"1\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m14\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\times 10^{-5}</annotation></semantics></math> within the first <math alttext=\"460K\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m15\" intent=\":literal\"><semantics><mrow><mn>460</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">460K</annotation></semantics></math> steps. The default length of motion reference and prediction is <math alttext=\"150\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx1.p1.m16\" intent=\":literal\"><semantics><mn>150</mn><annotation encoding=\"application/x-tex\">150</annotation></semantics></math>. More implementation details are provided in supplementary material.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "each",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;on four tasks: Text-to-Motion (T2M), Global Spatiotemporal Controllable Generation (GSTC), Music-to-Dance (M2D), and Speech-to-Gesture (S2G).\nFor T2M and GSTC evaluations, test sets are uniformly sampled across all datasets&#160;(<math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"Sx5.SSx2.p1.m1\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> samples from each).\nThe M2D benchmark integrates test sequences from AIST++&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib38\" title=\"\">2021</a>)</cite>, FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib39\" title=\"\">2023</a>)</cite>, and PhantomDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib36\" title=\"\">2022</a>)</cite>, with S2G evaluation conducted on BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib48\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "t2m",
                    "across",
                    "musictodance",
                    "texttomotion",
                    "speechtogesture",
                    "s2g",
                    "samples",
                    "m2d",
                    "each",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">T2M.</span>\nTab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx3.T4\" title=\"Table 4 &#8227; Consistent Visual-textual Motion Captions &#8227; OmniMoCap-X Dataset &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that previous methods, constrained by small-scale datasets, struggle to generalize in generating diverse and complex motions.\nIn contrast, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;demonstrates a significant advantage, outperforming not only baseline methods trained on small datasets (MDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib67\" title=\"\">2022</a>)</cite>, MLD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib14\" title=\"\">2023b</a>)</cite>, MoMask&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib22\" title=\"\">2024a</a>)</cite>, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite>) but also MoMask* and MotionCraft*, which are trained on our <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>.\nCompared to MoMask* and MotionCraft*, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;uses stronger text encoder (T5-XXL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Raffel et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib65\" title=\"\">2020</a>)</cite>), which models complex texts more effectively than CLIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib64\" title=\"\">2021</a>)</cite>.\nIn addition, it adopts a unified multimodal framework with reference motion during training, enabling the model to learn more detailed whole-body motion representations, leading to better generation quality and generalization.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "shows",
                    "text",
                    "datasets",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GSTC.</span>\nWe adopt the cross-joint setup from OmniControl&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>)</cite>, simulating spatially dense control by controlling all joints.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T5\" title=\"Table 5 &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, due to the small dataset size, OmniControl&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib72\" title=\"\">2024</a>)</cite> struggles to generalize in GSTC task.\nIn contrast, our method demonstrates clear advantages, effectively following spatially dense control signals.</p>\n\n",
                "matched_terms": [
                    "task",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">M2D and S2G.</span>\nWe directly compare our method with the MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib8\" title=\"\">2024</a>)</cite> on the S2G and M2D tasks.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T6\" title=\"Table 6 &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;achieves competitive performance. The lower FID is mainly due to the relatively small test sets in S2G and M2D, while <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is trained on the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>, leading to some distribution differences. Additionally, our method demonstrates superior diversity, which may also impact the FID.</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "m2d",
                    "s2g"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx5.T7\" title=\"Table 7 &#8227; Ablation Study &#8227; Experiments &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> presents the ablation results and demonstrates two key findings: (1) Ablating coarse-to-fine training compromises text-conditioned alignment, indicating finer-grained controls may override coarse semantic constraints; (2) Mixed-condition training impairs spatiotemporal control due to physical constraints introducing optimization conflicts, necessitating weak-to-strong conditioning.</p>\n\n",
                "matched_terms": [
                    "indicating",
                    "presents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx4.F3\" title=\"Figure 3 &#8227; Unified Multimodal Modeling &#8227; Method &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;handles various multimodal generation, including T2M, M2D, S2G, and GSTC (covering motion prediction, in-betweening, and joint guidance). When combined with reference motion, the model generates conditioned motions that align with the reference. More visualizations are in the supplementary video.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "t2m",
                    "m2d",
                    "s2g"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces the <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;and the <em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>.\n<em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;is an autoregressive diffusion model that integrates reference motion and multimodal conditions for precise whole-body motion control. It employs a progressive weak-to-strong conditioning strategy to effectively handle multi-granular constraints.\n<em class=\"ltx_emph ltx_font_italic\">OmniMoCap-X</em>&#160;is the largest multimodal MoCap dataset, comprising <math alttext=\"286.2\" class=\"ltx_Math\" display=\"inline\" id=\"Sx6.p1.m1\" intent=\":literal\"><semantics><mn>286.2</mn><annotation encoding=\"application/x-tex\">286.2</annotation></semantics></math> hours from <math alttext=\"28\" class=\"ltx_Math\" display=\"inline\" id=\"Sx6.p1.m2\" intent=\":literal\"><semantics><mn>28</mn><annotation encoding=\"application/x-tex\">28</annotation></semantics></math> motion capture datasets, unified in SMPL-X with structured and consistent captions across 10 tasks.\nExperiments demonstrate that <em class=\"ltx_emph ltx_font_italic\">OmniMotion-X</em>&#160;outperforms baselines, laying a strong foundation for large-scale multimodal motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "dataset",
                    "across",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitation and Future Work.</span> While effective, our method currently lacks scene, object, and human interaction constraints, restricting its use in complex real-world applications. Furthermore, the sample-space denoising process results in slower inference speeds. Future work should focus on integrating interaction constraints to be more versatile and developing more efficient motion representations for improved physical consistency and inference speed.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "interaction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Format Conversion.</span>\nSince many datasets are not in the SMPL-series format, we convert all datasets to the SMPL-X format for consistency. For BVH-format datasets such as Mixamo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib2\" title=\"\">Adobe </a>)</cite>, 100Style&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Mason, Starke, and Komura <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib56\" title=\"\">2022</a>)</cite>, Motorica&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alexanderson et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib3\" title=\"\">2023</a>)</cite>, and LaFAN1&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Harvey et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib26\" title=\"\">2020</a>)</cite>, we first standardize the reference pose of all BVH files to a T-pose to ensure consistent initialization. We then align the root node&#8217;s coordinate system with that of the SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib58\" title=\"\">2019</a>)</cite> model in Blender&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib9\" title=\"\">Blender Foundation </a>)</cite>, where the negative Y-axis is defined as the forward direction and the Z-axis as the vertical upward direction.\nTo adapt to the SMPL-X topology, we generate a corresponding BVH skeleton based on a predefined SMPL-X T-pose template. We perform skeleton retargeting in MotionBuilder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib6\" title=\"\">Autodesk Inc. </a>)</cite> to map the original animation data onto the SMPL-X hierarchy.\nFinally, we convert the retargeted BVH files from Euler angles to axis-angle representations and apply Gaussian filtering to smooth both joint rotations and translations over time, yielding stable SMPL-X parameters.\nFor the Choreomaster dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib11\" title=\"\">2021</a>)</cite>, originally in FBX format, we first convert it to BVH using Blender, and then process it using the same pipeline.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Normalization.</span>\nTo reduce the difficulty of model training, we perform temporal and spatial normalization on SMPL-X motion sequences. Specifically, we standardize all motion sequences in four steps. First, we align the initial frame of each sequence to face the positive Z-axis. Then, we reposition the starting frame to a common location with the feet just touching the ground. Next, we unify the frame rate across datasets to 30 fps. Finally, we segment each motion sequence into 5-second clips (150 frames).</p>\n\n",
                "matched_terms": [
                    "motion",
                    "across",
                    "each",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text and Motion Feature Extractors.</span>\nExisting research typically relies on pre-trained motion and text feature extractors for evaluation. However, due to significant differences in dataset scale, distribution, and motion representation compared to existing works, these pre-trained extractors are not directly applicable. Therefore, following the approach&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lu et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib52\" title=\"\">2024a</a>)</cite>, we re-train motion and text feature extractor using a contrastive learning framework tailored to this dataset. The text feature extractor is based on the Transformer encoder architecture&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Petrovich, Black, and Varol <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib61\" title=\"\">2023</a>)</cite>, which encodes raw text into a semantic vector <math alttext=\"s_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m1\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">s_{t}</annotation></semantics></math>. The motion feature extractor also uses a Transformer encoder to encode motion sequences of up to 150 frames into a semantic vector <math alttext=\"s_{m}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m2\" intent=\":literal\"><semantics><msub><mi>s</mi><mi>m</mi></msub><annotation encoding=\"application/x-tex\">s_{m}</annotation></semantics></math>. Both encoders include additional semantic tokens, with a structure similar to the encoder in ACTOR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Petrovich, Black, and Varol <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#bib.bib60\" title=\"\">2021</a>)</cite>, but without involving probabilistic distribution modeling. In implementation, the text encoder takes as input text features extracted from a pre-trained and frozen DistilBERT network, while the motion encoder directly processes raw motion sequence data. In the contrastive learning framework, we optimize the feature space such that matching text-motion feature pairs <math alttext=\"(s_{t},s_{m})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>s</mi><mi>m</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(s_{t},s_{m})</annotation></semantics></math> are brought closer in the embedding space, while ensuring that non-matching pairs are separated by at least a distance <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p1.m4\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>. This optimization objective is achieved through the following contrastive loss function:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "dataset",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Mask.</span> Our model leverages four specialized masking mechanisms: (1) Component attention mask, the standard source key padding mask in Transformers&#8212;regulates attention across different conditional inputs. In our DiT-based architecture, input conditions may be partially missing; this mask zeroes out attention weights for missing inputs, allowing the model to focus only on valid ones. (2) Global task-dependent mask, a spatiotemporal dynamic mask tailored to tasks like motion prediction, interpolation, completion, and trajectory-guided generation&#8212;distinguishes between observed and unobserved motion regions based on task requirements. (3) Global motion disentanglement mask transforms dense global motion conditions into a hybrid sparse-dense representation by selecting key joints (e.g., <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p2.m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> joints) for global representation optimization, while the remaining <math alttext=\"n-k\" class=\"ltx_Math\" display=\"inline\" id=\"Sx9.p2.m2\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8722;</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">n-k</annotation></semantics></math> joints contribute to local motion representation and noise injection. (4) Motion representation reconstruction mask filters out reconstruction errors of joints without ground-truth annotations during training. For instance, it suppresses hand-joint errors in datasets lacking hand annotations, effectively addressing cross-dataset joint count mismatches.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "count",
                    "across",
                    "task",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the quality and accuracy of our caption generation process, we selected GPT-4o, the current state-of-the-art closed-source multimodal model, as our primary captioning tool. To maximize caption accuracy and comprehensiveness, we implemented several key optimizations to the captioning pipeline. First, we significantly increased the number of input frames provided to the model, allowing for more comprehensive temporal understanding of the video content. Second, we enhanced the video rendering quality to ensure that visual details are clearly preserved and accurately conveyed to the model. Third, we conducted extensive prompt engineering to design optimal instructions that guide the model toward generating more precise and detailed captions that capture both visual elements and temporal dynamics (see Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx9.F4\" title=\"Figure 4 &#8227; More Implementation Details &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> for representative examples). These technical improvements collectively resulted in substantially improved caption quality compared to baseline approaches.</p>\n\n",
                "matched_terms": [
                    "third",
                    "number",
                    "primary",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our dataset supports various interaction types, including Human-Human Interaction (HHI), Human-Object Interaction (HOI), and Human-Scene Interaction (HSI), as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19789v1#Sx9.F5\" title=\"Figure 5 &#8227; More Implementation Details &#8227; OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Specifically, human-human interactions encompass a rich spectrum of social scenarios such as conversations, handshakes, embraces, and collaborative activities. Human-object interactions include everyday activities like grasping, manipulation, and tool usage. Human-scene interactions capture the dynamic relationships between individuals and their environments, such as sitting on indoor sofas with feet resting on carpets, and other contextual behavioral patterns. Each interaction type is accompanied by detailed spatio-temporal annotations and semantic descriptions, providing comprehensive training samples for multimodal whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "humanhuman",
                    "hoi",
                    "their",
                    "samples",
                    "interaction",
                    "humanscene",
                    "hhi",
                    "each",
                    "dataset",
                    "hsi",
                    "humanobject"
                ]
            }
        ]
    }
}