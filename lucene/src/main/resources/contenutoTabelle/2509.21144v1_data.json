{
    "S5.T1": {
        "source_file": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
        "caption": "Table 1: Main comparison results on the CVSS-T dataset. Results are presented as EN-ZH || ZH-EN. Higher scores indicate better performance. ‘-’ denotes unavailable results. Best scores are in bold and second-best scores are underlined.",
        "body": "Category\nModel\n#Size\nSpeech-BLEU\nText-BLEU\nA.PCP\nSLC 0.2\nSLC 0.4\nUTMOS\n\n\nGT\nCVSS-T\n-\n-\n-\n2.37\n0.32\n0.72\n2.27 || 3.56\n\n\nCascaded\n3-Stage\n2.6B\n25.02 || 16.62\n25.80 || 17.08\n2.80 || 2.85\n\n0.56 || 0.54\n0.82 || 0.88\n3.76 || 3.50\n\n\n2-Stage\n2.8B\n26.94 || 20.86\n27.38 || 22.20\n\n2.87 || 2.64\n0.67 || 0.52\n0.93 || 0.70\n\n3.79 || 3.48\n\n\nMLLM\nGPT-4o\n-\n\n31.64 || 19.27\n- || -\n2.66 || 2.58\n0.47 || 0.37\n0.71 || 0.61\n3.46 || 4.18\n\n\n\nQwen2.5-O\n7B\n7.10 || 22.66\n\n34.85 || 24.39\n1.90 || 1.92\n0.31 || 0.35\n0.57 || 0.61\n3.23 || 4.30\n\n\n\nS2ST\nSeamless-M\n1.2B\n14.53 || 14.36\n24.80 || 18.44\n2.34 || 2.29\n0.54 || 0.22\n0.82 || 0.45\n2.73 || 3.59\n\n\nSeamless-L\n2.3B\n25.05 || 17.67\n27.61 || 21.95\n2.41 || 2.15\n0.67 || 0.36\n0.95 || 0.62\n2.69 || 4.04\n\n\nSeamless-Ex\n1.7B\n24.45 || 15.84\n26.59 || 16.74\n\n2.83 || 2.87\n\n0.68 || 0.52\n0.94 || 0.77\n2.46 || 2.90\n\n\nUniSS (P)\n1.5B\n30.28 || 23.61\n30.93 || 24.45\n2.73 || 2.75\n0.98 || 0.84\n0.99 || 0.97\n\n3.77 || 3.86\n\n\n\nUniSS (Q)\n1.5B\n32.20 || 24.28\n\n32.95 || 26.28\n\n2.71 || 2.74\n0.98 || 0.87\n0.99 || 0.97\n3.76 || 3.86",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">Category</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">#Size</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">Speech-BLEU</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">Text-BLEU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">A.PCP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">SLC 0.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">SLC 0.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">UTMOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">GT</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">CVSS-T</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.27 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m3\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 3.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Cascaded</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">3-Stage</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.6B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">25.02 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m4\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 16.62</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">25.80 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m5\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 17.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.80 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m6\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_framed ltx_framed_underline\">2.85</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.56 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m7\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.82 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m8\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">3.76 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m9\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 3.50</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2-Stage</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.8B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">26.94 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m10\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 20.86</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">27.38 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m11\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 22.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">\n<span class=\"ltx_text ltx_font_bold\">2.87</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m12\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.67 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m13\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.52</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.93 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m14\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.70</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">\n<span class=\"ltx_text ltx_font_bold\">3.79</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m15\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 3.48</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">MLLM</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">GPT-4o</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">31.64</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m16\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 19.27</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">- <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m17\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> -</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.66 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m18\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.47 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m19\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.71 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m20\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">3.46 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m21\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_framed ltx_framed_underline\">4.18</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Qwen2.5-O</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">7B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">7.10 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m22\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 22.66</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">\n<span class=\"ltx_text ltx_font_bold\">34.85</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m23\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 24.39</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1.90 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m24\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 1.92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.31 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m25\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.35</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.57 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m26\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">3.23 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m27\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">4.30</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"6\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">S2ST</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Seamless-M</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1.2B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">14.53 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m28\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 14.36</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">24.80 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m29\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 18.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.34 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m30\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.54 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m31\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.82 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m32\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.73 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m33\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 3.59</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Seamless-L</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.3B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">25.05 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m34\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 17.67</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">27.61 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m35\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 21.95</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.41 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m36\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.15</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.67 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m37\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.36</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.95 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m38\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.62</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.69 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m39\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 4.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Seamless-Ex</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">1.7B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">24.45 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m40\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 15.84</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">26.59 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m41\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 16.74</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">2.83</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m42\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">2.87</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.68 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m43\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.52</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">0.94 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m44\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.77</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">2.46 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m45\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C2D1E5;\">UniSS (P)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C2D1E5;\">1.5B</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C2D1E5;\">30.28 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m46\" intent=\":literal\"><semantics><mo fence=\"false\" mathbackground=\"#C2D1E5\" stretchy=\"false\" style=\"--ltx-bg-color:#C2D1E5;\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_framed ltx_framed_underline\">23.61</span></span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C2D1E5;\">30.93 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m47\" intent=\":literal\"><semantics><mo fence=\"false\" mathbackground=\"#C2D1E5\" stretchy=\"false\" style=\"--ltx-bg-color:#C2D1E5;\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_framed ltx_framed_underline\">24.45</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C2D1E5;\">2.73 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m48\" intent=\":literal\"><semantics><mo fence=\"false\" mathbackground=\"#C2D1E5\" stretchy=\"false\" style=\"--ltx-bg-color:#C2D1E5;\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#C2D1E5;\">0.98<span class=\"ltx_text ltx_font_medium\"> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m49\" intent=\":literal\"><semantics><mo fence=\"false\" mathbackground=\"#C2D1E5\" stretchy=\"false\" style=\"--ltx-bg-color:#C2D1E5;\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_framed ltx_framed_underline\">0.84</span></span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#C2D1E5;\">0.99<span class=\"ltx_text ltx_font_medium\"> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m50\" intent=\":literal\"><semantics><mo fence=\"false\" mathbackground=\"#C2D1E5\" stretchy=\"false\" style=\"--ltx-bg-color:#C2D1E5;\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> </span>0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#C2D1E5;\">3.77</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#C2D1E5;\"> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m51\" intent=\":literal\"><semantics><mo fence=\"false\" mathbackground=\"#C2D1E5\" stretchy=\"false\" style=\"--ltx-bg-color:#C2D1E5;\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 3.86</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C2D1E5;\">UniSS (Q)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C2D1E5;\">1.5B</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#C2D1E5;\">32.20<span class=\"ltx_text ltx_font_medium\"> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m52\" intent=\":literal\"><semantics><mo fence=\"false\" mathbackground=\"#C2D1E5\" stretchy=\"false\" style=\"--ltx-bg-color:#C2D1E5;\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> </span>24.28</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#C2D1E5;\">32.95</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#C2D1E5;\"> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m53\" intent=\":literal\"><semantics><mo fence=\"false\" mathbackground=\"#C2D1E5\" stretchy=\"false\" style=\"--ltx-bg-color:#C2D1E5;\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">26.28</span></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C2D1E5;\">2.71 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m54\" intent=\":literal\"><semantics><mo fence=\"false\" mathbackground=\"#C2D1E5\" stretchy=\"false\" style=\"--ltx-bg-color:#C2D1E5;\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#C2D1E5;\">0.98<span class=\"ltx_text ltx_font_medium\"> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m55\" intent=\":literal\"><semantics><mo fence=\"false\" mathbackground=\"#C2D1E5\" stretchy=\"false\" style=\"--ltx-bg-color:#C2D1E5;\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> </span>0.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#C2D1E5;\">0.99<span class=\"ltx_text ltx_font_medium\"> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m56\" intent=\":literal\"><semantics><mo fence=\"false\" mathbackground=\"#C2D1E5\" stretchy=\"false\" style=\"--ltx-bg-color:#C2D1E5;\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> </span>0.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#C2D1E5;padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C2D1E5;\">3.76 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m57\" intent=\":literal\"><semantics><mo fence=\"false\" mathbackground=\"#C2D1E5\" stretchy=\"false\" style=\"--ltx-bg-color:#C2D1E5;\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 3.86</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "speechbleu",
            "17b",
            "textbleu",
            "main",
            "slc",
            "presented",
            "secondbest",
            "underlined",
            "3stage",
            "utmos",
            "qwen25o",
            "unavailable",
            "apcp",
            "denotes",
            "best",
            "seamlessex",
            "results",
            "cascaded",
            "gpt4o",
            "seamlessm",
            "bold",
            "scores",
            "zhen",
            "category",
            "performance",
            "26b",
            "enzh",
            "comparison",
            "15b",
            "12b",
            "mllm",
            "cvsst",
            "dataset",
            "higher",
            "indicate",
            "2stage",
            "size",
            "23b",
            "uniss",
            "better",
            "28b",
            "s2st",
            "seamlessl",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T1\" title=\"Table 1 &#8227; 5.3 Objective Performance &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main comparison results on the CVSS-T dataset.\nThe results clearly demonstrate that UniSS establishes a new SOTA in translation fidelity\nwhile maintaining voice characteristics, prosody, duration consistency, and speech quality, validating our core design principles. Additional results on other datasets and tasks are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4\" title=\"Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cmots.github.io/uniss-demo\" title=\"\">https://cmots.github.io/uniss-demo</a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "s2st",
                    "uniss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A conventional cascaded S2ST system typically consists of three sequential components: automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech (TTS) synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Casacuberta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib4\" title=\"\">2004</a>; Nakamura et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib5\" title=\"\">2006</a>)</cite>. However, this cascaded architecture often suffers from error accumulation across stages and struggles to retain paralinguistic features of the original speech. To address these limitations, subsequent research has shifted towards end-to-end approaches that aim to translate speech to another language while preserving expressive characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib7\" title=\"\">2024</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib8\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "cascaded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most recently, the application of large language models (LLMs) to generative speech tasks has further accelerated the development of S2ST systems. Current approaches typically fall into two categories: (1) single-stage methods, which directly predict multi-stream acoustic tokens autoregressively via multi-head outputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite>; and (2) two-stage pipelines, which first generate semantic tokens autoregressively, followed by another autoregressive (AR) model to predict acoustic tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>; Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Gong and Veluri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>)</cite> use a single AR language model to jointly model semantic and partial acoustic tokens, along with a non-autoregressive (NAR) model for complete acoustic information.\nWhile these approaches have shown promising results, they also introduce significantly more architectural complexity than textual LLMs. Additionally, they treat the LLM as a sequence-to-sequence converter, failing to leverage the pre-trained knowledge for textual translation embedded within the LLM.</p>\n\n",
                "matched_terms": [
                    "results",
                    "s2st",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In pursuit of high-fidelity, expressive S2ST, and to overcome the limitations outlined above, our vision is defined by three core principles: (1) a single-stage architecture to eliminate complexity; (2) a unified model that aligns speech and text modalities; and (3) a mechanism to explicitly leverage the proven text translation capabilities of LLMs. To our knowledge, no existing approach satisfies all three principles simultaneously.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of architectural complexity, we present <span class=\"ltx_text ltx_font_bold\">UniSS</span>, a unified single-stage S2ST framework that preserves timbre, emotion, and duration consistency.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> demonstrates our approach and performance results. UniSS builds upon pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> as the backbone, modeling content and speaker information in a single AR language model without architectural modifications. We transfer the LLM&#8217;s pre-trained text translation capabilities to the speech domain through a cross-modality chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib15\" title=\"\">2022</a>)</cite>. During inference, our Quality Mode guides the model to decompose the complex S2ST task into sequential listen, translate, and speak steps within a single inference pass. We also introduce a Performance Mode that trades translation fidelity for faster inference, enabling deployment across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "results",
                    "s2st",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realizing the vision outlined above demands large-scale, high-quality training data that preserves both translation accuracy and speaker expressiveness. However, existing S2ST datasets are either small-scale to train powerful unified models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite> or suffer from quality control issues when scraped from the web&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite>. To address this data challenge, we design a scalable synthesis pipeline and contribute UniST, a 44.8k-hour Chinese-English S2ST dataset offering high translation fidelity and rich speaker preservation.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Unified Single-Stage Architecture for S2ST:</span> We propose UniSS, a single-stage AR model for expressive S2ST that directly leverages a pre-trained textual LLM, eliminating the architectural complexity of prior work.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Large-Scale, High-Quality S2ST Dataset:</span> We design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English S2ST dataset with speaker voice and emotion preservation.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation must preserve semantic accuracy during cross-lingual conversion. Traditional cascaded systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Casacuberta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib4\" title=\"\">2004</a>; Nakamura et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib5\" title=\"\">2006</a>)</cite> chain ASR, MT, and TTS components sequentially, suffering from error accumulation and information loss through text bottlenecks. Early direct methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib1\" title=\"\">2019</a>)</cite> encountered significant challenges with translation quality and synthesis artifacts.\nThe breakthrough came with discrete unit-based methods, where speech-to-unit translation (S2UT) approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib2\" title=\"\">2022</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib17\" title=\"\">2021</a>; Inaguma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib3\" title=\"\">2023</a>)</cite> employ discrete speech units (subwords, phonemes, or semantic tokens) as intermediate representations, which enables effective disentanglement of linguistic content from acoustic properties. SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite> achieved robust multilingual performance via unified multitask optimization of translation and synthesis.\nRecent systems focus on enhancing expressiveness while maintaining translation fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib18\" title=\"\">2022b</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib19\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib7\" title=\"\">2024</a>)</cite>. SeamlessExpressive&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite> designs a PRETSSEL vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib20\" title=\"\">2024</a>)</cite> to enhance expressiveness preservation. TransVIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib8\" title=\"\">2024</a>)</cite> employs feature disentanglement to separately model semantic, acoustic, and temporal information for voice and isochrony control.</p>\n\n",
                "matched_terms": [
                    "cascaded",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of LLMs in text processing has inspired their adaptation to S2ST through speech tokenization. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib21\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib22\" title=\"\">2025</a>)</cite> focus primarily on semantic translation without preserving voice characteristics.\nTo address voice preservation, some works model acoustic features using multiple AR models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>; Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>)</cite>. Recent unified approaches attempt to model both semantic and acoustic features within a single AR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>; Gong and Veluri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite>. However, they still require an additional NAR model for complete acoustic generation. Hibiki&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite> proposes a multi-stream architecture processing source and target speech simultaneously, but necessitates substantial architectural modifications, including nested Transformers and training from scratch.\nA fundamental limitation across these approaches is their reliance on complex architectures to complete acoustic representations. Moreover, they fail to explicitly leverage the LLM&#8217;s pre-trained text translation capabilities, treating them merely as generic sequence converters. UniSS addresses both limitations through a single-stage architecture while effectively transferring LLM&#8217;s text translation capabilities to speech.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 UniSS Architecture &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, UniSS is a unified AR language model for expressive speech-to-speech translation, built upon a single-stage architecture with a cross-modal chain-of-thought prompting framework, and optimized via a progressive training strategy. This section details the three core components of UniSS: model architecture, cross-modal CoT prompting, and training methodology.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of UniSS is to perform expressive S2ST by modeling the conditional distribution <math alttext=\"P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo fence=\"false\">|</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})</annotation></semantics></math>, where <math alttext=\"\\mathbf{X}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{src}</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> denote the source and target speech waveforms, respectively. To achieve faithful content translation while preserving speaker identity and expressiveness, UniSS introduces three types of speech tokens: speaker tokens <math alttext=\"\\mathbf{S}^{spk}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{spk}</annotation></semantics></math> with fixed sequence length to capture global style attributes, e.g., timbre, prosody, and emotion; linguistic tokens <math alttext=\"\\mathbf{S}^{ling}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{ling}</annotation></semantics></math> to encode the content of the source utterance; and semantic tokens <math alttext=\"\\mathbf{S}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}</annotation></semantics></math> to represent the expressive target utterance and can be directly decoded into waveform when combined with speaker tokens.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This unified design allows UniSS to handle expressive S2ST without intermediate acoustic representations or cascaded systems, maintaining fidelity in both content and voice.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "cascaded",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build our generation model upon the pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite>, a strong LLM backbone. By expanding the model vocabulary to include the discrete speech tokens, UniSS treats speech and text uniformly as token sequences, allowing both modalities to be processed within the same transformer architecture.</p>\n\n",
                "matched_terms": [
                    "model",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the S2ST task, the source waveform <math alttext=\"\\mathbf{X_{src}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;&#119851;&#119836;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{src}}</annotation></semantics></math> is represented by <math alttext=\"(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})</annotation></semantics></math>, while the target waveform <math alttext=\"\\mathbf{Y_{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119853;&#119840;&#119853;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{tgt}}</annotation></semantics></math> is represented by <math alttext=\"\\mathbf{S}_{tgt}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}_{tgt}^{sem}</annotation></semantics></math>. This design is motivated by our preliminary experiments, which revealed that although BiCodec&#8217;s semantic tokens are highly effective for waveform reconstruction, their self-supervised nature makes them suboptimal for content understanding. By separately representing speaker identity, linguistic content, and generation-oriented semantics, UniSS enables more accurate and controllable S2ST modeling.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here <math alttext=\"c_{lang}^{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">c_{lang}^{src}</annotation></semantics></math> denotes the source language, and <math alttext=\"\\mathbf{S}^{sem}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}_{src}</annotation></semantics></math> denotes the semantic tokens from source speech. This phase endows the model with robust speech understanding and generation while prioritizing the preservation of text translation capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "denotes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 2: S2ST with CoT.</span>\nIn the second phase, we introduce the core S2ST task. The model is trained to generate outputs using the CoT prompting formats described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS2\" title=\"3.2 Cross-Modal Chain-of-Thought Prompting &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, as well as a simplified direct generation mode that bypasses intermediate text outputs:</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 3: Refinement.</span>\nIn the final phase, we fine-tune the model on the full S2ST task using both CoT prompting modes.\nThis phase uses an annealed learning rate to stabilize the learned CoT patterns and optimize the final translation performance.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training end-to-end expressive S2ST models faces a significant bottleneck due to the scarcity of large-scale, parallel data that preserves speaker characteristics. To address this limitation, we design a scalable data synthesis pipeline. The pipeline generates UniST, our large-scale Chinese-English expressive S2ST dataset. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) illustrates the data construction process.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS is trained in three progressive phases, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS3\" title=\"3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>. Phase 1 establishes text-speech alignment using 77.1k hours of speech data and MT text data from WMT17&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bojar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib28\" title=\"\">2017</a>)</cite>. Phase 2 introduces CoT prompting with our UniST General dataset, mixed with Phase 1 data at a 2:1 ratio. Phase 3 fine-tunes the model exclusively on the UniST High-Quality dataset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "uniss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training employs the AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov and Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib29\" title=\"\">2019</a>)</cite> with a 2.3M-token batch size, weight decay of 0.1, and momentum parameters (0.9, 0.95). All audio is resampled to 16&#160;kHz, and the LLM vocabulary is expanded to 180,407 to include speech and control tokens. Learning rates progress from 8e-4 in Phase 1 to 2e-4 in Phase 2, and finally anneal from 5e-5 to 5e-6 in Phase 3. The model is trained on 16 NVIDIA H800 80G GPUs using the Megatron-LM Framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shoeybi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib30\" title=\"\">2019</a>)</cite> for efficient large-model training. Complete training details and hyperparameters are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.SS1\" title=\"B.1 UniSS &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our primary S2ST evaluation, we use the Chinese (ZH) and English (EN) test sets from CVSS-T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite>, which contain 4,897 utterance pairs (8.2 hours for Chinese and 6.3 hours for English). For emotion preservation evaluation, we randomly sample 300 utterances from each emotional speech dataset (ESD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib31\" title=\"\">2021</a>)</cite> and CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib32\" title=\"\">2014</a>)</cite>), covering a selected set of 4 emotions (happy, sad, angry, neutral). We evaluate both English-to-Chinese (EN-ZH) and Chinese-to-English (ZH-EN) translation directions.</p>\n\n",
                "matched_terms": [
                    "enzh",
                    "s2st",
                    "zhen",
                    "cvsst",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ comprehensive objective and subjective metrics to evaluate S2ST performance. For translation fidelity, we measure BLEU scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib33\" title=\"\">2002</a>)</cite> on both ASR-transcribed speech outputs (Speech-BLEU) and intermediate text (Text-BLEU). Prosody preservation is assessed through prosody similarity (A.PCP)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Andrews et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib34\" title=\"\">2022</a>)</cite>. Duration consistency is evaluated using Speech Length Compliance (SLC) within 0.2 and 0.4 ratio tolerance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nSpeech quality is measured using UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib36\" title=\"\">2022</a>)</cite>. Additionally, we conduct a subjective Mean Opinion Score (MOS) evaluation where bilingual speakers rate emotion similarity, speaker similarity, and naturalness on a 5-point scale. Detailed descriptions of all evaluation metrics and implementation procedures are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A3\" title=\"Appendix C Evaluation Metrics &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "slc",
                    "textbleu",
                    "performance",
                    "s2st",
                    "utmos",
                    "scores",
                    "apcp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Systems.</span>\nTo establish an upper bound for modular approaches, we construct two cascaded baselines using top-performing open-source models: a <span class=\"ltx_text ltx_font_bold\">3-Stage</span> pipeline of Whisper-large-v3 (ASR), NLLB-200-distilled (MT)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Koishekenov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib37\" title=\"\">2023</a>)</cite>, and CosyVoice 2 (TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib38\" title=\"\">2024</a>)</cite>; and a <span class=\"ltx_text ltx_font_bold\">2-Stage</span> pipeline of SeamlessM4T-v2-Large (S2TT) and CosyVoice 2 (TTS).</p>\n\n",
                "matched_terms": [
                    "2stage",
                    "cascaded",
                    "3stage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Large Language Models (MLLMs).</span>\nTo benchmark against the latest generation of general-purpose models, we include two leading MLLMs: <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib39\" title=\"\">2024</a>)</cite>, an enterprise-level model from OpenAI with strong speech-to-speech capability; and <span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib40\" title=\"\">2025</a>)</cite>, a powerful open-source MLLM building on large-scale audio&#8211;language pretraining, enabling both speech understanding and synthesis.</p>\n\n",
                "matched_terms": [
                    "mllm",
                    "gpt4o",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End S2ST Systems.</span>\nWe compare against dedicated S2ST models that represent the current open-source SOTA: <span class=\"ltx_text ltx_font_bold\">SeamlessM4T</span> (Medium and Large V2, denoted Seamless-M and Seamless-L), and its expressive variant <span class=\"ltx_text ltx_font_bold\">SeamlessExpressive</span> (Seamless-Ex). For subjective metrics, we also evaluate <span class=\"ltx_text ltx_font_bold\">Seed LiveInterpret 2.0</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib41\" title=\"\">2025</a>)</cite>, denoted as Seed Live, an enterprise-level S2ST system with duplex speech understanding and generation abilities.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "seamlessl",
                    "seamlessex",
                    "seamlessm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For UniSS experiments, we deploy vLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib42\" title=\"\">2023</a>)</cite> to support inference. We set a decoding temperature of 0.7, top-k of -1, top-p of 0.8, and a repetition penalty of 1.1. We report results for both Performance mode, denoted as <span class=\"ltx_text ltx_font_bold\">UniSS (P)</span>, and Quality mode, denoted as <span class=\"ltx_text ltx_font_bold\">UniSS (Q)</span>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uniss",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation Fidelity.</span> UniSS achieves state-of-the-art translation fidelity on both EN-ZH and ZH-EN directions. The UniSS (Q) variant achieves a Speech-BLEU of 32.20 on EN-ZH and 24.28 on ZH-EN, substantially outperforming all prior end-to-end and cascaded baselines. The efficient UniSS (P) also delivers strong results, surpassing most existing systems. Notably, in terms of intermediate text metrics, UniSS models perform on par with or better than larger multimodal LLM-based approaches.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "enzh",
                    "better",
                    "results",
                    "cascaded",
                    "uniss",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody Preservation.</span> In terms of prosody, UniSS achieves competitive performance. UniSS (P) variant achieves the second-highest A.PCP score (2.73 and 2.75), closely following Seamless-Ex, which incorporates a dedicated prosody encoder. The performance gap is marginal at only 0.10 (EN-ZH) and 0.12 (ZH-EN), highlighting the efficacy of UniSS in preserving prosodic patterns without specialized modules.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "enzh",
                    "seamlessex",
                    "uniss",
                    "zhen",
                    "apcp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Duration Consistency.</span> UniSS demonstrates superior duration consistency. UniSS (Q) achieves near-optimal SLC 0.2 scores on EN-ZH and the best performance on ZH-EN, improving over the previous best end-to-end system (Seamless-Ex) by 44% and 67%. On the more relaxed SLC 0.4 metric, while competing systems achieve scores above 0.90, both UniSS variants deliver near-perfect performance with scores of 0.99 (EN-ZH) and 0.97 (ZH-EN).</p>\n\n",
                "matched_terms": [
                    "slc",
                    "performance",
                    "best",
                    "enzh",
                    "seamlessex",
                    "zhen",
                    "uniss",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Quality.</span> UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (P) achieves a competitive UTMOS score of 3.77, matching cascaded models (3.76 and 3.79) while surpassing all end-to-end models. In the ZH-EN direction, UniSS surpasses 3-stage systems with a score of 3.86 compared to 3.50.</p>\n\n",
                "matched_terms": [
                    "enzh",
                    "3stage",
                    "s2st",
                    "cascaded",
                    "utmos",
                    "uniss",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Preservation.</span>\nUniSS (Q) demonstrates strong capability in preserving emotions, achieving a MOS of 4.51. This represents a substantial 27% improvement over the expressive S2ST baseline Seamless-Ex (3.56) and surpasses the 3-stage cascaded system (4.48). Notably, UniSS (Q) also approaches Seed Live (4.56), indicating its ability in capturing emotional nuance.</p>\n\n",
                "matched_terms": [
                    "3stage",
                    "seamlessex",
                    "s2st",
                    "cascaded",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Preservation.</span>\nUniSS effectively retains speaker voice characteristics without requiring an additional NAR stage-2 model. For speaker similarity, UniSS (Q) achieves a score of 4.42, outperforming all other models. This represents a 0.07 improvement over the 2-stage system (4.35), which deploys a carefully designed TTS model. These results underscore the ability of UniSS to maintain voice characteristics in an end-to-end fashion.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model",
                    "uniss",
                    "2stage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Naturalness.</span>\nThe naturalness of speech from UniSS is also favorably rated. UniSS (Q) achieves a naturalness MOS of 4.45, surpassing Seamless-Ex (3.10) and the 3-stage cascaded baseline (4.31) that incorporates a specialized TTS model. This demonstrates UniSS&#8217;s ability to generate high-quality, natural-sounding speech without dedicated text-to-speech components.</p>\n\n",
                "matched_terms": [
                    "3stage",
                    "uniss",
                    "seamlessex",
                    "cascaded",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework provides flexible control over the quality-efficiency trade-off through its different CoT prompting modes.\nWe evaluate inference speed on the AR language model component using the Transformers library&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wolf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib43\" title=\"\">2020</a>)</cite>,\nusing 400 utterances (200 per direction) from CVSS-T on a single H800 GPU without batching.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cvsst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Subjective Results &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the Performance mode achieves a 1.07<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p2.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> speedup over Quality mode with only a 1.84 point reduction in Speech-BLEU, demonstrating a favorable speed-quality trade-off.\nFurthermore, we trained <span class=\"ltx_text ltx_font_bold\">UniSS-Small</span> based on Qwen2.5-0.5B-Instruct, achieving significant computational savings.\nUniSS-Small (P) delivers 1.25<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p2.m2\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> speedup with competitive translation fidelity (25.68 Speech-BLEU),\nmaking it suitable for resource-constrained deployment scenarios while maintaining the advantages of our unified architecture.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct ablation studies to validate the effect of our design.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T4\" title=\"Table 4 &#8227; 5.6 Ablation Studies &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the detailed results comparing variants against a base model trained through Phases 1 and 2. The results in type Base and Train are evaluated in the Performance mode.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our three-phase progressive training significantly impacts performance.\nPhase 3 refinement (<span class=\"ltx_text ltx_font_bold\">w/ Phase 3</span>) contributes improvements of +0.90 and +2.06 Speech-BLEU points, validating the importance of high-quality data fine-tuning for final optimization.\nRemoving the initial Phase 1 alignment (<span class=\"ltx_text ltx_font_bold\">UniST only</span>) causes severe performance degradation of -7.18 and -10.15 points. This drop demonstrates that Phase 1 text-speech alignment is essential for subsequent S2ST learning.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "s2st",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Replacing our content-focused GLM-4 linguistic tokenizer with BiCodec&#8217;s self-supervised semantic tokens (<span class=\"ltx_text ltx_font_bold\">w/o GLM</span>) leads to significant performance degradation of -15.01 and -8.73 Speech-BLEU points.\nThis drop reveals that while BiCodec&#8217;s semantic tokens excel at speech generation tasks, their self-supervised nature limits their effectiveness for content understanding in the S2ST task.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "s2st",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Removing the intermediate text generation and performing direct speech-to-speech translation (<span class=\"ltx_text ltx_font_bold\">Direct S2ST</span>) results in a severe performance degradation of -14.94 and -14.40 Speech-BLEU points in the inference. This demonstrates that our cross-modal CoT prompting enables the transfer of textual translation expertise to the speech domain and improves translation fidelity.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "s2st",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces UniSS, a unified single-stage expressive speech-to-speech translation framework that preserves voice and emotional style. Our approach eliminates architectural complexity through well-designed speech semantic and style modeling, enabling seamless integration with pre-trained LLMs. By transferring LLMs&#8217; text translation capabilities to speech via cross-modal CoT prompting, UniSS surpasses previous S2ST systems across multiple dimensions: translation fidelity, speech naturalness, and preservation of voice, emotion, and duration consistency.\nBeyond the model architecture, we design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English expressive S2ST dataset. Our work demonstrates a simple and effective approach for building the next generation of expressive S2ST systems.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Works. </span>\nCurrent limitations point to several research directions. (1) Language Support: We trained UniSS only on Chinese and English due to resource limitations. The data construction pipeline and training framework can be easily extended to multilingual scenarios, which represents our immediate next step. (2) Speech Tokenizer: While the linguistic, speaker, and semantic tokenizers perform well in UniSS, they originate from two different audio tokenizers, causing vocabulary size expansion. Future work will focus on training a unified tokenizer to merge these components and reduce vocabulary size.</p>\n\n",
                "matched_terms": [
                    "size",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present real examples from different models here, with speech converted to text using ASR for display purposes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A1.F1\" title=\"Figure A1 &#8227; Appendix A Examples &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">A1</span></a>, UniSS achieves competitive BLEU scores on challenging sentences where GPT-4o struggles with translation quality. Note Seamless-L and GPT-4o cannot preserve source voice and emotional style.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "gpt4o",
                    "scores",
                    "seamlessl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T1\" title=\"Table B1 &#8227; B.1 UniSS &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B1</span></a>, the UniSS is trained in three phases progressively.\nAll audio is resampled to 16&#160;kHz. The LLM vocabulary is expanded to 180,407 to include speech and control tokens.\nWe use the AdamW optimizer with a weight decay of 0.1 and momentum parameters (0.9, 0.95). The batch size is fixed at 2.3M tokens for all phases.</p>\n\n",
                "matched_terms": [
                    "size",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model is trained on a mixture of ASR, TTS, S2TT, and MT tasks.\nThis phase utilizes 77.1k hours of speech data and 2.3B translation tokens from WMT17,\ntotaling approximately 32B tokens per epoch.\nThe model is trained for 3 epochs with a constant learning rate of 8e-4 and a 1-epoch warm-up.</p>\n\n",
                "matched_terms": [
                    "23b",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce CoT prompting using our UniST General dataset.\nThe model learns three generation paths: Performance mode, Quality mode, and direct S2ST data without intermediate text.\nNew data is mixed with Phase 1 data at a 2:1 ratio, totaling approximately 55B tokens. We train for one epoch with a constant learning rate of 2e-4 and a 5% warm-up.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model",
                    "s2st",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune the model exclusively on the UniST High-quality dataset.\nThe learning rate follows a cosine schedule, gradually annealing from 5e-5 to 5e-6 over 1 epoch without warm-up. This phase consists of approximately 10B training tokens. During this stage, the training loss reaches its minimum around 0.9 epochs, and we select the checkpoint at this point as our final model weights.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our language model is trained on 16 NVIDIA H800 80G GPUs. We utilize the Megatron-LM Framework for efficient large-model training.\nBecause audio duration distribution is uneven, padding would waste significant computational resources. We use the sequence packing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Krell et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib44\" title=\"\">2022</a>]</cite> technique to concatenate multiple samples into a single 18k token long sequence. We use a global batch size of 128. Completing all three phases of training takes approximately 6 days.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide detailed descriptions of all baseline models evaluated against our proposed UniSS framework. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T2\" title=\"Table B2 &#8227; B.2 Baselines &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B2</span></a> summarizes the baselines, model versions, and parameter sizes used in our experiments. All baseline models are evaluated using their default inference parameters without any additional tuning or modifications. Evaluation is conducted directly on the CVSS-T and FLEURS test sets without extra data preprocessing, prompts, or output post-processing. We present the implementation specifics of each baseline in the following subsections.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model",
                    "cvsst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic Speech Recognition (ASR)</span>: We select Whisper-large-v3, a transformer-based encoder-decoder model trained on large-scale multilingual speech corpora, for its strong multilingual transcription performance and wide adoption in the community.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-to-Text Translation (S2TT)</span>: We select SeamlessM4T-v2-Large, a multilingual transformer-based model that jointly performs ASR and MT. Its end-to-end design reduces error propagation across stages and offers strong performance in speech translation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen2.5-Omni is selected for its ability to perform fully end-to-end speech-to-speech translation without requiring intermediate text generation, making it a promising multimodal LLM baseline for direct S2ST tasks. In our experiments, we find that the model&#8217;s performance is highly sensitive to the prompt format. When using a simple default instruction, the model sometimes appends assistant-like phrases (e.g., &#8220;Do you need anything else?&#8221;) to the translated speech, likely due to its conversational fine-tuning.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, we find that such detailed prompts often lead to degraded audio output quality. Specifically, in our experiments, the model occasionally produces abnormal acoustic artifacts, such as elongating a single word unnaturally or producing disfluent prosody. This suggests that Qwen2.5-Omni&#8217;s speech generation may be overly sensitive to instruction format, and that more verbose prompts can negatively impact generation fluency despite offering better control over semantic behavior.</p>\n\n",
                "matched_terms": [
                    "model",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen2-audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib45\" title=\"\">2024</a>]</cite> is used for direct speech-to-text translation. Considering its strong performance on multilingual speech comprehension tasks and its ability to follow structured prompts, we adopt it for S2TT evaluation. Prompts are designed to explicitly guide the model to translate input speech into target-language text. For example:</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the performance of speech-to-speech translation (S2ST), we report a range of objective metrics covering translation accuracy, speaker identity preservation, prosodic alignment, temporal consistency, and speech quality. For all metrics, higher values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "indicate",
                    "performance",
                    "better",
                    "s2st",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-BLEU</span> evaluates the fidelity of generated translations by computing corpus-level BLEU scores using the SacreBLEU library&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Post, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib46\" title=\"\">2018</a>]</cite>. Before scoring, we apply language-specific preprocessing: English text is lowercased and stripped of punctuation (excluding apostrophes), while Chinese text is normalized to simplified characters, punctuation is removed, and characters are separated by spaces. This ensures consistency with standard BLEU evaluation practices. We use the <span class=\"ltx_text ltx_font_italic\">corpus_score</span> function to calculate the BLEU score across the whole dataset. Chinese samples are scored in &#8216;zh&#8217; mode.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "textbleu",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-BLEU</span>, or ASR-BLEU, evaluates the translation fidelity of speech-to-speech translation systems. We transcribe the generated speech using ASR models, Whisper-large-v3 for English and Paraformer-zh for Chinese, and compute the BLEU score between the transcribed output and the ground-truth reference. Preprocessing follows the same pipeline as Text-BLEU.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "textbleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate on the Chinese and English test subsets of FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib48\" title=\"\">2022</a>]</cite>, a massively multilingual speech dataset spanning 102 languages with n-way parallel speech and text data. UniSS demonstrates robust performance on this challenging multilingual benchmark across multiple evaluation tasks. Additionally, UniSS achieves state-of-the-art performance on speech-to-text translation (S2TT) and competitive results on automatic speech recognition (ASR) and text-to-speech synthesis (TTS) tasks.</p>\n\n",
                "matched_terms": [
                    "results",
                    "dataset",
                    "uniss",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the robustness of our framework across datasets, we further evaluate performance on the FLEURS test set. FLEURS is a multilingual benchmark derived from the FLoRes corpus that provides high-quality parallel speech and text pairs across diverse languages, making it a valuable complement to CVSS-T for assessing speech translation systems under standardized multilingual conditions.</p>\n\n",
                "matched_terms": [
                    "cvsst",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T3\" title=\"Table D3 &#8227; D.1 S2ST Performance on Fleurs &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D3</span></a>, UniSS (Q) achieves strong translation performance with Speech-BLEU of 36.16 and Text-BLEU of 36.96 on EN-ZH, outperforming all S2ST baselines and cascaded systems. UniSS (P) also demonstrates competitive performance across both metrics. In the ZH-EN direction, UniSS surpasses all systems with comparable model size, demonstrating the parameter efficiency of our design.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "textbleu",
                    "performance",
                    "enzh",
                    "size",
                    "uniss",
                    "s2st",
                    "cascaded",
                    "model",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS exhibits excellent prosody transfer capabilities on the FLEURS benchmark. UniSS (Q) achieves A.PCP scores of 2.72 (EN-ZH) and 2.64 (ZH-EN), outperforming all end-to-end baselines and matching GPT-4o&#8217;s performance (2.72 and 2.39). Notably, UniSS achieves these results without incorporating dedicated prosody modeling modules, highlighting the effectiveness of our unified framework.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "enzh",
                    "results",
                    "zhen",
                    "uniss",
                    "scores",
                    "apcp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system demonstrates exceptional speech duration consistency on the FLEURS test set. Both UniSS variants achieve near-perfect scores on the SLC-0.2 and SLC-0.4 metrics, indicating that the generated speech closely mirrors the temporal length of the input audio. This precise duration alignment is critical for real-time applications and audiovisual translation scenarios where timing mismatches can disrupt user experience.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (Q) attains a UTMOS score of 3.41, outperforming all open-source baselines and surpassing GPT-4o (3.36). UniSS (Q) also leads all S2ST models in the ZH-EN direction with a score of 4.16. Despite its significantly smaller size, UniSS consistently produces high-quality speech across both translation directions, confirming its effectiveness as a practical open-source solution for speech-to-speech translation.</p>\n\n",
                "matched_terms": [
                    "enzh",
                    "size",
                    "s2st",
                    "gpt4o",
                    "utmos",
                    "uniss",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond speech-to-speech translation, UniSS also supports speech-to-text translation (S2TT). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>, UniSS achieves leading performance on this task as well. Whisper only supports X-EN translation and demonstrates poor performance on multilingual scenarios. UniSS&#8217;s Text-BLEU scores significantly outperform the 7B multimodal LLM Qwen2-audio across both datasets, demonstrating superior cross-lingual speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "scores",
                    "textbleu",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although UniSS was not specifically designed for automatic speech recognition, it demonstrates competitive ASR capabilities as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. We evaluate the ASR performance on the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib49\" title=\"\">2015</a>]</cite>. On the LibriSpeech test-clean subset, UniSS achieves a WER of 2.4, which is comparable to the open-source state-of-the-art ASR model, Whisper (1.8 WER). However, on the more challenging LibriSpeech test-others subset, UniSS shows degraded performance with a WER of 6.6. It is important to note that UniSS is primarily designed as a speech-to-speech translation model, and these ASR results demonstrate its versatility beyond its core translation capabilities.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "uniss",
                    "results",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although UniSS was not specifically designed for text-to-speech synthesis, we evaluate its TTS capabilities on the LibriSpeech test-clean dataset as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. UniSS achieves a WER of 4.75 and speaker similarity of 0.568, compared to SparkTTS, which obtains 1.98 WER and 0.584 similarity. The performance gap is expected given that SparkTTS is specifically optimized for TTS, whereas UniSS is primarily designed for speech-to-speech translation. These results demonstrate that UniSS can generate intelligible speech output.</p>\n\n",
                "matched_terms": [
                    "results",
                    "dataset",
                    "uniss",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CVSS-T</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>]</cite>: A benchmark dataset for speech-to-speech translation in English-Chinese language pairs.</p>\n\n",
                "matched_terms": [
                    "cvsst",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a subjective assessment on a randomly sampled subset of UniST to evaluate the quality of our dataset. Four raters listened to 150 examples and provided MOS scores across three dimensions: Emotion Similarity, Speaker Similarity, and Speech Naturalness. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.T5\" title=\"Table E5 &#8227; E.3 Analysis of UniST &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E5</span></a>, UniST synthesized by SparkTTS achieves high speech naturalness and effectively preserves both the source speaker&#8217;s voice characteristics and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The progressive training strategy pertains only to the training process and does not increase the model&#8217;s structural complexity. The alignment in phase 1 uses existing, easily accessible open-source data. In the ablation study section, we demonstrate the impact of different stages on model performance, proving that progressive training is effective.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speaker tokenizer is used to represent global speaker information. We hypothesize that this voice decoupling reduces the difficulty of speech translation by allowing the model to focus more on the linguistic content. We employ different linguistic and semantic tokenizers for understanding and generation to optimize each task. To generate more natural and expressive audio, the generation process uses semantic tokens, which retain information beyond the core content. However, this additional information makes content understanding more challenging. Therefore, the linguistic tokenizer, which focuses solely on content information, is better suited for the speech understanding phase. As a simple speech representation, we believe this design does not introduce greater complexity in either training or inference.</p>\n\n",
                "matched_terms": [
                    "model",
                    "better"
                ]
            }
        ]
    },
    "S5.T3.fig1": {
        "source_file": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
        "caption": "Table 2: Subjective MOS evaluation on the expressive emotion dataset. *Seed Live is closed source. Best scores are in bold and second-best scores are underlined.",
        "body": "Model\nEmo Sim.↑\\uparrow\nSpk Sim.↑\\uparrow\nNaturalness↑\\uparrow\n\n\n3-Stage\n4.48\n4.33\n4.31\n\n\n2-Stage\n4.27\n4.35\n4.27\n\n\nSeed Live*\n4.56\n4.19\n4.69\n\n\nSeamless-Ex\n3.56\n2.94\n3.10\n\n\nUniSS (Q)\n4.51\n4.42\n4.45",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:3.4pt;padding-right:3.4pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.4pt;padding-right:3.4pt;\"><span class=\"ltx_text ltx_font_bold\">Emo Sim.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.4pt;padding-right:3.4pt;\"><span class=\"ltx_text ltx_font_bold\">Spk Sim.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:3.4pt;padding-right:3.4pt;\"><span class=\"ltx_text ltx_font_bold\">Naturalness<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">3-Stage</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">4.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">4.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">4.31</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">2-Stage</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">4.27</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.4pt;padding-right:3.4pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.35</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">4.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">Seed Live*</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.4pt;padding-right:3.4pt;\"><span class=\"ltx_text ltx_font_bold\">4.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">4.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.4pt;padding-right:3.4pt;\"><span class=\"ltx_text ltx_font_bold\">4.69</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">Seamless-Ex</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">3.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">2.94</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.4pt;padding-right:3.4pt;\">3.10</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#C2D1E5;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.4pt;padding-right:3.4pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C2D1E5;\">UniSS (Q)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.4pt;padding-right:3.4pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#C2D1E5;\">4.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.4pt;padding-right:3.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#C2D1E5;\">4.42</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.4pt;padding-right:3.4pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#C2D1E5;\">4.45</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "secondbest",
            "3stage",
            "underlined",
            "subjective",
            "emotion",
            "expressive",
            "source",
            "evaluation",
            "best",
            "closed",
            "seamlessex",
            "naturalness↑uparrow",
            "bold",
            "scores",
            "emo",
            "mos",
            "seed",
            "sim↑uparrow",
            "dataset",
            "spk",
            "2stage",
            "uniss",
            "live",
            "model"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cmots.github.io/uniss-demo\" title=\"\">https://cmots.github.io/uniss-demo</a>.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "emotion",
                    "model",
                    "expressive",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In pursuit of high-fidelity, expressive S2ST, and to overcome the limitations outlined above, our vision is defined by three core principles: (1) a single-stage architecture to eliminate complexity; (2) a unified model that aligns speech and text modalities; and (3) a mechanism to explicitly leverage the proven text translation capabilities of LLMs. To our knowledge, no existing approach satisfies all three principles simultaneously.</p>\n\n",
                "matched_terms": [
                    "model",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of architectural complexity, we present <span class=\"ltx_text ltx_font_bold\">UniSS</span>, a unified single-stage S2ST framework that preserves timbre, emotion, and duration consistency.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> demonstrates our approach and performance results. UniSS builds upon pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> as the backbone, modeling content and speaker information in a single AR language model without architectural modifications. We transfer the LLM&#8217;s pre-trained text translation capabilities to the speech domain through a cross-modality chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib15\" title=\"\">2022</a>)</cite>. During inference, our Quality Mode guides the model to decompose the complex S2ST task into sequential listen, translate, and speak steps within a single inference pass. We also introduce a Performance Mode that trades translation fidelity for faster inference, enabling deployment across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Unified Single-Stage Architecture for S2ST:</span> We propose UniSS, a single-stage AR model for expressive S2ST that directly leverages a pre-trained textual LLM, eliminating the architectural complexity of prior work.</p>\n\n",
                "matched_terms": [
                    "model",
                    "uniss",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Large-Scale, High-Quality S2ST Dataset:</span> We design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English S2ST dataset with speaker voice and emotion preservation.</p>\n\n",
                "matched_terms": [
                    "expressive",
                    "emotion",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of LLMs in text processing has inspired their adaptation to S2ST through speech tokenization. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib21\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib22\" title=\"\">2025</a>)</cite> focus primarily on semantic translation without preserving voice characteristics.\nTo address voice preservation, some works model acoustic features using multiple AR models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>; Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>)</cite>. Recent unified approaches attempt to model both semantic and acoustic features within a single AR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>; Gong and Veluri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite>. However, they still require an additional NAR model for complete acoustic generation. Hibiki&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite> proposes a multi-stream architecture processing source and target speech simultaneously, but necessitates substantial architectural modifications, including nested Transformers and training from scratch.\nA fundamental limitation across these approaches is their reliance on complex architectures to complete acoustic representations. Moreover, they fail to explicitly leverage the LLM&#8217;s pre-trained text translation capabilities, treating them merely as generic sequence converters. UniSS addresses both limitations through a single-stage architecture while effectively transferring LLM&#8217;s text translation capabilities to speech.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 UniSS Architecture &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, UniSS is a unified AR language model for expressive speech-to-speech translation, built upon a single-stage architecture with a cross-modal chain-of-thought prompting framework, and optimized via a progressive training strategy. This section details the three core components of UniSS: model architecture, cross-modal CoT prompting, and training methodology.</p>\n\n",
                "matched_terms": [
                    "model",
                    "uniss",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of UniSS is to perform expressive S2ST by modeling the conditional distribution <math alttext=\"P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo fence=\"false\">|</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})</annotation></semantics></math>, where <math alttext=\"\\mathbf{X}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{src}</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> denote the source and target speech waveforms, respectively. To achieve faithful content translation while preserving speaker identity and expressiveness, UniSS introduces three types of speech tokens: speaker tokens <math alttext=\"\\mathbf{S}^{spk}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{spk}</annotation></semantics></math> with fixed sequence length to capture global style attributes, e.g., timbre, prosody, and emotion; linguistic tokens <math alttext=\"\\mathbf{S}^{ling}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{ling}</annotation></semantics></math> to encode the content of the source utterance; and semantic tokens <math alttext=\"\\mathbf{S}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}</annotation></semantics></math> to represent the expressive target utterance and can be directly decoded into waveform when combined with speaker tokens.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "expressive",
                    "emotion",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This unified design allows UniSS to handle expressive S2ST without intermediate acoustic representations or cascaded systems, maintaining fidelity in both content and voice.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build our generation model upon the pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite>, a strong LLM backbone. By expanding the model vocabulary to include the discrete speech tokens, UniSS treats speech and text uniformly as token sequences, allowing both modalities to be processed within the same transformer architecture.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To represent both the content and expressive characteristics of speech, UniSS adopts a triple-tokenizer strategy, transforming the waveform <math alttext=\"\\mathbf{W}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119830;</mi><annotation encoding=\"application/x-tex\">\\mathbf{W}</annotation></semantics></math> into single-stream discrete token sequences:</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the S2ST task, the source waveform <math alttext=\"\\mathbf{X_{src}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;&#119851;&#119836;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{src}}</annotation></semantics></math> is represented by <math alttext=\"(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})</annotation></semantics></math>, while the target waveform <math alttext=\"\\mathbf{Y_{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119853;&#119840;&#119853;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{tgt}}</annotation></semantics></math> is represented by <math alttext=\"\\mathbf{S}_{tgt}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}_{tgt}^{sem}</annotation></semantics></math>. This design is motivated by our preliminary experiments, which revealed that although BiCodec&#8217;s semantic tokens are highly effective for waveform reconstruction, their self-supervised nature makes them suboptimal for content understanding. By separately representing speaker identity, linguistic content, and generation-oriented semantics, UniSS enables more accurate and controllable S2ST modeling.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS is guided by an input prompt <math alttext=\"\\mathbf{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#119823;</mi><annotation encoding=\"application/x-tex\">\\mathbf{P}</annotation></semantics></math>, which is a structured sequence of control and source tokens:</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"c_{task}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{task}</annotation></semantics></math>, <math alttext=\"c_{lang}^{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">c_{lang}^{tgt}</annotation></semantics></math>, and <math alttext=\"c_{speed}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{speed}</annotation></semantics></math> are special tokens specifying the task mode, target language, and duration ratio between source and target speech, respectively. A special begin-of-translation token, <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">BOT</span>, signals the model to begin generation, producing an output sequence <math alttext=\"\\tau_{out}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mrow><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tau_{out}</annotation></semantics></math> that is terminated by an end-of-decoding token, <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">EOD</span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quality Mode.</span> This mode follows the full CoT prompting path to maximize translation fidelity. The model first <span class=\"ltx_text ltx_font_italic\">listens</span> by generating the source transcription <math alttext=\"\\mathbf{T}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{src}</annotation></semantics></math>, then <span class=\"ltx_text ltx_font_italic\">translates</span> it into the text in the target language <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>, and finally <span class=\"ltx_text ltx_font_italic\">speaks</span> by generating target semantic tokens <math alttext=\"\\mathbf{S}^{sem}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}_{tgt}</annotation></semantics></math>. Prompted with <math alttext=\"c_{task}=\\text{Quality Mode}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><mo>=</mo><mtext>Quality Mode</mtext></mrow><annotation encoding=\"application/x-tex\">c_{task}=\\text{Quality Mode}</annotation></semantics></math>, this explicit chain allows the model to leverage its robust text translation abilities, formally represented as:</p>\n\n",
                "matched_terms": [
                    "model",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here <math alttext=\"c_{lang}^{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">c_{lang}^{src}</annotation></semantics></math> denotes the source language, and <math alttext=\"\\mathbf{S}^{sem}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}_{src}</annotation></semantics></math> denotes the semantic tokens from source speech. This phase endows the model with robust speech understanding and generation while prioritizing the preservation of text translation capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training end-to-end expressive S2ST models faces a significant bottleneck due to the scarcity of large-scale, parallel data that preserves speaker characteristics. To address this limitation, we design a scalable data synthesis pipeline. The pipeline generates UniST, our large-scale Chinese-English expressive S2ST dataset. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) illustrates the data construction process.</p>\n\n",
                "matched_terms": [
                    "expressive",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation and Target Speech Synthesis.</span>\nAfter source cleaning, <math alttext=\"\\mathbf{T}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{src}</annotation></semantics></math> is translated by Qwen2.5-72B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> to produce target text <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math> in another language.\nPrompts used in translation are shown in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.SS2\" title=\"E.2 Text Translation Step &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.\nWe then apply an expressive TTS model, SparkTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib23\" title=\"\">2025</a>)</cite>, to synthesize the target speech <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> from <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m4\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>, conditioned by <math alttext=\"\\mathbf{X}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m5\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{src}</annotation></semantics></math> to preserve the source speaker&#8217;s voice. To enable fine-grained speed control, we calculate the duration ratio between source and target speech and discretize it into speed tokens <math alttext=\"\\mathbf{c}_{speed}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m6\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{speed}</annotation></semantics></math> with 0.1 intervals. This creates complete parallel samples <math alttext=\"(\\mathbf{X}_{src},\\mathbf{T}_{src},\\mathbf{T}_{tgt},\\mathbf{Y}_{tgt},\\mathbf{c}_{speed})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>&#119836;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{X}_{src},\\mathbf{T}_{src},\\mathbf{T}_{tgt},\\mathbf{Y}_{tgt},\\mathbf{c}_{speed})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "expressive",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quality Filtering and Dataset Variants.</span>\nThe synthesized data undergoes final quality filtering.\nWe apply ASR to <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> and discard samples with WER greater than 0.01 against <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>. We also apply a duration ratio filter to keep synthesized speech with duration within [0.5, 2.0] times of the source speech.\nThe filtered data forms our <span class=\"ltx_text ltx_font_bold\">UniST General</span> dataset (44.8k hours).\nOur refined <span class=\"ltx_text ltx_font_bold\">UniST High-Quality</span> dataset (19.8k hours) is created by applying additional Voice Activity Detection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib27\" title=\"\">2024</a>)</cite> to remove silence at the beginning and end of speech segments, and a stricter duration ratio filter of [0.7,1.5].</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS is trained in three progressive phases, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS3\" title=\"3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>. Phase 1 establishes text-speech alignment using 77.1k hours of speech data and MT text data from WMT17&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bojar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib28\" title=\"\">2017</a>)</cite>. Phase 2 introduces CoT prompting with our UniST General dataset, mixed with Phase 1 data at a 2:1 ratio. Phase 3 fine-tunes the model exclusively on the UniST High-Quality dataset.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our primary S2ST evaluation, we use the Chinese (ZH) and English (EN) test sets from CVSS-T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite>, which contain 4,897 utterance pairs (8.2 hours for Chinese and 6.3 hours for English). For emotion preservation evaluation, we randomly sample 300 utterances from each emotional speech dataset (ESD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib31\" title=\"\">2021</a>)</cite> and CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib32\" title=\"\">2014</a>)</cite>), covering a selected set of 4 emotions (happy, sad, angry, neutral). We evaluate both English-to-Chinese (EN-ZH) and Chinese-to-English (ZH-EN) translation directions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "emotion",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ comprehensive objective and subjective metrics to evaluate S2ST performance. For translation fidelity, we measure BLEU scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib33\" title=\"\">2002</a>)</cite> on both ASR-transcribed speech outputs (Speech-BLEU) and intermediate text (Text-BLEU). Prosody preservation is assessed through prosody similarity (A.PCP)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Andrews et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib34\" title=\"\">2022</a>)</cite>. Duration consistency is evaluated using Speech Length Compliance (SLC) within 0.2 and 0.4 ratio tolerance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nSpeech quality is measured using UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib36\" title=\"\">2022</a>)</cite>. Additionally, we conduct a subjective Mean Opinion Score (MOS) evaluation where bilingual speakers rate emotion similarity, speaker similarity, and naturalness on a 5-point scale. Detailed descriptions of all evaluation metrics and implementation procedures are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A3\" title=\"Appendix C Evaluation Metrics &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "subjective",
                    "mos",
                    "emotion",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Systems.</span>\nTo establish an upper bound for modular approaches, we construct two cascaded baselines using top-performing open-source models: a <span class=\"ltx_text ltx_font_bold\">3-Stage</span> pipeline of Whisper-large-v3 (ASR), NLLB-200-distilled (MT)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Koishekenov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib37\" title=\"\">2023</a>)</cite>, and CosyVoice 2 (TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib38\" title=\"\">2024</a>)</cite>; and a <span class=\"ltx_text ltx_font_bold\">2-Stage</span> pipeline of SeamlessM4T-v2-Large (S2TT) and CosyVoice 2 (TTS).</p>\n\n",
                "matched_terms": [
                    "2stage",
                    "3stage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End S2ST Systems.</span>\nWe compare against dedicated S2ST models that represent the current open-source SOTA: <span class=\"ltx_text ltx_font_bold\">SeamlessM4T</span> (Medium and Large V2, denoted Seamless-M and Seamless-L), and its expressive variant <span class=\"ltx_text ltx_font_bold\">SeamlessExpressive</span> (Seamless-Ex). For subjective metrics, we also evaluate <span class=\"ltx_text ltx_font_bold\">Seed LiveInterpret 2.0</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib41\" title=\"\">2025</a>)</cite>, denoted as Seed Live, an enterprise-level S2ST system with duplex speech understanding and generation abilities.</p>\n\n",
                "matched_terms": [
                    "seamlessex",
                    "subjective",
                    "live",
                    "seed",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T1\" title=\"Table 1 &#8227; 5.3 Objective Performance &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main comparison results on the CVSS-T dataset.\nThe results clearly demonstrate that UniSS establishes a new SOTA in translation fidelity\nwhile maintaining voice characteristics, prosody, duration consistency, and speech quality, validating our core design principles. Additional results on other datasets and tasks are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4\" title=\"Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody Preservation.</span> In terms of prosody, UniSS achieves competitive performance. UniSS (P) variant achieves the second-highest A.PCP score (2.73 and 2.75), closely following Seamless-Ex, which incorporates a dedicated prosody encoder. The performance gap is marginal at only 0.10 (EN-ZH) and 0.12 (ZH-EN), highlighting the efficacy of UniSS in preserving prosodic patterns without specialized modules.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "seamlessex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Duration Consistency.</span> UniSS demonstrates superior duration consistency. UniSS (Q) achieves near-optimal SLC 0.2 scores on EN-ZH and the best performance on ZH-EN, improving over the previous best end-to-end system (Seamless-Ex) by 44% and 67%. On the more relaxed SLC 0.4 metric, while competing systems achieve scores above 0.90, both UniSS variants deliver near-perfect performance with scores of 0.99 (EN-ZH) and 0.97 (ZH-EN).</p>\n\n",
                "matched_terms": [
                    "best",
                    "uniss",
                    "scores",
                    "seamlessex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Quality.</span> UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (P) achieves a competitive UTMOS score of 3.77, matching cascaded models (3.76 and 3.79) while surpassing all end-to-end models. In the ZH-EN direction, UniSS surpasses 3-stage systems with a score of 3.86 compared to 3.50.</p>\n\n",
                "matched_terms": [
                    "3stage",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the objective metrics, we conduct comprehensive subjective assessments on emotion preservation, voice preservation, and speech naturalness. The results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Subjective Results &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Preservation.</span>\nUniSS (Q) demonstrates strong capability in preserving emotions, achieving a MOS of 4.51. This represents a substantial 27% improvement over the expressive S2ST baseline Seamless-Ex (3.56) and surpasses the 3-stage cascaded system (4.48). Notably, UniSS (Q) also approaches Seed Live (4.56), indicating its ability in capturing emotional nuance.</p>\n\n",
                "matched_terms": [
                    "3stage",
                    "seamlessex",
                    "live",
                    "mos",
                    "emotion",
                    "seed",
                    "uniss",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Preservation.</span>\nUniSS effectively retains speaker voice characteristics without requiring an additional NAR stage-2 model. For speaker similarity, UniSS (Q) achieves a score of 4.42, outperforming all other models. This represents a 0.07 improvement over the 2-stage system (4.35), which deploys a carefully designed TTS model. These results underscore the ability of UniSS to maintain voice characteristics in an end-to-end fashion.</p>\n\n",
                "matched_terms": [
                    "2stage",
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Naturalness.</span>\nThe naturalness of speech from UniSS is also favorably rated. UniSS (Q) achieves a naturalness MOS of 4.45, surpassing Seamless-Ex (3.10) and the 3-stage cascaded baseline (4.31) that incorporates a specialized TTS model. This demonstrates UniSS&#8217;s ability to generate high-quality, natural-sounding speech without dedicated text-to-speech components.</p>\n\n",
                "matched_terms": [
                    "3stage",
                    "uniss",
                    "seamlessex",
                    "mos",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces UniSS, a unified single-stage expressive speech-to-speech translation framework that preserves voice and emotional style. Our approach eliminates architectural complexity through well-designed speech semantic and style modeling, enabling seamless integration with pre-trained LLMs. By transferring LLMs&#8217; text translation capabilities to speech via cross-modal CoT prompting, UniSS surpasses previous S2ST systems across multiple dimensions: translation fidelity, speech naturalness, and preservation of voice, emotion, and duration consistency.\nBeyond the model architecture, we design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English expressive S2ST dataset. Our work demonstrates a simple and effective approach for building the next generation of expressive S2ST systems.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "emotion",
                    "model",
                    "expressive",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present real examples from different models here, with speech converted to text using ASR for display purposes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A1.F1\" title=\"Figure A1 &#8227; Appendix A Examples &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">A1</span></a>, UniSS achieves competitive BLEU scores on challenging sentences where GPT-4o struggles with translation quality. Note Seamless-L and GPT-4o cannot preserve source voice and emotional style.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "scores",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce CoT prompting using our UniST General dataset.\nThe model learns three generation paths: Performance mode, Quality mode, and direct S2ST data without intermediate text.\nNew data is mixed with Phase 1 data at a 2:1 ratio, totaling approximately 55B tokens. We train for one epoch with a constant learning rate of 2e-4 and a 5% warm-up.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune the model exclusively on the UniST High-quality dataset.\nThe learning rate follows a cosine schedule, gradually annealing from 5e-5 to 5e-6 over 1 epoch without warm-up. This phase consists of approximately 10B training tokens. During this stage, the training loss reaches its minimum around 0.9 epochs, and we select the checkpoint at this point as our final model weights.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide detailed descriptions of all baseline models evaluated against our proposed UniSS framework. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T2\" title=\"Table B2 &#8227; B.2 Baselines &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B2</span></a> summarizes the baselines, model versions, and parameter sizes used in our experiments. All baseline models are evaluated using their default inference parameters without any additional tuning or modifications. Evaluation is conducted directly on the CVSS-T and FLEURS test sets without extra data preprocessing, prompts, or output post-processing. We present the implementation specifics of each baseline in the following subsections.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Speech (TTS)</span>: We employ CosyVoice 2 (0.5B), a transformer-based expressive TTS model designed for high-quality multilingual speech synthesis. Its ability to generate natural and expressive audio makes it suitable for our evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen2-audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib45\" title=\"\">2024</a>]</cite> is used for direct speech-to-text translation. Considering its strong performance on multilingual speech comprehension tasks and its ability to follow structured prompts, we adopt it for S2TT evaluation. Prompts are designed to explicitly guide the model to translate input speech into target-language text. For example:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeamlessExpressive extends SeamlessM4T by introducing expressiveness-aware modeling, enabling more natural and emotionally rich speech synthesis in the target language. The model conditions generation on prosodic and expressive cues from the source speech and is particularly well-suited for conversational and affective scenarios. We evaluate SeamlessExpressive using its official inference scripts and default hyperparameters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "expressive",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Seed LiveInterpret 2.0 is an enterprise-level simultaneous interpretation model that performs real-time speech-to-speech translation with voice cloning capabilities. The model employs a duplex framework that processes input audio and generates target-language speech directly without intermediate text representations. In our evaluation, we used the official inference API with default parameters.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "seed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the capability to perform direct speech-to-speech translation, current end-to-end models still face notable limitations in audio quality. In our experiments, the generated speech often contained audible artifacts or background noise, especially for longer or noisier input segments. While SeamlessExpressive improves upon previous models by attempting to clone speaker identity, prosody, and expressiveness, its expressive fidelity remains limited. The cloned voice frequently lacks the richness and nuance of natural human speech, and variations in emotion or emphasis are not always faithfully reproduced.</p>\n\n",
                "matched_terms": [
                    "expressive",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-BLEU</span> evaluates the fidelity of generated translations by computing corpus-level BLEU scores using the SacreBLEU library&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Post, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib46\" title=\"\">2018</a>]</cite>. Before scoring, we apply language-specific preprocessing: English text is lowercased and stripped of punctuation (excluding apostrophes), while Chinese text is normalized to simplified characters, punctuation is removed, and characters are separated by spaces. This ensures consistency with standard BLEU evaluation practices. We use the <span class=\"ltx_text ltx_font_italic\">corpus_score</span> function to calculate the BLEU score across the whole dataset. Chinese samples are scored in &#8216;zh&#8217; mode.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "scores",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate expressive qualities, we conducted a Mean Opinion Score (MOS) listening study. Six bilingual speakers rated synthesized speech on a 5-point scale across three dimensions: emotion similarity (Emo Sim.), speaker similarity (Spk Sim.), and speech naturalness (Naturalness). Similarity is compared against the original audio instead of the synthesized audio.</p>\n\n",
                "matched_terms": [
                    "spk",
                    "emo",
                    "mos",
                    "emotion",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implemented the MOS evaluation using webMUSHRA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Schoeffler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib47\" title=\"\">2018</a>]</cite>, a MUSHRA-compliant web-based audio evaluation framework that facilitates controlled listening experiments. The platform provided key features for audio assessment, including seamless audio switching and Likert scale questionnaires. Experimental configurations were defined through YAML files, with results automatically exported as CSV files. The user interface is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A3.F2\" title=\"Figure C2 &#8227; C.2 Subjective Evaluation &#8227; Appendix C Evaluation Metrics &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">C2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate on the Chinese and English test subsets of FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib48\" title=\"\">2022</a>]</cite>, a massively multilingual speech dataset spanning 102 languages with n-way parallel speech and text data. UniSS demonstrates robust performance on this challenging multilingual benchmark across multiple evaluation tasks. Additionally, UniSS achieves state-of-the-art performance on speech-to-text translation (S2TT) and competitive results on automatic speech recognition (ASR) and text-to-speech synthesis (TTS) tasks.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "uniss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T3\" title=\"Table D3 &#8227; D.1 S2ST Performance on Fleurs &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D3</span></a>, UniSS (Q) achieves strong translation performance with Speech-BLEU of 36.16 and Text-BLEU of 36.96 on EN-ZH, outperforming all S2ST baselines and cascaded systems. UniSS (P) also demonstrates competitive performance across both metrics. In the ZH-EN direction, UniSS surpasses all systems with comparable model size, demonstrating the parameter efficiency of our design.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS exhibits excellent prosody transfer capabilities on the FLEURS benchmark. UniSS (Q) achieves A.PCP scores of 2.72 (EN-ZH) and 2.64 (ZH-EN), outperforming all end-to-end baselines and matching GPT-4o&#8217;s performance (2.72 and 2.39). Notably, UniSS achieves these results without incorporating dedicated prosody modeling modules, highlighting the effectiveness of our unified framework.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system demonstrates exceptional speech duration consistency on the FLEURS test set. Both UniSS variants achieve near-perfect scores on the SLC-0.2 and SLC-0.4 metrics, indicating that the generated speech closely mirrors the temporal length of the input audio. This precise duration alignment is critical for real-time applications and audiovisual translation scenarios where timing mismatches can disrupt user experience.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond speech-to-speech translation, UniSS also supports speech-to-text translation (S2TT). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>, UniSS achieves leading performance on this task as well. Whisper only supports X-EN translation and demonstrates poor performance on multilingual scenarios. UniSS&#8217;s Text-BLEU scores significantly outperform the 7B multimodal LLM Qwen2-audio across both datasets, demonstrating superior cross-lingual speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although UniSS was not specifically designed for automatic speech recognition, it demonstrates competitive ASR capabilities as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. We evaluate the ASR performance on the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib49\" title=\"\">2015</a>]</cite>. On the LibriSpeech test-clean subset, UniSS achieves a WER of 2.4, which is comparable to the open-source state-of-the-art ASR model, Whisper (1.8 WER). However, on the more challenging LibriSpeech test-others subset, UniSS shows degraded performance with a WER of 6.6. It is important to note that UniSS is primarily designed as a speech-to-speech translation model, and these ASR results demonstrate its versatility beyond its core translation capabilities.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although UniSS was not specifically designed for text-to-speech synthesis, we evaluate its TTS capabilities on the LibriSpeech test-clean dataset as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. UniSS achieves a WER of 4.75 and speaker similarity of 0.568, compared to SparkTTS, which obtains 1.98 WER and 0.584 similarity. The performance gap is expected given that SparkTTS is specifically optimized for TTS, whereas UniSS is primarily designed for speech-to-speech translation. These results demonstrate that UniSS can generate intelligible speech output.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After cleaning the source dataset, we employ Qwen-2.5-72B-Instruct as our translation engine to translate source transcriptions into the target language. For source language Chinese audio, we use the following prompt:</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a subjective assessment on a randomly sampled subset of UniST to evaluate the quality of our dataset. Four raters listened to 150 examples and provided MOS scores across three dimensions: Emotion Similarity, Speaker Similarity, and Speech Naturalness. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.T5\" title=\"Table E5 &#8227; E.3 Analysis of UniST &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E5</span></a>, UniST synthesized by SparkTTS achieves high speech naturalness and effectively preserves both the source speaker&#8217;s voice characteristics and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "subjective",
                    "mos",
                    "emotion",
                    "scores",
                    "dataset",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speaker tokenizer is used to represent global speaker information. We hypothesize that this voice decoupling reduces the difficulty of speech translation by allowing the model to focus more on the linguistic content. We employ different linguistic and semantic tokenizers for understanding and generation to optimize each task. To generate more natural and expressive audio, the generation process uses semantic tokens, which retain information beyond the core content. However, this additional information makes content understanding more challenging. Therefore, the linguistic tokenizer, which focuses solely on content information, is better suited for the speech understanding phase. As a simple speech representation, we believe this design does not introduce greater complexity in either training or inference.</p>\n\n",
                "matched_terms": [
                    "model",
                    "expressive"
                ]
            }
        ]
    },
    "S5.T3.fig2": {
        "source_file": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
        "caption": "Table 3: Inference speed of AR language model and Speech-BLEU comparison. Speech-BLEU is the average of EN-ZH and ZH-EN. Time is the total inference time on 400 utterances without batching inference. Best results are in bold.",
        "body": "Model\n#Size\nSpeech-BLEU↑\\uparrow\nTime(s)↓\\downarrow\nSpeedup↑\\uparrow\n\n\n\n\nUniSS (Q)\n1.5B\n33.66\n1521.52\n1.00×\\times\n\n\n\nUniSS (P)\n1.5B\n31.82\n1426.54\n1.07×\\times\n\n\n\nUniSS-Small (Q)\n0.5B\n28.17\n1339.24\n1.14×\\times\n\n\n\nUniSS-Small (P)\n0.5B\n25.68\n1212.65\n1.25×\\times",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">#Size</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speech-BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Time(s)<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Speedup<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniSS (Q)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">33.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1521.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.00<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m7\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniSS (P)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.5B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">31.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1426.54</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.07<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m8\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniSS-Small (Q)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.5B</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">28.17</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1339.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.14<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m9\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">UniSS-Small (P)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.5B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">25.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">1212.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">1.25<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m10\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math></span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "speechbleu",
            "inference",
            "speed",
            "time",
            "best",
            "without",
            "results",
            "total",
            "bold",
            "zhen",
            "114×times",
            "times↓downarrow",
            "batching",
            "107×times",
            "enzh",
            "comparison",
            "100×times",
            "15b",
            "average",
            "utterances",
            "unisssmall",
            "language",
            "125×times",
            "size",
            "uniss",
            "05b",
            "speedup↑uparrow",
            "model",
            "speechbleu↑uparrow"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cmots.github.io/uniss-demo\" title=\"\">https://cmots.github.io/uniss-demo</a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "language",
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most recently, the application of large language models (LLMs) to generative speech tasks has further accelerated the development of S2ST systems. Current approaches typically fall into two categories: (1) single-stage methods, which directly predict multi-stream acoustic tokens autoregressively via multi-head outputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite>; and (2) two-stage pipelines, which first generate semantic tokens autoregressively, followed by another autoregressive (AR) model to predict acoustic tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>; Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Gong and Veluri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>)</cite> use a single AR language model to jointly model semantic and partial acoustic tokens, along with a non-autoregressive (NAR) model for complete acoustic information.\nWhile these approaches have shown promising results, they also introduce significantly more architectural complexity than textual LLMs. Additionally, they treat the LLM as a sequence-to-sequence converter, failing to leverage the pre-trained knowledge for textual translation embedded within the LLM.</p>\n\n",
                "matched_terms": [
                    "results",
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of architectural complexity, we present <span class=\"ltx_text ltx_font_bold\">UniSS</span>, a unified single-stage S2ST framework that preserves timbre, emotion, and duration consistency.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> demonstrates our approach and performance results. UniSS builds upon pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> as the backbone, modeling content and speaker information in a single AR language model without architectural modifications. We transfer the LLM&#8217;s pre-trained text translation capabilities to the speech domain through a cross-modality chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib15\" title=\"\">2022</a>)</cite>. During inference, our Quality Mode guides the model to decompose the complex S2ST task into sequential listen, translate, and speak steps within a single inference pass. We also introduce a Performance Mode that trades translation fidelity for faster inference, enabling deployment across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "without",
                    "inference",
                    "results",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Unified Single-Stage Architecture for S2ST:</span> We propose UniSS, a single-stage AR model for expressive S2ST that directly leverages a pre-trained textual LLM, eliminating the architectural complexity of prior work.</p>\n\n",
                "matched_terms": [
                    "model",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of LLMs in text processing has inspired their adaptation to S2ST through speech tokenization. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib21\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib22\" title=\"\">2025</a>)</cite> focus primarily on semantic translation without preserving voice characteristics.\nTo address voice preservation, some works model acoustic features using multiple AR models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>; Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>)</cite>. Recent unified approaches attempt to model both semantic and acoustic features within a single AR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>; Gong and Veluri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite>. However, they still require an additional NAR model for complete acoustic generation. Hibiki&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite> proposes a multi-stream architecture processing source and target speech simultaneously, but necessitates substantial architectural modifications, including nested Transformers and training from scratch.\nA fundamental limitation across these approaches is their reliance on complex architectures to complete acoustic representations. Moreover, they fail to explicitly leverage the LLM&#8217;s pre-trained text translation capabilities, treating them merely as generic sequence converters. UniSS addresses both limitations through a single-stage architecture while effectively transferring LLM&#8217;s text translation capabilities to speech.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 UniSS Architecture &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, UniSS is a unified AR language model for expressive speech-to-speech translation, built upon a single-stage architecture with a cross-modal chain-of-thought prompting framework, and optimized via a progressive training strategy. This section details the three core components of UniSS: model architecture, cross-modal CoT prompting, and training methodology.</p>\n\n",
                "matched_terms": [
                    "language",
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This unified design allows UniSS to handle expressive S2ST without intermediate acoustic representations or cascaded systems, maintaining fidelity in both content and voice.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build our generation model upon the pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite>, a strong LLM backbone. By expanding the model vocabulary to include the discrete speech tokens, UniSS treats speech and text uniformly as token sequences, allowing both modalities to be processed within the same transformer architecture.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"c_{task}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{task}</annotation></semantics></math>, <math alttext=\"c_{lang}^{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">c_{lang}^{tgt}</annotation></semantics></math>, and <math alttext=\"c_{speed}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{speed}</annotation></semantics></math> are special tokens specifying the task mode, target language, and duration ratio between source and target speech, respectively. A special begin-of-translation token, <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">BOT</span>, signals the model to begin generation, producing an output sequence <math alttext=\"\\tau_{out}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mrow><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tau_{out}</annotation></semantics></math> that is terminated by an end-of-decoding token, <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">EOD</span>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quality Mode.</span> This mode follows the full CoT prompting path to maximize translation fidelity. The model first <span class=\"ltx_text ltx_font_italic\">listens</span> by generating the source transcription <math alttext=\"\\mathbf{T}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{src}</annotation></semantics></math>, then <span class=\"ltx_text ltx_font_italic\">translates</span> it into the text in the target language <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>, and finally <span class=\"ltx_text ltx_font_italic\">speaks</span> by generating target semantic tokens <math alttext=\"\\mathbf{S}^{sem}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}_{tgt}</annotation></semantics></math>. Prompted with <math alttext=\"c_{task}=\\text{Quality Mode}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><mo>=</mo><mtext>Quality Mode</mtext></mrow><annotation encoding=\"application/x-tex\">c_{task}=\\text{Quality Mode}</annotation></semantics></math>, this explicit chain allows the model to leverage its robust text translation abilities, formally represented as:</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here <math alttext=\"c_{lang}^{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">c_{lang}^{src}</annotation></semantics></math> denotes the source language, and <math alttext=\"\\mathbf{S}^{sem}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}_{src}</annotation></semantics></math> denotes the semantic tokens from source speech. This phase endows the model with robust speech understanding and generation while prioritizing the preservation of text translation capabilities.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation and Target Speech Synthesis.</span>\nAfter source cleaning, <math alttext=\"\\mathbf{T}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{src}</annotation></semantics></math> is translated by Qwen2.5-72B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> to produce target text <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math> in another language.\nPrompts used in translation are shown in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.SS2\" title=\"E.2 Text Translation Step &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.\nWe then apply an expressive TTS model, SparkTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib23\" title=\"\">2025</a>)</cite>, to synthesize the target speech <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> from <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m4\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>, conditioned by <math alttext=\"\\mathbf{X}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m5\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{src}</annotation></semantics></math> to preserve the source speaker&#8217;s voice. To enable fine-grained speed control, we calculate the duration ratio between source and target speech and discretize it into speed tokens <math alttext=\"\\mathbf{c}_{speed}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m6\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{speed}</annotation></semantics></math> with 0.1 intervals. This creates complete parallel samples <math alttext=\"(\\mathbf{X}_{src},\\mathbf{T}_{src},\\mathbf{T}_{tgt},\\mathbf{Y}_{tgt},\\mathbf{c}_{speed})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>&#119836;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{X}_{src},\\mathbf{T}_{src},\\mathbf{T}_{tgt},\\mathbf{Y}_{tgt},\\mathbf{c}_{speed})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "speed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS is trained in three progressive phases, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS3\" title=\"3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>. Phase 1 establishes text-speech alignment using 77.1k hours of speech data and MT text data from WMT17&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bojar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib28\" title=\"\">2017</a>)</cite>. Phase 2 introduces CoT prompting with our UniST General dataset, mixed with Phase 1 data at a 2:1 ratio. Phase 3 fine-tunes the model exclusively on the UniST High-Quality dataset.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training employs the AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov and Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib29\" title=\"\">2019</a>)</cite> with a 2.3M-token batch size, weight decay of 0.1, and momentum parameters (0.9, 0.95). All audio is resampled to 16&#160;kHz, and the LLM vocabulary is expanded to 180,407 to include speech and control tokens. Learning rates progress from 8e-4 in Phase 1 to 2e-4 in Phase 2, and finally anneal from 5e-5 to 5e-6 in Phase 3. The model is trained on 16 NVIDIA H800 80G GPUs using the Megatron-LM Framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shoeybi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib30\" title=\"\">2019</a>)</cite> for efficient large-model training. Complete training details and hyperparameters are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.SS1\" title=\"B.1 UniSS &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our primary S2ST evaluation, we use the Chinese (ZH) and English (EN) test sets from CVSS-T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite>, which contain 4,897 utterance pairs (8.2 hours for Chinese and 6.3 hours for English). For emotion preservation evaluation, we randomly sample 300 utterances from each emotional speech dataset (ESD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib31\" title=\"\">2021</a>)</cite> and CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib32\" title=\"\">2014</a>)</cite>), covering a selected set of 4 emotions (happy, sad, angry, neutral). We evaluate both English-to-Chinese (EN-ZH) and Chinese-to-English (ZH-EN) translation directions.</p>\n\n",
                "matched_terms": [
                    "utterances",
                    "enzh",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Large Language Models (MLLMs).</span>\nTo benchmark against the latest generation of general-purpose models, we include two leading MLLMs: <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib39\" title=\"\">2024</a>)</cite>, an enterprise-level model from OpenAI with strong speech-to-speech capability; and <span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib40\" title=\"\">2025</a>)</cite>, a powerful open-source MLLM building on large-scale audio&#8211;language pretraining, enabling both speech understanding and synthesis.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For UniSS experiments, we deploy vLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib42\" title=\"\">2023</a>)</cite> to support inference. We set a decoding temperature of 0.7, top-k of -1, top-p of 0.8, and a repetition penalty of 1.1. We report results for both Performance mode, denoted as <span class=\"ltx_text ltx_font_bold\">UniSS (P)</span>, and Quality mode, denoted as <span class=\"ltx_text ltx_font_bold\">UniSS (Q)</span>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uniss",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T1\" title=\"Table 1 &#8227; 5.3 Objective Performance &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main comparison results on the CVSS-T dataset.\nThe results clearly demonstrate that UniSS establishes a new SOTA in translation fidelity\nwhile maintaining voice characteristics, prosody, duration consistency, and speech quality, validating our core design principles. Additional results on other datasets and tasks are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4\" title=\"Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uniss",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation Fidelity.</span> UniSS achieves state-of-the-art translation fidelity on both EN-ZH and ZH-EN directions. The UniSS (Q) variant achieves a Speech-BLEU of 32.20 on EN-ZH and 24.28 on ZH-EN, substantially outperforming all prior end-to-end and cascaded baselines. The efficient UniSS (P) also delivers strong results, surpassing most existing systems. Notably, in terms of intermediate text metrics, UniSS models perform on par with or better than larger multimodal LLM-based approaches.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "enzh",
                    "results",
                    "uniss",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody Preservation.</span> In terms of prosody, UniSS achieves competitive performance. UniSS (P) variant achieves the second-highest A.PCP score (2.73 and 2.75), closely following Seamless-Ex, which incorporates a dedicated prosody encoder. The performance gap is marginal at only 0.10 (EN-ZH) and 0.12 (ZH-EN), highlighting the efficacy of UniSS in preserving prosodic patterns without specialized modules.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "enzh",
                    "zhen",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Duration Consistency.</span> UniSS demonstrates superior duration consistency. UniSS (Q) achieves near-optimal SLC 0.2 scores on EN-ZH and the best performance on ZH-EN, improving over the previous best end-to-end system (Seamless-Ex) by 44% and 67%. On the more relaxed SLC 0.4 metric, while competing systems achieve scores above 0.90, both UniSS variants deliver near-perfect performance with scores of 0.99 (EN-ZH) and 0.97 (ZH-EN).</p>\n\n",
                "matched_terms": [
                    "best",
                    "uniss",
                    "enzh",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Quality.</span> UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (P) achieves a competitive UTMOS score of 3.77, matching cascaded models (3.76 and 3.79) while surpassing all end-to-end models. In the ZH-EN direction, UniSS surpasses 3-stage systems with a score of 3.86 compared to 3.50.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "enzh",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Preservation.</span>\nUniSS effectively retains speaker voice characteristics without requiring an additional NAR stage-2 model. For speaker similarity, UniSS (Q) achieves a score of 4.42, outperforming all other models. This represents a 0.07 improvement over the 2-stage system (4.35), which deploys a carefully designed TTS model. These results underscore the ability of UniSS to maintain voice characteristics in an end-to-end fashion.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uniss",
                    "model",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Naturalness.</span>\nThe naturalness of speech from UniSS is also favorably rated. UniSS (Q) achieves a naturalness MOS of 4.45, surpassing Seamless-Ex (3.10) and the 3-stage cascaded baseline (4.31) that incorporates a specialized TTS model. This demonstrates UniSS&#8217;s ability to generate high-quality, natural-sounding speech without dedicated text-to-speech components.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework provides flexible control over the quality-efficiency trade-off through its different CoT prompting modes.\nWe evaluate inference speed on the AR language model component using the Transformers library&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wolf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib43\" title=\"\">2020</a>)</cite>,\nusing 400 utterances (200 per direction) from CVSS-T on a single H800 GPU without batching.</p>\n\n",
                "matched_terms": [
                    "language",
                    "batching",
                    "without",
                    "inference",
                    "speed",
                    "utterances",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Subjective Results &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the Performance mode achieves a 1.07<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p2.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> speedup over Quality mode with only a 1.84 point reduction in Speech-BLEU, demonstrating a favorable speed-quality trade-off.\nFurthermore, we trained <span class=\"ltx_text ltx_font_bold\">UniSS-Small</span> based on Qwen2.5-0.5B-Instruct, achieving significant computational savings.\nUniSS-Small (P) delivers 1.25<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p2.m2\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> speedup with competitive translation fidelity (25.68 Speech-BLEU),\nmaking it suitable for resource-constrained deployment scenarios while maintaining the advantages of our unified architecture.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "125×times",
                    "107×times",
                    "unisssmall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct ablation studies to validate the effect of our design.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T4\" title=\"Table 4 &#8227; 5.6 Ablation Studies &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the detailed results comparing variants against a base model trained through Phases 1 and 2. The results in type Base and Train are evaluated in the Performance mode.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Removing the intermediate text generation and performing direct speech-to-speech translation (<span class=\"ltx_text ltx_font_bold\">Direct S2ST</span>) results in a severe performance degradation of -14.94 and -14.40 Speech-BLEU points in the inference. This demonstrates that our cross-modal CoT prompting enables the transfer of textual translation expertise to the speech domain and improves translation fidelity.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "results",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces UniSS, a unified single-stage expressive speech-to-speech translation framework that preserves voice and emotional style. Our approach eliminates architectural complexity through well-designed speech semantic and style modeling, enabling seamless integration with pre-trained LLMs. By transferring LLMs&#8217; text translation capabilities to speech via cross-modal CoT prompting, UniSS surpasses previous S2ST systems across multiple dimensions: translation fidelity, speech naturalness, and preservation of voice, emotion, and duration consistency.\nBeyond the model architecture, we design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English expressive S2ST dataset. Our work demonstrates a simple and effective approach for building the next generation of expressive S2ST systems.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Works. </span>\nCurrent limitations point to several research directions. (1) Language Support: We trained UniSS only on Chinese and English due to resource limitations. The data construction pipeline and training framework can be easily extended to multilingual scenarios, which represents our immediate next step. (2) Speech Tokenizer: While the linguistic, speaker, and semantic tokenizers perform well in UniSS, they originate from two different audio tokenizers, causing vocabulary size expansion. Future work will focus on training a unified tokenizer to merge these components and reduce vocabulary size.</p>\n\n",
                "matched_terms": [
                    "language",
                    "size",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T1\" title=\"Table B1 &#8227; B.1 UniSS &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B1</span></a>, the UniSS is trained in three phases progressively.\nAll audio is resampled to 16&#160;kHz. The LLM vocabulary is expanded to 180,407 to include speech and control tokens.\nWe use the AdamW optimizer with a weight decay of 0.1 and momentum parameters (0.9, 0.95). The batch size is fixed at 2.3M tokens for all phases.</p>\n\n",
                "matched_terms": [
                    "size",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce CoT prompting using our UniST General dataset.\nThe model learns three generation paths: Performance mode, Quality mode, and direct S2ST data without intermediate text.\nNew data is mixed with Phase 1 data at a 2:1 ratio, totaling approximately 55B tokens. We train for one epoch with a constant learning rate of 2e-4 and a 5% warm-up.</p>\n\n",
                "matched_terms": [
                    "model",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune the model exclusively on the UniST High-quality dataset.\nThe learning rate follows a cosine schedule, gradually annealing from 5e-5 to 5e-6 over 1 epoch without warm-up. This phase consists of approximately 10B training tokens. During this stage, the training loss reaches its minimum around 0.9 epochs, and we select the checkpoint at this point as our final model weights.</p>\n\n",
                "matched_terms": [
                    "model",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our language model is trained on 16 NVIDIA H800 80G GPUs. We utilize the Megatron-LM Framework for efficient large-model training.\nBecause audio duration distribution is uneven, padding would waste significant computational resources. We use the sequence packing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Krell et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib44\" title=\"\">2022</a>]</cite> technique to concatenate multiple samples into a single 18k token long sequence. We use a global batch size of 128. Completing all three phases of training takes approximately 6 days.</p>\n\n",
                "matched_terms": [
                    "language",
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide detailed descriptions of all baseline models evaluated against our proposed UniSS framework. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T2\" title=\"Table B2 &#8227; B.2 Baselines &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B2</span></a> summarizes the baselines, model versions, and parameter sizes used in our experiments. All baseline models are evaluated using their default inference parameters without any additional tuning or modifications. Evaluation is conducted directly on the CVSS-T and FLEURS test sets without extra data preprocessing, prompts, or output post-processing. We present the implementation specifics of each baseline in the following subsections.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model",
                    "without",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Speech (TTS)</span>: We employ CosyVoice 2 (0.5B), a transformer-based expressive TTS model designed for high-quality multilingual speech synthesis. Its ability to generate natural and expressive audio makes it suitable for our evaluation.</p>\n\n",
                "matched_terms": [
                    "05b",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen2.5-Omni is selected for its ability to perform fully end-to-end speech-to-speech translation without requiring intermediate text generation, making it a promising multimodal LLM baseline for direct S2ST tasks. In our experiments, we find that the model&#8217;s performance is highly sensitive to the prompt format. When using a simple default instruction, the model sometimes appends assistant-like phrases (e.g., &#8220;Do you need anything else?&#8221;) to the translated speech, likely due to its conversational fine-tuning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "without"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeamlessM4T Medium and Large-v2 are multilingual speech translation models that support speech-to-speech translation without explicit intermediate transcription or synthesis stages. These models adopt a unified encoder-decoder architecture and are designed to support both low-resource and high-resource language pairs. In our experiments, we use the official inference pipelines provided by Meta with default decoding parameters. No additional prompt tuning, input preprocessing, or output filtering is applied.</p>\n\n",
                "matched_terms": [
                    "language",
                    "without",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeamlessExpressive extends SeamlessM4T by introducing expressiveness-aware modeling, enabling more natural and emotionally rich speech synthesis in the target language. The model conditions generation on prosodic and expressive cues from the source speech and is particularly well-suited for conversational and affective scenarios. We evaluate SeamlessExpressive using its official inference scripts and default hyperparameters.</p>\n\n",
                "matched_terms": [
                    "language",
                    "model",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Seed LiveInterpret 2.0 is an enterprise-level simultaneous interpretation model that performs real-time speech-to-speech translation with voice cloning capabilities. The model employs a duplex framework that processes input audio and generates target-language speech directly without intermediate text representations. In our evaluation, we used the official inference API with default parameters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "without",
                    "inference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate on the Chinese and English test subsets of FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib48\" title=\"\">2022</a>]</cite>, a massively multilingual speech dataset spanning 102 languages with n-way parallel speech and text data. UniSS demonstrates robust performance on this challenging multilingual benchmark across multiple evaluation tasks. Additionally, UniSS achieves state-of-the-art performance on speech-to-text translation (S2TT) and competitive results on automatic speech recognition (ASR) and text-to-speech synthesis (TTS) tasks.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T3\" title=\"Table D3 &#8227; D.1 S2ST Performance on Fleurs &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D3</span></a>, UniSS (Q) achieves strong translation performance with Speech-BLEU of 36.16 and Text-BLEU of 36.96 on EN-ZH, outperforming all S2ST baselines and cascaded systems. UniSS (P) also demonstrates competitive performance across both metrics. In the ZH-EN direction, UniSS surpasses all systems with comparable model size, demonstrating the parameter efficiency of our design.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "enzh",
                    "size",
                    "model",
                    "uniss",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS exhibits excellent prosody transfer capabilities on the FLEURS benchmark. UniSS (Q) achieves A.PCP scores of 2.72 (EN-ZH) and 2.64 (ZH-EN), outperforming all end-to-end baselines and matching GPT-4o&#8217;s performance (2.72 and 2.39). Notably, UniSS achieves these results without incorporating dedicated prosody modeling modules, highlighting the effectiveness of our unified framework.</p>\n\n",
                "matched_terms": [
                    "enzh",
                    "without",
                    "results",
                    "uniss",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (Q) attains a UTMOS score of 3.41, outperforming all open-source baselines and surpassing GPT-4o (3.36). UniSS (Q) also leads all S2ST models in the ZH-EN direction with a score of 4.16. Despite its significantly smaller size, UniSS consistently produces high-quality speech across both translation directions, confirming its effectiveness as a practical open-source solution for speech-to-speech translation.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "enzh",
                    "zhen",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although UniSS was not specifically designed for automatic speech recognition, it demonstrates competitive ASR capabilities as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. We evaluate the ASR performance on the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib49\" title=\"\">2015</a>]</cite>. On the LibriSpeech test-clean subset, UniSS achieves a WER of 2.4, which is comparable to the open-source state-of-the-art ASR model, Whisper (1.8 WER). However, on the more challenging LibriSpeech test-others subset, UniSS shows degraded performance with a WER of 6.6. It is important to note that UniSS is primarily designed as a speech-to-speech translation model, and these ASR results demonstrate its versatility beyond its core translation capabilities.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although UniSS was not specifically designed for text-to-speech synthesis, we evaluate its TTS capabilities on the LibriSpeech test-clean dataset as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. UniSS achieves a WER of 4.75 and speaker similarity of 0.568, compared to SparkTTS, which obtains 1.98 WER and 0.584 similarity. The performance gap is expected given that SparkTTS is specifically optimized for TTS, whereas UniSS is primarily designed for speech-to-speech translation. These results demonstrate that UniSS can generate intelligible speech output.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speaker tokenizer is used to represent global speaker information. We hypothesize that this voice decoupling reduces the difficulty of speech translation by allowing the model to focus more on the linguistic content. We employ different linguistic and semantic tokenizers for understanding and generation to optimize each task. To generate more natural and expressive audio, the generation process uses semantic tokens, which retain information beyond the core content. However, this additional information makes content understanding more challenging. Therefore, the linguistic tokenizer, which focuses solely on content information, is better suited for the speech understanding phase. As a simple speech representation, we believe this design does not introduce greater complexity in either training or inference.</p>\n\n",
                "matched_terms": [
                    "model",
                    "inference"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
        "caption": "Table 4: Ablation study on UniSS components evaluated on CVSS-T test set. w/ and w/o denote ‘with’ and ‘without’, respectively. Best scores are in bold.",
        "body": "Type\nVariant\nSpeech-BLEU↑\\uparrow\nΔ\\Delta\n\n\nEN-ZH\nZH-EN\nEN-ZH\nZH-EN\n\n\nBase\nPhase 1+2\n29.38\n21.55\n-\n-\n\n\nTrain\nw/ Phase 3\n30.28\n23.61\n+0.90\n+2.06\n\n\nUniST only\n22.20\n11.40\n-7.18\n-10.15\n\n\nw/o GLM\n14.37\n12.82\n-15.01\n-8.73\n\n\nInfer\nDirect S2ST\n14.44\n7.15\n-14.94\n-14.40",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">Type</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">Variant</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">Speech-BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">EN-ZH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">ZH-EN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">EN-ZH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">ZH-EN</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Base</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Phase 1+2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">29.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">21.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Train</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">w/ Phase 3</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">30.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\"><span class=\"ltx_text ltx_font_bold\">23.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">+0.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">+2.06</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">UniST only</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">22.20</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">11.40</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-7.18</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-10.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">w/o GLM</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">14.37</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">12.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-15.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-8.73</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Infer</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">Direct S2ST</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">14.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">7.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-14.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.3pt;padding-right:4.3pt;\">-14.40</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "‘without’",
            "components",
            "train",
            "respectively",
            "type",
            "study",
            "‘with’",
            "base",
            "best",
            "only",
            "phase",
            "bold",
            "scores",
            "zhen",
            "direct",
            "infer",
            "ablation",
            "evaluated",
            "enzh",
            "δdelta",
            "unist",
            "cvsst",
            "test",
            "glm",
            "set",
            "denote",
            "s2st",
            "uniss",
            "speechbleu↑uparrow",
            "variant"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We conduct ablation studies to validate the effect of our design.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T4\" title=\"Table 4 &#8227; 5.6 Ablation Studies &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the detailed results comparing variants against a base model trained through Phases 1 and 2. The results in type Base and Train are evaluated in the Performance mode.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cmots.github.io/uniss-demo\" title=\"\">https://cmots.github.io/uniss-demo</a>.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "unist",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A conventional cascaded S2ST system typically consists of three sequential components: automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech (TTS) synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Casacuberta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib4\" title=\"\">2004</a>; Nakamura et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib5\" title=\"\">2006</a>)</cite>. However, this cascaded architecture often suffers from error accumulation across stages and struggles to retain paralinguistic features of the original speech. To address these limitations, subsequent research has shifted towards end-to-end approaches that aim to translate speech to another language while preserving expressive characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib7\" title=\"\">2024</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib8\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "components"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of architectural complexity, we present <span class=\"ltx_text ltx_font_bold\">UniSS</span>, a unified single-stage S2ST framework that preserves timbre, emotion, and duration consistency.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> demonstrates our approach and performance results. UniSS builds upon pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> as the backbone, modeling content and speaker information in a single AR language model without architectural modifications. We transfer the LLM&#8217;s pre-trained text translation capabilities to the speech domain through a cross-modality chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib15\" title=\"\">2022</a>)</cite>. During inference, our Quality Mode guides the model to decompose the complex S2ST task into sequential listen, translate, and speak steps within a single inference pass. We also introduce a Performance Mode that trades translation fidelity for faster inference, enabling deployment across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realizing the vision outlined above demands large-scale, high-quality training data that preserves both translation accuracy and speaker expressiveness. However, existing S2ST datasets are either small-scale to train powerful unified models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite> or suffer from quality control issues when scraped from the web&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite>. To address this data challenge, we design a scalable synthesis pipeline and contribute UniST, a 44.8k-hour Chinese-English S2ST dataset offering high translation fidelity and rich speaker preservation.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "unist",
                    "train"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Unified Single-Stage Architecture for S2ST:</span> We propose UniSS, a single-stage AR model for expressive S2ST that directly leverages a pre-trained textual LLM, eliminating the architectural complexity of prior work.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Large-Scale, High-Quality S2ST Dataset:</span> We design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English S2ST dataset with speaker voice and emotion preservation.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "unist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation must preserve semantic accuracy during cross-lingual conversion. Traditional cascaded systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Casacuberta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib4\" title=\"\">2004</a>; Nakamura et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib5\" title=\"\">2006</a>)</cite> chain ASR, MT, and TTS components sequentially, suffering from error accumulation and information loss through text bottlenecks. Early direct methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib1\" title=\"\">2019</a>)</cite> encountered significant challenges with translation quality and synthesis artifacts.\nThe breakthrough came with discrete unit-based methods, where speech-to-unit translation (S2UT) approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib2\" title=\"\">2022</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib17\" title=\"\">2021</a>; Inaguma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib3\" title=\"\">2023</a>)</cite> employ discrete speech units (subwords, phonemes, or semantic tokens) as intermediate representations, which enables effective disentanglement of linguistic content from acoustic properties. SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite> achieved robust multilingual performance via unified multitask optimization of translation and synthesis.\nRecent systems focus on enhancing expressiveness while maintaining translation fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib18\" title=\"\">2022b</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib19\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib7\" title=\"\">2024</a>)</cite>. SeamlessExpressive&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite> designs a PRETSSEL vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib20\" title=\"\">2024</a>)</cite> to enhance expressiveness preservation. TransVIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib8\" title=\"\">2024</a>)</cite> employs feature disentanglement to separately model semantic, acoustic, and temporal information for voice and isochrony control.</p>\n\n",
                "matched_terms": [
                    "components",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of LLMs in text processing has inspired their adaptation to S2ST through speech tokenization. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib21\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib22\" title=\"\">2025</a>)</cite> focus primarily on semantic translation without preserving voice characteristics.\nTo address voice preservation, some works model acoustic features using multiple AR models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>; Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>)</cite>. Recent unified approaches attempt to model both semantic and acoustic features within a single AR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>; Gong and Veluri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite>. However, they still require an additional NAR model for complete acoustic generation. Hibiki&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite> proposes a multi-stream architecture processing source and target speech simultaneously, but necessitates substantial architectural modifications, including nested Transformers and training from scratch.\nA fundamental limitation across these approaches is their reliance on complex architectures to complete acoustic representations. Moreover, they fail to explicitly leverage the LLM&#8217;s pre-trained text translation capabilities, treating them merely as generic sequence converters. UniSS addresses both limitations through a single-stage architecture while effectively transferring LLM&#8217;s text translation capabilities to speech.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 UniSS Architecture &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, UniSS is a unified AR language model for expressive speech-to-speech translation, built upon a single-stage architecture with a cross-modal chain-of-thought prompting framework, and optimized via a progressive training strategy. This section details the three core components of UniSS: model architecture, cross-modal CoT prompting, and training methodology.</p>\n\n",
                "matched_terms": [
                    "components",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of UniSS is to perform expressive S2ST by modeling the conditional distribution <math alttext=\"P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo fence=\"false\">|</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})</annotation></semantics></math>, where <math alttext=\"\\mathbf{X}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{src}</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> denote the source and target speech waveforms, respectively. To achieve faithful content translation while preserving speaker identity and expressiveness, UniSS introduces three types of speech tokens: speaker tokens <math alttext=\"\\mathbf{S}^{spk}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{spk}</annotation></semantics></math> with fixed sequence length to capture global style attributes, e.g., timbre, prosody, and emotion; linguistic tokens <math alttext=\"\\mathbf{S}^{ling}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{ling}</annotation></semantics></math> to encode the content of the source utterance; and semantic tokens <math alttext=\"\\mathbf{S}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}</annotation></semantics></math> to represent the expressive target utterance and can be directly decoded into waveform when combined with speaker tokens.</p>\n\n",
                "matched_terms": [
                    "denote",
                    "uniss",
                    "s2st",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This unified design allows UniSS to handle expressive S2ST without intermediate acoustic representations or cascaded systems, maintaining fidelity in both content and voice.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the S2ST task, the source waveform <math alttext=\"\\mathbf{X_{src}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;&#119851;&#119836;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{src}}</annotation></semantics></math> is represented by <math alttext=\"(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})</annotation></semantics></math>, while the target waveform <math alttext=\"\\mathbf{Y_{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119853;&#119840;&#119853;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{tgt}}</annotation></semantics></math> is represented by <math alttext=\"\\mathbf{S}_{tgt}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}_{tgt}^{sem}</annotation></semantics></math>. This design is motivated by our preliminary experiments, which revealed that although BiCodec&#8217;s semantic tokens are highly effective for waveform reconstruction, their self-supervised nature makes them suboptimal for content understanding. By separately representing speaker identity, linguistic content, and generation-oriented semantics, UniSS enables more accurate and controllable S2ST modeling.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 2: S2ST with CoT.</span>\nIn the second phase, we introduce the core S2ST task. The model is trained to generate outputs using the CoT prompting formats described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS2\" title=\"3.2 Cross-Modal Chain-of-Thought Prompting &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, as well as a simplified direct generation mode that bypasses intermediate text outputs:</p>\n\n",
                "matched_terms": [
                    "phase",
                    "s2st",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 3: Refinement.</span>\nIn the final phase, we fine-tune the model on the full S2ST task using both CoT prompting modes.\nThis phase uses an annealed learning rate to stabilize the learned CoT patterns and optimize the final translation performance.</p>\n\n",
                "matched_terms": [
                    "phase",
                    "s2st"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training end-to-end expressive S2ST models faces a significant bottleneck due to the scarcity of large-scale, parallel data that preserves speaker characteristics. To address this limitation, we design a scalable data synthesis pipeline. The pipeline generates UniST, our large-scale Chinese-English expressive S2ST dataset. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) illustrates the data construction process.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "unist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b,c), UniST General provides greater data diversity and generalization potential\ndue to its larger range of duration ratios and speech lengths. UniST High-Quality focuses on temporal consistency, making it ideal for the final refinement phase of our progressive training. In Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.SS3\" title=\"E.3 Analysis of UniST &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E.3</span></a>, we show that the UniST dataset also preserves emotional style in synthesis.</p>\n\n",
                "matched_terms": [
                    "phase",
                    "unist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS is trained in three progressive phases, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS3\" title=\"3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>. Phase 1 establishes text-speech alignment using 77.1k hours of speech data and MT text data from WMT17&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bojar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib28\" title=\"\">2017</a>)</cite>. Phase 2 introduces CoT prompting with our UniST General dataset, mixed with Phase 1 data at a 2:1 ratio. Phase 3 fine-tunes the model exclusively on the UniST High-Quality dataset.</p>\n\n",
                "matched_terms": [
                    "phase",
                    "unist",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our primary S2ST evaluation, we use the Chinese (ZH) and English (EN) test sets from CVSS-T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite>, which contain 4,897 utterance pairs (8.2 hours for Chinese and 6.3 hours for English). For emotion preservation evaluation, we randomly sample 300 utterances from each emotional speech dataset (ESD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib31\" title=\"\">2021</a>)</cite> and CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib32\" title=\"\">2014</a>)</cite>), covering a selected set of 4 emotions (happy, sad, angry, neutral). We evaluate both English-to-Chinese (EN-ZH) and Chinese-to-English (ZH-EN) translation directions.</p>\n\n",
                "matched_terms": [
                    "enzh",
                    "set",
                    "s2st",
                    "zhen",
                    "cvsst",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ comprehensive objective and subjective metrics to evaluate S2ST performance. For translation fidelity, we measure BLEU scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib33\" title=\"\">2002</a>)</cite> on both ASR-transcribed speech outputs (Speech-BLEU) and intermediate text (Text-BLEU). Prosody preservation is assessed through prosody similarity (A.PCP)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Andrews et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib34\" title=\"\">2022</a>)</cite>. Duration consistency is evaluated using Speech Length Compliance (SLC) within 0.2 and 0.4 ratio tolerance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nSpeech quality is measured using UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib36\" title=\"\">2022</a>)</cite>. Additionally, we conduct a subjective Mean Opinion Score (MOS) evaluation where bilingual speakers rate emotion similarity, speaker similarity, and naturalness on a 5-point scale. Detailed descriptions of all evaluation metrics and implementation procedures are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A3\" title=\"Appendix C Evaluation Metrics &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "evaluated",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare UniSS against a strong and diverse set of state-of-the-art (SOTA) systems. Details of baseline implementation are introduced in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.SS2\" title=\"B.2 Baselines &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End S2ST Systems.</span>\nWe compare against dedicated S2ST models that represent the current open-source SOTA: <span class=\"ltx_text ltx_font_bold\">SeamlessM4T</span> (Medium and Large V2, denoted Seamless-M and Seamless-L), and its expressive variant <span class=\"ltx_text ltx_font_bold\">SeamlessExpressive</span> (Seamless-Ex). For subjective metrics, we also evaluate <span class=\"ltx_text ltx_font_bold\">Seed LiveInterpret 2.0</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib41\" title=\"\">2025</a>)</cite>, denoted as Seed Live, an enterprise-level S2ST system with duplex speech understanding and generation abilities.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "variant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For UniSS experiments, we deploy vLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib42\" title=\"\">2023</a>)</cite> to support inference. We set a decoding temperature of 0.7, top-k of -1, top-p of 0.8, and a repetition penalty of 1.1. We report results for both Performance mode, denoted as <span class=\"ltx_text ltx_font_bold\">UniSS (P)</span>, and Quality mode, denoted as <span class=\"ltx_text ltx_font_bold\">UniSS (Q)</span>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T1\" title=\"Table 1 &#8227; 5.3 Objective Performance &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main comparison results on the CVSS-T dataset.\nThe results clearly demonstrate that UniSS establishes a new SOTA in translation fidelity\nwhile maintaining voice characteristics, prosody, duration consistency, and speech quality, validating our core design principles. Additional results on other datasets and tasks are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4\" title=\"Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "cvsst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation Fidelity.</span> UniSS achieves state-of-the-art translation fidelity on both EN-ZH and ZH-EN directions. The UniSS (Q) variant achieves a Speech-BLEU of 32.20 on EN-ZH and 24.28 on ZH-EN, substantially outperforming all prior end-to-end and cascaded baselines. The efficient UniSS (P) also delivers strong results, surpassing most existing systems. Notably, in terms of intermediate text metrics, UniSS models perform on par with or better than larger multimodal LLM-based approaches.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "enzh",
                    "zhen",
                    "variant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody Preservation.</span> In terms of prosody, UniSS achieves competitive performance. UniSS (P) variant achieves the second-highest A.PCP score (2.73 and 2.75), closely following Seamless-Ex, which incorporates a dedicated prosody encoder. The performance gap is marginal at only 0.10 (EN-ZH) and 0.12 (ZH-EN), highlighting the efficacy of UniSS in preserving prosodic patterns without specialized modules.</p>\n\n",
                "matched_terms": [
                    "only",
                    "enzh",
                    "uniss",
                    "zhen",
                    "variant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Duration Consistency.</span> UniSS demonstrates superior duration consistency. UniSS (Q) achieves near-optimal SLC 0.2 scores on EN-ZH and the best performance on ZH-EN, improving over the previous best end-to-end system (Seamless-Ex) by 44% and 67%. On the more relaxed SLC 0.4 metric, while competing systems achieve scores above 0.90, both UniSS variants deliver near-perfect performance with scores of 0.99 (EN-ZH) and 0.97 (ZH-EN).</p>\n\n",
                "matched_terms": [
                    "best",
                    "enzh",
                    "zhen",
                    "uniss",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Quality.</span> UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (P) achieves a competitive UTMOS score of 3.77, matching cascaded models (3.76 and 3.79) while surpassing all end-to-end models. In the ZH-EN direction, UniSS surpasses 3-stage systems with a score of 3.86 compared to 3.50.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss",
                    "enzh",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Preservation.</span>\nUniSS (Q) demonstrates strong capability in preserving emotions, achieving a MOS of 4.51. This represents a substantial 27% improvement over the expressive S2ST baseline Seamless-Ex (3.56) and surpasses the 3-stage cascaded system (4.48). Notably, UniSS (Q) also approaches Seed Live (4.56), indicating its ability in capturing emotional nuance.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Naturalness.</span>\nThe naturalness of speech from UniSS is also favorably rated. UniSS (Q) achieves a naturalness MOS of 4.45, surpassing Seamless-Ex (3.10) and the 3-stage cascaded baseline (4.31) that incorporates a specialized TTS model. This demonstrates UniSS&#8217;s ability to generate high-quality, natural-sounding speech without dedicated text-to-speech components.</p>\n\n",
                "matched_terms": [
                    "components",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our three-phase progressive training significantly impacts performance.\nPhase 3 refinement (<span class=\"ltx_text ltx_font_bold\">w/ Phase 3</span>) contributes improvements of +0.90 and +2.06 Speech-BLEU points, validating the importance of high-quality data fine-tuning for final optimization.\nRemoving the initial Phase 1 alignment (<span class=\"ltx_text ltx_font_bold\">UniST only</span>) causes severe performance degradation of -7.18 and -10.15 points. This drop demonstrates that Phase 1 text-speech alignment is essential for subsequent S2ST learning.</p>\n\n",
                "matched_terms": [
                    "phase",
                    "s2st",
                    "only",
                    "unist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Replacing our content-focused GLM-4 linguistic tokenizer with BiCodec&#8217;s self-supervised semantic tokens (<span class=\"ltx_text ltx_font_bold\">w/o GLM</span>) leads to significant performance degradation of -15.01 and -8.73 Speech-BLEU points.\nThis drop reveals that while BiCodec&#8217;s semantic tokens excel at speech generation tasks, their self-supervised nature limits their effectiveness for content understanding in the S2ST task.</p>\n\n",
                "matched_terms": [
                    "glm",
                    "s2st"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Removing the intermediate text generation and performing direct speech-to-speech translation (<span class=\"ltx_text ltx_font_bold\">Direct S2ST</span>) results in a severe performance degradation of -14.94 and -14.40 Speech-BLEU points in the inference. This demonstrates that our cross-modal CoT prompting enables the transfer of textual translation expertise to the speech domain and improves translation fidelity.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces UniSS, a unified single-stage expressive speech-to-speech translation framework that preserves voice and emotional style. Our approach eliminates architectural complexity through well-designed speech semantic and style modeling, enabling seamless integration with pre-trained LLMs. By transferring LLMs&#8217; text translation capabilities to speech via cross-modal CoT prompting, UniSS surpasses previous S2ST systems across multiple dimensions: translation fidelity, speech naturalness, and preservation of voice, emotion, and duration consistency.\nBeyond the model architecture, we design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English expressive S2ST dataset. Our work demonstrates a simple and effective approach for building the next generation of expressive S2ST systems.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "unist",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Works. </span>\nCurrent limitations point to several research directions. (1) Language Support: We trained UniSS only on Chinese and English due to resource limitations. The data construction pipeline and training framework can be easily extended to multilingual scenarios, which represents our immediate next step. (2) Speech Tokenizer: While the linguistic, speaker, and semantic tokenizers perform well in UniSS, they originate from two different audio tokenizers, causing vocabulary size expansion. Future work will focus on training a unified tokenizer to merge these components and reduce vocabulary size.</p>\n\n",
                "matched_terms": [
                    "components",
                    "uniss",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present real examples from different models here, with speech converted to text using ASR for display purposes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A1.F1\" title=\"Figure A1 &#8227; Appendix A Examples &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">A1</span></a>, UniSS achieves competitive BLEU scores on challenging sentences where GPT-4o struggles with translation quality. Note Seamless-L and GPT-4o cannot preserve source voice and emotional style.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce CoT prompting using our UniST General dataset.\nThe model learns three generation paths: Performance mode, Quality mode, and direct S2ST data without intermediate text.\nNew data is mixed with Phase 1 data at a 2:1 ratio, totaling approximately 55B tokens. We train for one epoch with a constant learning rate of 2e-4 and a 5% warm-up.</p>\n\n",
                "matched_terms": [
                    "train",
                    "s2st",
                    "phase",
                    "unist",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune the model exclusively on the UniST High-quality dataset.\nThe learning rate follows a cosine schedule, gradually annealing from 5e-5 to 5e-6 over 1 epoch without warm-up. This phase consists of approximately 10B training tokens. During this stage, the training loss reaches its minimum around 0.9 epochs, and we select the checkpoint at this point as our final model weights.</p>\n\n",
                "matched_terms": [
                    "phase",
                    "unist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide detailed descriptions of all baseline models evaluated against our proposed UniSS framework. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T2\" title=\"Table B2 &#8227; B.2 Baselines &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B2</span></a> summarizes the baselines, model versions, and parameter sizes used in our experiments. All baseline models are evaluated using their default inference parameters without any additional tuning or modifications. Evaluation is conducted directly on the CVSS-T and FLEURS test sets without extra data preprocessing, prompts, or output post-processing. We present the implementation specifics of each baseline in the following subsections.</p>\n\n",
                "matched_terms": [
                    "evaluated",
                    "uniss",
                    "cvsst",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen2.5-Omni is selected for its ability to perform fully end-to-end speech-to-speech translation without requiring intermediate text generation, making it a promising multimodal LLM baseline for direct S2ST tasks. In our experiments, we find that the model&#8217;s performance is highly sensitive to the prompt format. When using a simple default instruction, the model sometimes appends assistant-like phrases (e.g., &#8220;Do you need anything else?&#8221;) to the translated speech, likely due to its conversational fine-tuning.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For S2ST evaluation, we employ GPT-4o-audio-preview, which supports multimodal speech translation tasks with direct speech-to-speech generation capabilities. Given the instability of Qwen2.5-Omni under complex prompt instructions observed in preliminary experiments, we select GPT-4o-audio-preview as a more robust alternative. Our experiments confirm that GPT-4o-audio-preview demonstrates superior stability and consistently produces higher-quality outputs compared to previous models.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For direct end-to-end baselines, we evaluate several state-of-the-art S2ST systems that translate input audio directly into target-language speech, bypassing intermediate textual representations. Specifically, we include SeamlessM4T Medium, SeamlessM4T Large-v2, and SeamlessExpressive in our evaluation.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate on the Chinese and English test subsets of FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib48\" title=\"\">2022</a>]</cite>, a massively multilingual speech dataset spanning 102 languages with n-way parallel speech and text data. UniSS demonstrates robust performance on this challenging multilingual benchmark across multiple evaluation tasks. Additionally, UniSS achieves state-of-the-art performance on speech-to-text translation (S2TT) and competitive results on automatic speech recognition (ASR) and text-to-speech synthesis (TTS) tasks.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the robustness of our framework across datasets, we further evaluate performance on the FLEURS test set. FLEURS is a multilingual benchmark derived from the FLoRes corpus that provides high-quality parallel speech and text pairs across diverse languages, making it a valuable complement to CVSS-T for assessing speech translation systems under standardized multilingual conditions.</p>\n\n",
                "matched_terms": [
                    "set",
                    "cvsst",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T3\" title=\"Table D3 &#8227; D.1 S2ST Performance on Fleurs &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D3</span></a>, UniSS (Q) achieves strong translation performance with Speech-BLEU of 36.16 and Text-BLEU of 36.96 on EN-ZH, outperforming all S2ST baselines and cascaded systems. UniSS (P) also demonstrates competitive performance across both metrics. In the ZH-EN direction, UniSS surpasses all systems with comparable model size, demonstrating the parameter efficiency of our design.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss",
                    "enzh",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS exhibits excellent prosody transfer capabilities on the FLEURS benchmark. UniSS (Q) achieves A.PCP scores of 2.72 (EN-ZH) and 2.64 (ZH-EN), outperforming all end-to-end baselines and matching GPT-4o&#8217;s performance (2.72 and 2.39). Notably, UniSS achieves these results without incorporating dedicated prosody modeling modules, highlighting the effectiveness of our unified framework.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "enzh",
                    "scores",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system demonstrates exceptional speech duration consistency on the FLEURS test set. Both UniSS variants achieve near-perfect scores on the SLC-0.2 and SLC-0.4 metrics, indicating that the generated speech closely mirrors the temporal length of the input audio. This precise duration alignment is critical for real-time applications and audiovisual translation scenarios where timing mismatches can disrupt user experience.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "set",
                    "scores",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (Q) attains a UTMOS score of 3.41, outperforming all open-source baselines and surpassing GPT-4o (3.36). UniSS (Q) also leads all S2ST models in the ZH-EN direction with a score of 4.16. Despite its significantly smaller size, UniSS consistently produces high-quality speech across both translation directions, confirming its effectiveness as a practical open-source solution for speech-to-speech translation.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss",
                    "enzh",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond speech-to-speech translation, UniSS also supports speech-to-text translation (S2TT). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>, UniSS achieves leading performance on this task as well. Whisper only supports X-EN translation and demonstrates poor performance on multilingual scenarios. UniSS&#8217;s Text-BLEU scores significantly outperform the 7B multimodal LLM Qwen2-audio across both datasets, demonstrating superior cross-lingual speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "only",
                    "uniss",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a subjective assessment on a randomly sampled subset of UniST to evaluate the quality of our dataset. Four raters listened to 150 examples and provided MOS scores across three dimensions: Emotion Similarity, Speaker Similarity, and Speech Naturalness. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.T5\" title=\"Table E5 &#8227; E.3 Analysis of UniST &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E5</span></a>, UniST synthesized by SparkTTS achieves high speech naturalness and effectively preserves both the source speaker&#8217;s voice characteristics and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "unist",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The progressive training strategy pertains only to the training process and does not increase the model&#8217;s structural complexity. The alignment in phase 1 uses existing, easily accessible open-source data. In the ablation study section, we demonstrate the impact of different stages on model performance, proving that progressive training is effective.</p>\n\n",
                "matched_terms": [
                    "phase",
                    "only",
                    "study",
                    "ablation"
                ]
            }
        ]
    },
    "A2.T1": {
        "source_file": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
        "caption": "Table B1: UniSS training configuration showing datasets, hyperparameters, and training settings for each phase.",
        "body": "Phase\nDataset\nTraining Tokens\nEpochs\nLearning Rate\nWarm-up\nBatch Size\n\n\nPhase 1\n77.1k hours speech data\n\n∼\\sim32B per epoch\n3\n8e-4 (constant)\n1 epoch\n2.3M\n\n\n2.3B MT tokens\n\n\nPhase 2\nUniST General dataset\n\n∼\\sim55B total\n1\n2e-4 (constant)\n5% epoch\n2.3M\n\n\nMixed with Phase 1 data (2:1)\n\n\nPhase 3\nUniST High-quality\n\n∼\\sim10B total\n0.9/1\n5e-5 →\\to 5e-6 (cosine)\nNone\n2.3M",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Phase</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Training Tokens</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Epochs</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Learning Rate</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Warm-up</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Batch Size</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Phase 1</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">77.1k hours speech data</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>32B per epoch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">8e-4 (constant)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">1 epoch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.3M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.3B MT tokens</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Phase 2</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">UniST General dataset</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>55B total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2e-4 (constant)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">5% epoch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.3M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Mixed with Phase 1 data (2:1)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Phase 3</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">UniST High-quality</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>10B total</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.9/1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">5e-5 <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> 5e-6 (cosine)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">None</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.3M</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "highquality",
            "mixed",
            "speech",
            "datasets",
            "warmup",
            "data",
            "→to",
            "∼sim10b",
            "rate",
            "constant",
            "hyperparameters",
            "total",
            "general",
            "∼sim55b",
            "batch",
            "5e6",
            "none",
            "phase",
            "each",
            "learning",
            "8e4",
            "cosine",
            "showing",
            "configuration",
            "771k",
            "training",
            "∼sim32b",
            "unist",
            "hours",
            "tokens",
            "dataset",
            "5e5",
            "epoch",
            "settings",
            "size",
            "23b",
            "2e4",
            "23m",
            "uniss",
            "epochs"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T1\" title=\"Table B1 &#8227; B.1 UniSS &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B1</span></a>, the UniSS is trained in three phases progressively.\nAll audio is resampled to 16&#160;kHz. The LLM vocabulary is expanded to 180,407 to include speech and control tokens.\nWe use the AdamW optimizer with a weight decay of 0.1 and momentum parameters (0.9, 0.95). The batch size is fixed at 2.3M tokens for all phases.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cmots.github.io/uniss-demo\" title=\"\">https://cmots.github.io/uniss-demo</a>.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "speech",
                    "uniss",
                    "data",
                    "unist",
                    "hours",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation (S2ST) enables conversion from a spoken utterance in one language to a spoken utterance in another, facilitating critical applications like real-time interpretation and cross-lingual video dubbing. Recent advancements have significantly improved translation fidelity from a semantic perspective&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib1\" title=\"\">2019</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib2\" title=\"\">2022</a>; Inaguma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib3\" title=\"\">2023</a>)</cite>. However, achieving high-quality translation that also faithfully preserves the expressive aspects of speech, such as speaker identity and emotional style, still remains an open challenge.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most recently, the application of large language models (LLMs) to generative speech tasks has further accelerated the development of S2ST systems. Current approaches typically fall into two categories: (1) single-stage methods, which directly predict multi-stream acoustic tokens autoregressively via multi-head outputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite>; and (2) two-stage pipelines, which first generate semantic tokens autoregressively, followed by another autoregressive (AR) model to predict acoustic tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>; Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Gong and Veluri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>)</cite> use a single AR language model to jointly model semantic and partial acoustic tokens, along with a non-autoregressive (NAR) model for complete acoustic information.\nWhile these approaches have shown promising results, they also introduce significantly more architectural complexity than textual LLMs. Additionally, they treat the LLM as a sequence-to-sequence converter, failing to leverage the pre-trained knowledge for textual translation embedded within the LLM.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of architectural complexity, we present <span class=\"ltx_text ltx_font_bold\">UniSS</span>, a unified single-stage S2ST framework that preserves timbre, emotion, and duration consistency.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> demonstrates our approach and performance results. UniSS builds upon pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> as the backbone, modeling content and speaker information in a single AR language model without architectural modifications. We transfer the LLM&#8217;s pre-trained text translation capabilities to the speech domain through a cross-modality chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib15\" title=\"\">2022</a>)</cite>. During inference, our Quality Mode guides the model to decompose the complex S2ST task into sequential listen, translate, and speak steps within a single inference pass. We also introduce a Performance Mode that trades translation fidelity for faster inference, enabling deployment across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realizing the vision outlined above demands large-scale, high-quality training data that preserves both translation accuracy and speaker expressiveness. However, existing S2ST datasets are either small-scale to train powerful unified models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite> or suffer from quality control issues when scraped from the web&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite>. To address this data challenge, we design a scalable synthesis pipeline and contribute UniST, a 44.8k-hour Chinese-English S2ST dataset offering high translation fidelity and rich speaker preservation.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "data",
                    "datasets",
                    "training",
                    "unist",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Large-Scale, High-Quality S2ST Dataset:</span> We design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English S2ST dataset with speaker voice and emotion preservation.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "dataset",
                    "unist",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation must preserve semantic accuracy during cross-lingual conversion. Traditional cascaded systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Casacuberta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib4\" title=\"\">2004</a>; Nakamura et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib5\" title=\"\">2006</a>)</cite> chain ASR, MT, and TTS components sequentially, suffering from error accumulation and information loss through text bottlenecks. Early direct methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib1\" title=\"\">2019</a>)</cite> encountered significant challenges with translation quality and synthesis artifacts.\nThe breakthrough came with discrete unit-based methods, where speech-to-unit translation (S2UT) approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib2\" title=\"\">2022</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib17\" title=\"\">2021</a>; Inaguma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib3\" title=\"\">2023</a>)</cite> employ discrete speech units (subwords, phonemes, or semantic tokens) as intermediate representations, which enables effective disentanglement of linguistic content from acoustic properties. SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite> achieved robust multilingual performance via unified multitask optimization of translation and synthesis.\nRecent systems focus on enhancing expressiveness while maintaining translation fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib18\" title=\"\">2022b</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib19\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib7\" title=\"\">2024</a>)</cite>. SeamlessExpressive&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite> designs a PRETSSEL vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib20\" title=\"\">2024</a>)</cite> to enhance expressiveness preservation. TransVIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib8\" title=\"\">2024</a>)</cite> employs feature disentanglement to separately model semantic, acoustic, and temporal information for voice and isochrony control.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of LLMs in text processing has inspired their adaptation to S2ST through speech tokenization. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib21\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib22\" title=\"\">2025</a>)</cite> focus primarily on semantic translation without preserving voice characteristics.\nTo address voice preservation, some works model acoustic features using multiple AR models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>; Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>)</cite>. Recent unified approaches attempt to model both semantic and acoustic features within a single AR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>; Gong and Veluri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite>. However, they still require an additional NAR model for complete acoustic generation. Hibiki&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite> proposes a multi-stream architecture processing source and target speech simultaneously, but necessitates substantial architectural modifications, including nested Transformers and training from scratch.\nA fundamental limitation across these approaches is their reliance on complex architectures to complete acoustic representations. Moreover, they fail to explicitly leverage the LLM&#8217;s pre-trained text translation capabilities, treating them merely as generic sequence converters. UniSS addresses both limitations through a single-stage architecture while effectively transferring LLM&#8217;s text translation capabilities to speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 UniSS Architecture &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, UniSS is a unified AR language model for expressive speech-to-speech translation, built upon a single-stage architecture with a cross-modal chain-of-thought prompting framework, and optimized via a progressive training strategy. This section details the three core components of UniSS: model architecture, cross-modal CoT prompting, and training methodology.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of UniSS is to perform expressive S2ST by modeling the conditional distribution <math alttext=\"P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo fence=\"false\">|</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})</annotation></semantics></math>, where <math alttext=\"\\mathbf{X}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{src}</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> denote the source and target speech waveforms, respectively. To achieve faithful content translation while preserving speaker identity and expressiveness, UniSS introduces three types of speech tokens: speaker tokens <math alttext=\"\\mathbf{S}^{spk}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{spk}</annotation></semantics></math> with fixed sequence length to capture global style attributes, e.g., timbre, prosody, and emotion; linguistic tokens <math alttext=\"\\mathbf{S}^{ling}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{ling}</annotation></semantics></math> to encode the content of the source utterance; and semantic tokens <math alttext=\"\\mathbf{S}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}</annotation></semantics></math> to represent the expressive target utterance and can be directly decoded into waveform when combined with speaker tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build our generation model upon the pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite>, a strong LLM backbone. By expanding the model vocabulary to include the discrete speech tokens, UniSS treats speech and text uniformly as token sequences, allowing both modalities to be processed within the same transformer architecture.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To represent both the content and expressive characteristics of speech, UniSS adopts a triple-tokenizer strategy, transforming the waveform <math alttext=\"\\mathbf{W}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119830;</mi><annotation encoding=\"application/x-tex\">\\mathbf{W}</annotation></semantics></math> into single-stream discrete token sequences:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Meanwhile, <math alttext=\"\\mathbf{S}^{ling}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m1\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{ling}</annotation></semantics></math> is generated by the <span class=\"ltx_text ltx_font_bold\">linguistic tokenizer</span>, which adopts the GLM-4 speech tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib24\" title=\"\">2024</a>)</cite>. It leverages a quantized Whisper encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib25\" title=\"\">2023</a>)</cite> to convert speech into a variable-length sequence of linguistic tokens at a rate of 12.5 tokens per second, enabling robust content understanding.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the S2ST task, the source waveform <math alttext=\"\\mathbf{X_{src}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;&#119851;&#119836;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{src}}</annotation></semantics></math> is represented by <math alttext=\"(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})</annotation></semantics></math>, while the target waveform <math alttext=\"\\mathbf{Y_{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119853;&#119840;&#119853;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{tgt}}</annotation></semantics></math> is represented by <math alttext=\"\\mathbf{S}_{tgt}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}_{tgt}^{sem}</annotation></semantics></math>. This design is motivated by our preliminary experiments, which revealed that although BiCodec&#8217;s semantic tokens are highly effective for waveform reconstruction, their self-supervised nature makes them suboptimal for content understanding. By separately representing speaker identity, linguistic content, and generation-oriented semantics, UniSS enables more accurate and controllable S2ST modeling.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS is guided by an input prompt <math alttext=\"\\mathbf{P}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#119823;</mi><annotation encoding=\"application/x-tex\">\\mathbf{P}</annotation></semantics></math>, which is a structured sequence of control and source tokens:</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"c_{task}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{task}</annotation></semantics></math>, <math alttext=\"c_{lang}^{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">c_{lang}^{tgt}</annotation></semantics></math>, and <math alttext=\"c_{speed}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{speed}</annotation></semantics></math> are special tokens specifying the task mode, target language, and duration ratio between source and target speech, respectively. A special begin-of-translation token, <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">BOT</span>, signals the model to begin generation, producing an output sequence <math alttext=\"\\tau_{out}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mrow><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tau_{out}</annotation></semantics></math> that is terminated by an end-of-decoding token, <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">EOD</span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 1: Speech-Text Alignment.</span>\nWe begin by adapting the pre-trained LLM to the speech modality through a multi-task learning stage involving four foundational tasks: ASR, TTS, Speech-to-Text Translation (S2TT), and MT. The ASR, TTS, and S2TT tasks align speech and text, while MT preserves the model&#8217;s foundational translation capabilities.\nEach task is defined by a specific prompt structure and target output sequence:</p>\n\n",
                "matched_terms": [
                    "phase",
                    "learning",
                    "speech",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here <math alttext=\"c_{lang}^{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">c_{lang}^{src}</annotation></semantics></math> denotes the source language, and <math alttext=\"\\mathbf{S}^{sem}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}_{src}</annotation></semantics></math> denotes the semantic tokens from source speech. This phase endows the model with robust speech understanding and generation while prioritizing the preservation of text translation capabilities.</p>\n\n",
                "matched_terms": [
                    "phase",
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phase utilizes the alignment from Phase 1 and transfers the LLM&#8217;s text translation capabilities to the speech domain.</p>\n\n",
                "matched_terms": [
                    "phase",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 3: Refinement.</span>\nIn the final phase, we fine-tune the model on the full S2ST task using both CoT prompting modes.\nThis phase uses an annealed learning rate to stabilize the learned CoT patterns and optimize the final translation performance.</p>\n\n",
                "matched_terms": [
                    "phase",
                    "rate",
                    "learning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training end-to-end expressive S2ST models faces a significant bottleneck due to the scarcity of large-scale, parallel data that preserves speaker characteristics. To address this limitation, we design a scalable data synthesis pipeline. The pipeline generates UniST, our large-scale Chinese-English expressive S2ST dataset. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) illustrates the data construction process.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "unist",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Cleaning.</span>\nOur pipeline begins with large-scale TTS corpora containing paired speech and transcription <math alttext=\"(\\mathbf{X}_{src},\\mathbf{T}_{src})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{X}_{src},\\mathbf{T}_{src})</annotation></semantics></math>, combined from several Chinese and English public datasets (details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.SS1\" title=\"E.1 Source Dataset &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a>). Following VoxBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib23\" title=\"\">2025</a>)</cite>, we first clean the corpus using Paraformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib26\" title=\"\">2022</a>)</cite> to re-recognize speech and compute Word Error Rate (WER) between re-recognized and original transcriptions, discarding samples with WER <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> 0.05.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "speech",
                    "data",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation and Target Speech Synthesis.</span>\nAfter source cleaning, <math alttext=\"\\mathbf{T}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{src}</annotation></semantics></math> is translated by Qwen2.5-72B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> to produce target text <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math> in another language.\nPrompts used in translation are shown in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.SS2\" title=\"E.2 Text Translation Step &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.\nWe then apply an expressive TTS model, SparkTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib23\" title=\"\">2025</a>)</cite>, to synthesize the target speech <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> from <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m4\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>, conditioned by <math alttext=\"\\mathbf{X}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m5\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{src}</annotation></semantics></math> to preserve the source speaker&#8217;s voice. To enable fine-grained speed control, we calculate the duration ratio between source and target speech and discretize it into speed tokens <math alttext=\"\\mathbf{c}_{speed}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m6\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{speed}</annotation></semantics></math> with 0.1 intervals. This creates complete parallel samples <math alttext=\"(\\mathbf{X}_{src},\\mathbf{T}_{src},\\mathbf{T}_{tgt},\\mathbf{Y}_{tgt},\\mathbf{c}_{speed})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>&#119836;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{X}_{src},\\mathbf{T}_{src},\\mathbf{T}_{tgt},\\mathbf{Y}_{tgt},\\mathbf{c}_{speed})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quality Filtering and Dataset Variants.</span>\nThe synthesized data undergoes final quality filtering.\nWe apply ASR to <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> and discard samples with WER greater than 0.01 against <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>. We also apply a duration ratio filter to keep synthesized speech with duration within [0.5, 2.0] times of the source speech.\nThe filtered data forms our <span class=\"ltx_text ltx_font_bold\">UniST General</span> dataset (44.8k hours).\nOur refined <span class=\"ltx_text ltx_font_bold\">UniST High-Quality</span> dataset (19.8k hours) is created by applying additional Voice Activity Detection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib27\" title=\"\">2024</a>)</cite> to remove silence at the beginning and end of speech segments, and a stricter duration ratio filter of [0.7,1.5].</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "speech",
                    "data",
                    "general",
                    "unist",
                    "hours",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b,c), UniST General provides greater data diversity and generalization potential\ndue to its larger range of duration ratios and speech lengths. UniST High-Quality focuses on temporal consistency, making it ideal for the final refinement phase of our progressive training. In Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.SS3\" title=\"E.3 Analysis of UniST &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E.3</span></a>, we show that the UniST dataset also preserves emotional style in synthesis.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "speech",
                    "data",
                    "general",
                    "training",
                    "phase",
                    "unist",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS is trained in three progressive phases, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS3\" title=\"3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>. Phase 1 establishes text-speech alignment using 77.1k hours of speech data and MT text data from WMT17&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bojar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib28\" title=\"\">2017</a>)</cite>. Phase 2 introduces CoT prompting with our UniST General dataset, mixed with Phase 1 data at a 2:1 ratio. Phase 3 fine-tunes the model exclusively on the UniST High-Quality dataset.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "mixed",
                    "speech",
                    "uniss",
                    "data",
                    "general",
                    "771k",
                    "phase",
                    "unist",
                    "hours",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training employs the AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov and Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib29\" title=\"\">2019</a>)</cite> with a 2.3M-token batch size, weight decay of 0.1, and momentum parameters (0.9, 0.95). All audio is resampled to 16&#160;kHz, and the LLM vocabulary is expanded to 180,407 to include speech and control tokens. Learning rates progress from 8e-4 in Phase 1 to 2e-4 in Phase 2, and finally anneal from 5e-5 to 5e-6 in Phase 3. The model is trained on 16 NVIDIA H800 80G GPUs using the Megatron-LM Framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shoeybi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib30\" title=\"\">2019</a>)</cite> for efficient large-model training. Complete training details and hyperparameters are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.SS1\" title=\"B.1 UniSS &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "5e5",
                    "speech",
                    "8e4",
                    "size",
                    "hyperparameters",
                    "2e4",
                    "5e6",
                    "batch",
                    "training",
                    "phase",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our primary S2ST evaluation, we use the Chinese (ZH) and English (EN) test sets from CVSS-T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite>, which contain 4,897 utterance pairs (8.2 hours for Chinese and 6.3 hours for English). For emotion preservation evaluation, we randomly sample 300 utterances from each emotional speech dataset (ESD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib31\" title=\"\">2021</a>)</cite> and CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib32\" title=\"\">2014</a>)</cite>), covering a selected set of 4 emotions (happy, sad, angry, neutral). We evaluate both English-to-Chinese (EN-ZH) and Chinese-to-English (ZH-EN) translation directions.</p>\n\n",
                "matched_terms": [
                    "each",
                    "speech",
                    "hours",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ comprehensive objective and subjective metrics to evaluate S2ST performance. For translation fidelity, we measure BLEU scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib33\" title=\"\">2002</a>)</cite> on both ASR-transcribed speech outputs (Speech-BLEU) and intermediate text (Text-BLEU). Prosody preservation is assessed through prosody similarity (A.PCP)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Andrews et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib34\" title=\"\">2022</a>)</cite>. Duration consistency is evaluated using Speech Length Compliance (SLC) within 0.2 and 0.4 ratio tolerance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nSpeech quality is measured using UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib36\" title=\"\">2022</a>)</cite>. Additionally, we conduct a subjective Mean Opinion Score (MOS) evaluation where bilingual speakers rate emotion similarity, speaker similarity, and naturalness on a 5-point scale. Detailed descriptions of all evaluation metrics and implementation procedures are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A3\" title=\"Appendix C Evaluation Metrics &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T1\" title=\"Table 1 &#8227; 5.3 Objective Performance &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main comparison results on the CVSS-T dataset.\nThe results clearly demonstrate that UniSS establishes a new SOTA in translation fidelity\nwhile maintaining voice characteristics, prosody, duration consistency, and speech quality, validating our core design principles. Additional results on other datasets and tasks are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4\" title=\"Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "speech",
                    "uniss",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Quality.</span> UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (P) achieves a competitive UTMOS score of 3.77, matching cascaded models (3.76 and 3.79) while surpassing all end-to-end models. In the ZH-EN direction, UniSS surpasses 3-stage systems with a score of 3.86 compared to 3.50.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Naturalness.</span>\nThe naturalness of speech from UniSS is also favorably rated. UniSS (Q) achieves a naturalness MOS of 4.45, surpassing Seamless-Ex (3.10) and the 3-stage cascaded baseline (4.31) that incorporates a specialized TTS model. This demonstrates UniSS&#8217;s ability to generate high-quality, natural-sounding speech without dedicated text-to-speech components.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "speech",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our three-phase progressive training significantly impacts performance.\nPhase 3 refinement (<span class=\"ltx_text ltx_font_bold\">w/ Phase 3</span>) contributes improvements of +0.90 and +2.06 Speech-BLEU points, validating the importance of high-quality data fine-tuning for final optimization.\nRemoving the initial Phase 1 alignment (<span class=\"ltx_text ltx_font_bold\">UniST only</span>) causes severe performance degradation of -7.18 and -10.15 points. This drop demonstrates that Phase 1 text-speech alignment is essential for subsequent S2ST learning.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "learning",
                    "data",
                    "training",
                    "phase",
                    "unist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Replacing our content-focused GLM-4 linguistic tokenizer with BiCodec&#8217;s self-supervised semantic tokens (<span class=\"ltx_text ltx_font_bold\">w/o GLM</span>) leads to significant performance degradation of -15.01 and -8.73 Speech-BLEU points.\nThis drop reveals that while BiCodec&#8217;s semantic tokens excel at speech generation tasks, their self-supervised nature limits their effectiveness for content understanding in the S2ST task.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces UniSS, a unified single-stage expressive speech-to-speech translation framework that preserves voice and emotional style. Our approach eliminates architectural complexity through well-designed speech semantic and style modeling, enabling seamless integration with pre-trained LLMs. By transferring LLMs&#8217; text translation capabilities to speech via cross-modal CoT prompting, UniSS surpasses previous S2ST systems across multiple dimensions: translation fidelity, speech naturalness, and preservation of voice, emotion, and duration consistency.\nBeyond the model architecture, we design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English expressive S2ST dataset. Our work demonstrates a simple and effective approach for building the next generation of expressive S2ST systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss",
                    "datasets",
                    "unist",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Works. </span>\nCurrent limitations point to several research directions. (1) Language Support: We trained UniSS only on Chinese and English due to resource limitations. The data construction pipeline and training framework can be easily extended to multilingual scenarios, which represents our immediate next step. (2) Speech Tokenizer: While the linguistic, speaker, and semantic tokenizers perform well in UniSS, they originate from two different audio tokenizers, causing vocabulary size expansion. Future work will focus on training a unified tokenizer to merge these components and reduce vocabulary size.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "size",
                    "data",
                    "training",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present real examples from different models here, with speech converted to text using ASR for display purposes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A1.F1\" title=\"Figure A1 &#8227; Appendix A Examples &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">A1</span></a>, UniSS achieves competitive BLEU scores on challenging sentences where GPT-4o struggles with translation quality. Note Seamless-L and GPT-4o cannot preserve source voice and emotional style.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model is trained on a mixture of ASR, TTS, S2TT, and MT tasks.\nThis phase utilizes 77.1k hours of speech data and 2.3B translation tokens from WMT17,\ntotaling approximately 32B tokens per epoch.\nThe model is trained for 3 epochs with a constant learning rate of 8e-4 and a 1-epoch warm-up.</p>\n\n",
                "matched_terms": [
                    "learning",
                    "epoch",
                    "rate",
                    "speech",
                    "8e4",
                    "hours",
                    "constant",
                    "data",
                    "warmup",
                    "23b",
                    "epochs",
                    "771k",
                    "phase",
                    "tokens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce CoT prompting using our UniST General dataset.\nThe model learns three generation paths: Performance mode, Quality mode, and direct S2ST data without intermediate text.\nNew data is mixed with Phase 1 data at a 2:1 ratio, totaling approximately 55B tokens. We train for one epoch with a constant learning rate of 2e-4 and a 5% warm-up.</p>\n\n",
                "matched_terms": [
                    "mixed",
                    "learning",
                    "epoch",
                    "rate",
                    "constant",
                    "data",
                    "warmup",
                    "2e4",
                    "general",
                    "phase",
                    "unist",
                    "tokens",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune the model exclusively on the UniST High-quality dataset.\nThe learning rate follows a cosine schedule, gradually annealing from 5e-5 to 5e-6 over 1 epoch without warm-up. This phase consists of approximately 10B training tokens. During this stage, the training loss reaches its minimum around 0.9 epochs, and we select the checkpoint at this point as our final model weights.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "learning",
                    "5e5",
                    "epoch",
                    "rate",
                    "cosine",
                    "epochs",
                    "warmup",
                    "5e6",
                    "training",
                    "phase",
                    "unist",
                    "tokens",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our language model is trained on 16 NVIDIA H800 80G GPUs. We utilize the Megatron-LM Framework for efficient large-model training.\nBecause audio duration distribution is uneven, padding would waste significant computational resources. We use the sequence packing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Krell et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib44\" title=\"\">2022</a>]</cite> technique to concatenate multiple samples into a single 18k token long sequence. We use a global batch size of 128. Completing all three phases of training takes approximately 6 days.</p>\n\n",
                "matched_terms": [
                    "size",
                    "batch",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide detailed descriptions of all baseline models evaluated against our proposed UniSS framework. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T2\" title=\"Table B2 &#8227; B.2 Baselines &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B2</span></a> summarizes the baselines, model versions, and parameter sizes used in our experiments. All baseline models are evaluated using their default inference parameters without any additional tuning or modifications. Evaluation is conducted directly on the CVSS-T and FLEURS test sets without extra data preprocessing, prompts, or output post-processing. We present the implementation specifics of each baseline in the following subsections.</p>\n\n",
                "matched_terms": [
                    "each",
                    "uniss",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Speech (TTS)</span>: We employ CosyVoice 2 (0.5B), a transformer-based expressive TTS model designed for high-quality multilingual speech synthesis. Its ability to generate natural and expressive audio makes it suitable for our evaluation.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeamlessExpressive extends SeamlessM4T by introducing expressiveness-aware modeling, enabling more natural and emotionally rich speech synthesis in the target language. The model conditions generation on prosodic and expressive cues from the source speech and is particularly well-suited for conversational and affective scenarios. We evaluate SeamlessExpressive using its official inference scripts and default hyperparameters.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "hyperparameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate on the Chinese and English test subsets of FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib48\" title=\"\">2022</a>]</cite>, a massively multilingual speech dataset spanning 102 languages with n-way parallel speech and text data. UniSS demonstrates robust performance on this challenging multilingual benchmark across multiple evaluation tasks. Additionally, UniSS achieves state-of-the-art performance on speech-to-text translation (S2TT) and competitive results on automatic speech recognition (ASR) and text-to-speech synthesis (TTS) tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "speech",
                    "uniss",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the robustness of our framework across datasets, we further evaluate performance on the FLEURS test set. FLEURS is a multilingual benchmark derived from the FLoRes corpus that provides high-quality parallel speech and text pairs across diverse languages, making it a valuable complement to CVSS-T for assessing speech translation systems under standardized multilingual conditions.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T3\" title=\"Table D3 &#8227; D.1 S2ST Performance on Fleurs &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D3</span></a>, UniSS (Q) achieves strong translation performance with Speech-BLEU of 36.16 and Text-BLEU of 36.96 on EN-ZH, outperforming all S2ST baselines and cascaded systems. UniSS (P) also demonstrates competitive performance across both metrics. In the ZH-EN direction, UniSS surpasses all systems with comparable model size, demonstrating the parameter efficiency of our design.</p>\n\n",
                "matched_terms": [
                    "size",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system demonstrates exceptional speech duration consistency on the FLEURS test set. Both UniSS variants achieve near-perfect scores on the SLC-0.2 and SLC-0.4 metrics, indicating that the generated speech closely mirrors the temporal length of the input audio. This precise duration alignment is critical for real-time applications and audiovisual translation scenarios where timing mismatches can disrupt user experience.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (Q) attains a UTMOS score of 3.41, outperforming all open-source baselines and surpassing GPT-4o (3.36). UniSS (Q) also leads all S2ST models in the ZH-EN direction with a score of 4.16. Despite its significantly smaller size, UniSS consistently produces high-quality speech across both translation directions, confirming its effectiveness as a practical open-source solution for speech-to-speech translation.</p>\n\n",
                "matched_terms": [
                    "highquality",
                    "speech",
                    "uniss",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond speech-to-speech translation, UniSS also supports speech-to-text translation (S2TT). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>, UniSS achieves leading performance on this task as well. Whisper only supports X-EN translation and demonstrates poor performance on multilingual scenarios. UniSS&#8217;s Text-BLEU scores significantly outperform the 7B multimodal LLM Qwen2-audio across both datasets, demonstrating superior cross-lingual speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although UniSS was not specifically designed for automatic speech recognition, it demonstrates competitive ASR capabilities as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. We evaluate the ASR performance on the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib49\" title=\"\">2015</a>]</cite>. On the LibriSpeech test-clean subset, UniSS achieves a WER of 2.4, which is comparable to the open-source state-of-the-art ASR model, Whisper (1.8 WER). However, on the more challenging LibriSpeech test-others subset, UniSS shows degraded performance with a WER of 6.6. It is important to note that UniSS is primarily designed as a speech-to-speech translation model, and these ASR results demonstrate its versatility beyond its core translation capabilities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although UniSS was not specifically designed for text-to-speech synthesis, we evaluate its TTS capabilities on the LibriSpeech test-clean dataset as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. UniSS achieves a WER of 4.75 and speaker similarity of 0.568, compared to SparkTTS, which obtains 1.98 WER and 0.584 similarity. The performance gap is expected given that SparkTTS is specifically optimized for TTS, whereas UniSS is primarily designed for speech-to-speech translation. These results demonstrate that UniSS can generate intelligible speech output.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select a diverse collection of publicly available Chinese and English speech datasets as the foundation for UniSS training data. Our data sources encompass multiple domains and speaking styles to ensure robust cross-lingual speech translation capabilities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "datasets",
                    "training",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FLEURS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib48\" title=\"\">2022</a>]</cite>: A multilingual benchmark derived from FLoRes, containing parallel speech and text data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">HQ-Conversations</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Magic Data Technology&#160;Co., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib57\" title=\"\">2024</a>]</cite>: A multi-speaker Mandarin conversational dataset.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NCSSD-C and NCSSD-R</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib60\" title=\"\">2024</a>]</cite>: Multi-speaker bilingual datasets for speech synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a subjective assessment on a randomly sampled subset of UniST to evaluate the quality of our dataset. Four raters listened to 150 examples and provided MOS scores across three dimensions: Emotion Similarity, Speaker Similarity, and Speech Naturalness. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.T5\" title=\"Table E5 &#8227; E.3 Analysis of UniST &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E5</span></a>, UniST synthesized by SparkTTS achieves high speech naturalness and effectively preserves both the source speaker&#8217;s voice characteristics and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "speech",
                    "unist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although Spark-TTS does not achieve state-of-the-art performance in objective evaluations of speaker similarity, we observe that its performance in subjective evaluations is comparable. Furthermore, we assume that its voice decoupling capability reduces the modeling complexity for large language models, which is why we deploy BiCodec for the generation speech tokenizer in the LLM. Additionally, Spark-TTS generates BiCodec tokens directly, making training more efficient by eliminating an extra encoding step.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tokens",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The progressive training strategy pertains only to the training process and does not increase the model&#8217;s structural complexity. The alignment in phase 1 uses existing, easily accessible open-source data. In the ablation study section, we demonstrate the impact of different stages on model performance, proving that progressive training is effective.</p>\n\n",
                "matched_terms": [
                    "phase",
                    "training",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speaker tokenizer is used to represent global speaker information. We hypothesize that this voice decoupling reduces the difficulty of speech translation by allowing the model to focus more on the linguistic content. We employ different linguistic and semantic tokenizers for understanding and generation to optimize each task. To generate more natural and expressive audio, the generation process uses semantic tokens, which retain information beyond the core content. However, this additional information makes content understanding more challenging. Therefore, the linguistic tokenizer, which focuses solely on content information, is better suited for the speech understanding phase. As a simple speech representation, we believe this design does not introduce greater complexity in either training or inference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training",
                    "phase",
                    "tokens",
                    "each"
                ]
            }
        ]
    },
    "A2.T2": {
        "source_file": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
        "caption": "Table B2: Overview of baseline models used in experiments.",
        "body": "Pipeline\nStage\nModel Name\nModel Source\nSize\n\n\n\n\nThree-stage\nASR\nWhisper-large-v3\nopenai/whisper-large-v3\nLarge (∼\\sim1.5B)\n\n\nMT\nNLLB-200-distilled-600M\nfacebook/nllb-200-distilled-600M\n600M\n\n\nTTS\nCosyVoice 2\nFunAudioLLM/CosyVoice2-0.5B\n0.5B\n\n\nTwo-stage\nSpeech Translation\nSeamlessM4T-v2-Large\nfacebook/seamless-m4t-v2-large\nLarge (∼\\sim2.3B)\n\n\nTTS\nCosyVoice 2\nFunAudioLLM/CosyVoice2-0.5B\n0.5B\n\n\nMultimodal LLMs\nSpeech-to-Text\nQwen2-audio\nQwen/Qwen2-audio\n7B\n\n\nSpeech-to-Speech\nQwen2.5 Omni\nQwen/Qwen2.5-Omni\n7B\n\n\nSpeech-to-Speech\nGPT-4o-Audio-Preview\nOpenAI Official API\nNot disclosed\n\n\nEnd-to-End S2ST\nS2ST\nSeamlessM4T Medium\nfacebook/seamless-m4t-medium\nMedium (∼\\sim1.2B)\n\n\nS2ST\nSeamlessM4T Large-v2\nfacebook/seamless-m4t-large-v2\nLarge (∼\\sim2.3B)\n\n\nS2ST\nSeamlessExpressive\nfacebook/seamless-expressive\n\n∼\\sim1.7B",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Pipeline</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Stage</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model Name</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model Source</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Size</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\" style=\"padding-bottom:2.0pt;\">Three-stage</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:2.0pt;\">ASR</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:2.0pt;\">Whisper-large-v3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;\">openai/whisper-large-v3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:2.0pt;\">Large (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T2.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1.5B)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:2.0pt;\">MT</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:2.0pt;\">NLLB-200-distilled-600M</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;\">facebook/nllb-200-distilled-600M</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:2.0pt;\">600M</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">TTS</td>\n<td class=\"ltx_td ltx_align_left\">CosyVoice 2</td>\n<td class=\"ltx_td ltx_align_center\">FunAudioLLM/CosyVoice2-0.5B</td>\n<td class=\"ltx_td ltx_align_left\">0.5B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding-bottom:2.0pt;\">Two-stage</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:2.0pt;\">Speech Translation</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:2.0pt;\">SeamlessM4T-v2-Large</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;\">facebook/seamless-m4t-v2-large</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:2.0pt;\">Large (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T2.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.3B)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">TTS</td>\n<td class=\"ltx_td ltx_align_left\">CosyVoice 2</td>\n<td class=\"ltx_td ltx_align_center\">FunAudioLLM/CosyVoice2-0.5B</td>\n<td class=\"ltx_td ltx_align_left\">0.5B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\" style=\"padding-bottom:2.0pt;\">Multimodal LLMs</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:2.0pt;\">Speech-to-Text</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:2.0pt;\">Qwen2-audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;\">Qwen/Qwen2-audio</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:2.0pt;\">7B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:2.0pt;\">Speech-to-Speech</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:2.0pt;\">Qwen2.5 Omni</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;\">Qwen/Qwen2.5-Omni</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:2.0pt;\">7B</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Speech-to-Speech</td>\n<td class=\"ltx_td ltx_align_left\">GPT-4o-Audio-Preview</td>\n<td class=\"ltx_td ltx_align_center\">OpenAI Official API</td>\n<td class=\"ltx_td ltx_align_left\">Not disclosed</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"3\" style=\"padding-bottom:2.0pt;\">End-to-End S2ST</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:2.0pt;\">S2ST</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:2.0pt;\">SeamlessM4T Medium</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;\">facebook/seamless-m4t-medium</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-bottom:2.0pt;\">Medium (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T2.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1.2B)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:2.0pt;\">S2ST</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:2.0pt;\">SeamlessM4T Large-v2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;\">facebook/seamless-m4t-large-v2</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-bottom:2.0pt;\">Large (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T2.m4\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.3B)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">S2ST</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">SeamlessExpressive</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">facebook/seamless-expressive</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.T2.m5\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1.7B</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "facebookseamlessm4tlargev2",
            "speechtotext",
            "speech",
            "qwen25",
            "experiments",
            "seamlessm4t",
            "qwen2audio",
            "used",
            "cosyvoice",
            "source",
            "overview",
            "gpt4oaudiopreview",
            "pipeline",
            "stage",
            "asr",
            "llms",
            "nllb200distilled600m",
            "openaiwhisperlargev3",
            "qwenqwen25omni",
            "disclosed",
            "multimodal",
            "endtoend",
            "facebookseamlessm4tv2large",
            "speechtospeech",
            "threestage",
            "openai",
            "facebooknllb200distilled600m",
            "600m",
            "name",
            "medium",
            "∼sim12b",
            "large",
            "facebookseamlessm4tmedium",
            "omni",
            "seamlessm4tv2large",
            "official",
            "∼sim15b",
            "seamlessexpressive",
            "translation",
            "baseline",
            "qwenqwen2audio",
            "models",
            "size",
            "largev2",
            "tts",
            "05b",
            "twostage",
            "s2st",
            "funaudiollmcosyvoice205b",
            "api",
            "∼sim17b",
            "whisperlargev3",
            "model",
            "facebookseamlessexpressive",
            "∼sim23b",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In this section, we provide detailed descriptions of all baseline models evaluated against our proposed UniSS framework. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T2\" title=\"Table B2 &#8227; B.2 Baselines &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B2</span></a> summarizes the baselines, model versions, and parameter sizes used in our experiments. All baseline models are evaluated using their default inference parameters without any additional tuning or modifications. Evaluation is conducted directly on the CVSS-T and FLEURS test sets without extra data preprocessing, prompts, or output post-processing. We present the implementation specifics of each baseline in the following subsections.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cmots.github.io/uniss-demo\" title=\"\">https://cmots.github.io/uniss-demo</a>.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation",
                    "models",
                    "speech",
                    "llms",
                    "s2st",
                    "large",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation (S2ST) enables conversion from a spoken utterance in one language to a spoken utterance in another, facilitating critical applications like real-time interpretation and cross-lingual video dubbing. Recent advancements have significantly improved translation fidelity from a semantic perspective&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib1\" title=\"\">2019</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib2\" title=\"\">2022</a>; Inaguma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib3\" title=\"\">2023</a>)</cite>. However, achieving high-quality translation that also faithfully preserves the expressive aspects of speech, such as speaker identity and emotional style, still remains an open challenge.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "speech",
                    "translation",
                    "speechtospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A conventional cascaded S2ST system typically consists of three sequential components: automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech (TTS) synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Casacuberta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib4\" title=\"\">2004</a>; Nakamura et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib5\" title=\"\">2006</a>)</cite>. However, this cascaded architecture often suffers from error accumulation across stages and struggles to retain paralinguistic features of the original speech. To address these limitations, subsequent research has shifted towards end-to-end approaches that aim to translate speech to another language while preserving expressive characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib7\" title=\"\">2024</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib8\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "speech",
                    "asr",
                    "tts",
                    "s2st",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most recently, the application of large language models (LLMs) to generative speech tasks has further accelerated the development of S2ST systems. Current approaches typically fall into two categories: (1) single-stage methods, which directly predict multi-stream acoustic tokens autoregressively via multi-head outputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite>; and (2) two-stage pipelines, which first generate semantic tokens autoregressively, followed by another autoregressive (AR) model to predict acoustic tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>; Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Gong and Veluri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>)</cite> use a single AR language model to jointly model semantic and partial acoustic tokens, along with a non-autoregressive (NAR) model for complete acoustic information.\nWhile these approaches have shown promising results, they also introduce significantly more architectural complexity than textual LLMs. Additionally, they treat the LLM as a sequence-to-sequence converter, failing to leverage the pre-trained knowledge for textual translation embedded within the LLM.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "models",
                    "speech",
                    "llms",
                    "s2st",
                    "large",
                    "twostage",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In pursuit of high-fidelity, expressive S2ST, and to overcome the limitations outlined above, our vision is defined by three core principles: (1) a single-stage architecture to eliminate complexity; (2) a unified model that aligns speech and text modalities; and (3) a mechanism to explicitly leverage the proven text translation capabilities of LLMs. To our knowledge, no existing approach satisfies all three principles simultaneously.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "speech",
                    "llms",
                    "s2st",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of architectural complexity, we present <span class=\"ltx_text ltx_font_bold\">UniSS</span>, a unified single-stage S2ST framework that preserves timbre, emotion, and duration consistency.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> demonstrates our approach and performance results. UniSS builds upon pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> as the backbone, modeling content and speaker information in a single AR language model without architectural modifications. We transfer the LLM&#8217;s pre-trained text translation capabilities to the speech domain through a cross-modality chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib15\" title=\"\">2022</a>)</cite>. During inference, our Quality Mode guides the model to decompose the complex S2ST task into sequential listen, translate, and speak steps within a single inference pass. We also introduce a Performance Mode that trades translation fidelity for faster inference, enabling deployment across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "speech",
                    "translation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realizing the vision outlined above demands large-scale, high-quality training data that preserves both translation accuracy and speaker expressiveness. However, existing S2ST datasets are either small-scale to train powerful unified models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite> or suffer from quality control issues when scraped from the web&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite>. To address this data challenge, we design a scalable synthesis pipeline and contribute UniST, a 44.8k-hour Chinese-English S2ST dataset offering high translation fidelity and rich speaker preservation.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "s2st",
                    "models",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Unified Single-Stage Architecture for S2ST:</span> We propose UniSS, a single-stage AR model for expressive S2ST that directly leverages a pre-trained textual LLM, eliminating the architectural complexity of prior work.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transfer of Text Translation Capabilities to Speech:</span> We demonstrate how to effectively leverage pre-trained LLMs&#8217; superior text translation abilities for speech translation, bridging the modality gap through cross-modal CoT prompting and enabling flexible control over quality-efficiency trade-offs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Large-Scale, High-Quality S2ST Dataset:</span> We design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English S2ST dataset with speaker voice and emotion preservation.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "s2st"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation must preserve semantic accuracy during cross-lingual conversion. Traditional cascaded systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Casacuberta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib4\" title=\"\">2004</a>; Nakamura et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib5\" title=\"\">2006</a>)</cite> chain ASR, MT, and TTS components sequentially, suffering from error accumulation and information loss through text bottlenecks. Early direct methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib1\" title=\"\">2019</a>)</cite> encountered significant challenges with translation quality and synthesis artifacts.\nThe breakthrough came with discrete unit-based methods, where speech-to-unit translation (S2UT) approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib2\" title=\"\">2022</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib17\" title=\"\">2021</a>; Inaguma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib3\" title=\"\">2023</a>)</cite> employ discrete speech units (subwords, phonemes, or semantic tokens) as intermediate representations, which enables effective disentanglement of linguistic content from acoustic properties. SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite> achieved robust multilingual performance via unified multitask optimization of translation and synthesis.\nRecent systems focus on enhancing expressiveness while maintaining translation fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib18\" title=\"\">2022b</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib19\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib7\" title=\"\">2024</a>)</cite>. SeamlessExpressive&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite> designs a PRETSSEL vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib20\" title=\"\">2024</a>)</cite> to enhance expressiveness preservation. TransVIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib8\" title=\"\">2024</a>)</cite> employs feature disentanglement to separately model semantic, acoustic, and temporal information for voice and isochrony control.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation",
                    "speech",
                    "asr",
                    "tts",
                    "seamlessm4t",
                    "model",
                    "seamlessexpressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of LLMs in text processing has inspired their adaptation to S2ST through speech tokenization. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib21\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib22\" title=\"\">2025</a>)</cite> focus primarily on semantic translation without preserving voice characteristics.\nTo address voice preservation, some works model acoustic features using multiple AR models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>; Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>)</cite>. Recent unified approaches attempt to model both semantic and acoustic features within a single AR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>; Gong and Veluri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite>. However, they still require an additional NAR model for complete acoustic generation. Hibiki&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite> proposes a multi-stream architecture processing source and target speech simultaneously, but necessitates substantial architectural modifications, including nested Transformers and training from scratch.\nA fundamental limitation across these approaches is their reliance on complex architectures to complete acoustic representations. Moreover, they fail to explicitly leverage the LLM&#8217;s pre-trained text translation capabilities, treating them merely as generic sequence converters. UniSS addresses both limitations through a single-stage architecture while effectively transferring LLM&#8217;s text translation capabilities to speech.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "models",
                    "speech",
                    "llms",
                    "s2st",
                    "model",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 UniSS Architecture &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, UniSS is a unified AR language model for expressive speech-to-speech translation, built upon a single-stage architecture with a cross-modal chain-of-thought prompting framework, and optimized via a progressive training strategy. This section details the three core components of UniSS: model architecture, cross-modal CoT prompting, and training methodology.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechtospeech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of UniSS is to perform expressive S2ST by modeling the conditional distribution <math alttext=\"P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo fence=\"false\">|</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})</annotation></semantics></math>, where <math alttext=\"\\mathbf{X}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{src}</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> denote the source and target speech waveforms, respectively. To achieve faithful content translation while preserving speaker identity and expressiveness, UniSS introduces three types of speech tokens: speaker tokens <math alttext=\"\\mathbf{S}^{spk}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{spk}</annotation></semantics></math> with fixed sequence length to capture global style attributes, e.g., timbre, prosody, and emotion; linguistic tokens <math alttext=\"\\mathbf{S}^{ling}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{ling}</annotation></semantics></math> to encode the content of the source utterance; and semantic tokens <math alttext=\"\\mathbf{S}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}</annotation></semantics></math> to represent the expressive target utterance and can be directly decoded into waveform when combined with speaker tokens.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "speech",
                    "translation",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build our generation model upon the pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite>, a strong LLM backbone. By expanding the model vocabulary to include the discrete speech tokens, UniSS treats speech and text uniformly as token sequences, allowing both modalities to be processed within the same transformer architecture.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the S2ST task, the source waveform <math alttext=\"\\mathbf{X_{src}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;&#119851;&#119836;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{src}}</annotation></semantics></math> is represented by <math alttext=\"(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})</annotation></semantics></math>, while the target waveform <math alttext=\"\\mathbf{Y_{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119853;&#119840;&#119853;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{tgt}}</annotation></semantics></math> is represented by <math alttext=\"\\mathbf{S}_{tgt}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}_{tgt}^{sem}</annotation></semantics></math>. This design is motivated by our preliminary experiments, which revealed that although BiCodec&#8217;s semantic tokens are highly effective for waveform reconstruction, their self-supervised nature makes them suboptimal for content understanding. By separately representing speaker identity, linguistic content, and generation-oriented semantics, UniSS enables more accurate and controllable S2ST modeling.</p>\n\n",
                "matched_terms": [
                    "experiments",
                    "s2st",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Direct speech-to-speech translation is an inherently complex task. Inspired by the success of chain-of-thought (CoT) prompting in LLM, we introduce a cross-modal CoT prompting framework to decompose this task into a more manageable sequence of <span class=\"ltx_text ltx_font_italic\">listen</span>, <span class=\"ltx_text ltx_font_italic\">translate</span>, and <span class=\"ltx_text ltx_font_italic\">speak</span> steps. This approach is designed to effectively transfer LLM&#8217;s powerful, pre-trained text translation capabilities to the speech domain. Our framework provides two controllable prompts to trade off translation fidelity and efficiency.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation",
                    "speechtospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"c_{task}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{task}</annotation></semantics></math>, <math alttext=\"c_{lang}^{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">c_{lang}^{tgt}</annotation></semantics></math>, and <math alttext=\"c_{speed}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{speed}</annotation></semantics></math> are special tokens specifying the task mode, target language, and duration ratio between source and target speech, respectively. A special begin-of-translation token, <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">BOT</span>, signals the model to begin generation, producing an output sequence <math alttext=\"\\tau_{out}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mrow><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tau_{out}</annotation></semantics></math> that is terminated by an end-of-decoding token, <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">EOD</span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quality Mode.</span> This mode follows the full CoT prompting path to maximize translation fidelity. The model first <span class=\"ltx_text ltx_font_italic\">listens</span> by generating the source transcription <math alttext=\"\\mathbf{T}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{src}</annotation></semantics></math>, then <span class=\"ltx_text ltx_font_italic\">translates</span> it into the text in the target language <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>, and finally <span class=\"ltx_text ltx_font_italic\">speaks</span> by generating target semantic tokens <math alttext=\"\\mathbf{S}^{sem}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}_{tgt}</annotation></semantics></math>. Prompted with <math alttext=\"c_{task}=\\text{Quality Mode}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><mrow><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><mo>=</mo><mtext>Quality Mode</mtext></mrow><annotation encoding=\"application/x-tex\">c_{task}=\\text{Quality Mode}</annotation></semantics></math>, this explicit chain allows the model to leverage its robust text translation abilities, formally represented as:</p>\n\n",
                "matched_terms": [
                    "model",
                    "translation",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt a three-phase progressive training strategy to optimize the model, aiming to gradually build cross-modal capabilities while mitigating\ncatastrophic forgetting of the LLM&#8217;s foundational text translation abilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 1: Speech-Text Alignment.</span>\nWe begin by adapting the pre-trained LLM to the speech modality through a multi-task learning stage involving four foundational tasks: ASR, TTS, Speech-to-Text Translation (S2TT), and MT. The ASR, TTS, and S2TT tasks align speech and text, while MT preserves the model&#8217;s foundational translation capabilities.\nEach task is defined by a specific prompt structure and target output sequence:</p>\n\n",
                "matched_terms": [
                    "translation",
                    "stage",
                    "speechtotext",
                    "speech",
                    "asr",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here <math alttext=\"c_{lang}^{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">c_{lang}^{src}</annotation></semantics></math> denotes the source language, and <math alttext=\"\\mathbf{S}^{sem}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}_{src}</annotation></semantics></math> denotes the semantic tokens from source speech. This phase endows the model with robust speech understanding and generation while prioritizing the preservation of text translation capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speech",
                    "translation",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 2: S2ST with CoT.</span>\nIn the second phase, we introduce the core S2ST task. The model is trained to generate outputs using the CoT prompting formats described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS2\" title=\"3.2 Cross-Modal Chain-of-Thought Prompting &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, as well as a simplified direct generation mode that bypasses intermediate text outputs:</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This phase utilizes the alignment from Phase 1 and transfers the LLM&#8217;s text translation capabilities to the speech domain.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 3: Refinement.</span>\nIn the final phase, we fine-tune the model on the full S2ST task using both CoT prompting modes.\nThis phase uses an annealed learning rate to stabilize the learned CoT patterns and optimize the final translation performance.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training end-to-end expressive S2ST models faces a significant bottleneck due to the scarcity of large-scale, parallel data that preserves speaker characteristics. To address this limitation, we design a scalable data synthesis pipeline. The pipeline generates UniST, our large-scale Chinese-English expressive S2ST dataset. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) illustrates the data construction process.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "s2st",
                    "models",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Cleaning.</span>\nOur pipeline begins with large-scale TTS corpora containing paired speech and transcription <math alttext=\"(\\mathbf{X}_{src},\\mathbf{T}_{src})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{X}_{src},\\mathbf{T}_{src})</annotation></semantics></math>, combined from several Chinese and English public datasets (details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.SS1\" title=\"E.1 Source Dataset &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a>). Following VoxBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib23\" title=\"\">2025</a>)</cite>, we first clean the corpus using Paraformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib26\" title=\"\">2022</a>)</cite> to re-recognize speech and compute Word Error Rate (WER) between re-recognized and original transcriptions, discarding samples with WER <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> 0.05.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "speech",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation and Target Speech Synthesis.</span>\nAfter source cleaning, <math alttext=\"\\mathbf{T}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{src}</annotation></semantics></math> is translated by Qwen2.5-72B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> to produce target text <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math> in another language.\nPrompts used in translation are shown in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.SS2\" title=\"E.2 Text Translation Step &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.\nWe then apply an expressive TTS model, SparkTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib23\" title=\"\">2025</a>)</cite>, to synthesize the target speech <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> from <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m4\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>, conditioned by <math alttext=\"\\mathbf{X}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m5\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{src}</annotation></semantics></math> to preserve the source speaker&#8217;s voice. To enable fine-grained speed control, we calculate the duration ratio between source and target speech and discretize it into speed tokens <math alttext=\"\\mathbf{c}_{speed}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m6\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{speed}</annotation></semantics></math> with 0.1 intervals. This creates complete parallel samples <math alttext=\"(\\mathbf{X}_{src},\\mathbf{T}_{src},\\mathbf{T}_{tgt},\\mathbf{Y}_{tgt},\\mathbf{c}_{speed})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>&#119836;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{X}_{src},\\mathbf{T}_{src},\\mathbf{T}_{tgt},\\mathbf{Y}_{tgt},\\mathbf{c}_{speed})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "speech",
                    "tts",
                    "used",
                    "model",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quality Filtering and Dataset Variants.</span>\nThe synthesized data undergoes final quality filtering.\nWe apply ASR to <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> and discard samples with WER greater than 0.01 against <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>. We also apply a duration ratio filter to keep synthesized speech with duration within [0.5, 2.0] times of the source speech.\nThe filtered data forms our <span class=\"ltx_text ltx_font_bold\">UniST General</span> dataset (44.8k hours).\nOur refined <span class=\"ltx_text ltx_font_bold\">UniST High-Quality</span> dataset (19.8k hours) is created by applying additional Voice Activity Detection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib27\" title=\"\">2024</a>)</cite> to remove silence at the beginning and end of speech segments, and a stricter duration ratio filter of [0.7,1.5].</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS is trained in three progressive phases, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS3\" title=\"3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>. Phase 1 establishes text-speech alignment using 77.1k hours of speech data and MT text data from WMT17&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bojar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib28\" title=\"\">2017</a>)</cite>. Phase 2 introduces CoT prompting with our UniST General dataset, mixed with Phase 1 data at a 2:1 ratio. Phase 3 fine-tunes the model exclusively on the UniST High-Quality dataset.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training employs the AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov and Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib29\" title=\"\">2019</a>)</cite> with a 2.3M-token batch size, weight decay of 0.1, and momentum parameters (0.9, 0.95). All audio is resampled to 16&#160;kHz, and the LLM vocabulary is expanded to 180,407 to include speech and control tokens. Learning rates progress from 8e-4 in Phase 1 to 2e-4 in Phase 2, and finally anneal from 5e-5 to 5e-6 in Phase 3. The model is trained on 16 NVIDIA H800 80G GPUs using the Megatron-LM Framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shoeybi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib30\" title=\"\">2019</a>)</cite> for efficient large-model training. Complete training details and hyperparameters are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.SS1\" title=\"B.1 UniSS &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our primary S2ST evaluation, we use the Chinese (ZH) and English (EN) test sets from CVSS-T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite>, which contain 4,897 utterance pairs (8.2 hours for Chinese and 6.3 hours for English). For emotion preservation evaluation, we randomly sample 300 utterances from each emotional speech dataset (ESD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib31\" title=\"\">2021</a>)</cite> and CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib32\" title=\"\">2014</a>)</cite>), covering a selected set of 4 emotions (happy, sad, angry, neutral). We evaluate both English-to-Chinese (EN-ZH) and Chinese-to-English (ZH-EN) translation directions.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ comprehensive objective and subjective metrics to evaluate S2ST performance. For translation fidelity, we measure BLEU scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib33\" title=\"\">2002</a>)</cite> on both ASR-transcribed speech outputs (Speech-BLEU) and intermediate text (Text-BLEU). Prosody preservation is assessed through prosody similarity (A.PCP)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Andrews et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib34\" title=\"\">2022</a>)</cite>. Duration consistency is evaluated using Speech Length Compliance (SLC) within 0.2 and 0.4 ratio tolerance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nSpeech quality is measured using UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib36\" title=\"\">2022</a>)</cite>. Additionally, we conduct a subjective Mean Opinion Score (MOS) evaluation where bilingual speakers rate emotion similarity, speaker similarity, and naturalness on a 5-point scale. Detailed descriptions of all evaluation metrics and implementation procedures are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A3\" title=\"Appendix C Evaluation Metrics &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Systems.</span>\nTo establish an upper bound for modular approaches, we construct two cascaded baselines using top-performing open-source models: a <span class=\"ltx_text ltx_font_bold\">3-Stage</span> pipeline of Whisper-large-v3 (ASR), NLLB-200-distilled (MT)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Koishekenov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib37\" title=\"\">2023</a>)</cite>, and CosyVoice 2 (TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib38\" title=\"\">2024</a>)</cite>; and a <span class=\"ltx_text ltx_font_bold\">2-Stage</span> pipeline of SeamlessM4T-v2-Large (S2TT) and CosyVoice 2 (TTS).</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "models",
                    "asr",
                    "tts",
                    "whisperlargev3",
                    "cosyvoice",
                    "seamlessm4tv2large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Large Language Models (MLLMs).</span>\nTo benchmark against the latest generation of general-purpose models, we include two leading MLLMs: <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib39\" title=\"\">2024</a>)</cite>, an enterprise-level model from OpenAI with strong speech-to-speech capability; and <span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib40\" title=\"\">2025</a>)</cite>, a powerful open-source MLLM building on large-scale audio&#8211;language pretraining, enabling both speech understanding and synthesis.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "openai",
                    "models",
                    "speech",
                    "large",
                    "multimodal",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End S2ST Systems.</span>\nWe compare against dedicated S2ST models that represent the current open-source SOTA: <span class=\"ltx_text ltx_font_bold\">SeamlessM4T</span> (Medium and Large V2, denoted Seamless-M and Seamless-L), and its expressive variant <span class=\"ltx_text ltx_font_bold\">SeamlessExpressive</span> (Seamless-Ex). For subjective metrics, we also evaluate <span class=\"ltx_text ltx_font_bold\">Seed LiveInterpret 2.0</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib41\" title=\"\">2025</a>)</cite>, denoted as Seed Live, an enterprise-level S2ST system with duplex speech understanding and generation abilities.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "medium",
                    "s2st",
                    "large",
                    "seamlessm4t",
                    "seamlessexpressive",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T1\" title=\"Table 1 &#8227; 5.3 Objective Performance &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main comparison results on the CVSS-T dataset.\nThe results clearly demonstrate that UniSS establishes a new SOTA in translation fidelity\nwhile maintaining voice characteristics, prosody, duration consistency, and speech quality, validating our core design principles. Additional results on other datasets and tasks are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4\" title=\"Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation Fidelity.</span> UniSS achieves state-of-the-art translation fidelity on both EN-ZH and ZH-EN directions. The UniSS (Q) variant achieves a Speech-BLEU of 32.20 on EN-ZH and 24.28 on ZH-EN, substantially outperforming all prior end-to-end and cascaded baselines. The efficient UniSS (P) also delivers strong results, surpassing most existing systems. Notably, in terms of intermediate text metrics, UniSS models perform on par with or better than larger multimodal LLM-based approaches.</p>\n\n",
                "matched_terms": [
                    "endtoend",
                    "models",
                    "translation",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Quality.</span> UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (P) achieves a competitive UTMOS score of 3.77, matching cascaded models (3.76 and 3.79) while surpassing all end-to-end models. In the ZH-EN direction, UniSS surpasses 3-stage systems with a score of 3.86 compared to 3.50.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "models",
                    "speech",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Preservation.</span>\nUniSS (Q) demonstrates strong capability in preserving emotions, achieving a MOS of 4.51. This represents a substantial 27% improvement over the expressive S2ST baseline Seamless-Ex (3.56) and surpasses the 3-stage cascaded system (4.48). Notably, UniSS (Q) also approaches Seed Live (4.56), indicating its ability in capturing emotional nuance.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Preservation.</span>\nUniSS effectively retains speaker voice characteristics without requiring an additional NAR stage-2 model. For speaker similarity, UniSS (Q) achieves a score of 4.42, outperforming all other models. This represents a 0.07 improvement over the 2-stage system (4.35), which deploys a carefully designed TTS model. These results underscore the ability of UniSS to maintain voice characteristics in an end-to-end fashion.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "tts",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Naturalness.</span>\nThe naturalness of speech from UniSS is also favorably rated. UniSS (Q) achieves a naturalness MOS of 4.45, surpassing Seamless-Ex (3.10) and the 3-stage cascaded baseline (4.31) that incorporates a specialized TTS model. This demonstrates UniSS&#8217;s ability to generate high-quality, natural-sounding speech without dedicated text-to-speech components.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Replacing our content-focused GLM-4 linguistic tokenizer with BiCodec&#8217;s self-supervised semantic tokens (<span class=\"ltx_text ltx_font_bold\">w/o GLM</span>) leads to significant performance degradation of -15.01 and -8.73 Speech-BLEU points.\nThis drop reveals that while BiCodec&#8217;s semantic tokens excel at speech generation tasks, their self-supervised nature limits their effectiveness for content understanding in the S2ST task.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Removing the intermediate text generation and performing direct speech-to-speech translation (<span class=\"ltx_text ltx_font_bold\">Direct S2ST</span>) results in a severe performance degradation of -14.94 and -14.40 Speech-BLEU points in the inference. This demonstrates that our cross-modal CoT prompting enables the transfer of textual translation expertise to the speech domain and improves translation fidelity.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "speech",
                    "translation",
                    "speechtospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces UniSS, a unified single-stage expressive speech-to-speech translation framework that preserves voice and emotional style. Our approach eliminates architectural complexity through well-designed speech semantic and style modeling, enabling seamless integration with pre-trained LLMs. By transferring LLMs&#8217; text translation capabilities to speech via cross-modal CoT prompting, UniSS surpasses previous S2ST systems across multiple dimensions: translation fidelity, speech naturalness, and preservation of voice, emotion, and duration consistency.\nBeyond the model architecture, we design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English expressive S2ST dataset. Our work demonstrates a simple and effective approach for building the next generation of expressive S2ST systems.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation",
                    "pipeline",
                    "speech",
                    "llms",
                    "s2st",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Works. </span>\nCurrent limitations point to several research directions. (1) Language Support: We trained UniSS only on Chinese and English due to resource limitations. The data construction pipeline and training framework can be easily extended to multilingual scenarios, which represents our immediate next step. (2) Speech Tokenizer: While the linguistic, speaker, and semantic tokenizers perform well in UniSS, they originate from two different audio tokenizers, causing vocabulary size expansion. Future work will focus on training a unified tokenizer to merge these components and reduce vocabulary size.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "speech",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present real examples from different models here, with speech converted to text using ASR for display purposes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A1.F1\" title=\"Figure A1 &#8227; Appendix A Examples &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">A1</span></a>, UniSS achieves competitive BLEU scores on challenging sentences where GPT-4o struggles with translation quality. Note Seamless-L and GPT-4o cannot preserve source voice and emotional style.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "models",
                    "speech",
                    "asr",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T1\" title=\"Table B1 &#8227; B.1 UniSS &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B1</span></a>, the UniSS is trained in three phases progressively.\nAll audio is resampled to 16&#160;kHz. The LLM vocabulary is expanded to 180,407 to include speech and control tokens.\nWe use the AdamW optimizer with a weight decay of 0.1 and momentum parameters (0.9, 0.95). The batch size is fixed at 2.3M tokens for all phases.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model is trained on a mixture of ASR, TTS, S2TT, and MT tasks.\nThis phase utilizes 77.1k hours of speech data and 2.3B translation tokens from WMT17,\ntotaling approximately 32B tokens per epoch.\nThe model is trained for 3 epochs with a constant learning rate of 8e-4 and a 1-epoch warm-up.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "speech",
                    "asr",
                    "tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce CoT prompting using our UniST General dataset.\nThe model learns three generation paths: Performance mode, Quality mode, and direct S2ST data without intermediate text.\nNew data is mixed with Phase 1 data at a 2:1 ratio, totaling approximately 55B tokens. We train for one epoch with a constant learning rate of 2e-4 and a 5% warm-up.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune the model exclusively on the UniST High-quality dataset.\nThe learning rate follows a cosine schedule, gradually annealing from 5e-5 to 5e-6 over 1 epoch without warm-up. This phase consists of approximately 10B training tokens. During this stage, the training loss reaches its minimum around 0.9 epochs, and we select the checkpoint at this point as our final model weights.</p>\n\n",
                "matched_terms": [
                    "stage",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our language model is trained on 16 NVIDIA H800 80G GPUs. We utilize the Megatron-LM Framework for efficient large-model training.\nBecause audio duration distribution is uneven, padding would waste significant computational resources. We use the sequence packing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Krell et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib44\" title=\"\">2022</a>]</cite> technique to concatenate multiple samples into a single 18k token long sequence. We use a global batch size of 128. Completing all three phases of training takes approximately 6 days.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implement two cascaded baselines: a conventional three-stage baseline (ASR <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> MT <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> TTS) and a two-stage baseline (S2TT <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> TTS). We select state-of-the-art, widely adopted models at each stage to represent strong and practically relevant baselines.</p>\n\n",
                "matched_terms": [
                    "threestage",
                    "baseline",
                    "stage",
                    "models",
                    "asr",
                    "tts",
                    "twostage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic Speech Recognition (ASR)</span>: We select Whisper-large-v3, a transformer-based encoder-decoder model trained on large-scale multilingual speech corpora, for its strong multilingual transcription performance and wide adoption in the community.</p>\n\n",
                "matched_terms": [
                    "whisperlargev3",
                    "speech",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Machine Translation (MT)</span>: We use Meta&#8217;s NLLB-200-distilled-600M model, a multilingual transformer trained on parallel corpora covering 200 languages. We select this model for its balance between translation quality and computational efficiency.</p>\n\n",
                "matched_terms": [
                    "model",
                    "translation",
                    "nllb200distilled600m"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Speech (TTS)</span>: We employ CosyVoice 2 (0.5B), a transformer-based expressive TTS model designed for high-quality multilingual speech synthesis. Its ability to generate natural and expressive audio makes it suitable for our evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts",
                    "05b",
                    "cosyvoice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two-stage pipeline simplifies the cascaded approach by combining speech recognition and translation into one direct Speech-to-Text Translation (S2TT) module:</p>\n\n",
                "matched_terms": [
                    "translation",
                    "pipeline",
                    "speechtotext",
                    "speech",
                    "twostage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-to-Text Translation (S2TT)</span>: We select SeamlessM4T-v2-Large, a multilingual transformer-based model that jointly performs ASR and MT. Its end-to-end design reduces error propagation across stages and offers strong performance in speech translation.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "speechtotext",
                    "speech",
                    "asr",
                    "model",
                    "seamlessm4tv2large",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Speech (TTS)</span>: We generate synthesized speech using the same CosyVoice 2 model described in the three-stage pipeline.</p>\n\n",
                "matched_terms": [
                    "threestage",
                    "pipeline",
                    "cosyvoice",
                    "speech",
                    "tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, we observe several limitations associated with cascaded baselines. First, the inference process is noticeably slower due to the sequential execution of multiple large models, making the pipeline less suitable for real-time applications. Second, we find that errors introduced in earlier stages, particularly in ASR, tend to propagate downstream, often leading to degraded translation quality or unnatural speech synthesis. These limitations highlight the challenges of building efficient and robust cascaded speech translation systems.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "pipeline",
                    "models",
                    "speech",
                    "asr",
                    "experiments",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ three advanced multimodal large language models as baselines: Qwen2-audio, Qwen2.5-Omni, and GPT-4o-audio. These models are capable of handling speech input and producing either textual or speech output, depending on the task.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speech",
                    "large",
                    "multimodal",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen2.5-Omni is selected for its ability to perform fully end-to-end speech-to-speech translation without requiring intermediate text generation, making it a promising multimodal LLM baseline for direct S2ST tasks. In our experiments, we find that the model&#8217;s performance is highly sensitive to the prompt format. When using a simple default instruction, the model sometimes appends assistant-like phrases (e.g., &#8220;Do you need anything else?&#8221;) to the translated speech, likely due to its conversational fine-tuning.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation",
                    "baseline",
                    "speech",
                    "experiments",
                    "s2st",
                    "multimodal",
                    "model",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, we find that such detailed prompts often lead to degraded audio output quality. Specifically, in our experiments, the model occasionally produces abnormal acoustic artifacts, such as elongating a single word unnaturally or producing disfluent prosody. This suggests that Qwen2.5-Omni&#8217;s speech generation may be overly sensitive to instruction format, and that more verbose prompts can negatively impact generation fluency despite offering better control over semantic behavior.</p>\n\n",
                "matched_terms": [
                    "experiments",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen2-audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib45\" title=\"\">2024</a>]</cite> is used for direct speech-to-text translation. Considering its strong performance on multilingual speech comprehension tasks and its ability to follow structured prompts, we adopt it for S2TT evaluation. Prompts are designed to explicitly guide the model to translate input speech into target-language text. For example:</p>\n\n",
                "matched_terms": [
                    "translation",
                    "speechtotext",
                    "speech",
                    "qwen2audio",
                    "used",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For S2ST evaluation, we employ GPT-4o-audio-preview, which supports multimodal speech translation tasks with direct speech-to-speech generation capabilities. Given the instability of Qwen2.5-Omni under complex prompt instructions observed in preliminary experiments, we select GPT-4o-audio-preview as a more robust alternative. Our experiments confirm that GPT-4o-audio-preview demonstrates superior stability and consistently produces higher-quality outputs compared to previous models.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudiopreview",
                    "speechtospeech",
                    "translation",
                    "models",
                    "speech",
                    "experiments",
                    "s2st",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although we occasionally encounter conversational artifacts, such as appended phrases like &#8220;Do you need help with anything else?&#8221; or generic fallback responses like &#8220;I&#8217;m sorry, I can&#8217;t process this audio&#8221;, the overall translation fluency and speech quality are significantly improved compared to earlier baselines. However, GPT-4o is not open-sourced, and using its API for large-scale translation incurs considerable cost, which poses a practical constraint in real-world deployment scenarios.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation",
                    "not",
                    "api"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For direct end-to-end baselines, we evaluate several state-of-the-art S2ST systems that translate input audio directly into target-language speech, bypassing intermediate textual representations. Specifically, we include SeamlessM4T Medium, SeamlessM4T Large-v2, and SeamlessExpressive in our evaluation.</p>\n\n",
                "matched_terms": [
                    "largev2",
                    "speech",
                    "medium",
                    "s2st",
                    "seamlessm4t",
                    "seamlessexpressive",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeamlessM4T Medium and Large-v2 are multilingual speech translation models that support speech-to-speech translation without explicit intermediate transcription or synthesis stages. These models adopt a unified encoder-decoder architecture and are designed to support both low-resource and high-resource language pairs. In our experiments, we use the official inference pipelines provided by Meta with default decoding parameters. No additional prompt tuning, input preprocessing, or output filtering is applied.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation",
                    "largev2",
                    "speech",
                    "models",
                    "medium",
                    "experiments",
                    "seamlessm4t",
                    "official"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeamlessExpressive extends SeamlessM4T by introducing expressiveness-aware modeling, enabling more natural and emotionally rich speech synthesis in the target language. The model conditions generation on prosodic and expressive cues from the source speech and is particularly well-suited for conversational and affective scenarios. We evaluate SeamlessExpressive using its official inference scripts and default hyperparameters.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "source",
                    "seamlessm4t",
                    "model",
                    "seamlessexpressive",
                    "official"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Seed LiveInterpret 2.0 is an enterprise-level simultaneous interpretation model that performs real-time speech-to-speech translation with voice cloning capabilities. The model employs a duplex framework that processes input audio and generates target-language speech directly without intermediate text representations. In our evaluation, we used the official inference API with default parameters.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation",
                    "speech",
                    "api",
                    "used",
                    "model",
                    "official"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the capability to perform direct speech-to-speech translation, current end-to-end models still face notable limitations in audio quality. In our experiments, the generated speech often contained audible artifacts or background noise, especially for longer or noisier input segments. While SeamlessExpressive improves upon previous models by attempting to clone speaker identity, prosody, and expressiveness, its expressive fidelity remains limited. The cloned voice frequently lacks the richness and nuance of natural human speech, and variations in emotion or emphasis are not always faithfully reproduced.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation",
                    "models",
                    "speech",
                    "experiments",
                    "seamlessexpressive",
                    "endtoend",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the performance of speech-to-speech translation (S2ST), we report a range of objective metrics covering translation accuracy, speaker identity preservation, prosodic alignment, temporal consistency, and speech quality. For all metrics, higher values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "speech",
                    "translation",
                    "speechtospeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-BLEU</span>, or ASR-BLEU, evaluates the translation fidelity of speech-to-speech translation systems. We transcribe the generated speech using ASR models, Whisper-large-v3 for English and Paraformer-zh for Chinese, and compute the BLEU score between the transcribed output and the ground-truth reference. Preprocessing follows the same pipeline as Text-BLEU.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation",
                    "pipeline",
                    "models",
                    "speech",
                    "asr",
                    "whisperlargev3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SLC-0.2</span> and <span class=\"ltx_text ltx_font_bold\">SLC-0.4</span> (Speech Length Compliance) assess the temporal consistency between the source and translated audio by measuring whether the output duration falls within &#177;20% or &#177;40% of the original speech length. These metrics are important in real-world scenarios such as simultaneous interpretation and audiovisual dubbing, where maintaining similar speech pacing and timing is essential for naturalness, synchronization, and listener comprehension.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In emotion similarity and speaker similarity tasks, the scale options are: &#8220;1. Definitely Different&#8221;, &#8220;2. Probably Different&#8221;, &#8220;3. Not be sure&#8221;, &#8220;4. Probably the same&#8221;, &#8220;5. Definitely the same&#8221;. In speech naturalness task, the scale options are: &#8220;1. Definitely unnatural&#8221;, &#8220;2. Probably unnatural&#8221;, &#8220;3. Not be sure&#8221;, &#8220;4. Probably natural&#8221;, &#8220;5. Definitely natural&#8221;.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate on the Chinese and English test subsets of FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib48\" title=\"\">2022</a>]</cite>, a massively multilingual speech dataset spanning 102 languages with n-way parallel speech and text data. UniSS demonstrates robust performance on this challenging multilingual benchmark across multiple evaluation tasks. Additionally, UniSS achieves state-of-the-art performance on speech-to-text translation (S2TT) and competitive results on automatic speech recognition (ASR) and text-to-speech synthesis (TTS) tasks.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "speechtotext",
                    "speech",
                    "asr",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the robustness of our framework across datasets, we further evaluate performance on the FLEURS test set. FLEURS is a multilingual benchmark derived from the FLoRes corpus that provides high-quality parallel speech and text pairs across diverse languages, making it a valuable complement to CVSS-T for assessing speech translation systems under standardized multilingual conditions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T3\" title=\"Table D3 &#8227; D.1 S2ST Performance on Fleurs &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D3</span></a>, UniSS (Q) achieves strong translation performance with Speech-BLEU of 36.16 and Text-BLEU of 36.96 on EN-ZH, outperforming all S2ST baselines and cascaded systems. UniSS (P) also demonstrates competitive performance across both metrics. In the ZH-EN direction, UniSS surpasses all systems with comparable model size, demonstrating the parameter efficiency of our design.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model",
                    "translation",
                    "size"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system demonstrates exceptional speech duration consistency on the FLEURS test set. Both UniSS variants achieve near-perfect scores on the SLC-0.2 and SLC-0.4 metrics, indicating that the generated speech closely mirrors the temporal length of the input audio. This precise duration alignment is critical for real-time applications and audiovisual translation scenarios where timing mismatches can disrupt user experience.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (Q) attains a UTMOS score of 3.41, outperforming all open-source baselines and surpassing GPT-4o (3.36). UniSS (Q) also leads all S2ST models in the ZH-EN direction with a score of 4.16. Despite its significantly smaller size, UniSS consistently produces high-quality speech across both translation directions, confirming its effectiveness as a practical open-source solution for speech-to-speech translation.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation",
                    "models",
                    "speech",
                    "size",
                    "s2st"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond speech-to-speech translation, UniSS also supports speech-to-text translation (S2TT). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>, UniSS achieves leading performance on this task as well. Whisper only supports X-EN translation and demonstrates poor performance on multilingual scenarios. UniSS&#8217;s Text-BLEU scores significantly outperform the 7B multimodal LLM Qwen2-audio across both datasets, demonstrating superior cross-lingual speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation",
                    "speechtotext",
                    "speech",
                    "multimodal",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although UniSS was not specifically designed for automatic speech recognition, it demonstrates competitive ASR capabilities as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. We evaluate the ASR performance on the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib49\" title=\"\">2015</a>]</cite>. On the LibriSpeech test-clean subset, UniSS achieves a WER of 2.4, which is comparable to the open-source state-of-the-art ASR model, Whisper (1.8 WER). However, on the more challenging LibriSpeech test-others subset, UniSS shows degraded performance with a WER of 6.6. It is important to note that UniSS is primarily designed as a speech-to-speech translation model, and these ASR results demonstrate its versatility beyond its core translation capabilities.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation",
                    "speech",
                    "asr",
                    "model",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although UniSS was not specifically designed for text-to-speech synthesis, we evaluate its TTS capabilities on the LibriSpeech test-clean dataset as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. UniSS achieves a WER of 4.75 and speaker similarity of 0.568, compared to SparkTTS, which obtains 1.98 WER and 0.584 similarity. The performance gap is expected given that SparkTTS is specifically optimized for TTS, whereas UniSS is primarily designed for speech-to-speech translation. These results demonstrate that UniSS can generate intelligible speech output.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation",
                    "speech",
                    "tts",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select a diverse collection of publicly available Chinese and English speech datasets as the foundation for UniSS training data. Our data sources encompass multiple domains and speaking styles to ensure robust cross-lingual speech translation capabilities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Below is the list of source corpora we used:</p>\n\n",
                "matched_terms": [
                    "used",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CoVoST2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib51\" title=\"\">2020</a>]</cite>: A multilingual speech-to-text translation corpus derived from Common Voice.</p>\n\n",
                "matched_terms": [
                    "speechtotext",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CVSS-T</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>]</cite>: A benchmark dataset for speech-to-speech translation in English-Chinese language pairs.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LibriSpeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib49\" title=\"\">2015</a>]</cite>: A multi-speaker English speech corpus with reading audiobooks for TTS.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After cleaning the source dataset, we employ Qwen-2.5-72B-Instruct as our translation engine to translate source transcriptions into the target language. For source language Chinese audio, we use the following prompt:</p>\n\n",
                "matched_terms": [
                    "translation",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For source language English audio, we use the corresponding Chinese version of this prompt to ensure consistent translation quality across language directions.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the translation process, we clean the translated text based on the reply format characteristics of Qwen-2.5-72B-Instruct through several steps: (1) removing common prefixes such as &#8220;Sure, here is the translation:&#8221; and its corresponding Chinese version that are automatically generated by the model; (2) eliminating explanatory notes and comments (e.g., &#8220;Note:&#8221; and corresponding Chinese version) that often accompany the translations; (3) normalizing text formatting by removing line breaks and excessive whitespace characters; and (4) filtering out mixed-language content to ensure linguistic purity, where translations containing both Chinese and English characters are discarded as invalid outputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a subjective assessment on a randomly sampled subset of UniST to evaluate the quality of our dataset. Four raters listened to 150 examples and provided MOS scores across three dimensions: Emotion Similarity, Speaker Similarity, and Speech Naturalness. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.T5\" title=\"Table E5 &#8227; E.3 Analysis of UniST &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E5</span></a>, UniST synthesized by SparkTTS achieves high speech naturalness and effectively preserves both the source speaker&#8217;s voice characteristics and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We acknowledge that our work on speech-to-speech translation, which preserves a speaker&#8217;s vocal identity, has the potential for dual use. While our primary goal is to foster more natural and personal cross-lingual communication, benefiting applications like international conferencing, film dubbing, and assistive technologies. However, the technology could be misused by malicious actors. Potential risks include the creation of convincing audio deepfakes for disinformation campaigns, impersonation for fraud, or targeted harassment.</p>\n\n",
                "matched_terms": [
                    "speechtospeech",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although Spark-TTS does not achieve state-of-the-art performance in objective evaluations of speaker similarity, we observe that its performance in subjective evaluations is comparable. Furthermore, we assume that its voice decoupling capability reduces the modeling complexity for large language models, which is why we deploy BiCodec for the generation speech tokenizer in the LLM. Additionally, Spark-TTS generates BiCodec tokens directly, making training more efficient by eliminating an extra encoding step.</p>\n\n",
                "matched_terms": [
                    "large",
                    "speech",
                    "models",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The progressive training strategy pertains only to the training process and does not increase the model&#8217;s structural complexity. The alignment in phase 1 uses existing, easily accessible open-source data. In the ablation study section, we demonstrate the impact of different stages on model performance, proving that progressive training is effective.</p>\n\n",
                "matched_terms": [
                    "model",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speaker tokenizer is used to represent global speaker information. We hypothesize that this voice decoupling reduces the difficulty of speech translation by allowing the model to focus more on the linguistic content. We employ different linguistic and semantic tokenizers for understanding and generation to optimize each task. To generate more natural and expressive audio, the generation process uses semantic tokens, which retain information beyond the core content. However, this additional information makes content understanding more challenging. Therefore, the linguistic tokenizer, which focuses solely on content information, is better suited for the speech understanding phase. As a simple speech representation, we believe this design does not introduce greater complexity in either training or inference.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "speech",
                    "used",
                    "model",
                    "not"
                ]
            }
        ]
    },
    "A4.T3": {
        "source_file": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
        "caption": "Table D3: Main comparison results on the Fleurs dataset. Results are presented as EN-ZH || ZH-EN. Higher scores indicate better performance. ‘-’ denotes unavailable results. Best scores are in bold and second-best scores are underlined.",
        "body": "Category\nModel\n#Size\nSpeech-BLEU\nText-BLEU\nA.PCP\nSLC 0.2\nSLC 0.4\nUTMOS\n\n\n\n\nCascaded\n3-Stage\n2.6B\n22.00 || 21.49\n24.56 || 22.09\n\n2.86 || 2.73\n\n0.27 || 0.37\n0.48 || 0.60\n2.14 || 3.82\n\n\n2-Stage\n2.8B\n31.42 || 23.01\n34.72 || 23.67\n\n2.94 || 2.06\n0.46 || 0.38\n0.74 || 0.61\n2.13 || 3.83\n\n\nMLLM\nGPT-4o\n-\n\n38.58 || 23.21\n\n- || -\n2.72 || 2.39\n0.45 || 0.42\n0.71 || 0.64\n3.36 || 4.25\n\n\n\nQwen2.5-O\n7B\n3.37 || 24.20\n\n\n37.76 || 25.15\n\n1.93 || 2.21\n0.07 || 0.33\n0.27 || 0.56\n3.05 || 4.33\n\n\n\nS2ST\nSeamless-M\n1.2B\n14.10 || 13.30\n27.92 || 17.71\n2.19 || 2.10\n0.24 || 0.15\n0.47 || 0.29\n2.64 || 3.52\n\n\nSeamless-L\n2.3B\n28.86 || 23.84\n32.99 || 24.06\n\n2.18 || 2.06\n0.42 || 0.14\n0.66 || 0.32\n2.42 || 3.90\n\n\nSeamless-Ex\n1.7B\n18.80 || 18.88\n34.53 || 18.89\n2.64 || 2.64\n0.69 || 0.32\n0.88 || 0.58\n2.06 || 3.19\n\n\nUniSS (P)\n1.5B\n34.74 || 22.42\n36.12 || 22.71\n2.72 || 2.65\n\n0.95 || 0.75\n0.99 || 0.94\n\n3.40 || 4.15\n\n\nUniSS (Q)\n1.5B\n\n36.16 || 23.06\n\n36.96 || 23.37\n2.72 || 2.64\n\n0.95 || 0.76\n\n\n0.99 || 0.93\n\n\n3.41 || 4.16",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Category</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">#Size</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Speech-BLEU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">Text-BLEU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">A.PCP</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">SLC 0.2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">SLC 0.4</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\"><span class=\"ltx_text ltx_font_bold\">UTMOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Cascaded</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">3-Stage</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.6B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">22.00 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m3\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 21.49</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">24.56 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m4\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 22.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">2.86</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m5\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">2.73</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.27 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m6\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.48 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m7\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.14 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m8\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 3.82</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2-Stage</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.8B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">31.42 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m9\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 23.01</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">34.72 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m10\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 23.67</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<span class=\"ltx_text ltx_font_bold\">2.94</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m11\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.06</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.46 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m12\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.38</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.74 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m13\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.61</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.13 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m14\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 3.83</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">MLLM</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">GPT-4o</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">-</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<span class=\"ltx_text ltx_font_bold\">38.58</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m15\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_framed ltx_framed_underline\">23.21</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">- <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m16\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> -</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.72 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m17\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.45 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m18\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.71 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m19\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">3.36 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m20\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_framed ltx_framed_underline\">4.25</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Qwen2.5-O</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">7B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">3.37 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m21\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">24.20</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<span class=\"ltx_text ltx_font_bold\">37.76</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m22\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">25.15</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">1.93 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m23\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.07 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m24\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.33</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.27 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m25\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.56</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">3.05 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m26\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">4.33</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"6\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">S2ST</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Seamless-M</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">1.2B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">14.10 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m27\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 13.30</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">27.92 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m28\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 17.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.19 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m29\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.24 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m30\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.47 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m31\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.64 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m32\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 3.52</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Seamless-L</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.3B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">28.86 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m33\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 23.84</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">32.99 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m34\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_framed ltx_framed_underline\">24.06</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.18 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m35\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.06</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.42 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m36\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.14</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.66 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m37\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.42 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m38\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 3.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">Seamless-Ex</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">1.7B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">18.80 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m39\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 18.88</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">34.53 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m40\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 18.89</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.64 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m41\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.64</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.69 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m42\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.88 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m43\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.58</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.06 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m44\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 3.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">UniSS (P)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">1.5B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">34.74 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m45\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 22.42</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">36.12 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m46\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 22.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.72 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m47\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_framed ltx_framed_underline\">2.65</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.95 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m48\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">0.99 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m49\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">3.40</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m50\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 4.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">UniSS (Q)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">1.5B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">36.16</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m51\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 23.06</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\">36.96</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m52\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 23.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">2.72 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m53\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 2.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<span class=\"ltx_text ltx_font_bold\">0.95</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m54\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">0.76</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<span class=\"ltx_text ltx_font_bold\">0.99</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m55\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_framed ltx_framed_underline\">0.93</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.7pt;padding-right:5.7pt;\">\n<span class=\"ltx_text ltx_font_bold\">3.41</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T3.m56\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 4.16</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "speechbleu",
            "17b",
            "textbleu",
            "main",
            "slc",
            "presented",
            "secondbest",
            "underlined",
            "3stage",
            "utmos",
            "qwen25o",
            "unavailable",
            "apcp",
            "denotes",
            "best",
            "seamlessex",
            "results",
            "cascaded",
            "gpt4o",
            "seamlessm",
            "bold",
            "scores",
            "zhen",
            "category",
            "performance",
            "fleurs",
            "26b",
            "enzh",
            "comparison",
            "15b",
            "12b",
            "mllm",
            "dataset",
            "higher",
            "indicate",
            "2stage",
            "size",
            "23b",
            "uniss",
            "better",
            "28b",
            "s2st",
            "seamlessl",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T3\" title=\"Table D3 &#8227; D.1 S2ST Performance on Fleurs &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D3</span></a>, UniSS (Q) achieves strong translation performance with Speech-BLEU of 36.16 and Text-BLEU of 36.96 on EN-ZH, outperforming all S2ST baselines and cascaded systems. UniSS (P) also demonstrates competitive performance across both metrics. In the ZH-EN direction, UniSS surpasses all systems with comparable model size, demonstrating the parameter efficiency of our design.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cmots.github.io/uniss-demo\" title=\"\">https://cmots.github.io/uniss-demo</a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "results",
                    "s2st",
                    "uniss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A conventional cascaded S2ST system typically consists of three sequential components: automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech (TTS) synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Casacuberta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib4\" title=\"\">2004</a>; Nakamura et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib5\" title=\"\">2006</a>)</cite>. However, this cascaded architecture often suffers from error accumulation across stages and struggles to retain paralinguistic features of the original speech. To address these limitations, subsequent research has shifted towards end-to-end approaches that aim to translate speech to another language while preserving expressive characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib7\" title=\"\">2024</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib8\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "cascaded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most recently, the application of large language models (LLMs) to generative speech tasks has further accelerated the development of S2ST systems. Current approaches typically fall into two categories: (1) single-stage methods, which directly predict multi-stream acoustic tokens autoregressively via multi-head outputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite>; and (2) two-stage pipelines, which first generate semantic tokens autoregressively, followed by another autoregressive (AR) model to predict acoustic tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>; Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Gong and Veluri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>)</cite> use a single AR language model to jointly model semantic and partial acoustic tokens, along with a non-autoregressive (NAR) model for complete acoustic information.\nWhile these approaches have shown promising results, they also introduce significantly more architectural complexity than textual LLMs. Additionally, they treat the LLM as a sequence-to-sequence converter, failing to leverage the pre-trained knowledge for textual translation embedded within the LLM.</p>\n\n",
                "matched_terms": [
                    "results",
                    "s2st",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In pursuit of high-fidelity, expressive S2ST, and to overcome the limitations outlined above, our vision is defined by three core principles: (1) a single-stage architecture to eliminate complexity; (2) a unified model that aligns speech and text modalities; and (3) a mechanism to explicitly leverage the proven text translation capabilities of LLMs. To our knowledge, no existing approach satisfies all three principles simultaneously.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of architectural complexity, we present <span class=\"ltx_text ltx_font_bold\">UniSS</span>, a unified single-stage S2ST framework that preserves timbre, emotion, and duration consistency.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> demonstrates our approach and performance results. UniSS builds upon pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> as the backbone, modeling content and speaker information in a single AR language model without architectural modifications. We transfer the LLM&#8217;s pre-trained text translation capabilities to the speech domain through a cross-modality chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib15\" title=\"\">2022</a>)</cite>. During inference, our Quality Mode guides the model to decompose the complex S2ST task into sequential listen, translate, and speak steps within a single inference pass. We also introduce a Performance Mode that trades translation fidelity for faster inference, enabling deployment across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "results",
                    "s2st",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realizing the vision outlined above demands large-scale, high-quality training data that preserves both translation accuracy and speaker expressiveness. However, existing S2ST datasets are either small-scale to train powerful unified models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite> or suffer from quality control issues when scraped from the web&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite>. To address this data challenge, we design a scalable synthesis pipeline and contribute UniST, a 44.8k-hour Chinese-English S2ST dataset offering high translation fidelity and rich speaker preservation.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Unified Single-Stage Architecture for S2ST:</span> We propose UniSS, a single-stage AR model for expressive S2ST that directly leverages a pre-trained textual LLM, eliminating the architectural complexity of prior work.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Large-Scale, High-Quality S2ST Dataset:</span> We design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English S2ST dataset with speaker voice and emotion preservation.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation must preserve semantic accuracy during cross-lingual conversion. Traditional cascaded systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Casacuberta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib4\" title=\"\">2004</a>; Nakamura et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib5\" title=\"\">2006</a>)</cite> chain ASR, MT, and TTS components sequentially, suffering from error accumulation and information loss through text bottlenecks. Early direct methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib1\" title=\"\">2019</a>)</cite> encountered significant challenges with translation quality and synthesis artifacts.\nThe breakthrough came with discrete unit-based methods, where speech-to-unit translation (S2UT) approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib2\" title=\"\">2022</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib17\" title=\"\">2021</a>; Inaguma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib3\" title=\"\">2023</a>)</cite> employ discrete speech units (subwords, phonemes, or semantic tokens) as intermediate representations, which enables effective disentanglement of linguistic content from acoustic properties. SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite> achieved robust multilingual performance via unified multitask optimization of translation and synthesis.\nRecent systems focus on enhancing expressiveness while maintaining translation fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib18\" title=\"\">2022b</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib19\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib7\" title=\"\">2024</a>)</cite>. SeamlessExpressive&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite> designs a PRETSSEL vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib20\" title=\"\">2024</a>)</cite> to enhance expressiveness preservation. TransVIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib8\" title=\"\">2024</a>)</cite> employs feature disentanglement to separately model semantic, acoustic, and temporal information for voice and isochrony control.</p>\n\n",
                "matched_terms": [
                    "cascaded",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of LLMs in text processing has inspired their adaptation to S2ST through speech tokenization. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib21\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib22\" title=\"\">2025</a>)</cite> focus primarily on semantic translation without preserving voice characteristics.\nTo address voice preservation, some works model acoustic features using multiple AR models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>; Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>)</cite>. Recent unified approaches attempt to model both semantic and acoustic features within a single AR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>; Gong and Veluri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite>. However, they still require an additional NAR model for complete acoustic generation. Hibiki&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite> proposes a multi-stream architecture processing source and target speech simultaneously, but necessitates substantial architectural modifications, including nested Transformers and training from scratch.\nA fundamental limitation across these approaches is their reliance on complex architectures to complete acoustic representations. Moreover, they fail to explicitly leverage the LLM&#8217;s pre-trained text translation capabilities, treating them merely as generic sequence converters. UniSS addresses both limitations through a single-stage architecture while effectively transferring LLM&#8217;s text translation capabilities to speech.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 UniSS Architecture &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, UniSS is a unified AR language model for expressive speech-to-speech translation, built upon a single-stage architecture with a cross-modal chain-of-thought prompting framework, and optimized via a progressive training strategy. This section details the three core components of UniSS: model architecture, cross-modal CoT prompting, and training methodology.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of UniSS is to perform expressive S2ST by modeling the conditional distribution <math alttext=\"P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo fence=\"false\">|</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})</annotation></semantics></math>, where <math alttext=\"\\mathbf{X}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{src}</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> denote the source and target speech waveforms, respectively. To achieve faithful content translation while preserving speaker identity and expressiveness, UniSS introduces three types of speech tokens: speaker tokens <math alttext=\"\\mathbf{S}^{spk}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{spk}</annotation></semantics></math> with fixed sequence length to capture global style attributes, e.g., timbre, prosody, and emotion; linguistic tokens <math alttext=\"\\mathbf{S}^{ling}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{ling}</annotation></semantics></math> to encode the content of the source utterance; and semantic tokens <math alttext=\"\\mathbf{S}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}</annotation></semantics></math> to represent the expressive target utterance and can be directly decoded into waveform when combined with speaker tokens.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This unified design allows UniSS to handle expressive S2ST without intermediate acoustic representations or cascaded systems, maintaining fidelity in both content and voice.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "cascaded",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build our generation model upon the pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite>, a strong LLM backbone. By expanding the model vocabulary to include the discrete speech tokens, UniSS treats speech and text uniformly as token sequences, allowing both modalities to be processed within the same transformer architecture.</p>\n\n",
                "matched_terms": [
                    "model",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the S2ST task, the source waveform <math alttext=\"\\mathbf{X_{src}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;&#119851;&#119836;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{src}}</annotation></semantics></math> is represented by <math alttext=\"(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})</annotation></semantics></math>, while the target waveform <math alttext=\"\\mathbf{Y_{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119853;&#119840;&#119853;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{tgt}}</annotation></semantics></math> is represented by <math alttext=\"\\mathbf{S}_{tgt}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}_{tgt}^{sem}</annotation></semantics></math>. This design is motivated by our preliminary experiments, which revealed that although BiCodec&#8217;s semantic tokens are highly effective for waveform reconstruction, their self-supervised nature makes them suboptimal for content understanding. By separately representing speaker identity, linguistic content, and generation-oriented semantics, UniSS enables more accurate and controllable S2ST modeling.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here <math alttext=\"c_{lang}^{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">c_{lang}^{src}</annotation></semantics></math> denotes the source language, and <math alttext=\"\\mathbf{S}^{sem}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}_{src}</annotation></semantics></math> denotes the semantic tokens from source speech. This phase endows the model with robust speech understanding and generation while prioritizing the preservation of text translation capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "denotes"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 2: S2ST with CoT.</span>\nIn the second phase, we introduce the core S2ST task. The model is trained to generate outputs using the CoT prompting formats described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS2\" title=\"3.2 Cross-Modal Chain-of-Thought Prompting &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, as well as a simplified direct generation mode that bypasses intermediate text outputs:</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 3: Refinement.</span>\nIn the final phase, we fine-tune the model on the full S2ST task using both CoT prompting modes.\nThis phase uses an annealed learning rate to stabilize the learned CoT patterns and optimize the final translation performance.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training end-to-end expressive S2ST models faces a significant bottleneck due to the scarcity of large-scale, parallel data that preserves speaker characteristics. To address this limitation, we design a scalable data synthesis pipeline. The pipeline generates UniST, our large-scale Chinese-English expressive S2ST dataset. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) illustrates the data construction process.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS is trained in three progressive phases, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS3\" title=\"3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>. Phase 1 establishes text-speech alignment using 77.1k hours of speech data and MT text data from WMT17&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bojar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib28\" title=\"\">2017</a>)</cite>. Phase 2 introduces CoT prompting with our UniST General dataset, mixed with Phase 1 data at a 2:1 ratio. Phase 3 fine-tunes the model exclusively on the UniST High-Quality dataset.</p>\n\n",
                "matched_terms": [
                    "model",
                    "uniss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training employs the AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov and Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib29\" title=\"\">2019</a>)</cite> with a 2.3M-token batch size, weight decay of 0.1, and momentum parameters (0.9, 0.95). All audio is resampled to 16&#160;kHz, and the LLM vocabulary is expanded to 180,407 to include speech and control tokens. Learning rates progress from 8e-4 in Phase 1 to 2e-4 in Phase 2, and finally anneal from 5e-5 to 5e-6 in Phase 3. The model is trained on 16 NVIDIA H800 80G GPUs using the Megatron-LM Framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shoeybi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib30\" title=\"\">2019</a>)</cite> for efficient large-model training. Complete training details and hyperparameters are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.SS1\" title=\"B.1 UniSS &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our primary S2ST evaluation, we use the Chinese (ZH) and English (EN) test sets from CVSS-T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite>, which contain 4,897 utterance pairs (8.2 hours for Chinese and 6.3 hours for English). For emotion preservation evaluation, we randomly sample 300 utterances from each emotional speech dataset (ESD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib31\" title=\"\">2021</a>)</cite> and CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib32\" title=\"\">2014</a>)</cite>), covering a selected set of 4 emotions (happy, sad, angry, neutral). We evaluate both English-to-Chinese (EN-ZH) and Chinese-to-English (ZH-EN) translation directions.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "enzh",
                    "zhen",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ comprehensive objective and subjective metrics to evaluate S2ST performance. For translation fidelity, we measure BLEU scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib33\" title=\"\">2002</a>)</cite> on both ASR-transcribed speech outputs (Speech-BLEU) and intermediate text (Text-BLEU). Prosody preservation is assessed through prosody similarity (A.PCP)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Andrews et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib34\" title=\"\">2022</a>)</cite>. Duration consistency is evaluated using Speech Length Compliance (SLC) within 0.2 and 0.4 ratio tolerance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nSpeech quality is measured using UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib36\" title=\"\">2022</a>)</cite>. Additionally, we conduct a subjective Mean Opinion Score (MOS) evaluation where bilingual speakers rate emotion similarity, speaker similarity, and naturalness on a 5-point scale. Detailed descriptions of all evaluation metrics and implementation procedures are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A3\" title=\"Appendix C Evaluation Metrics &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "slc",
                    "textbleu",
                    "performance",
                    "s2st",
                    "utmos",
                    "scores",
                    "apcp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Systems.</span>\nTo establish an upper bound for modular approaches, we construct two cascaded baselines using top-performing open-source models: a <span class=\"ltx_text ltx_font_bold\">3-Stage</span> pipeline of Whisper-large-v3 (ASR), NLLB-200-distilled (MT)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Koishekenov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib37\" title=\"\">2023</a>)</cite>, and CosyVoice 2 (TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib38\" title=\"\">2024</a>)</cite>; and a <span class=\"ltx_text ltx_font_bold\">2-Stage</span> pipeline of SeamlessM4T-v2-Large (S2TT) and CosyVoice 2 (TTS).</p>\n\n",
                "matched_terms": [
                    "2stage",
                    "cascaded",
                    "3stage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Large Language Models (MLLMs).</span>\nTo benchmark against the latest generation of general-purpose models, we include two leading MLLMs: <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib39\" title=\"\">2024</a>)</cite>, an enterprise-level model from OpenAI with strong speech-to-speech capability; and <span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib40\" title=\"\">2025</a>)</cite>, a powerful open-source MLLM building on large-scale audio&#8211;language pretraining, enabling both speech understanding and synthesis.</p>\n\n",
                "matched_terms": [
                    "mllm",
                    "gpt4o",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End S2ST Systems.</span>\nWe compare against dedicated S2ST models that represent the current open-source SOTA: <span class=\"ltx_text ltx_font_bold\">SeamlessM4T</span> (Medium and Large V2, denoted Seamless-M and Seamless-L), and its expressive variant <span class=\"ltx_text ltx_font_bold\">SeamlessExpressive</span> (Seamless-Ex). For subjective metrics, we also evaluate <span class=\"ltx_text ltx_font_bold\">Seed LiveInterpret 2.0</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib41\" title=\"\">2025</a>)</cite>, denoted as Seed Live, an enterprise-level S2ST system with duplex speech understanding and generation abilities.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "seamlessl",
                    "seamlessex",
                    "seamlessm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For UniSS experiments, we deploy vLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib42\" title=\"\">2023</a>)</cite> to support inference. We set a decoding temperature of 0.7, top-k of -1, top-p of 0.8, and a repetition penalty of 1.1. We report results for both Performance mode, denoted as <span class=\"ltx_text ltx_font_bold\">UniSS (P)</span>, and Quality mode, denoted as <span class=\"ltx_text ltx_font_bold\">UniSS (Q)</span>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uniss",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T1\" title=\"Table 1 &#8227; 5.3 Objective Performance &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main comparison results on the CVSS-T dataset.\nThe results clearly demonstrate that UniSS establishes a new SOTA in translation fidelity\nwhile maintaining voice characteristics, prosody, duration consistency, and speech quality, validating our core design principles. Additional results on other datasets and tasks are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4\" title=\"Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "main",
                    "comparison",
                    "results",
                    "uniss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation Fidelity.</span> UniSS achieves state-of-the-art translation fidelity on both EN-ZH and ZH-EN directions. The UniSS (Q) variant achieves a Speech-BLEU of 32.20 on EN-ZH and 24.28 on ZH-EN, substantially outperforming all prior end-to-end and cascaded baselines. The efficient UniSS (P) also delivers strong results, surpassing most existing systems. Notably, in terms of intermediate text metrics, UniSS models perform on par with or better than larger multimodal LLM-based approaches.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "enzh",
                    "better",
                    "results",
                    "cascaded",
                    "uniss",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody Preservation.</span> In terms of prosody, UniSS achieves competitive performance. UniSS (P) variant achieves the second-highest A.PCP score (2.73 and 2.75), closely following Seamless-Ex, which incorporates a dedicated prosody encoder. The performance gap is marginal at only 0.10 (EN-ZH) and 0.12 (ZH-EN), highlighting the efficacy of UniSS in preserving prosodic patterns without specialized modules.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "enzh",
                    "seamlessex",
                    "uniss",
                    "zhen",
                    "apcp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Duration Consistency.</span> UniSS demonstrates superior duration consistency. UniSS (Q) achieves near-optimal SLC 0.2 scores on EN-ZH and the best performance on ZH-EN, improving over the previous best end-to-end system (Seamless-Ex) by 44% and 67%. On the more relaxed SLC 0.4 metric, while competing systems achieve scores above 0.90, both UniSS variants deliver near-perfect performance with scores of 0.99 (EN-ZH) and 0.97 (ZH-EN).</p>\n\n",
                "matched_terms": [
                    "slc",
                    "performance",
                    "best",
                    "enzh",
                    "seamlessex",
                    "zhen",
                    "uniss",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Quality.</span> UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (P) achieves a competitive UTMOS score of 3.77, matching cascaded models (3.76 and 3.79) while surpassing all end-to-end models. In the ZH-EN direction, UniSS surpasses 3-stage systems with a score of 3.86 compared to 3.50.</p>\n\n",
                "matched_terms": [
                    "enzh",
                    "3stage",
                    "s2st",
                    "cascaded",
                    "utmos",
                    "uniss",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion Preservation.</span>\nUniSS (Q) demonstrates strong capability in preserving emotions, achieving a MOS of 4.51. This represents a substantial 27% improvement over the expressive S2ST baseline Seamless-Ex (3.56) and surpasses the 3-stage cascaded system (4.48). Notably, UniSS (Q) also approaches Seed Live (4.56), indicating its ability in capturing emotional nuance.</p>\n\n",
                "matched_terms": [
                    "3stage",
                    "seamlessex",
                    "s2st",
                    "cascaded",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Preservation.</span>\nUniSS effectively retains speaker voice characteristics without requiring an additional NAR stage-2 model. For speaker similarity, UniSS (Q) achieves a score of 4.42, outperforming all other models. This represents a 0.07 improvement over the 2-stage system (4.35), which deploys a carefully designed TTS model. These results underscore the ability of UniSS to maintain voice characteristics in an end-to-end fashion.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model",
                    "uniss",
                    "2stage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Naturalness.</span>\nThe naturalness of speech from UniSS is also favorably rated. UniSS (Q) achieves a naturalness MOS of 4.45, surpassing Seamless-Ex (3.10) and the 3-stage cascaded baseline (4.31) that incorporates a specialized TTS model. This demonstrates UniSS&#8217;s ability to generate high-quality, natural-sounding speech without dedicated text-to-speech components.</p>\n\n",
                "matched_terms": [
                    "3stage",
                    "uniss",
                    "seamlessex",
                    "cascaded",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Subjective Results &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the Performance mode achieves a 1.07<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p2.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> speedup over Quality mode with only a 1.84 point reduction in Speech-BLEU, demonstrating a favorable speed-quality trade-off.\nFurthermore, we trained <span class=\"ltx_text ltx_font_bold\">UniSS-Small</span> based on Qwen2.5-0.5B-Instruct, achieving significant computational savings.\nUniSS-Small (P) delivers 1.25<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS5.p2.m2\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> speedup with competitive translation fidelity (25.68 Speech-BLEU),\nmaking it suitable for resource-constrained deployment scenarios while maintaining the advantages of our unified architecture.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct ablation studies to validate the effect of our design.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T4\" title=\"Table 4 &#8227; 5.6 Ablation Studies &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the detailed results comparing variants against a base model trained through Phases 1 and 2. The results in type Base and Train are evaluated in the Performance mode.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our three-phase progressive training significantly impacts performance.\nPhase 3 refinement (<span class=\"ltx_text ltx_font_bold\">w/ Phase 3</span>) contributes improvements of +0.90 and +2.06 Speech-BLEU points, validating the importance of high-quality data fine-tuning for final optimization.\nRemoving the initial Phase 1 alignment (<span class=\"ltx_text ltx_font_bold\">UniST only</span>) causes severe performance degradation of -7.18 and -10.15 points. This drop demonstrates that Phase 1 text-speech alignment is essential for subsequent S2ST learning.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "s2st",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Replacing our content-focused GLM-4 linguistic tokenizer with BiCodec&#8217;s self-supervised semantic tokens (<span class=\"ltx_text ltx_font_bold\">w/o GLM</span>) leads to significant performance degradation of -15.01 and -8.73 Speech-BLEU points.\nThis drop reveals that while BiCodec&#8217;s semantic tokens excel at speech generation tasks, their self-supervised nature limits their effectiveness for content understanding in the S2ST task.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "s2st",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Removing the intermediate text generation and performing direct speech-to-speech translation (<span class=\"ltx_text ltx_font_bold\">Direct S2ST</span>) results in a severe performance degradation of -14.94 and -14.40 Speech-BLEU points in the inference. This demonstrates that our cross-modal CoT prompting enables the transfer of textual translation expertise to the speech domain and improves translation fidelity.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "s2st",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces UniSS, a unified single-stage expressive speech-to-speech translation framework that preserves voice and emotional style. Our approach eliminates architectural complexity through well-designed speech semantic and style modeling, enabling seamless integration with pre-trained LLMs. By transferring LLMs&#8217; text translation capabilities to speech via cross-modal CoT prompting, UniSS surpasses previous S2ST systems across multiple dimensions: translation fidelity, speech naturalness, and preservation of voice, emotion, and duration consistency.\nBeyond the model architecture, we design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English expressive S2ST dataset. Our work demonstrates a simple and effective approach for building the next generation of expressive S2ST systems.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "uniss",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Works. </span>\nCurrent limitations point to several research directions. (1) Language Support: We trained UniSS only on Chinese and English due to resource limitations. The data construction pipeline and training framework can be easily extended to multilingual scenarios, which represents our immediate next step. (2) Speech Tokenizer: While the linguistic, speaker, and semantic tokenizers perform well in UniSS, they originate from two different audio tokenizers, causing vocabulary size expansion. Future work will focus on training a unified tokenizer to merge these components and reduce vocabulary size.</p>\n\n",
                "matched_terms": [
                    "size",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present real examples from different models here, with speech converted to text using ASR for display purposes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A1.F1\" title=\"Figure A1 &#8227; Appendix A Examples &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">A1</span></a>, UniSS achieves competitive BLEU scores on challenging sentences where GPT-4o struggles with translation quality. Note Seamless-L and GPT-4o cannot preserve source voice and emotional style.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "gpt4o",
                    "scores",
                    "seamlessl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T1\" title=\"Table B1 &#8227; B.1 UniSS &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B1</span></a>, the UniSS is trained in three phases progressively.\nAll audio is resampled to 16&#160;kHz. The LLM vocabulary is expanded to 180,407 to include speech and control tokens.\nWe use the AdamW optimizer with a weight decay of 0.1 and momentum parameters (0.9, 0.95). The batch size is fixed at 2.3M tokens for all phases.</p>\n\n",
                "matched_terms": [
                    "size",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model is trained on a mixture of ASR, TTS, S2TT, and MT tasks.\nThis phase utilizes 77.1k hours of speech data and 2.3B translation tokens from WMT17,\ntotaling approximately 32B tokens per epoch.\nThe model is trained for 3 epochs with a constant learning rate of 8e-4 and a 1-epoch warm-up.</p>\n\n",
                "matched_terms": [
                    "23b",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce CoT prompting using our UniST General dataset.\nThe model learns three generation paths: Performance mode, Quality mode, and direct S2ST data without intermediate text.\nNew data is mixed with Phase 1 data at a 2:1 ratio, totaling approximately 55B tokens. We train for one epoch with a constant learning rate of 2e-4 and a 5% warm-up.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model",
                    "s2st",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune the model exclusively on the UniST High-quality dataset.\nThe learning rate follows a cosine schedule, gradually annealing from 5e-5 to 5e-6 over 1 epoch without warm-up. This phase consists of approximately 10B training tokens. During this stage, the training loss reaches its minimum around 0.9 epochs, and we select the checkpoint at this point as our final model weights.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our language model is trained on 16 NVIDIA H800 80G GPUs. We utilize the Megatron-LM Framework for efficient large-model training.\nBecause audio duration distribution is uneven, padding would waste significant computational resources. We use the sequence packing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Krell et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib44\" title=\"\">2022</a>]</cite> technique to concatenate multiple samples into a single 18k token long sequence. We use a global batch size of 128. Completing all three phases of training takes approximately 6 days.</p>\n\n",
                "matched_terms": [
                    "size",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide detailed descriptions of all baseline models evaluated against our proposed UniSS framework. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T2\" title=\"Table B2 &#8227; B.2 Baselines &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B2</span></a> summarizes the baselines, model versions, and parameter sizes used in our experiments. All baseline models are evaluated using their default inference parameters without any additional tuning or modifications. Evaluation is conducted directly on the CVSS-T and FLEURS test sets without extra data preprocessing, prompts, or output post-processing. We present the implementation specifics of each baseline in the following subsections.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic Speech Recognition (ASR)</span>: We select Whisper-large-v3, a transformer-based encoder-decoder model trained on large-scale multilingual speech corpora, for its strong multilingual transcription performance and wide adoption in the community.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-to-Text Translation (S2TT)</span>: We select SeamlessM4T-v2-Large, a multilingual transformer-based model that jointly performs ASR and MT. Its end-to-end design reduces error propagation across stages and offers strong performance in speech translation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen2.5-Omni is selected for its ability to perform fully end-to-end speech-to-speech translation without requiring intermediate text generation, making it a promising multimodal LLM baseline for direct S2ST tasks. In our experiments, we find that the model&#8217;s performance is highly sensitive to the prompt format. When using a simple default instruction, the model sometimes appends assistant-like phrases (e.g., &#8220;Do you need anything else?&#8221;) to the translated speech, likely due to its conversational fine-tuning.</p>\n\n",
                "matched_terms": [
                    "s2st",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, we find that such detailed prompts often lead to degraded audio output quality. Specifically, in our experiments, the model occasionally produces abnormal acoustic artifacts, such as elongating a single word unnaturally or producing disfluent prosody. This suggests that Qwen2.5-Omni&#8217;s speech generation may be overly sensitive to instruction format, and that more verbose prompts can negatively impact generation fluency despite offering better control over semantic behavior.</p>\n\n",
                "matched_terms": [
                    "model",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen2-audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib45\" title=\"\">2024</a>]</cite> is used for direct speech-to-text translation. Considering its strong performance on multilingual speech comprehension tasks and its ability to follow structured prompts, we adopt it for S2TT evaluation. Prompts are designed to explicitly guide the model to translate input speech into target-language text. For example:</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the performance of speech-to-speech translation (S2ST), we report a range of objective metrics covering translation accuracy, speaker identity preservation, prosodic alignment, temporal consistency, and speech quality. For all metrics, higher values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "indicate",
                    "performance",
                    "better",
                    "s2st",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-BLEU</span> evaluates the fidelity of generated translations by computing corpus-level BLEU scores using the SacreBLEU library&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Post, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib46\" title=\"\">2018</a>]</cite>. Before scoring, we apply language-specific preprocessing: English text is lowercased and stripped of punctuation (excluding apostrophes), while Chinese text is normalized to simplified characters, punctuation is removed, and characters are separated by spaces. This ensures consistency with standard BLEU evaluation practices. We use the <span class=\"ltx_text ltx_font_italic\">corpus_score</span> function to calculate the BLEU score across the whole dataset. Chinese samples are scored in &#8216;zh&#8217; mode.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "textbleu",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-BLEU</span>, or ASR-BLEU, evaluates the translation fidelity of speech-to-speech translation systems. We transcribe the generated speech using ASR models, Whisper-large-v3 for English and Paraformer-zh for Chinese, and compute the BLEU score between the transcribed output and the ground-truth reference. Preprocessing follows the same pipeline as Text-BLEU.</p>\n\n",
                "matched_terms": [
                    "speechbleu",
                    "textbleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate on the Chinese and English test subsets of FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib48\" title=\"\">2022</a>]</cite>, a massively multilingual speech dataset spanning 102 languages with n-way parallel speech and text data. UniSS demonstrates robust performance on this challenging multilingual benchmark across multiple evaluation tasks. Additionally, UniSS achieves state-of-the-art performance on speech-to-text translation (S2TT) and competitive results on automatic speech recognition (ASR) and text-to-speech synthesis (TTS) tasks.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "fleurs",
                    "results",
                    "uniss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the robustness of our framework across datasets, we further evaluate performance on the FLEURS test set. FLEURS is a multilingual benchmark derived from the FLoRes corpus that provides high-quality parallel speech and text pairs across diverse languages, making it a valuable complement to CVSS-T for assessing speech translation systems under standardized multilingual conditions.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS exhibits excellent prosody transfer capabilities on the FLEURS benchmark. UniSS (Q) achieves A.PCP scores of 2.72 (EN-ZH) and 2.64 (ZH-EN), outperforming all end-to-end baselines and matching GPT-4o&#8217;s performance (2.72 and 2.39). Notably, UniSS achieves these results without incorporating dedicated prosody modeling modules, highlighting the effectiveness of our unified framework.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "fleurs",
                    "enzh",
                    "results",
                    "zhen",
                    "uniss",
                    "scores",
                    "apcp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system demonstrates exceptional speech duration consistency on the FLEURS test set. Both UniSS variants achieve near-perfect scores on the SLC-0.2 and SLC-0.4 metrics, indicating that the generated speech closely mirrors the temporal length of the input audio. This precise duration alignment is critical for real-time applications and audiovisual translation scenarios where timing mismatches can disrupt user experience.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "scores",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (Q) attains a UTMOS score of 3.41, outperforming all open-source baselines and surpassing GPT-4o (3.36). UniSS (Q) also leads all S2ST models in the ZH-EN direction with a score of 4.16. Despite its significantly smaller size, UniSS consistently produces high-quality speech across both translation directions, confirming its effectiveness as a practical open-source solution for speech-to-speech translation.</p>\n\n",
                "matched_terms": [
                    "enzh",
                    "size",
                    "s2st",
                    "gpt4o",
                    "utmos",
                    "uniss",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond speech-to-speech translation, UniSS also supports speech-to-text translation (S2TT). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>, UniSS achieves leading performance on this task as well. Whisper only supports X-EN translation and demonstrates poor performance on multilingual scenarios. UniSS&#8217;s Text-BLEU scores significantly outperform the 7B multimodal LLM Qwen2-audio across both datasets, demonstrating superior cross-lingual speech understanding capabilities.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "scores",
                    "textbleu",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although UniSS was not specifically designed for automatic speech recognition, it demonstrates competitive ASR capabilities as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. We evaluate the ASR performance on the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib49\" title=\"\">2015</a>]</cite>. On the LibriSpeech test-clean subset, UniSS achieves a WER of 2.4, which is comparable to the open-source state-of-the-art ASR model, Whisper (1.8 WER). However, on the more challenging LibriSpeech test-others subset, UniSS shows degraded performance with a WER of 6.6. It is important to note that UniSS is primarily designed as a speech-to-speech translation model, and these ASR results demonstrate its versatility beyond its core translation capabilities.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "uniss",
                    "results",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although UniSS was not specifically designed for text-to-speech synthesis, we evaluate its TTS capabilities on the LibriSpeech test-clean dataset as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. UniSS achieves a WER of 4.75 and speaker similarity of 0.568, compared to SparkTTS, which obtains 1.98 WER and 0.584 similarity. The performance gap is expected given that SparkTTS is specifically optimized for TTS, whereas UniSS is primarily designed for speech-to-speech translation. These results demonstrate that UniSS can generate intelligible speech output.</p>\n\n",
                "matched_terms": [
                    "results",
                    "dataset",
                    "uniss",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a subjective assessment on a randomly sampled subset of UniST to evaluate the quality of our dataset. Four raters listened to 150 examples and provided MOS scores across three dimensions: Emotion Similarity, Speaker Similarity, and Speech Naturalness. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.T5\" title=\"Table E5 &#8227; E.3 Analysis of UniST &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E5</span></a>, UniST synthesized by SparkTTS achieves high speech naturalness and effectively preserves both the source speaker&#8217;s voice characteristics and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The progressive training strategy pertains only to the training process and does not increase the model&#8217;s structural complexity. The alignment in phase 1 uses existing, easily accessible open-source data. In the ablation study section, we demonstrate the impact of different stages on model performance, proving that progressive training is effective.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speaker tokenizer is used to represent global speaker information. We hypothesize that this voice decoupling reduces the difficulty of speech translation by allowing the model to focus more on the linguistic content. We employ different linguistic and semantic tokenizers for understanding and generation to optimize each task. To generate more natural and expressive audio, the generation process uses semantic tokens, which retain information beyond the core content. However, this additional information makes content understanding more challenging. Therefore, the linguistic tokenizer, which focuses solely on content information, is better suited for the speech understanding phase. As a simple speech representation, we believe this design does not introduce greater complexity in either training or inference.</p>\n\n",
                "matched_terms": [
                    "model",
                    "better"
                ]
            }
        ]
    },
    "A4.T4": {
        "source_file": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
        "caption": "Table D4: Performance comparison across different speech tasks. For S2TT, results are reported as EN-ZH || ZH-EN BLEU scores. Higher Text-BLEU and lower WER indicate better performance.",
        "body": "Task\nDataset\nModel\nMetric\nResult\n\n\nS2TT\nCVSS-T\nQwen2-audio\nText-BLEU ↑\\uparrow\n\n21.86 || 18.69\n\n\nWhisper\nText-BLEU ↑\\uparrow\n\n- || 12.34\n\n\nUniSS\nText-BLEU ↑\\uparrow\n\n\n33.17 || 27.33\n\n\n\nFLEURS\nQwen2-audio\nText-BLEU ↑\\uparrow\n\n6.81 || 17.79\n\n\nWhisper\nText-BLEU ↑\\uparrow\n\n- || 15.65\n\n\nUniSS\nText-BLEU ↑\\uparrow\n\n\n34.91 || 22.18\n\n\n\nASR\nLibriSpeech test-clean\nWhisper\nWER ↓\\downarrow\n\n1.8\n\n\nUniSS\nWER ↓\\downarrow\n\n2.4\n\n\nLibriSpeech test-others\nWhisper\nWER ↓\\downarrow\n\n3.6\n\n\nUniSS\nWER ↓\\downarrow\n\n6.6\n\n\nTTS\nLibriSpeech test-clean\nSparkTTS\nWER ↓\\downarrow || SIM ↑\\uparrow\n\n\n1.98 || 0.584\n\n\n\nUniSS\nWER ↓\\downarrow || SIM ↑\\uparrow\n\n4.75 || 0.568",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:0.75pt 7.1pt;\"><span class=\"ltx_text ltx_font_bold\">Task</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:0.75pt 7.1pt;\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.75pt 7.1pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.75pt 7.1pt;\"><span class=\"ltx_text ltx_font_bold\">Metric</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" style=\"padding:0.75pt 7.1pt;\"><span class=\"ltx_text ltx_font_bold\">Result</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"6\" style=\"padding:0.75pt 7.1pt;\">S2TT</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\" style=\"padding:0.75pt 7.1pt;\">CVSS-T</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">Qwen2-audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">Text-BLEU <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">21.86 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m4\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 18.69</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.75pt 7.1pt;\">Whisper</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.75pt 7.1pt;\">Text-BLEU <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.75pt 7.1pt;\">- <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m6\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 12.34</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.75pt 7.1pt;\">UniSS</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.75pt 7.1pt;\">Text-BLEU <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.75pt 7.1pt;\">\n<span class=\"ltx_text ltx_font_bold\">33.17</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m8\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">27.33</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\" style=\"padding:0.75pt 7.1pt;\">FLEURS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">Qwen2-audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">Text-BLEU <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">6.81 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m10\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 17.79</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.75pt 7.1pt;\">Whisper</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.75pt 7.1pt;\">Text-BLEU <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m11\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.75pt 7.1pt;\">- <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m12\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 15.65</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.75pt 7.1pt;\">UniSS</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.75pt 7.1pt;\">Text-BLEU <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m13\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.75pt 7.1pt;\">\n<span class=\"ltx_text ltx_font_bold\">34.91</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m14\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">22.18</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\" style=\"padding:0.75pt 7.1pt;\">ASR</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding:0.75pt 7.1pt;\">LibriSpeech test-clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">Whisper</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">WER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.75pt 7.1pt;\"><span class=\"ltx_text ltx_font_bold\">1.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.75pt 7.1pt;\">UniSS</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.75pt 7.1pt;\">WER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m16\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.75pt 7.1pt;\">2.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding:0.75pt 7.1pt;\">LibriSpeech test-others</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">Whisper</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">WER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m17\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.75pt 7.1pt;\"><span class=\"ltx_text ltx_font_bold\">3.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.75pt 7.1pt;\">UniSS</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.75pt 7.1pt;\">WER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m18\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right\" style=\"padding:0.75pt 7.1pt;\">6.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding:0.75pt 7.1pt;\">TTS</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding:0.75pt 7.1pt;\">LibriSpeech test-clean</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">SparkTTS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">WER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m19\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m20\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> SIM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m21\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding:0.75pt 7.1pt;\">\n<span class=\"ltx_text ltx_font_bold\">1.98</span> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m22\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">0.584</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.75pt 7.1pt;\">UniSS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.75pt 7.1pt;\">WER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m23\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m24\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> SIM <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m25\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding:0.75pt 7.1pt;\">4.75 <math alttext=\"|\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T4.m26\" intent=\":literal\"><semantics><mo fence=\"false\" stretchy=\"false\">|</mo><annotation encoding=\"application/x-tex\">|</annotation></semantics></math> 0.568</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "textbleu",
            "wer",
            "speech",
            "reported",
            "lower",
            "qwen2audio",
            "s2tt",
            "metric",
            "↑uparrow",
            "asr",
            "different",
            "results",
            "scores",
            "zhen",
            "performance",
            "fleurs",
            "↓downarrow",
            "enzh",
            "sim",
            "comparison",
            "sparktts",
            "testclean",
            "cvsst",
            "dataset",
            "higher",
            "result",
            "across",
            "indicate",
            "whisper",
            "librispeech",
            "task",
            "uniss",
            "tts",
            "better",
            "bleu",
            "testothers",
            "tasks",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Beyond speech-to-speech translation, UniSS also supports speech-to-text translation (S2TT). As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>, UniSS achieves leading performance on this task as well. Whisper only supports X-EN translation and demonstrates poor performance on multilingual scenarios. UniSS&#8217;s Text-BLEU scores significantly outperform the 7B multimodal LLM Qwen2-audio across both datasets, demonstrating superior cross-lingual speech understanding capabilities.</p>\n\n",
            "<p class=\"ltx_p\">Although UniSS was not specifically designed for automatic speech recognition, it demonstrates competitive ASR capabilities as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. We evaluate the ASR performance on the LibriSpeech dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib49\" title=\"\">2015</a>]</cite>. On the LibriSpeech test-clean subset, UniSS achieves a WER of 2.4, which is comparable to the open-source state-of-the-art ASR model, Whisper (1.8 WER). However, on the more challenging LibriSpeech test-others subset, UniSS shows degraded performance with a WER of 6.6. It is important to note that UniSS is primarily designed as a speech-to-speech translation model, and these ASR results demonstrate its versatility beyond its core translation capabilities.</p>\n\n",
            "<p class=\"ltx_p\">Although UniSS was not specifically designed for text-to-speech synthesis, we evaluate its TTS capabilities on the LibriSpeech test-clean dataset as shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T4\" title=\"Table D4 &#8227; D.2 Comparison with S2TT Systems &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D4</span></a>. UniSS achieves a WER of 4.75 and speaker similarity of 0.568, compared to SparkTTS, which obtains 1.98 WER and 0.584 similarity. The performance gap is expected given that SparkTTS is specifically optimized for TTS, whereas UniSS is primarily designed for speech-to-speech translation. These results demonstrate that UniSS can generate intelligible speech output.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cmots.github.io/uniss-demo\" title=\"\">https://cmots.github.io/uniss-demo</a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss",
                    "results",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A conventional cascaded S2ST system typically consists of three sequential components: automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech (TTS) synthesis&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Casacuberta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib4\" title=\"\">2004</a>; Nakamura et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib5\" title=\"\">2006</a>)</cite>. However, this cascaded architecture often suffers from error accumulation across stages and struggles to retain paralinguistic features of the original speech. To address these limitations, subsequent research has shifted towards end-to-end approaches that aim to translate speech to another language while preserving expressive characteristics&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib7\" title=\"\">2024</a>; Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib8\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "speech",
                    "across",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most recently, the application of large language models (LLMs) to generative speech tasks has further accelerated the development of S2ST systems. Current approaches typically fall into two categories: (1) single-stage methods, which directly predict multi-stream acoustic tokens autoregressively via multi-head outputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite>; and (2) two-stage pipelines, which first generate semantic tokens autoregressively, followed by another autoregressive (AR) model to predict acoustic tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>; Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Gong and Veluri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Peng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>)</cite> use a single AR language model to jointly model semantic and partial acoustic tokens, along with a non-autoregressive (NAR) model for complete acoustic information.\nWhile these approaches have shown promising results, they also introduce significantly more architectural complexity than textual LLMs. Additionally, they treat the LLM as a sequence-to-sequence converter, failing to leverage the pre-trained knowledge for textual translation embedded within the LLM.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model",
                    "speech",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In pursuit of high-fidelity, expressive S2ST, and to overcome the limitations outlined above, our vision is defined by three core principles: (1) a single-stage architecture to eliminate complexity; (2) a unified model that aligns speech and text modalities; and (3) a mechanism to explicitly leverage the proven text translation capabilities of LLMs. To our knowledge, no existing approach satisfies all three principles simultaneously.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address the challenge of architectural complexity, we present <span class=\"ltx_text ltx_font_bold\">UniSS</span>, a unified single-stage S2ST framework that preserves timbre, emotion, and duration consistency.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> demonstrates our approach and performance results. UniSS builds upon pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> as the backbone, modeling content and speaker information in a single AR language model without architectural modifications. We transfer the LLM&#8217;s pre-trained text translation capabilities to the speech domain through a cross-modality chain-of-thought (CoT) prompting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib15\" title=\"\">2022</a>)</cite>. During inference, our Quality Mode guides the model to decompose the complex S2ST task into sequential listen, translate, and speak steps within a single inference pass. We also introduce a Performance Mode that trades translation fidelity for faster inference, enabling deployment across diverse scenarios.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "speech",
                    "task",
                    "model",
                    "results",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Unified Single-Stage Architecture for S2ST:</span> We propose UniSS, a single-stage AR model for expressive S2ST that directly leverages a pre-trained textual LLM, eliminating the architectural complexity of prior work.</p>\n\n",
                "matched_terms": [
                    "model",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation must preserve semantic accuracy during cross-lingual conversion. Traditional cascaded systems&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Casacuberta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib4\" title=\"\">2004</a>; Nakamura et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib5\" title=\"\">2006</a>)</cite> chain ASR, MT, and TTS components sequentially, suffering from error accumulation and information loss through text bottlenecks. Early direct methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib1\" title=\"\">2019</a>)</cite> encountered significant challenges with translation quality and synthesis artifacts.\nThe breakthrough came with discrete unit-based methods, where speech-to-unit translation (S2UT) approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib2\" title=\"\">2022</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib17\" title=\"\">2021</a>; Inaguma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib3\" title=\"\">2023</a>)</cite> employ discrete speech units (subwords, phonemes, or semantic tokens) as intermediate representations, which enables effective disentanglement of linguistic content from acoustic properties. SeamlessM4T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite> achieved robust multilingual performance via unified multitask optimization of translation and synthesis.\nRecent systems focus on enhancing expressiveness while maintaining translation fidelity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib18\" title=\"\">2022b</a>; Song et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib19\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib7\" title=\"\">2024</a>)</cite>. SeamlessExpressive&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite> designs a PRETSSEL vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib20\" title=\"\">2024</a>)</cite> to enhance expressiveness preservation. TransVIP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Le et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib8\" title=\"\">2024</a>)</cite> employs feature disentanglement to separately model semantic, acoustic, and temporal information for voice and isochrony control.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "asr",
                    "tts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of LLMs in text processing has inspired their adaptation to S2ST through speech tokenization. <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib21\" title=\"\">2023</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib22\" title=\"\">2025</a>)</cite> focus primarily on semantic translation without preserving voice characteristics.\nTo address voice preservation, some works model acoustic features using multiple AR models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Rubenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib11\" title=\"\">2023</a>; Dong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib10\" title=\"\">2024</a>)</cite>. Recent unified approaches attempt to model both semantic and acoustic features within a single AR model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib13\" title=\"\">2024</a>; Gong and Veluri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib12\" title=\"\">2024</a>)</cite>. However, they still require an additional NAR model for complete acoustic generation. Hibiki&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Labiausse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib9\" title=\"\">2025</a>)</cite> proposes a multi-stream architecture processing source and target speech simultaneously, but necessitates substantial architectural modifications, including nested Transformers and training from scratch.\nA fundamental limitation across these approaches is their reliance on complex architectures to complete acoustic representations. Moreover, they fail to explicitly leverage the LLM&#8217;s pre-trained text translation capabilities, treating them merely as generic sequence converters. UniSS addresses both limitations through a single-stage architecture while effectively transferring LLM&#8217;s text translation capabilities to speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "uniss",
                    "speech",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F2\" title=\"Figure 2 &#8227; 3.1 UniSS Architecture &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, UniSS is a unified AR language model for expressive speech-to-speech translation, built upon a single-stage architecture with a cross-modal chain-of-thought prompting framework, and optimized via a progressive training strategy. This section details the three core components of UniSS: model architecture, cross-modal CoT prompting, and training methodology.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of UniSS is to perform expressive S2ST by modeling the conditional distribution <math alttext=\"P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo fence=\"false\">|</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(\\mathbf{Y}_{tgt}|\\mathbf{X}_{src})</annotation></semantics></math>, where <math alttext=\"\\mathbf{X}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{src}</annotation></semantics></math> and <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> denote the source and target speech waveforms, respectively. To achieve faithful content translation while preserving speaker identity and expressiveness, UniSS introduces three types of speech tokens: speaker tokens <math alttext=\"\\mathbf{S}^{spk}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{spk}</annotation></semantics></math> with fixed sequence length to capture global style attributes, e.g., timbre, prosody, and emotion; linguistic tokens <math alttext=\"\\mathbf{S}^{ling}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{ling}</annotation></semantics></math> to encode the content of the source utterance; and semantic tokens <math alttext=\"\\mathbf{S}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}</annotation></semantics></math> to represent the expressive target utterance and can be directly decoded into waveform when combined with speaker tokens.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We build our generation model upon the pre-trained Qwen2.5-1.5B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite>, a strong LLM backbone. By expanding the model vocabulary to include the discrete speech tokens, UniSS treats speech and text uniformly as token sequences, allowing both modalities to be processed within the same transformer architecture.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To represent both the content and expressive characteristics of speech, UniSS adopts a triple-tokenizer strategy, transforming the waveform <math alttext=\"\\mathbf{W}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p1.m1\" intent=\":literal\"><semantics><mi>&#119830;</mi><annotation encoding=\"application/x-tex\">\\mathbf{W}</annotation></semantics></math> into single-stream discrete token sequences:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Meanwhile, <math alttext=\"\\mathbf{S}^{ling}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p2.m1\" intent=\":literal\"><semantics><msup><mi>&#119826;</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{ling}</annotation></semantics></math> is generated by the <span class=\"ltx_text ltx_font_bold\">linguistic tokenizer</span>, which adopts the GLM-4 speech tokenizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib24\" title=\"\">2024</a>)</cite>. It leverages a quantized Whisper encoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib25\" title=\"\">2023</a>)</cite> to convert speech into a variable-length sequence of linguistic tokens at a rate of 12.5 tokens per second, enabling robust content understanding.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the S2ST task, the source waveform <math alttext=\"\\mathbf{X_{src}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mi>&#119852;&#119851;&#119836;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{X_{src}}</annotation></semantics></math> is represented by <math alttext=\"(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msubsup><mo>,</mo><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{S}_{src}^{spk},\\mathbf{S}_{src}^{ling})</annotation></semantics></math>, while the target waveform <math alttext=\"\\mathbf{Y_{tgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mi>&#119853;&#119840;&#119853;</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y_{tgt}}</annotation></semantics></math> is represented by <math alttext=\"\\mathbf{S}_{tgt}^{sem}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS2.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}_{tgt}^{sem}</annotation></semantics></math>. This design is motivated by our preliminary experiments, which revealed that although BiCodec&#8217;s semantic tokens are highly effective for waveform reconstruction, their self-supervised nature makes them suboptimal for content understanding. By separately representing speaker identity, linguistic content, and generation-oriented semantics, UniSS enables more accurate and controllable S2ST modeling.</p>\n\n",
                "matched_terms": [
                    "task",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Direct speech-to-speech translation is an inherently complex task. Inspired by the success of chain-of-thought (CoT) prompting in LLM, we introduce a cross-modal CoT prompting framework to decompose this task into a more manageable sequence of <span class=\"ltx_text ltx_font_italic\">listen</span>, <span class=\"ltx_text ltx_font_italic\">translate</span>, and <span class=\"ltx_text ltx_font_italic\">speak</span> steps. This approach is designed to effectively transfer LLM&#8217;s powerful, pre-trained text translation capabilities to the speech domain. Our framework provides two controllable prompts to trade off translation fidelity and efficiency.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"c_{task}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{task}</annotation></semantics></math>, <math alttext=\"c_{lang}^{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">c_{lang}^{tgt}</annotation></semantics></math>, and <math alttext=\"c_{speed}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{speed}</annotation></semantics></math> are special tokens specifying the task mode, target language, and duration ratio between source and target speech, respectively. A special begin-of-translation token, <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">BOT</span>, signals the model to begin generation, producing an output sequence <math alttext=\"\\tau_{out}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m6\" intent=\":literal\"><semantics><msub><mi>&#964;</mi><mrow><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\tau_{out}</annotation></semantics></math> that is terminated by an end-of-decoding token, <span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">EOD</span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 1: Speech-Text Alignment.</span>\nWe begin by adapting the pre-trained LLM to the speech modality through a multi-task learning stage involving four foundational tasks: ASR, TTS, Speech-to-Text Translation (S2TT), and MT. The ASR, TTS, and S2TT tasks align speech and text, while MT preserves the model&#8217;s foundational translation capabilities.\nEach task is defined by a specific prompt structure and target output sequence:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr",
                    "tts",
                    "task",
                    "tasks",
                    "s2tt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here <math alttext=\"c_{lang}^{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><msubsup><mi>c</mi><mrow><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">c_{lang}^{src}</annotation></semantics></math> denotes the source language, and <math alttext=\"\\mathbf{S}^{sem}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><msubsup><mi>&#119826;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\mathbf{S}^{sem}_{src}</annotation></semantics></math> denotes the semantic tokens from source speech. This phase endows the model with robust speech understanding and generation while prioritizing the preservation of text translation capabilities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 2: S2ST with CoT.</span>\nIn the second phase, we introduce the core S2ST task. The model is trained to generate outputs using the CoT prompting formats described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS2\" title=\"3.2 Cross-Modal Chain-of-Thought Prompting &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, as well as a simplified direct generation mode that bypasses intermediate text outputs:</p>\n\n",
                "matched_terms": [
                    "task",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Phase 3: Refinement.</span>\nIn the final phase, we fine-tune the model on the full S2ST task using both CoT prompting modes.\nThis phase uses an annealed learning rate to stabilize the learned CoT patterns and optimize the final translation performance.</p>\n\n",
                "matched_terms": [
                    "task",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Cleaning.</span>\nOur pipeline begins with large-scale TTS corpora containing paired speech and transcription <math alttext=\"(\\mathbf{X}_{src},\\mathbf{T}_{src})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{X}_{src},\\mathbf{T}_{src})</annotation></semantics></math>, combined from several Chinese and English public datasets (details in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.SS1\" title=\"E.1 Source Dataset &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a>). Following VoxBox&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib23\" title=\"\">2025</a>)</cite>, we first clean the corpus using Paraformer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib26\" title=\"\">2022</a>)</cite> to re-recognize speech and compute Word Error Rate (WER) between re-recognized and original transcriptions, discarding samples with WER <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math> 0.05.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation and Target Speech Synthesis.</span>\nAfter source cleaning, <math alttext=\"\\mathbf{T}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m1\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{src}</annotation></semantics></math> is translated by Qwen2.5-72B-Instruct&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib14\" title=\"\">2024</a>)</cite> to produce target text <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math> in another language.\nPrompts used in translation are shown in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.SS2\" title=\"E.2 Text Translation Step &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>.\nWe then apply an expressive TTS model, SparkTTS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib23\" title=\"\">2025</a>)</cite>, to synthesize the target speech <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m3\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> from <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m4\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>, conditioned by <math alttext=\"\\mathbf{X}_{src}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m5\" intent=\":literal\"><semantics><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{X}_{src}</annotation></semantics></math> to preserve the source speaker&#8217;s voice. To enable fine-grained speed control, we calculate the duration ratio between source and target speech and discretize it into speed tokens <math alttext=\"\\mathbf{c}_{speed}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m6\" intent=\":literal\"><semantics><msub><mi>&#119836;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{c}_{speed}</annotation></semantics></math> with 0.1 intervals. This creates complete parallel samples <math alttext=\"(\\mathbf{X}_{src},\\mathbf{T}_{src},\\mathbf{T}_{tgt},\\mathbf{Y}_{tgt},\\mathbf{c}_{speed})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p3.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119831;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi></mrow></msub><mo>,</mo><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>&#119836;</mi><mrow><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>d</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\mathbf{X}_{src},\\mathbf{T}_{src},\\mathbf{T}_{tgt},\\mathbf{Y}_{tgt},\\mathbf{c}_{speed})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts",
                    "sparktts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quality Filtering and Dataset Variants.</span>\nThe synthesized data undergoes final quality filtering.\nWe apply ASR to <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> and discard samples with WER greater than 0.01 against <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>. We also apply a duration ratio filter to keep synthesized speech with duration within [0.5, 2.0] times of the source speech.\nThe filtered data forms our <span class=\"ltx_text ltx_font_bold\">UniST General</span> dataset (44.8k hours).\nOur refined <span class=\"ltx_text ltx_font_bold\">UniST High-Quality</span> dataset (19.8k hours) is created by applying additional Voice Activity Detection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib27\" title=\"\">2024</a>)</cite> to remove silence at the beginning and end of speech segments, and a stricter duration ratio filter of [0.7,1.5].</p>\n\n",
                "matched_terms": [
                    "wer",
                    "speech",
                    "asr",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b,c), UniST General provides greater data diversity and generalization potential\ndue to its larger range of duration ratios and speech lengths. UniST High-Quality focuses on temporal consistency, making it ideal for the final refinement phase of our progressive training. In Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.SS3\" title=\"E.3 Analysis of UniST &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E.3</span></a>, we show that the UniST dataset also preserves emotional style in synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS is trained in three progressive phases, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS3\" title=\"3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>. Phase 1 establishes text-speech alignment using 77.1k hours of speech data and MT text data from WMT17&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bojar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib28\" title=\"\">2017</a>)</cite>. Phase 2 introduces CoT prompting with our UniST General dataset, mixed with Phase 1 data at a 2:1 ratio. Phase 3 fine-tunes the model exclusively on the UniST High-Quality dataset.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "speech",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training employs the AdamW optimizer&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov and Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib29\" title=\"\">2019</a>)</cite> with a 2.3M-token batch size, weight decay of 0.1, and momentum parameters (0.9, 0.95). All audio is resampled to 16&#160;kHz, and the LLM vocabulary is expanded to 180,407 to include speech and control tokens. Learning rates progress from 8e-4 in Phase 1 to 2e-4 in Phase 2, and finally anneal from 5e-5 to 5e-6 in Phase 3. The model is trained on 16 NVIDIA H800 80G GPUs using the Megatron-LM Framework&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Shoeybi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib30\" title=\"\">2019</a>)</cite> for efficient large-model training. Complete training details and hyperparameters are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.SS1\" title=\"B.1 UniSS &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our primary S2ST evaluation, we use the Chinese (ZH) and English (EN) test sets from CVSS-T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite>, which contain 4,897 utterance pairs (8.2 hours for Chinese and 6.3 hours for English). For emotion preservation evaluation, we randomly sample 300 utterances from each emotional speech dataset (ESD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib31\" title=\"\">2021</a>)</cite> and CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib32\" title=\"\">2014</a>)</cite>), covering a selected set of 4 emotions (happy, sad, angry, neutral). We evaluate both English-to-Chinese (EN-ZH) and Chinese-to-English (ZH-EN) translation directions.</p>\n\n",
                "matched_terms": [
                    "enzh",
                    "speech",
                    "zhen",
                    "cvsst",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ comprehensive objective and subjective metrics to evaluate S2ST performance. For translation fidelity, we measure BLEU scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib33\" title=\"\">2002</a>)</cite> on both ASR-transcribed speech outputs (Speech-BLEU) and intermediate text (Text-BLEU). Prosody preservation is assessed through prosody similarity (A.PCP)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Andrews et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib34\" title=\"\">2022</a>)</cite>. Duration consistency is evaluated using Speech Length Compliance (SLC) within 0.2 and 0.4 ratio tolerance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nSpeech quality is measured using UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib36\" title=\"\">2022</a>)</cite>. Additionally, we conduct a subjective Mean Opinion Score (MOS) evaluation where bilingual speakers rate emotion similarity, speaker similarity, and naturalness on a 5-point scale. Detailed descriptions of all evaluation metrics and implementation procedures are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A3\" title=\"Appendix C Evaluation Metrics &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "textbleu",
                    "performance",
                    "speech",
                    "bleu",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Systems.</span>\nTo establish an upper bound for modular approaches, we construct two cascaded baselines using top-performing open-source models: a <span class=\"ltx_text ltx_font_bold\">3-Stage</span> pipeline of Whisper-large-v3 (ASR), NLLB-200-distilled (MT)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Koishekenov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib37\" title=\"\">2023</a>)</cite>, and CosyVoice 2 (TTS)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib38\" title=\"\">2024</a>)</cite>; and a <span class=\"ltx_text ltx_font_bold\">2-Stage</span> pipeline of SeamlessM4T-v2-Large (S2TT) and CosyVoice 2 (TTS).</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "asr",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Large Language Models (MLLMs).</span>\nTo benchmark against the latest generation of general-purpose models, we include two leading MLLMs: <span class=\"ltx_text ltx_font_bold\">GPT-4o</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hurst et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib39\" title=\"\">2024</a>)</cite>, an enterprise-level model from OpenAI with strong speech-to-speech capability; and <span class=\"ltx_text ltx_font_bold\">Qwen2.5-Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib40\" title=\"\">2025</a>)</cite>, a powerful open-source MLLM building on large-scale audio&#8211;language pretraining, enabling both speech understanding and synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For UniSS experiments, we deploy vLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kwon et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib42\" title=\"\">2023</a>)</cite> to support inference. We set a decoding temperature of 0.7, top-k of -1, top-p of 0.8, and a repetition penalty of 1.1. We report results for both Performance mode, denoted as <span class=\"ltx_text ltx_font_bold\">UniSS (P)</span>, and Quality mode, denoted as <span class=\"ltx_text ltx_font_bold\">UniSS (Q)</span>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uniss",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T1\" title=\"Table 1 &#8227; 5.3 Objective Performance &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the main comparison results on the CVSS-T dataset.\nThe results clearly demonstrate that UniSS establishes a new SOTA in translation fidelity\nwhile maintaining voice characteristics, prosody, duration consistency, and speech quality, validating our core design principles. Additional results on other datasets and tasks are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4\" title=\"Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "comparison",
                    "results",
                    "tasks",
                    "uniss",
                    "cvsst",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Translation Fidelity.</span> UniSS achieves state-of-the-art translation fidelity on both EN-ZH and ZH-EN directions. The UniSS (Q) variant achieves a Speech-BLEU of 32.20 on EN-ZH and 24.28 on ZH-EN, substantially outperforming all prior end-to-end and cascaded baselines. The efficient UniSS (P) also delivers strong results, surpassing most existing systems. Notably, in terms of intermediate text metrics, UniSS models perform on par with or better than larger multimodal LLM-based approaches.</p>\n\n",
                "matched_terms": [
                    "enzh",
                    "better",
                    "results",
                    "uniss",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody Preservation.</span> In terms of prosody, UniSS achieves competitive performance. UniSS (P) variant achieves the second-highest A.PCP score (2.73 and 2.75), closely following Seamless-Ex, which incorporates a dedicated prosody encoder. The performance gap is marginal at only 0.10 (EN-ZH) and 0.12 (ZH-EN), highlighting the efficacy of UniSS in preserving prosodic patterns without specialized modules.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "enzh",
                    "zhen",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Duration Consistency.</span> UniSS demonstrates superior duration consistency. UniSS (Q) achieves near-optimal SLC 0.2 scores on EN-ZH and the best performance on ZH-EN, improving over the previous best end-to-end system (Seamless-Ex) by 44% and 67%. On the more relaxed SLC 0.4 metric, while competing systems achieve scores above 0.90, both UniSS variants deliver near-perfect performance with scores of 0.99 (EN-ZH) and 0.97 (ZH-EN).</p>\n\n",
                "matched_terms": [
                    "performance",
                    "enzh",
                    "scores",
                    "metric",
                    "uniss",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Quality.</span> UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (P) achieves a competitive UTMOS score of 3.77, matching cascaded models (3.76 and 3.79) while surpassing all end-to-end models. In the ZH-EN direction, UniSS surpasses 3-stage systems with a score of 3.86 compared to 3.50.</p>\n\n",
                "matched_terms": [
                    "enzh",
                    "speech",
                    "zhen",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the objective metrics, we conduct comprehensive subjective assessments on emotion preservation, voice preservation, and speech naturalness. The results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T3\" title=\"Table 3 &#8227; 5.4 Subjective Results &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Preservation.</span>\nUniSS effectively retains speaker voice characteristics without requiring an additional NAR stage-2 model. For speaker similarity, UniSS (Q) achieves a score of 4.42, outperforming all other models. This represents a 0.07 improvement over the 2-stage system (4.35), which deploys a carefully designed TTS model. These results underscore the ability of UniSS to maintain voice characteristics in an end-to-end fashion.</p>\n\n",
                "matched_terms": [
                    "results",
                    "uniss",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Naturalness.</span>\nThe naturalness of speech from UniSS is also favorably rated. UniSS (Q) achieves a naturalness MOS of 4.45, surpassing Seamless-Ex (3.10) and the 3-stage cascaded baseline (4.31) that incorporates a specialized TTS model. This demonstrates UniSS&#8217;s ability to generate high-quality, natural-sounding speech without dedicated text-to-speech components.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speech",
                    "uniss",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework provides flexible control over the quality-efficiency trade-off through its different CoT prompting modes.\nWe evaluate inference speed on the AR language model component using the Transformers library&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wolf et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib43\" title=\"\">2020</a>)</cite>,\nusing 400 utterances (200 per direction) from CVSS-T on a single H800 GPU without batching.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cvsst",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct ablation studies to validate the effect of our design.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S5.T4\" title=\"Table 4 &#8227; 5.6 Ablation Studies &#8227; 5 Experiments and Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the detailed results comparing variants against a base model trained through Phases 1 and 2. The results in type Base and Train are evaluated in the Performance mode.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Replacing our content-focused GLM-4 linguistic tokenizer with BiCodec&#8217;s self-supervised semantic tokens (<span class=\"ltx_text ltx_font_bold\">w/o GLM</span>) leads to significant performance degradation of -15.01 and -8.73 Speech-BLEU points.\nThis drop reveals that while BiCodec&#8217;s semantic tokens excel at speech generation tasks, their self-supervised nature limits their effectiveness for content understanding in the S2ST task.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tasks",
                    "performance",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Removing the intermediate text generation and performing direct speech-to-speech translation (<span class=\"ltx_text ltx_font_bold\">Direct S2ST</span>) results in a severe performance degradation of -14.94 and -14.40 Speech-BLEU points in the inference. This demonstrates that our cross-modal CoT prompting enables the transfer of textual translation expertise to the speech domain and improves translation fidelity.</p>\n\n",
                "matched_terms": [
                    "results",
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces UniSS, a unified single-stage expressive speech-to-speech translation framework that preserves voice and emotional style. Our approach eliminates architectural complexity through well-designed speech semantic and style modeling, enabling seamless integration with pre-trained LLMs. By transferring LLMs&#8217; text translation capabilities to speech via cross-modal CoT prompting, UniSS surpasses previous S2ST systems across multiple dimensions: translation fidelity, speech naturalness, and preservation of voice, emotion, and duration consistency.\nBeyond the model architecture, we design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English expressive S2ST dataset. Our work demonstrates a simple and effective approach for building the next generation of expressive S2ST systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "uniss",
                    "speech",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations and Future Works. </span>\nCurrent limitations point to several research directions. (1) Language Support: We trained UniSS only on Chinese and English due to resource limitations. The data construction pipeline and training framework can be easily extended to multilingual scenarios, which represents our immediate next step. (2) Speech Tokenizer: While the linguistic, speaker, and semantic tokenizers perform well in UniSS, they originate from two different audio tokenizers, causing vocabulary size expansion. Future work will focus on training a unified tokenizer to merge these components and reduce vocabulary size.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present real examples from different models here, with speech converted to text using ASR for display purposes. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A1.F1\" title=\"Figure A1 &#8227; Appendix A Examples &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">A1</span></a>, UniSS achieves competitive BLEU scores on challenging sentences where GPT-4o struggles with translation quality. Note Seamless-L and GPT-4o cannot preserve source voice and emotional style.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr",
                    "different",
                    "bleu",
                    "uniss",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T1\" title=\"Table B1 &#8227; B.1 UniSS &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B1</span></a>, the UniSS is trained in three phases progressively.\nAll audio is resampled to 16&#160;kHz. The LLM vocabulary is expanded to 180,407 to include speech and control tokens.\nWe use the AdamW optimizer with a weight decay of 0.1 and momentum parameters (0.9, 0.95). The batch size is fixed at 2.3M tokens for all phases.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model is trained on a mixture of ASR, TTS, S2TT, and MT tasks.\nThis phase utilizes 77.1k hours of speech data and 2.3B translation tokens from WMT17,\ntotaling approximately 32B tokens per epoch.\nThe model is trained for 3 epochs with a constant learning rate of 8e-4 and a 1-epoch warm-up.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr",
                    "tts",
                    "tasks",
                    "s2tt",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce CoT prompting using our UniST General dataset.\nThe model learns three generation paths: Performance mode, Quality mode, and direct S2ST data without intermediate text.\nNew data is mixed with Phase 1 data at a 2:1 ratio, totaling approximately 55B tokens. We train for one epoch with a constant learning rate of 2e-4 and a 5% warm-up.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune the model exclusively on the UniST High-quality dataset.\nThe learning rate follows a cosine schedule, gradually annealing from 5e-5 to 5e-6 over 1 epoch without warm-up. This phase consists of approximately 10B training tokens. During this stage, the training loss reaches its minimum around 0.9 epochs, and we select the checkpoint at this point as our final model weights.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we provide detailed descriptions of all baseline models evaluated against our proposed UniSS framework. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A2.T2\" title=\"Table B2 &#8227; B.2 Baselines &#8227; Appendix B Implementation Details &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">B2</span></a> summarizes the baselines, model versions, and parameter sizes used in our experiments. All baseline models are evaluated using their default inference parameters without any additional tuning or modifications. Evaluation is conducted directly on the CVSS-T and FLEURS test sets without extra data preprocessing, prompts, or output post-processing. We present the implementation specifics of each baseline in the following subsections.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "model",
                    "cvsst",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implement two cascaded baselines: a conventional three-stage baseline (ASR <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> MT <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> TTS) and a two-stage baseline (S2TT <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.SSS1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> TTS). We select state-of-the-art, widely adopted models at each stage to represent strong and practically relevant baselines.</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "asr",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic Speech Recognition (ASR)</span>: We select Whisper-large-v3, a transformer-based encoder-decoder model trained on large-scale multilingual speech corpora, for its strong multilingual transcription performance and wide adoption in the community.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "asr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Speech (TTS)</span>: We employ CosyVoice 2 (0.5B), a transformer-based expressive TTS model designed for high-quality multilingual speech synthesis. Its ability to generate natural and expressive audio makes it suitable for our evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The two-stage pipeline simplifies the cascaded approach by combining speech recognition and translation into one direct Speech-to-Text Translation (S2TT) module:</p>\n\n",
                "matched_terms": [
                    "s2tt",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-to-Text Translation (S2TT)</span>: We select SeamlessM4T-v2-Large, a multilingual transformer-based model that jointly performs ASR and MT. Its end-to-end design reduces error propagation across stages and offers strong performance in speech translation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "speech",
                    "asr",
                    "s2tt",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-to-Speech (TTS)</span>: We generate synthesized speech using the same CosyVoice 2 model described in the three-stage pipeline.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our experiments, we observe several limitations associated with cascaded baselines. First, the inference process is noticeably slower due to the sequential execution of multiple large models, making the pipeline less suitable for real-time applications. Second, we find that errors introduced in earlier stages, particularly in ASR, tend to propagate downstream, often leading to degraded translation quality or unnatural speech synthesis. These limitations highlight the challenges of building efficient and robust cascaded speech translation systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ three advanced multimodal large language models as baselines: Qwen2-audio, Qwen2.5-Omni, and GPT-4o-audio. These models are capable of handling speech input and producing either textual or speech output, depending on the task.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "task",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen2.5-Omni is selected for its ability to perform fully end-to-end speech-to-speech translation without requiring intermediate text generation, making it a promising multimodal LLM baseline for direct S2ST tasks. In our experiments, we find that the model&#8217;s performance is highly sensitive to the prompt format. When using a simple default instruction, the model sometimes appends assistant-like phrases (e.g., &#8220;Do you need anything else?&#8221;) to the translated speech, likely due to its conversational fine-tuning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speech",
                    "tasks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, we find that such detailed prompts often lead to degraded audio output quality. Specifically, in our experiments, the model occasionally produces abnormal acoustic artifacts, such as elongating a single word unnaturally or producing disfluent prosody. This suggests that Qwen2.5-Omni&#8217;s speech generation may be overly sensitive to instruction format, and that more verbose prompts can negatively impact generation fluency despite offering better control over semantic behavior.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "better"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Qwen2-audio&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib45\" title=\"\">2024</a>]</cite> is used for direct speech-to-text translation. Considering its strong performance on multilingual speech comprehension tasks and its ability to follow structured prompts, we adopt it for S2TT evaluation. Prompts are designed to explicitly guide the model to translate input speech into target-language text. For example:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "speech",
                    "tasks",
                    "qwen2audio",
                    "s2tt",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All other parameters are kept at their default values. However, using Qwen2-audio requires additional input-output processing. Specifically, all input audio is resampled to 16&#160;kHz and converted to mono channel to meet the model&#8217;s input requirements. For output handling, we filter the returned results to remove prompt-related metadata and retain only the translated text for evaluation purposes.</p>\n\n",
                "matched_terms": [
                    "results",
                    "qwen2audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For S2ST evaluation, we employ GPT-4o-audio-preview, which supports multimodal speech translation tasks with direct speech-to-speech generation capabilities. Given the instability of Qwen2.5-Omni under complex prompt instructions observed in preliminary experiments, we select GPT-4o-audio-preview as a more robust alternative. Our experiments confirm that GPT-4o-audio-preview demonstrates superior stability and consistently produces higher-quality outputs compared to previous models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SeamlessExpressive extends SeamlessM4T by introducing expressiveness-aware modeling, enabling more natural and emotionally rich speech synthesis in the target language. The model conditions generation on prosodic and expressive cues from the source speech and is particularly well-suited for conversational and affective scenarios. We evaluate SeamlessExpressive using its official inference scripts and default hyperparameters.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Seed LiveInterpret 2.0 is an enterprise-level simultaneous interpretation model that performs real-time speech-to-speech translation with voice cloning capabilities. The model employs a duplex framework that processes input audio and generates target-language speech directly without intermediate text representations. In our evaluation, we used the official inference API with default parameters.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the performance of speech-to-speech translation (S2ST), we report a range of objective metrics covering translation accuracy, speaker identity preservation, prosodic alignment, temporal consistency, and speech quality. For all metrics, higher values indicate better performance.</p>\n\n",
                "matched_terms": [
                    "indicate",
                    "performance",
                    "speech",
                    "better",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-BLEU</span> evaluates the fidelity of generated translations by computing corpus-level BLEU scores using the SacreBLEU library&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Post, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib46\" title=\"\">2018</a>]</cite>. Before scoring, we apply language-specific preprocessing: English text is lowercased and stripped of punctuation (excluding apostrophes), while Chinese text is normalized to simplified characters, punctuation is removed, and characters are separated by spaces. This ensures consistency with standard BLEU evaluation practices. We use the <span class=\"ltx_text ltx_font_italic\">corpus_score</span> function to calculate the BLEU score across the whole dataset. Chinese samples are scored in &#8216;zh&#8217; mode.</p>\n\n",
                "matched_terms": [
                    "across",
                    "textbleu",
                    "bleu",
                    "scores",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-BLEU</span>, or ASR-BLEU, evaluates the translation fidelity of speech-to-speech translation systems. We transcribe the generated speech using ASR models, Whisper-large-v3 for English and Paraformer-zh for Chinese, and compute the BLEU score between the transcribed output and the ground-truth reference. Preprocessing follows the same pipeline as Text-BLEU.</p>\n\n",
                "matched_terms": [
                    "bleu",
                    "asr",
                    "speech",
                    "textbleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">UTMOS</span> is a neural network-based estimator that predicts the Mean Opinion Score (MOS) for synthesized speech, serving as a proxy for human judgment of audio quality. It produces a scalar value typically ranging from 1 to 5, reflecting aspects such as naturalness, fluency, and intelligibility. This objective metric helps mitigate the subjectivity and variability of human MOS ratings, enabling reproducible and scalable quality assessment.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate expressive qualities, we conducted a Mean Opinion Score (MOS) listening study. Six bilingual speakers rated synthesized speech on a 5-point scale across three dimensions: emotion similarity (Emo Sim.), speaker similarity (Spk Sim.), and speech naturalness (Naturalness). Similarity is compared against the original audio instead of the synthesized audio.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "sim",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In emotion similarity and speaker similarity tasks, the scale options are: &#8220;1. Definitely Different&#8221;, &#8220;2. Probably Different&#8221;, &#8220;3. Not be sure&#8221;, &#8220;4. Probably the same&#8221;, &#8220;5. Definitely the same&#8221;. In speech naturalness task, the scale options are: &#8220;1. Definitely unnatural&#8221;, &#8220;2. Probably unnatural&#8221;, &#8220;3. Not be sure&#8221;, &#8220;4. Probably natural&#8221;, &#8220;5. Definitely natural&#8221;.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tasks",
                    "task"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate on the Chinese and English test subsets of FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib48\" title=\"\">2022</a>]</cite>, a massively multilingual speech dataset spanning 102 languages with n-way parallel speech and text data. UniSS demonstrates robust performance on this challenging multilingual benchmark across multiple evaluation tasks. Additionally, UniSS achieves state-of-the-art performance on speech-to-text translation (S2TT) and competitive results on automatic speech recognition (ASR) and text-to-speech synthesis (TTS) tasks.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "fleurs",
                    "speech",
                    "asr",
                    "tts",
                    "results",
                    "tasks",
                    "s2tt",
                    "uniss",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the robustness of our framework across datasets, we further evaluate performance on the FLEURS test set. FLEURS is a multilingual benchmark derived from the FLoRes corpus that provides high-quality parallel speech and text pairs across diverse languages, making it a valuable complement to CVSS-T for assessing speech translation systems under standardized multilingual conditions.</p>\n\n",
                "matched_terms": [
                    "across",
                    "performance",
                    "fleurs",
                    "speech",
                    "cvsst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A4.T3\" title=\"Table D3 &#8227; D.1 S2ST Performance on Fleurs &#8227; Appendix D Additional Experimental Results &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">D3</span></a>, UniSS (Q) achieves strong translation performance with Speech-BLEU of 36.16 and Text-BLEU of 36.96 on EN-ZH, outperforming all S2ST baselines and cascaded systems. UniSS (P) also demonstrates competitive performance across both metrics. In the ZH-EN direction, UniSS surpasses all systems with comparable model size, demonstrating the parameter efficiency of our design.</p>\n\n",
                "matched_terms": [
                    "across",
                    "textbleu",
                    "performance",
                    "enzh",
                    "uniss",
                    "model",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS exhibits excellent prosody transfer capabilities on the FLEURS benchmark. UniSS (Q) achieves A.PCP scores of 2.72 (EN-ZH) and 2.64 (ZH-EN), outperforming all end-to-end baselines and matching GPT-4o&#8217;s performance (2.72 and 2.39). Notably, UniSS achieves these results without incorporating dedicated prosody modeling modules, highlighting the effectiveness of our unified framework.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "fleurs",
                    "enzh",
                    "results",
                    "zhen",
                    "uniss",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system demonstrates exceptional speech duration consistency on the FLEURS test set. Both UniSS variants achieve near-perfect scores on the SLC-0.2 and SLC-0.4 metrics, indicating that the generated speech closely mirrors the temporal length of the input audio. This precise duration alignment is critical for real-time applications and audiovisual translation scenarios where timing mismatches can disrupt user experience.</p>\n\n",
                "matched_terms": [
                    "uniss",
                    "speech",
                    "scores",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS achieves state-of-the-art speech quality among S2ST models. In the EN-ZH direction, UniSS (Q) attains a UTMOS score of 3.41, outperforming all open-source baselines and surpassing GPT-4o (3.36). UniSS (Q) also leads all S2ST models in the ZH-EN direction with a score of 4.16. Despite its significantly smaller size, UniSS consistently produces high-quality speech across both translation directions, confirming its effectiveness as a practical open-source solution for speech-to-speech translation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "enzh",
                    "speech",
                    "uniss",
                    "zhen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We select a diverse collection of publicly available Chinese and English speech datasets as the foundation for UniSS training data. Our data sources encompass multiple domains and speaking styles to ensure robust cross-lingual speech translation capabilities.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "uniss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CVSS-T</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>]</cite>: A benchmark dataset for speech-to-speech translation in English-Chinese language pairs.</p>\n\n",
                "matched_terms": [
                    "cvsst",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FLEURS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib48\" title=\"\">2022</a>]</cite>: A multilingual benchmark derived from FLoRes, containing parallel speech and text data.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LibriSpeech</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib49\" title=\"\">2015</a>]</cite>: A multi-speaker English speech corpus with reading audiobooks for TTS.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a subjective assessment on a randomly sampled subset of UniST to evaluate the quality of our dataset. Four raters listened to 150 examples and provided MOS scores across three dimensions: Emotion Similarity, Speaker Similarity, and Speech Naturalness. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.T5\" title=\"Table E5 &#8227; E.3 Analysis of UniST &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E5</span></a>, UniST synthesized by SparkTTS achieves high speech naturalness and effectively preserves both the source speaker&#8217;s voice characteristics and emotional expressiveness.</p>\n\n",
                "matched_terms": [
                    "across",
                    "speech",
                    "sparktts",
                    "scores",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although Spark-TTS does not achieve state-of-the-art performance in objective evaluations of speaker similarity, we observe that its performance in subjective evaluations is comparable. Furthermore, we assume that its voice decoupling capability reduces the modeling complexity for large language models, which is why we deploy BiCodec for the generation speech tokenizer in the LLM. Additionally, Spark-TTS generates BiCodec tokens directly, making training more efficient by eliminating an extra encoding step.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance",
                    "sparktts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The progressive training strategy pertains only to the training process and does not increase the model&#8217;s structural complexity. The alignment in phase 1 uses existing, easily accessible open-source data. In the ablation study section, we demonstrate the impact of different stages on model performance, proving that progressive training is effective.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The speaker tokenizer is used to represent global speaker information. We hypothesize that this voice decoupling reduces the difficulty of speech translation by allowing the model to focus more on the linguistic content. We employ different linguistic and semantic tokenizers for understanding and generation to optimize each task. To generate more natural and expressive audio, the generation process uses semantic tokens, which retain information beyond the core content. However, this additional information makes content understanding more challenging. Therefore, the linguistic tokenizer, which focuses solely on content information, is better suited for the speech understanding phase. As a simple speech representation, we believe this design does not introduce greater complexity in either training or inference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "task",
                    "better",
                    "different",
                    "model"
                ]
            }
        ]
    },
    "A5.T5": {
        "source_file": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
        "caption": "Table E5: Subjective MOS evaluation on the UniST General dataset.",
        "body": "Dataset\nEmo Sim.↑\\uparrow\nSpk Sim.↑\\uparrow\nNaturalness↑\\uparrow\n\n\nUniST General\n4.69\n4.31\n4.61",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.6pt;padding-right:4.6pt;\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.6pt;padding-right:4.6pt;\"><span class=\"ltx_text ltx_font_bold\">Emo Sim.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.6pt;padding-right:4.6pt;\"><span class=\"ltx_text ltx_font_bold\">Spk Sim.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.6pt;padding-right:4.6pt;\"><span class=\"ltx_text ltx_font_bold\">Naturalness<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:4.6pt;padding-right:4.6pt;\">UniST General</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.6pt;padding-right:4.6pt;\">4.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.6pt;padding-right:4.6pt;\">4.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:4.6pt;padding-right:4.6pt;\">4.61</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "spk",
            "evaluation",
            "emo",
            "subjective",
            "mos",
            "general",
            "naturalness↑uparrow",
            "unist",
            "sim↑uparrow",
            "dataset"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We conduct a subjective assessment on a randomly sampled subset of UniST to evaluate the quality of our dataset. Four raters listened to 150 examples and provided MOS scores across three dimensions: Emotion Similarity, Speaker Similarity, and Speech Naturalness. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.T5\" title=\"Table E5 &#8227; E.3 Analysis of UniST &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E5</span></a>, UniST synthesized by SparkTTS achieves high speech naturalness and effectively preserves both the source speaker&#8217;s voice characteristics and emotional expressiveness.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The ultimate goal of expressive speech-to-speech translation (S2ST) is to accurately translate spoken content while preserving the speaker identity and emotional style. However, progress in this field is largely hindered by three key challenges: the scarcity of paired speech data that retains expressive styles, the complexity of multi-stage processing pipelines, and the limited transfer of translation capabilities from large language models (LLMs). In this work, we address these challenges by introducing UniSS, a novel single-stage framework for expressive S2ST. Our approach features carefully designed speech semantic and style modeling, enabling seamless integration with existing text-based LLM frameworks to develop a unified text-speech language model. To transfer translation capabilities from text to speech, we propose a cross-modal chain-of-thought prompting process that progressively aligns audio semantics with text and ensures style preservation in the decoded results. Furthermore, we construct and release a large-scale, high-quality expressive S2ST dataset, UniST, comprising 44.8k hours of data. Experimental results show that UniSS significantly outperforms previous methods in translation fidelity and speech quality while preserving voice, emotion, and duration consistency. Our work establishes a simpler and more effective paradigm for building the next generation of expressive S2ST systems. Audio samples are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cmots.github.io/uniss-demo\" title=\"\">https://cmots.github.io/uniss-demo</a>.</p>\n\n",
                "matched_terms": [
                    "unist",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realizing the vision outlined above demands large-scale, high-quality training data that preserves both translation accuracy and speaker expressiveness. However, existing S2ST datasets are either small-scale to train powerful unified models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite> or suffer from quality control issues when scraped from the web&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib6\" title=\"\">2023</a>)</cite>. To address this data challenge, we design a scalable synthesis pipeline and contribute UniST, a 44.8k-hour Chinese-English S2ST dataset offering high translation fidelity and rich speaker preservation.</p>\n\n",
                "matched_terms": [
                    "unist",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">A Large-Scale, High-Quality S2ST Dataset:</span> We design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English S2ST dataset with speaker voice and emotion preservation.</p>\n\n",
                "matched_terms": [
                    "unist",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Training end-to-end expressive S2ST models faces a significant bottleneck due to the scarcity of large-scale, parallel data that preserves speaker characteristics. To address this limitation, we design a scalable data synthesis pipeline. The pipeline generates UniST, our large-scale Chinese-English expressive S2ST dataset. Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(a) illustrates the data construction process.</p>\n\n",
                "matched_terms": [
                    "unist",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quality Filtering and Dataset Variants.</span>\nThe synthesized data undergoes final quality filtering.\nWe apply ASR to <math alttext=\"\\mathbf{Y}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><msub><mi>&#119832;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{Y}_{tgt}</annotation></semantics></math> and discard samples with WER greater than 0.01 against <math alttext=\"\\mathbf{T}_{tgt}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m2\" intent=\":literal\"><semantics><msub><mi>&#119827;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>g</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{T}_{tgt}</annotation></semantics></math>. We also apply a duration ratio filter to keep synthesized speech with duration within [0.5, 2.0] times of the source speech.\nThe filtered data forms our <span class=\"ltx_text ltx_font_bold\">UniST General</span> dataset (44.8k hours).\nOur refined <span class=\"ltx_text ltx_font_bold\">UniST High-Quality</span> dataset (19.8k hours) is created by applying additional Voice Activity Detection&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib27\" title=\"\">2024</a>)</cite> to remove silence at the beginning and end of speech segments, and a stricter duration ratio filter of [0.7,1.5].</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "general",
                    "unist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.F3\" title=\"Figure 3 &#8227; 3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>(b,c), UniST General provides greater data diversity and generalization potential\ndue to its larger range of duration ratios and speech lengths. UniST High-Quality focuses on temporal consistency, making it ideal for the final refinement phase of our progressive training. In Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A5.SS3\" title=\"E.3 Analysis of UniST &#8227; Appendix E UniST and Construction Pipeline &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">E.3</span></a>, we show that the UniST dataset also preserves emotional style in synthesis.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "general",
                    "unist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">UniSS is trained in three progressive phases, as introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#S3.SS3\" title=\"3.3 Progressive Training Strategy &#8227; 3 Method &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>. Phase 1 establishes text-speech alignment using 77.1k hours of speech data and MT text data from WMT17&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bojar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib28\" title=\"\">2017</a>)</cite>. Phase 2 introduces CoT prompting with our UniST General dataset, mixed with Phase 1 data at a 2:1 ratio. Phase 3 fine-tunes the model exclusively on the UniST High-Quality dataset.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "general",
                    "unist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For our primary S2ST evaluation, we use the Chinese (ZH) and English (EN) test sets from CVSS-T&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jia et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib16\" title=\"\">2022a</a>)</cite>, which contain 4,897 utterance pairs (8.2 hours for Chinese and 6.3 hours for English). For emotion preservation evaluation, we randomly sample 300 utterances from each emotional speech dataset (ESD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib31\" title=\"\">2021</a>)</cite> and CREMA-D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib32\" title=\"\">2014</a>)</cite>), covering a selected set of 4 emotions (happy, sad, angry, neutral). We evaluate both English-to-Chinese (EN-ZH) and Chinese-to-English (ZH-EN) translation directions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ comprehensive objective and subjective metrics to evaluate S2ST performance. For translation fidelity, we measure BLEU scores&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib33\" title=\"\">2002</a>)</cite> on both ASR-transcribed speech outputs (Speech-BLEU) and intermediate text (Text-BLEU). Prosody preservation is assessed through prosody similarity (A.PCP)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Andrews et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib34\" title=\"\">2022</a>)</cite>. Duration consistency is evaluated using Speech Length Compliance (SLC) within 0.2 and 0.4 ratio tolerance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib35\" title=\"\">2023</a>)</cite>.\nSpeech quality is measured using UTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Saeki et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib36\" title=\"\">2022</a>)</cite>. Additionally, we conduct a subjective Mean Opinion Score (MOS) evaluation where bilingual speakers rate emotion similarity, speaker similarity, and naturalness on a 5-point scale. Detailed descriptions of all evaluation metrics and implementation procedures are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A3\" title=\"Appendix C Evaluation Metrics &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mos",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces UniSS, a unified single-stage expressive speech-to-speech translation framework that preserves voice and emotional style. Our approach eliminates architectural complexity through well-designed speech semantic and style modeling, enabling seamless integration with pre-trained LLMs. By transferring LLMs&#8217; text translation capabilities to speech via cross-modal CoT prompting, UniSS surpasses previous S2ST systems across multiple dimensions: translation fidelity, speech naturalness, and preservation of voice, emotion, and duration consistency.\nBeyond the model architecture, we design a reproducible pipeline to create expressive S2ST datasets and construct UniST, a large-scale Chinese-English expressive S2ST dataset. Our work demonstrates a simple and effective approach for building the next generation of expressive S2ST systems.</p>\n\n",
                "matched_terms": [
                    "unist",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce CoT prompting using our UniST General dataset.\nThe model learns three generation paths: Performance mode, Quality mode, and direct S2ST data without intermediate text.\nNew data is mixed with Phase 1 data at a 2:1 ratio, totaling approximately 55B tokens. We train for one epoch with a constant learning rate of 2e-4 and a 5% warm-up.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "general",
                    "unist"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune the model exclusively on the UniST High-quality dataset.\nThe learning rate follows a cosine schedule, gradually annealing from 5e-5 to 5e-6 over 1 epoch without warm-up. This phase consists of approximately 10B training tokens. During this stage, the training loss reaches its minimum around 0.9 epochs, and we select the checkpoint at this point as our final model weights.</p>\n\n",
                "matched_terms": [
                    "unist",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-BLEU</span> evaluates the fidelity of generated translations by computing corpus-level BLEU scores using the SacreBLEU library&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Post, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib46\" title=\"\">2018</a>]</cite>. Before scoring, we apply language-specific preprocessing: English text is lowercased and stripped of punctuation (excluding apostrophes), while Chinese text is normalized to simplified characters, punctuation is removed, and characters are separated by spaces. This ensures consistency with standard BLEU evaluation practices. We use the <span class=\"ltx_text ltx_font_italic\">corpus_score</span> function to calculate the BLEU score across the whole dataset. Chinese samples are scored in &#8216;zh&#8217; mode.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To comprehensively evaluate expressive qualities, we conducted a Mean Opinion Score (MOS) listening study. Six bilingual speakers rated synthesized speech on a 5-point scale across three dimensions: emotion similarity (Emo Sim.), speaker similarity (Spk Sim.), and speech naturalness (Naturalness). Similarity is compared against the original audio instead of the synthesized audio.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "emo",
                    "spk"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implemented the MOS evaluation using webMUSHRA&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Schoeffler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib47\" title=\"\">2018</a>]</cite>, a MUSHRA-compliant web-based audio evaluation framework that facilitates controlled listening experiments. The platform provided key features for audio assessment, including seamless audio switching and Likert scale questionnaires. Experimental configurations were defined through YAML files, with results automatically exported as CSV files. The user interface is shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#A3.F2\" title=\"Figure C2 &#8227; C.2 Subjective Evaluation &#8227; Appendix C Evaluation Metrics &#8227; UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice\"><span class=\"ltx_text ltx_ref_tag\">C2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also evaluate on the Chinese and English test subsets of FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.21144v1#bib.bib48\" title=\"\">2022</a>]</cite>, a massively multilingual speech dataset spanning 102 languages with n-way parallel speech and text data. UniSS demonstrates robust performance on this challenging multilingual benchmark across multiple evaluation tasks. Additionally, UniSS achieves state-of-the-art performance on speech-to-text translation (S2TT) and competitive results on automatic speech recognition (ASR) and text-to-speech synthesis (TTS) tasks.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dataset"
                ]
            }
        ]
    }
}