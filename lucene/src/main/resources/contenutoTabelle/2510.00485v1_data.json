{
    "S2.T1": {
        "source_file": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
        "caption": "Table 1: Comparison of podcast generation systems.",
        "body": "System\nOpen-Source?\n# Speaker\nSupport Voice Selection?\nIs Dialogue TTS?\nSupport Music/Sound?\n\n\nNotebookLM\n✗\n2\n✗\n-\n✗\n\n\nDia\n✓\n2\nPreset\n✓\n✗\n\n\nMuyan-TTS\n✓\n1\nPreset\n✗\n✗\n\n\nMoonCast\n✓\n2\nPreset\n✓\n✗\n\n\nMOSS-TTSD\n✓\n2\nPreset\n✓\n✗\n\n\nPodAgent*\n\n✓\nN\nAuto\n✗\n✓",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">System</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Open-Source?</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\"># Speaker</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Support Voice Selection?</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Is Dialogue TTS?</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Support Music/Sound?</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">NotebookLM</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10007;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Dia</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10003;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Preset</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Muyan-TTS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10003;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Preset</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MoonCast</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10003;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Preset</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MOSS-TTSD</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10003;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Preset</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PodAgent<sup class=\"ltx_sup\">*</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10003;</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">N</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Auto</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">&#10003;</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "musicsound",
            "preset",
            "muyantts",
            "mossttsd",
            "selection",
            "system",
            "systems",
            "voice",
            "notebooklm",
            "podagent",
            "generation",
            "comparison",
            "podcast",
            "mooncast",
            "dia",
            "opensource",
            "auto",
            "speaker",
            "tts",
            "support",
            "dialogue"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Podcasts are a popular audio format, with platforms like Apple Podcasts and Spotify leading the way. The rise of the AI podcast began with Google&#8217;s NotebookLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib17\" title=\"\">2023</a>)</cite>, which gained popularity in late 2024 for its &#8220;Audio Overviews&#8221; feature. This feature converts materials into conversational, two-person podcasts, praised for its highly natural dialogue speech. Similarly, most open-source podcast generation systems focus on dialogue speech synthesis, like Dia <cite class=\"ltx_cite ltx_citemacro_citep\">(Nari Labs, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib27\" title=\"\">2025</a>)</cite>, Muyan-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib23\" title=\"\">2025</a>)</cite>, MoonCast <cite class=\"ltx_cite ltx_citemacro_citep\">(Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib21\" title=\"\">2025</a>)</cite> and MOSS-TTSD <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenMOSS Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib29\" title=\"\">2025</a>)</cite>. These systems function primarily as dialogue Text-to-Speech (TTS) engines for text-given scenarios. Another type of podcast generation system takes a more holistic approach, incorporating elements beyond speech, such as text and music/sound. For example, WavJourney <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib25\" title=\"\">2023</a>)</cite> leverages LLMs to connect components like TTS and Text-to-Audio (TTA), generating element-rich audio programs. Upon this, PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite> introduces a &#8220;Host-Guest-Writer&#8221; multi-agent system to create informative conversation scripts and builds a voice pool for appropriate voice selection. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Podcast generation &#8227; 2 Related work &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> compares the systems leveraged in subsequent experiments.</p>\n\n",
            "<p class=\"ltx_p\">The text-based evaluation is conducted among GPT-4, PodAgent, MoonCast, and Real-Pod. Other systems in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Podcast generation &#8227; 2 Related work &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> are excluded as they do not provide conversation scripts. PodAgent, with its Host-Guest-Writer multi-agent system, can directly generate podcast scripts based on a given topic. While MoonCast functions similarly to NotebookLM, requiring external knowledge sources but providing prompt template for spontaneous script generation. For this evaluation, the MoonCast system uses the podcast scripts generated by PodAgent as input and transforms them into a spontaneous version.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recently, an increasing number of multimodal (text and audio) benchmarks have emerged, primarily focusing on evaluating models&#8217; understanding capability. However, exploration into assessing generative capabilities remains limited, especially for open-ended long-form content generation. Significant challenges lie in no reference standard answer, no unified evaluation metrics and uncontrollable human judgments. In this work, we take podcast-like audio generation as a starting point and propose PodEval, a comprehensive and well-designed open-source evaluation framework. In this framework: 1) We construct a real-world podcast dataset spanning diverse topics, serving as a reference for human-level creative quality. 2) We introduce a multimodal evaluation strategy and decompose the complex task into three dimensions: text, speech and audio, with different evaluation emphasis on &#8220;Content&#8221; and &#8220;Format&#8221;. 3) For each modality, we design corresponding evaluation methods, involving both objective metrics and subjective listening test. We leverage representative podcast generation systems (including open-source, close-source, and human-made) in our experiments. The results offer in-depth analysis and insights into podcast generation, demonstrating the effectiveness of PodEval in evaluating open-ended long-form audio. This project is open-source to facilitate public use: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/yujxx/PodEval\" title=\"\">https://github.com/yujxx/PodEval</a>.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "generation",
                    "systems",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most content-related objective metrics rely on reference scripts to measure quality and relevance. However, podcast generation lacks standardized references as it is an open-ended generation task. Moreover, relying on such references limits the diversity and creativity of the generated content.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">General speech evaluation focuses on individual sentences, while podcasts require natural and interactive dialogue, emphasizing dialogue-level naturalness. Additionally, voice presentation in multi-speaker scenarios is critical to ensuring role distinction and overall listener engagement.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each modality, we design tailored metrics to address diversity considerations. For text, we combine quantitative metrics with LLM-based evaluations to assess conversation scripts. For speech and audio, we design objective metrics and subjective listening tests to evaluate spoken dialogue and overall audio performance. All evaluation methods are organized into open-source tools for ease of use. Subjective tests are enhanced by spammer detection to improve data validity.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We utilize representative podcast generation systems in our experiments, including open-source, closed-source, and human-made ones. The results offer detailed analyses of these systems, provide insights for podcast generation, and validate the effectiveness of our evaluation framework.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "generation",
                    "systems",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PodAgent uses CosyVoice2<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib14\" title=\"\">2024</a>)</cite> as its backend TTS model, which is a single-sentence TTS system.</p>\n\n",
                "matched_terms": [
                    "system",
                    "tts",
                    "podagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Various evaluation works have emerged along with the development of LLMs and multimodal generative models. <span class=\"ltx_text ltx_font_bold\">Text-related Evaluation</span>, such as SuperGLUE, MMLU, and BIG-bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib41\" title=\"\">2019</a>; Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib18\" title=\"\">2020</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib39\" title=\"\">2022</a>)</cite>, assesses the capabilities of LLMs across diverse tasks with preset ground truth. Subsequently, MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib47\" title=\"\">2023</a>)</cite> explores the potential of LLMs as evaluators, and Chatbot Arena <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib12\" title=\"\">2024</a>)</cite> provides an open platform for assessing LLMs based on human preferences. <span class=\"ltx_text ltx_font_bold\">Speech-related Evaluation</span>, such as SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib45\" title=\"\">2021</a>)</cite>, is designed for discriminative tasks like speech recognition and speaker identification. However, evaluations for generative tasks are scarce due to their inherent diversity and subjectivity, making subjective evaluation essential for speech generation tasks. For instance, VOCBENCH <cite class=\"ltx_cite ltx_citemacro_citep\">(AlBadawy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib3\" title=\"\">2022</a>)</cite> incorporates both subjective and objective evaluations to assess vocoder performance in speech synthesis. Similarly, numerous <span class=\"ltx_text ltx_font_bold\">Audio-related Evaluation</span> work, such as AIR-Bench, Audiobench, MMAU, and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib44\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib42\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib35\" title=\"\">2024</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib26\" title=\"\">2025</a>)</cite>, focus on audio understanding and reasoning. Subjective evaluation remains crucial for assessing audio generation systems and is typically tailored to specific generation tasks. Unlike existing evaluation works, <span class=\"ltx_text ltx_font_bold\">PodEval</span> introduces a comprehensive framework specifically designed for podcast-like audio generation. It emphasizes both subjective and objective evaluations across text, speech, and audio, with all metrics closely aligned with real-world user experience.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "generation",
                    "systems",
                    "preset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Podcast Episode</span>. After finalizing the topic collection, we manually searched and screened podcast episodes to identify those most relevant to the selected topics. The selection process was guided by: (1) Topic Relevance: Episodes were selected based on their alignment with the predefined topics. (2) Rich Format: Preference was given to episodes that featured multi-speaker conversations, included background music and sound effects, and exhibited high audio quality.</p>\n\n",
                "matched_terms": [
                    "selection",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dialogue content in podcasts is extracted in text format for evaluation, representing the core message the podcast aims to convey. Podcast dialogues often center around specific topics, showcasing participants&#8217; unique perspectives and insights, which makes reference-correlation-based methods infeasible. Instead, the richness of perspectives conveyed (to provide informative takeaways for the listener) and the presentation style of the dialogue (to enhance listener comprehension) should be the primary focus of evaluation. Therefore, we follow the dialogue script-based evaluation methods proposed in PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite>, which adopt a two-fold approach: (1) <span class=\"ltx_text ltx_font_bold\">Quantitative Metrics</span> such as Distinct-N, Semantic-Div, MATTR, and Info-Dens to assess lexical diversity, semantic richness, vocabulary richness, and information density, respectively. These metrics operate independently of reference texts and focus on intrinsic text characteristics; (2) <span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge</span>, leveraging GPT-4 to replace human evaluators for complex and comprehensive assessments. Evaluation criteria include coherence, engagingness, diversity, informativeness and speaker diversity. It incorporates comparative evaluations to reduce bias and evidence-based scoring for robust and reliable results.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "podagent",
                    "dialogue",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span> (Word Error Rate) measures pronunciation accuracy, a critical indicator of the robustness of TTS-based podcast generation systems, powered by Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib32\" title=\"\">2022</a>)</cite> in our toolkit.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "systems",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SIM</span> stands for Speaker Similarity. In podcast generation systems, zero-shot TTS is often employed to replicate the voice of a preset speaker. The SIM between the synthesized voice and the reference voice serves as a crucial metric about vocal fidelity. In PodEval, SIM is quantified using the cosine similarity of extracted speaker embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Plaquet &amp; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib31\" title=\"\">2023</a>; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib8\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "systems",
                    "generation",
                    "speaker",
                    "preset",
                    "podcast",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Objective metrics can be calculated efficiently at a low cost without human involvement. However, the <span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> remains a necessary indicator of human perception. Unlike general speech synthesis, which emphasizes sentence-level pronunciation accuracy and naturalness, podcast speech focuses on achieving human-like natural dialogue. Subjective tests for such long-form speech present several key <span class=\"ltx_text ltx_font_bold\">challenges</span>: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> the length of dialogue in podcasts ranges from a few minutes to over an hour, making it impractical to evaluate the entire speech directly; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> the difficulty of comparing more than two systems simultaneously; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> guiding user focus toward dialogue naturalness, rather than on factors like content; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">4)</span> balancing topic diversity within a fixed testing capacity; and <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">5)</span> ensuring that crowdsourced evaluators remain focused and provide reliable feedback.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "dialogue",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In PodEval, we design the <span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation</span> based on the MUSHRA framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Schoeffler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib36\" title=\"\">2018</a>)</cite>. The key insight from this framework, <span class=\"ltx_text ltx_font_italic\">incorporating both high-quality and low-quality anchors</span>, helps evaluators establish a reliable reference of quality range. For researchers, analyzing scores for these anchors helps identify inattentive evaluators, enabling the <span class=\"ltx_text ltx_font_italic\">filtering of invalid submissions</span> and improving the data vadility. In our task, we use real podcast segments from the <span class=\"ltx_text ltx_font_italic\">Real-Pod dataset as the high-quality anchor</span> and synthesized dialogue segments from <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib16\" title=\"\">eSpeak Contributors </a></cite><span class=\"ltx_text ltx_font_italic\"> as the low-quality anchor</span>. For podcast samples from different systems, we provide an automatic toolkit to extract dialogue segments featuring <span class=\"ltx_text ltx_font_italic\">turn-taking</span> between speakers, representing a typical dialogue flow. Each dialogue segment is extracted with a <span class=\"ltx_text ltx_font_italic\">preset length</span> (e.g. 15&#8211;25 seconds) to ensure the speech samples are of similar duration. We select dialogue segments from <span class=\"ltx_text ltx_font_italic\">all 17 categories</span> in the Real-Pod dataset to ensure content diversity while keeping the total listening test duration <span class=\"ltx_text ltx_font_italic\">within 30 minutes</span>. In each test group, samples from different systems are presented <span class=\"ltx_text ltx_font_italic\">on the same page</span>, along with a <span class=\"ltx_text ltx_font_italic\">reference Real-Pod sample</span> to guide evaluators on what a natural dialogue sounds like. The scoring is adjusted using a slider ranging from 0 to 100, divided into <span class=\"ltx_text ltx_font_italic\">five stages with a clear definition</span>. Detailed instructions and website design can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS3\" title=\"A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The demo website is hosted at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml\" title=\"\">https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml</a>. Everyone is welcome to try it out and view the results at the end.</span></span></span></p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "systems",
                    "preset",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SMR</span> (Speech-to-Music Ratio): MSE are typically integrated into podcast audio to enhance the overall listening experience. Since speech is the primary focus in podcasts, it is essential to ensure that MSE dose not overpower or mask the speech, maintaining clarity and intelligibility of the dialogue. SMR measures the balance between speech and MSE, with a minimum requirement of being greater than 0. SMR_SCORE is the proportion of cases where SMR exceeds 0.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> is primarily designed based on the perceptions of real users. A key challenge lies in how to evaluate extra-long audios. As we mentioned above, podcasts in the real world range from a few minutes to over an hour in length. Conducting listening tests on full-length podcast episodes is impractical due to the time, effort, and financial resources required. Moreover, it is hard to judge podcasts of vastly different lengths in a fair and consistent manner. Research on long-form audio evaluation is limited. <cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib13\" title=\"\">2019</a>)</cite> did investigation on long-form speech evaluation and found that multiple evaluations are necessary due to the low correlation observed across different experimental settings. <cite class=\"ltx_cite ltx_citemacro_cite\">Cambre et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib11\" title=\"\">2020</a>)</cite> conducted a comprehensive evaluation of voice selection for long-form content; however, the minimum required listening time was only 10 seconds. A podcast-related evaluation study <cite class=\"ltx_cite ltx_citemacro_citep\">(Austria, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib6\" title=\"\">2007</a>)</cite> designed a questionnaire with carefully crafted questions in terms of both content and presentation to assess domain-specific podcasts. Different from that, PodEval does not constrain the domain of podcasts, and open-ended content evaluation is separately conducted in the text-based evaluation section. In this audio-based evaluation, we focus on assessing the overall performance of the audios. The design approach is as follows:</p>\n\n",
                "matched_terms": [
                    "selection",
                    "voice",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Metrics.</span> Detailed scores calculated across 17 podcast categories for each system are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS1\" title=\"A.2.1 Quantitative metrics &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.1</span></a>. For a concise and clearer comparison, we present the overall performance (averaged across all 17 categories) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F2\" title=\"Figure 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where we can observe that: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> For each quantitative metric, PodAgent outperforms directly prompting GPT-4; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> When comparing LLM-based methods (GPT-4, PodAgent) with human-created podcasts (Real-Pod), Real-Pod scores lower on lexical diversity (Distinct-2 and MATTR) but higher on information density and semantic diversity (Info-Dens and Sem-Div). This is reasonable for: i) real human interactions often include filler words and use simpler language; ii) most real podcasts are significantly longer (30 minutes to an hour), leading to higher information richness compared to generated podcasts, which are usually only a few minutes long; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> As a spontaneous version of PodAgent, MoonCast shows reduced lexical diversity and information density. While its semantic diversity remains comparable to PodAgent.</p>\n\n",
                "matched_terms": [
                    "system",
                    "podagent",
                    "comparison",
                    "podcast",
                    "mooncast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> This evaluation compares PodAgent (scored from -3 to 3) with GPT-4 (reference score as 0), both of which generate conversation scripts without external knowledge resources. Detailed scores for each category are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS2\" title=\"A.2.2 LLM-as-a-Judge &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.2</span></a>. We present the overall performance and results for five specific categories in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T2\" title=\"Table 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for analysis. We can see that scores across all metrics and all categories are positive, demonstrating that PodAgent significantly outperforms directly prompting GPT-4 in generating podcast scripts across all evaluated dimensions.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure fairness, all open-source TTS systems use the same PodAgent-generated scripts. Subjective tests use a spontaneous version from MoonCast, while objective evaluations use the original PodAgent scripts, as filler words in the spontaneous version challenge metrics like WER.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "systems",
                    "tts",
                    "mooncast",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(1) shows the WER results calculated for the entire conversation script. All systems, except MuyanTTS, achieve WER scores below 20%. Analysis of sampled MuyanTTS outputs reveals robustness issues like repeated sentences and the insertion of unknown content.</p>\n\n",
                "matched_terms": [
                    "muyantts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SIM.</span> The SIM metric evaluates zero-shot TTS systems&#8217; ability to replicate the timbre of a reference voice. PodAgent, MoonCast, MuyanTTS, Dia, and MOSS-TTSD&#8212;are assessed as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(2). Each system uses the reference voice selected by PodAgent for the topic. The performance rankings are: MuyanTTS, MOSS-TTSD, Dia, MoonCast, and PodAgent. PodAgent&#8217;s relatively low score in this metric likely stems from its instruction-following style control strategy. While this approach enhances overall conversational expressiveness, it can reduce speaker similarity.</p>\n\n",
                "matched_terms": [
                    "system",
                    "podagent",
                    "voice",
                    "mossttsd",
                    "speaker",
                    "tts",
                    "muyantts",
                    "mooncast",
                    "dia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SPTD.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(3) shows timbre variation across speakers in the conversation for three systems: Real-Pod, PodAgent, and NotebookLM. Real-Pod reflects real-world podcasts, PodAgent uses a voice selection mechanism for distinct voices, and NotebookLM fixed voices (one male, one female). The SPTD scores rank as follows: PodAgent, NotebookLM, and Real-Pod. This likely reflects that real-world podcasts prioritize guest expertise and availability over timbre differences. PodAgent demonstrates an effective automated voice selection process for podcast creation.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "podagent",
                    "systems",
                    "notebooklm",
                    "selection",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DNSMOS.</span> The DNSMOS metric was applied to all systems to evaluate speech quality as in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(4). PodAgent, MoonCast, MuyanTTS, MOSS-TTSD, and NotebookLM achieve similar scores, while Real-Pod and Dia show noticeable declines in speech quality. For Real-Pod, the lower scores are due to: (1) real podcasts often use MSE for enhancement, requiring speech-MSE separation before evaluation, which may leave residual MSE artifacts, and (2) human-created podcasts involve recording, editing, or post-processing that introduce noise or instability. Dia struggles with long-form speech synthesis. Its outputs for lengthy podcast scripts frequently feature overly fast speaking speeds and occasional sentence truncations, leading to its relatively low DNSMOS performance.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "systems",
                    "notebooklm",
                    "mossttsd",
                    "podcast",
                    "muyantts",
                    "mooncast",
                    "dia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation.</span> We released the task on Prolific<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.prolific.com/\" title=\"\">https://www.prolific.com/</a></span></span></span>, requesting 20 native English-speaking participants from the US/UK. We set the filter rules as: 1) Over 90% of LQ samples must be marked as the worst, as the synthesized samples from eSpeak are obviously robotic and unnatural. 2) Over 50% of HQ samples must rank in the top-2 best. While it is possible for other systems to achieve a better score than the real podcast, the evaluation of the real podcast should also remain above average. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T3\" title=\"Table 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the two statistical metrics for the submission results. Based on these rules, Judger-7, 18 and 20 can be excluded. We also provide the box plot for each Judger in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F9\" title=\"Figure 9 &#8227; A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix for more advanced analysis. For instance, apart from the LQ samples, Judger-20 assigns similar scores to all other systems, further confirming the invalidity of this submission.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "dialogue",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Result Analysis.</span> After excluding unqualified submissions, we analyzed system performance based on the remaining 17 valid submissions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F4\" title=\"Figure 4 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the final results. We can observe that dialogue segments from real podcasts (HQ) achieved the highest scores, which aligns with expectations. NotebookLM, a closed-source product, ranked second, reflecting the high naturalness of its synthesized dialogue speech. Among the three open-source podcast generation systems, PodAgent scored the lowest, which is reasonable since its backend TTS system, CosyVoice2, is limited to single-sentence synthesis. In contrast, MoonCast and MOSS-TTSD, which support direct dialogue synthesis, performed better in dialogue naturalness evaluations. Overall, the evaluation results align with expectations, validating the rationality and effectiveness of our evaluation method design.</p>\n\n",
                "matched_terms": [
                    "system",
                    "podagent",
                    "systems",
                    "notebooklm",
                    "mossttsd",
                    "generation",
                    "tts",
                    "podcast",
                    "mooncast",
                    "support",
                    "opensource",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the density distribution of loudness-related metrics, enabling a comparative analysis with the reference range. All seven systems are included. For <span class=\"ltx_text ltx_font_bold\">LOUD_IT</span>, Dia and MoonCast align well with the reference range, while NotebookLM&#8217;s loudness centers around -25. Real-Pod, as manually produced audio, shows a highly scattered loudness distribution. For <span class=\"ltx_text ltx_font_bold\">LOUD_TP</span>, Muyan-TTS performs best, with all samples maintaining a true peak loudness below -1. In contrast, MoonCast, Dia and MOSS-TTSD perform poorly, while Real-Pod continues to exhibit scattered results. For <span class=\"ltx_text ltx_font_bold\">LOUD_RA</span>, MoonCast has a relatively narrow loudness variation range, while PodAgent and MOSS-TTSD display richer variance. Quantitative scores are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "systems",
                    "mossttsd",
                    "muyantts",
                    "mooncast",
                    "dia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SMR and CASP.</span> PodAgent and Real-Pod are evaluated for these MSE-related metrics. From the density distribution in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, PodAgent exhibits a more concentrated distribution compared to Real-Pod. For <span class=\"ltx_text ltx_font_bold\">SMR</span>, the SMR_SCORE in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows that all PodAgent samples achieve an SMR greater than 0, whereas some Real-Pod cases fail to meet this requirement. For <span class=\"ltx_text ltx_font_bold\">CASP</span>, a higher score indicates better MSE-Speech harmony. Real-Pod demonstrates a higher upper limit, which is expected as exceptional human artistic creations naturally surpass AI-generated outputs. However, PodAgent delivers more consistent performance, and the overall gap between the two systems is not significant, making it an alternative way to enhance creative efficiency.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to direct scores, we also derive a corresponding score based on users&#8217; justifications. Specifically, given justification texts from multiple systems for the same question, GPT-4 uses the following prompt to score: <span class=\"ltx_text ltx_font_italic\">&#8221;For each system, summarize the corresponding comments into one sentence and assign a score between 1 and 5.&#8221;</span> A detailed experiment setup is provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS5.SSS2\" title=\"A.5.2 Questionnaire-based MOS test &#8227; A.5 Audio-based evaluation (subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.5.2</span></a>, and separate scores are listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T10\" title=\"Table 10 &#8227; A.5.2 Questionnaire-based MOS test &#8227; A.5 Audio-based evaluation (subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F6\" title=\"Figure 6 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the final results, averaging the direct score and the justification-based score. From the result, we can observe that:</p>\n\n",
                "matched_terms": [
                    "system",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Speech (naturalness and authenticity) is the most dominant factor affecting the listener&#8217;s experience</span>. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, PodAgent scored low in dialogue naturalness due to using a single-sentence synthesis TTS system, leading to consistently poor results in this MOS test. This outcome is expected, as dialogue speech is the core component of podcast-like audio programs. Although PodAgent&#8217;s Music/Sound (harmony) score is below Real-Pod (consistent with the results of objective metric - CASP), it is significantly higher than its scores in other metrics, indicating that <span class=\"ltx_text ltx_font_italic\">the gap between PodAgent and Real-Pod in music harmony is smaller than in speech naturalness</span>.</p>\n\n",
                "matched_terms": [
                    "musicsound",
                    "system",
                    "podagent",
                    "tts",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Real podcasts perform best in most metrics (5/7).</span> Real-Pod significantly outperforms other systems on holistic metrics like Engagement Level (EL) and Human Likelihood (HL). However, Full Episode Willingness (FEW) scores are low across all systems, with NotebookLM and Real-Pod scoring similarly. <span class=\"ltx_text ltx_font_italic\">This highlights the value of perceptual and preference-based question design in the test.</span> FEW, a preference-based question, garnered justifications like &#8220;the topic is not of interest to me&#8221; for lower scores. In contrast, higher scores for EL and HL indicate that users tend to exclude subjective factors (e.g., personal topic interest) when rating audio performance. A similar pattern is observed in Information Delivery (effectiveness) and Speaker Expression (preference).</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "systems",
                    "notebooklm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Audio Quality metric, while PodAgent and MOSS-TTSD score lower than Real-Pod, PodAgent performs better here than in other metrics, and NotebookLM slightly surpasses Real-Pod. As noted, human-made podcasts often exhibit inconsistent audio quality due to complex production. User feedback, like &#8220;Little bit of mic hiss/bloom but otherwise fine,&#8221; supports this observation. This highlights that <span class=\"ltx_text ltx_font_italic\">when conversational realism approaches that of real speech, AI-based methods offers an advantage in their controllability and consistency in producing high-quality audio</span>.</p>\n\n",
                "matched_terms": [
                    "mossttsd",
                    "podagent",
                    "notebooklm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PodEval is the first comprehensive evaluation framework for podcast-like audio generation, tackling the challenges of assessing open-ended, long-form content. We constructed a real-world podcast dataset as a benchmark for human-level creative quality across diverse topics and formats. By decomposing evaluation into text, speech, and audio, PodEval introduced multidimensional methods combining objective metrics and well-designed subjective listening tests. Experiments with various podcast generation systems, including open-source, closed-source, and human-made examples, validated the framework&#8217;s effectiveness. The results offer insights into the strengths and weaknesses of different systems (e.g. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F14\" title=\"Figure 14 &#8227; A.6 System analysis report &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>), highlighting PodEval&#8217;s role in advancing podcast generation research and inspiring future work on evaluating open-ended, long-form content generation task.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "generation",
                    "systems",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces PodEval, a comprehensive framework for evaluating podcast-like audio generation, with careful consideration of ethical implications. The <span class=\"ltx_text ltx_font_italic\">Real-Pod dataset</span> was constructed using publicly available podcasts in alignment with fair use, avoiding sensitive or private data. Instead of directly providing audio files, the dataset offers publicly accessible download links and download toolkit to reduce the risk of misuse and ensure proper attribution. <span class=\"ltx_text ltx_font_italic\">Subjective evaluations</span> were conducted using crowdsourced workers recruited through the Prolific platform, with compensation exceeding the platform&#8217;s minimum wage requirements. Reliability was ensured through attention-check questions and clear instructions for participants. To mitigate bias, the framework incorporates <span class=\"ltx_text ltx_font_italic\">diverse topics and evaluators</span>, promoting inclusivity and fairness. While PodEval aims to advance AI-assisted podcast generation, we emphasize its role as a tool to enhance, not replace, human creativity. PodEval is designed to foster innovation while adhering to principles of transparency, fairness, and ethical AI development.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Framework for <span class=\"ltx_text ltx_font_italic\">subjective human evaluations</span> of podcast speech and audio. One is <span class=\"ltx_text ltx_font_italic\">Dialogue Naturalness Evaluation</span> and the other one is <span class=\"ltx_text ltx_font_italic\">Questionnaire-based MOS Test</span>.</p>\n\n",
                "matched_terms": [
                    "dialogue",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Experiment Settings:</span> Lengthy listening tests can be exhausting and may lead to inaccurate feedback. It is essential to ensure the overall test duration does not exceed 30 minutes. In the Questionnaire-based MOS Test, each audio sample is around 3 minutes and requires answering 10 questions with corresponding justifications. Based on the Dialogue Naturalness Test results shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, we selected 4 representative systems. Each test group included four podcast samples from different systems but within the same podcast category. According to actual test results, each group took an average of 24 minutes to complete. The 4 representative systems are:</p>\n\n",
                "matched_terms": [
                    "systems",
                    "dialogue",
                    "podcast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PodAgent:</span> An open-source podcast generation framework incorporating conversation script generation, automatic voice selection, speech synthesis, and BMSE enhancement.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "podagent",
                    "generation",
                    "selection",
                    "podcast",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MOSS-TTSD:</span> Achieved the highest score among the open-source systems utilized in the Dialogue Naturalness Evaluation (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "mossttsd",
                    "opensource",
                    "systems",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NotebookLM:</span> A pioneering podcast generation product, widely recognized for its exceptional performance, is nearly indistinguishable from real podcasts.</p>\n\n",
                "matched_terms": [
                    "podcast",
                    "generation",
                    "notebooklm"
                ]
            }
        ]
    },
    "S7.T2": {
        "source_file": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
        "caption": "Table 2: LLM-as-a-Judge: comparison between GPT-4 and PodAgent (overall performance and 5 specific categories). Scores range from -3 to 3, where positive values favor PodAgent.",
        "body": "Metrics\nOverall\nFiction\nEducation\nBusiness\nTrueCrime\nHealth & Fitness\n\n\n\n\nCoherence\n0.7059\n0.5000\n0.8333\n1.0000\n1.0000\n0.6667\n\n\nEngagingness\n1.0294\n1.1667\n1.0000\n1.1667\n0.6667\n1.1667\n\n\nDiversity\n1.1765\n1.3333\n1.0000\n1.3333\n0.8333\n1.5000\n\n\nInformativeness\n1.6078\n1.5000\n1.6667\n2.0000\n1.1667\n1.6667\n\n\nSpeaker Difference\n1.0637\n0.9167\n1.0000\n1.1667\n0.6667\n1.0000\n\n\nOverall\n1.3064\n1.2500\n1.3333\n1.6667\n0.8333\n1.2500",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"--ltx-bg-color:#FFFF00;padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFFF00;\">Overall</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Fiction</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Education</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Business</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">TrueCrime</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Health &amp; Fitness</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 4.0pt;\">Coherence</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.7059</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 4.0pt;\">0.5000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 4.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 4.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 4.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 4.0pt;\">0.6667</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 4.0pt;\">Engagingness</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">1.0294</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.6667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.1667</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 4.0pt;\">Diversity</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">1.1765</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.3333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.3333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.5000</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 4.0pt;\">Informativeness</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">1.6078</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.6667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">2.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.6667</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 4.0pt;\">Speaker Difference</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">1.0637</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.9167</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.6667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.0000</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:1.5pt 4.0pt;\">Overall</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 4.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">1.3064</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 4.0pt;\">1.2500</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 4.0pt;\">1.3333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 4.0pt;\">1.6667</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 4.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 4.0pt;\">1.2500</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "difference",
            "business",
            "coherence",
            "categories",
            "where",
            "range",
            "fiction",
            "truecrime",
            "specific",
            "overall",
            "scores",
            "metrics",
            "gpt4",
            "podagent",
            "performance",
            "comparison",
            "positive",
            "fitness",
            "from",
            "engagingness",
            "health",
            "llmasajudge",
            "education",
            "informativeness",
            "diversity",
            "speaker",
            "favor",
            "values",
            "between"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> This evaluation compares PodAgent (scored from -3 to 3) with GPT-4 (reference score as 0), both of which generate conversation scripts without external knowledge resources. Detailed scores for each category are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS2\" title=\"A.2.2 LLM-as-a-Judge &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.2</span></a>. We present the overall performance and results for five specific categories in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T2\" title=\"Table 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for analysis. We can see that scores across all metrics and all categories are positive, demonstrating that PodAgent significantly outperforms directly prompting GPT-4 in generating podcast scripts across all evaluated dimensions.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Most content-related objective metrics rely on reference scripts to measure quality and relevance. However, podcast generation lacks standardized references as it is an open-ended generation task. Moreover, relying on such references limits the diversity and creativity of the generated content.</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct a real-world podcast dataset spanning a wide range of podcast categories and topics, serving as a reference for human-level creative quality. Model-based samples are also provided.</p>\n\n",
                "matched_terms": [
                    "categories",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each modality, we design tailored metrics to address diversity considerations. For text, we combine quantitative metrics with LLM-based evaluations to assess conversation scripts. For speech and audio, we design objective metrics and subjective listening tests to evaluate spoken dialogue and overall audio performance. All evaluation methods are organized into open-source tools for ease of use. Subjective tests are enhanced by spammer detection to improve data validity.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "diversity",
                    "metrics",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Various evaluation works have emerged along with the development of LLMs and multimodal generative models. <span class=\"ltx_text ltx_font_bold\">Text-related Evaluation</span>, such as SuperGLUE, MMLU, and BIG-bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib41\" title=\"\">2019</a>; Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib18\" title=\"\">2020</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib39\" title=\"\">2022</a>)</cite>, assesses the capabilities of LLMs across diverse tasks with preset ground truth. Subsequently, MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib47\" title=\"\">2023</a>)</cite> explores the potential of LLMs as evaluators, and Chatbot Arena <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib12\" title=\"\">2024</a>)</cite> provides an open platform for assessing LLMs based on human preferences. <span class=\"ltx_text ltx_font_bold\">Speech-related Evaluation</span>, such as SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib45\" title=\"\">2021</a>)</cite>, is designed for discriminative tasks like speech recognition and speaker identification. However, evaluations for generative tasks are scarce due to their inherent diversity and subjectivity, making subjective evaluation essential for speech generation tasks. For instance, VOCBENCH <cite class=\"ltx_cite ltx_citemacro_citep\">(AlBadawy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib3\" title=\"\">2022</a>)</cite> incorporates both subjective and objective evaluations to assess vocoder performance in speech synthesis. Similarly, numerous <span class=\"ltx_text ltx_font_bold\">Audio-related Evaluation</span> work, such as AIR-Bench, Audiobench, MMAU, and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib44\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib42\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib35\" title=\"\">2024</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib26\" title=\"\">2025</a>)</cite>, focus on audio understanding and reasoning. Subjective evaluation remains crucial for assessing audio generation systems and is typically tailored to specific generation tasks. Unlike existing evaluation works, <span class=\"ltx_text ltx_font_bold\">PodEval</span> introduces a comprehensive framework specifically designed for podcast-like audio generation. It emphasizes both subjective and objective evaluations across text, speech, and audio, with all metrics closely aligned with real-world user experience.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "diversity",
                    "speaker",
                    "specific",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There is no unified standard for defining &#8220;what makes a good podcast episode.&#8221; Unlike textbooks or official TV programs, podcasts can be created by anyone to share their unique ideas or insights. We do not make direct comparisons between generated podcasts and real podcasts&#8212;such comparisons are inherently unfeasible, especially when they approach topics from entirely different perspectives. Instead, we construct a real-world podcast dataset, called <span class=\"ltx_text ltx_font_bold\">Real-Pod</span> dataset, to serve as a reference for human-level creative quality. It is important to note that this dataset acts as a &#8220;reference&#8221; rather than an absolute &#8220;answer&#8221;. The design principles of the Real-Pod dataset are <span class=\"ltx_text ltx_font_bold\">real</span> (consists of human-made podcasts), <span class=\"ltx_text ltx_font_bold\">broad</span> (diverse topic coverage) and <span class=\"ltx_text ltx_font_bold\">rich</span> (varied formats, like multi-speaker, music and sound). The workflow for constructing the Real-Pod dataset is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S3.F1\" title=\"Figure 1 &#8227; 3 Real-Pod: Real-world podcast dataset &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>:</p>\n\n",
                "matched_terms": [
                    "between",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Podcast Category</span>. We began by compiling a comprehensive list of podcast categories based on the taxonomy from Apple Podcast <cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib5\" title=\"\">Apple Inc., </a>)</cite>. The 17 categories are shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S3.F1\" title=\"Figure 1 &#8227; 3 Real-Pod: Real-world podcast dataset &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>-left.</p>\n\n",
                "matched_terms": [
                    "categories",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Podcast Topic</span>. Next, we established relevant topics for each category through a two-step process: (1) using GPT-4 <cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib1\" title=\"\">2023</a>)</cite> to generate 5 popular and distinct topics per category, reflecting current trends and listener interests; and (2) manually reviewing and refining these topics to ensure their uniqueness and relevance with real-world podcasts, selecting 3 representative topics for each category, resulting in a final collection of 51 topics (17 categories &#215; 3 topics).</p>\n\n",
                "matched_terms": [
                    "gpt4",
                    "categories"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dialogue content in podcasts is extracted in text format for evaluation, representing the core message the podcast aims to convey. Podcast dialogues often center around specific topics, showcasing participants&#8217; unique perspectives and insights, which makes reference-correlation-based methods infeasible. Instead, the richness of perspectives conveyed (to provide informative takeaways for the listener) and the presentation style of the dialogue (to enhance listener comprehension) should be the primary focus of evaluation. Therefore, we follow the dialogue script-based evaluation methods proposed in PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite>, which adopt a two-fold approach: (1) <span class=\"ltx_text ltx_font_bold\">Quantitative Metrics</span> such as Distinct-N, Semantic-Div, MATTR, and Info-Dens to assess lexical diversity, semantic richness, vocabulary richness, and information density, respectively. These metrics operate independently of reference texts and focus on intrinsic text characteristics; (2) <span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge</span>, leveraging GPT-4 to replace human evaluators for complex and comprehensive assessments. Evaluation criteria include coherence, engagingness, diversity, informativeness and speaker diversity. It incorporates comparative evaluations to reduce bias and evidence-based scoring for robust and reliable results.</p>\n\n",
                "matched_terms": [
                    "gpt4",
                    "podagent",
                    "llmasajudge",
                    "informativeness",
                    "diversity",
                    "speaker",
                    "coherence",
                    "specific",
                    "engagingness",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SIM</span> stands for Speaker Similarity. In podcast generation systems, zero-shot TTS is often employed to replicate the voice of a preset speaker. The SIM between the synthesized voice and the reference voice serves as a crucial metric about vocal fidelity. In PodEval, SIM is quantified using the cosine similarity of extracted speaker embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Plaquet &amp; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib31\" title=\"\">2023</a>; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib8\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "between",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SPTD</span> is a brand new metric we proposed, standing for Speaker Timbre Difference. As audio programs, podcasts are accessible only through listening. In multi-speaker conversations, voices with greater timbre differences enhance clarity and make the information easier to follow. SPTD is to assess the overall timbre variation across speakers. Equation <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S5.E1\" title=\"In 5 Speech-based evaluation &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> calculates the SPTD among <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I1.i4.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> distinct speakers.</p>\n\n",
                "matched_terms": [
                    "difference",
                    "speaker",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Objective metrics can be calculated efficiently at a low cost without human involvement. However, the <span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> remains a necessary indicator of human perception. Unlike general speech synthesis, which emphasizes sentence-level pronunciation accuracy and naturalness, podcast speech focuses on achieving human-like natural dialogue. Subjective tests for such long-form speech present several key <span class=\"ltx_text ltx_font_bold\">challenges</span>: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> the length of dialogue in podcasts ranges from a few minutes to over an hour, making it impractical to evaluate the entire speech directly; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> the difficulty of comparing more than two systems simultaneously; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> guiding user focus toward dialogue naturalness, rather than on factors like content; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">4)</span> balancing topic diversity within a fixed testing capacity; and <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">5)</span> ensuring that crowdsourced evaluators remain focused and provide reliable feedback.</p>\n\n",
                "matched_terms": [
                    "from",
                    "diversity",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In PodEval, we design the <span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation</span> based on the MUSHRA framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Schoeffler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib36\" title=\"\">2018</a>)</cite>. The key insight from this framework, <span class=\"ltx_text ltx_font_italic\">incorporating both high-quality and low-quality anchors</span>, helps evaluators establish a reliable reference of quality range. For researchers, analyzing scores for these anchors helps identify inattentive evaluators, enabling the <span class=\"ltx_text ltx_font_italic\">filtering of invalid submissions</span> and improving the data vadility. In our task, we use real podcast segments from the <span class=\"ltx_text ltx_font_italic\">Real-Pod dataset as the high-quality anchor</span> and synthesized dialogue segments from <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib16\" title=\"\">eSpeak Contributors </a></cite><span class=\"ltx_text ltx_font_italic\"> as the low-quality anchor</span>. For podcast samples from different systems, we provide an automatic toolkit to extract dialogue segments featuring <span class=\"ltx_text ltx_font_italic\">turn-taking</span> between speakers, representing a typical dialogue flow. Each dialogue segment is extracted with a <span class=\"ltx_text ltx_font_italic\">preset length</span> (e.g. 15&#8211;25 seconds) to ensure the speech samples are of similar duration. We select dialogue segments from <span class=\"ltx_text ltx_font_italic\">all 17 categories</span> in the Real-Pod dataset to ensure content diversity while keeping the total listening test duration <span class=\"ltx_text ltx_font_italic\">within 30 minutes</span>. In each test group, samples from different systems are presented <span class=\"ltx_text ltx_font_italic\">on the same page</span>, along with a <span class=\"ltx_text ltx_font_italic\">reference Real-Pod sample</span> to guide evaluators on what a natural dialogue sounds like. The scoring is adjusted using a slider ranging from 0 to 100, divided into <span class=\"ltx_text ltx_font_italic\">five stages with a clear definition</span>. Detailed instructions and website design can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS3\" title=\"A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The demo website is hosted at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml\" title=\"\">https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml</a>. Everyone is welcome to try it out and view the results at the end.</span></span></span></p>\n\n",
                "matched_terms": [
                    "diversity",
                    "between",
                    "categories",
                    "from",
                    "scores",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce the audio-based evaluation for podcasts, which treats speech as one component and assesses the overall audio performance, including speech, music and sound effects (MSE), and their interactions. Similarly, we first introduce the following <span class=\"ltx_text ltx_font_bold\">Objective metrics</span>:</p>\n\n",
                "matched_terms": [
                    "overall",
                    "metrics",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness</span>: Loudness ensures audio falls within an acceptable volume range. The ITU-R BS.1770-4 standard <cite class=\"ltx_cite ltx_citemacro_citep\">(BS Series, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib9\" title=\"\">2011</a>)</cite> is widely recognized for measuring audio loudness and true-peak levels. Based on this, the <cite class=\"ltx_cite ltx_citemacro_citep\">(EBU R128, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib15\" title=\"\">2011</a>)</cite> standard has been broadly adopted by broadcast and streaming platforms, recommend a target Integrated Loudness (LOUD-IT) of -23 LUFS (&#177;1 LUFS), True Peak (LOUD-TP) <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP and Loudness Range (LOUD-RA) <math alttext=\"&lt;20\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">&lt;20</annotation></semantics></math> LU. For podcast-like streaming, adjustments are made for typical listening environments, such as mobile devices where headphones are commonly used. In these cases, the LOUD-IT is recommended as -18, -16 (&#177;1) or -14 LUFS <cite class=\"ltx_cite ltx_citemacro_citep\">(AES, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib2\" title=\"\">2021</a>; Apple, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib4\" title=\"\">2023</a>; Spotify, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib38\" title=\"\">2023</a>)</cite>. Netflix recommends keeping LOUD-RA between 4 and 18 LU <cite class=\"ltx_cite ltx_citemacro_citep\">(Netflix, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib28\" title=\"\">2024</a>)</cite>. There is no &#8220;absolute right&#8221; reference for loudness metrics. We propose the following reference standards considering all above guidelines: <span class=\"ltx_text ltx_font_bold\">LOUD-IT:</span> <math alttext=\"-18\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>18</mn></mrow><annotation encoding=\"application/x-tex\">-18</annotation></semantics></math> to <math alttext=\"-14\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>14</mn></mrow><annotation encoding=\"application/x-tex\">-14</annotation></semantics></math> LUFS; <span class=\"ltx_text ltx_font_bold\">LOUD-TP:</span> <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP; <span class=\"ltx_text ltx_font_bold\">LOUD-RA:</span> 4 to 18 LU. Based on this &#8220;relatively correct&#8221; reference, we can analyze the distribution of loudness metrics across different systems. We also provide a quantitative scoring strategy in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS4\" title=\"A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "between",
                    "where",
                    "metrics",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SMR</span> (Speech-to-Music Ratio): MSE are typically integrated into podcast audio to enhance the overall listening experience. Since speech is the primary focus in podcasts, it is essential to ensure that MSE dose not overpower or mask the speech, maintaining clarity and intelligibility of the dialogue. SMR measures the balance between speech and MSE, with a minimum requirement of being greater than 0. SMR_SCORE is the proportion of cases where SMR exceeds 0.</p>\n\n",
                "matched_terms": [
                    "between",
                    "where",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> is primarily designed based on the perceptions of real users. A key challenge lies in how to evaluate extra-long audios. As we mentioned above, podcasts in the real world range from a few minutes to over an hour in length. Conducting listening tests on full-length podcast episodes is impractical due to the time, effort, and financial resources required. Moreover, it is hard to judge podcasts of vastly different lengths in a fair and consistent manner. Research on long-form audio evaluation is limited. <cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib13\" title=\"\">2019</a>)</cite> did investigation on long-form speech evaluation and found that multiple evaluations are necessary due to the low correlation observed across different experimental settings. <cite class=\"ltx_cite ltx_citemacro_cite\">Cambre et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib11\" title=\"\">2020</a>)</cite> conducted a comprehensive evaluation of voice selection for long-form content; however, the minimum required listening time was only 10 seconds. A podcast-related evaluation study <cite class=\"ltx_cite ltx_citemacro_citep\">(Austria, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib6\" title=\"\">2007</a>)</cite> designed a questionnaire with carefully crafted questions in terms of both content and presentation to assess domain-specific podcasts. Different from that, PodEval does not constrain the domain of podcasts, and open-ended content evaluation is separately conducted in the text-based evaluation section. In this audio-based evaluation, we focus on assessing the overall performance of the audios. The design approach is as follows:</p>\n\n",
                "matched_terms": [
                    "from",
                    "overall",
                    "performance",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The test data are preprocessed by extracting <span class=\"ltx_text ltx_font_bold\">the first / middle / final minute</span>. These segments are concatenated into a single audio, separated by a beep signal. This method unifies podcast length, captures overall performance from diverse positions, and minimizes content-related biases.</p>\n\n",
                "matched_terms": [
                    "from",
                    "overall",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text-based evaluation is conducted among GPT-4, PodAgent, MoonCast, and Real-Pod. Other systems in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Podcast generation &#8227; 2 Related work &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> are excluded as they do not provide conversation scripts. PodAgent, with its Host-Guest-Writer multi-agent system, can directly generate podcast scripts based on a given topic. While MoonCast functions similarly to NotebookLM, requiring external knowledge sources but providing prompt template for spontaneous script generation. For this evaluation, the MoonCast system uses the podcast scripts generated by PodAgent as input and transforms them into a spontaneous version.</p>\n\n",
                "matched_terms": [
                    "gpt4",
                    "podagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Metrics.</span> Detailed scores calculated across 17 podcast categories for each system are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS1\" title=\"A.2.1 Quantitative metrics &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.1</span></a>. For a concise and clearer comparison, we present the overall performance (averaged across all 17 categories) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F2\" title=\"Figure 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where we can observe that: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> For each quantitative metric, PodAgent outperforms directly prompting GPT-4; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> When comparing LLM-based methods (GPT-4, PodAgent) with human-created podcasts (Real-Pod), Real-Pod scores lower on lexical diversity (Distinct-2 and MATTR) but higher on information density and semantic diversity (Info-Dens and Sem-Div). This is reasonable for: i) real human interactions often include filler words and use simpler language; ii) most real podcasts are significantly longer (30 minutes to an hour), leading to higher information richness compared to generated podcasts, which are usually only a few minutes long; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> As a spontaneous version of PodAgent, MoonCast shows reduced lexical diversity and information density. While its semantic diversity remains comparable to PodAgent.</p>\n\n",
                "matched_terms": [
                    "gpt4",
                    "podagent",
                    "performance",
                    "diversity",
                    "comparison",
                    "categories",
                    "where",
                    "overall",
                    "scores",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure fairness, all open-source TTS systems use the same PodAgent-generated scripts. Subjective tests use a spontaneous version from MoonCast, while objective evaluations use the original PodAgent scripts, as filler words in the spontaneous version challenge metrics like WER.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "from",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SIM.</span> The SIM metric evaluates zero-shot TTS systems&#8217; ability to replicate the timbre of a reference voice. PodAgent, MoonCast, MuyanTTS, Dia, and MOSS-TTSD&#8212;are assessed as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(2). Each system uses the reference voice selected by PodAgent for the topic. The performance rankings are: MuyanTTS, MOSS-TTSD, Dia, MoonCast, and PodAgent. PodAgent&#8217;s relatively low score in this metric likely stems from its instruction-following style control strategy. While this approach enhances overall conversational expressiveness, it can reduce speaker similarity.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "performance",
                    "speaker",
                    "from",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SPTD.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(3) shows timbre variation across speakers in the conversation for three systems: Real-Pod, PodAgent, and NotebookLM. Real-Pod reflects real-world podcasts, PodAgent uses a voice selection mechanism for distinct voices, and NotebookLM fixed voices (one male, one female). The SPTD scores rank as follows: PodAgent, NotebookLM, and Real-Pod. This likely reflects that real-world podcasts prioritize guest expertise and availability over timbre differences. PodAgent demonstrates an effective automated voice selection process for podcast creation.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "podagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DNSMOS.</span> The DNSMOS metric was applied to all systems to evaluate speech quality as in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(4). PodAgent, MoonCast, MuyanTTS, MOSS-TTSD, and NotebookLM achieve similar scores, while Real-Pod and Dia show noticeable declines in speech quality. For Real-Pod, the lower scores are due to: (1) real podcasts often use MSE for enhancement, requiring speech-MSE separation before evaluation, which may leave residual MSE artifacts, and (2) human-created podcasts involve recording, editing, or post-processing that introduce noise or instability. Dia struggles with long-form speech synthesis. Its outputs for lengthy podcast scripts frequently feature overly fast speaking speeds and occasional sentence truncations, leading to its relatively low DNSMOS performance.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "podagent",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation.</span> We released the task on Prolific<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.prolific.com/\" title=\"\">https://www.prolific.com/</a></span></span></span>, requesting 20 native English-speaking participants from the US/UK. We set the filter rules as: 1) Over 90% of LQ samples must be marked as the worst, as the synthesized samples from eSpeak are obviously robotic and unnatural. 2) Over 50% of HQ samples must rank in the top-2 best. While it is possible for other systems to achieve a better score than the real podcast, the evaluation of the real podcast should also remain above average. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T3\" title=\"Table 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the two statistical metrics for the submission results. Based on these rules, Judger-7, 18 and 20 can be excluded. We also provide the box plot for each Judger in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F9\" title=\"Figure 9 &#8227; A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix for more advanced analysis. For instance, apart from the LQ samples, Judger-20 assigns similar scores to all other systems, further confirming the invalidity of this submission.</p>\n\n",
                "matched_terms": [
                    "from",
                    "scores",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Result Analysis.</span> After excluding unqualified submissions, we analyzed system performance based on the remaining 17 valid submissions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F4\" title=\"Figure 4 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the final results. We can observe that dialogue segments from real podcasts (HQ) achieved the highest scores, which aligns with expectations. NotebookLM, a closed-source product, ranked second, reflecting the high naturalness of its synthesized dialogue speech. Among the three open-source podcast generation systems, PodAgent scored the lowest, which is reasonable since its backend TTS system, CosyVoice2, is limited to single-sentence synthesis. In contrast, MoonCast and MOSS-TTSD, which support direct dialogue synthesis, performed better in dialogue naturalness evaluations. Overall, the evaluation results align with expectations, validating the rationality and effectiveness of our evaluation method design.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "performance",
                    "from",
                    "overall",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the density distribution of loudness-related metrics, enabling a comparative analysis with the reference range. All seven systems are included. For <span class=\"ltx_text ltx_font_bold\">LOUD_IT</span>, Dia and MoonCast align well with the reference range, while NotebookLM&#8217;s loudness centers around -25. Real-Pod, as manually produced audio, shows a highly scattered loudness distribution. For <span class=\"ltx_text ltx_font_bold\">LOUD_TP</span>, Muyan-TTS performs best, with all samples maintaining a true peak loudness below -1. In contrast, MoonCast, Dia and MOSS-TTSD perform poorly, while Real-Pod continues to exhibit scattered results. For <span class=\"ltx_text ltx_font_bold\">LOUD_RA</span>, MoonCast has a relatively narrow loudness variation range, while PodAgent and MOSS-TTSD display richer variance. Quantitative scores are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "scores",
                    "metrics",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SMR and CASP.</span> PodAgent and Real-Pod are evaluated for these MSE-related metrics. From the density distribution in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, PodAgent exhibits a more concentrated distribution compared to Real-Pod. For <span class=\"ltx_text ltx_font_bold\">SMR</span>, the SMR_SCORE in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows that all PodAgent samples achieve an SMR greater than 0, whereas some Real-Pod cases fail to meet this requirement. For <span class=\"ltx_text ltx_font_bold\">CASP</span>, a higher score indicates better MSE-Speech harmony. Real-Pod demonstrates a higher upper limit, which is expected as exceptional human artistic creations naturally surpass AI-generated outputs. However, PodAgent delivers more consistent performance, and the overall gap between the two systems is not significant, making it an alternative way to enhance creative efficiency.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "performance",
                    "between",
                    "from",
                    "overall",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to direct scores, we also derive a corresponding score based on users&#8217; justifications. Specifically, given justification texts from multiple systems for the same question, GPT-4 uses the following prompt to score: <span class=\"ltx_text ltx_font_italic\">&#8221;For each system, summarize the corresponding comments into one sentence and assign a score between 1 and 5.&#8221;</span> A detailed experiment setup is provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS5.SSS2\" title=\"A.5.2 Questionnaire-based MOS test &#8227; A.5 Audio-based evaluation (subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.5.2</span></a>, and separate scores are listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T10\" title=\"Table 10 &#8227; A.5.2 Questionnaire-based MOS test &#8227; A.5 Audio-based evaluation (subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F6\" title=\"Figure 6 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the final results, averaging the direct score and the justification-based score. From the result, we can observe that:</p>\n\n",
                "matched_terms": [
                    "gpt4",
                    "between",
                    "from",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Speech (naturalness and authenticity) is the most dominant factor affecting the listener&#8217;s experience</span>. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, PodAgent scored low in dialogue naturalness due to using a single-sentence synthesis TTS system, leading to consistently poor results in this MOS test. This outcome is expected, as dialogue speech is the core component of podcast-like audio programs. Although PodAgent&#8217;s Music/Sound (harmony) score is below Real-Pod (consistent with the results of objective metric - CASP), it is significantly higher than its scores in other metrics, indicating that <span class=\"ltx_text ltx_font_italic\">the gap between PodAgent and Real-Pod in music harmony is smaller than in speech naturalness</span>.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "between",
                    "scores",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Real podcasts perform best in most metrics (5/7).</span> Real-Pod significantly outperforms other systems on holistic metrics like Engagement Level (EL) and Human Likelihood (HL). However, Full Episode Willingness (FEW) scores are low across all systems, with NotebookLM and Real-Pod scoring similarly. <span class=\"ltx_text ltx_font_italic\">This highlights the value of perceptual and preference-based question design in the test.</span> FEW, a preference-based question, garnered justifications like &#8220;the topic is not of interest to me&#8221; for lower scores. In contrast, higher scores for EL and HL indicate that users tend to exclude subjective factors (e.g., personal topic interest) when rating audio performance. A similar pattern is observed in Information Delivery (effectiveness) and Speaker Expression (preference).</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "scores",
                    "metrics",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Audio Quality metric, while PodAgent and MOSS-TTSD score lower than Real-Pod, PodAgent performs better here than in other metrics, and NotebookLM slightly surpasses Real-Pod. As noted, human-made podcasts often exhibit inconsistent audio quality due to complex production. User feedback, like &#8220;Little bit of mic hiss/bloom but otherwise fine,&#8221; supports this observation. This highlights that <span class=\"ltx_text ltx_font_italic\">when conversational realism approaches that of real speech, AI-based methods offers an advantage in their controllability and consistency in producing high-quality audio</span>.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tools for <span class=\"ltx_text ltx_font_italic\">text-based evaluation</span> of dialogue scripts. Includes both <span class=\"ltx_text ltx_font_italic\">Quantitative Metrics</span> and <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> methods.</p>\n\n",
                "matched_terms": [
                    "llmasajudge",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Experiment Settings:</span> Lengthy listening tests can be exhausting and may lead to inaccurate feedback. It is essential to ensure the overall test duration does not exceed 30 minutes. In the Questionnaire-based MOS Test, each audio sample is around 3 minutes and requires answering 10 questions with corresponding justifications. Based on the Dialogue Naturalness Test results shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, we selected 4 representative systems. Each test group included four podcast samples from different systems but within the same podcast category. According to actual test results, each group took an average of 24 minutes to complete. The 4 representative systems are:</p>\n\n",
                "matched_terms": [
                    "from",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NotebookLM:</span> A pioneering podcast generation product, widely recognized for its exceptional performance, is nearly indistinguishable from real podcasts.</p>\n\n",
                "matched_terms": [
                    "from",
                    "performance"
                ]
            }
        ]
    },
    "S7.T3": {
        "source_file": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
        "caption": "Table 3: Dialogue Naturalness Evaluation - statistical information for filtering.",
        "body": "Judger\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nLQ Last (%)\n94.12\n100\n100\n100\n100\n100\n100\n100\n100\n100\n\n\nHQ Top2 (%)\n88.24\n88.24\n58.82\n58.82\n94.12\n64.71\n17.65\n58.82\n64.71\n94.12\n\n\nJudger\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n\n\nLQ Last (%)\n94.12\n100\n100\n100\n100\n100\n100\n76.47\n100\n100\n\n\nHQ Top2 (%)\n94.12\n82.35\n64.71\n82.35\n88.24\n58.82\n64.71\n47.06\n52.94\n35.29",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Judger</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"--ltx-bg-color:#FFB3B3;padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFB3B3;\">7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">LQ Last (%)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">94.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">HQ Top2 (%)</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">88.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">88.24</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">58.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">58.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">94.12</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">64.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFE6E6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFE6E6;\">17.65</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">58.82</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">64.71</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">94.12</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Judger</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFB3B3;padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFB3B3;\">18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFB3B3;padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFB3B3;\">20</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">LQ Last (%)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">94.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFE6E6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFE6E6;\">76.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">100</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">HQ Top2 (%)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">94.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">82.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">64.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">82.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">88.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">58.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">64.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#FFE6E6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFE6E6;\">47.06</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">52.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"--ltx-bg-color:#FFE6E6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFE6E6;\">35.29</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "information",
            "filtering",
            "evaluation",
            "last",
            "statistical",
            "judger",
            "naturalness",
            "top2",
            "dialogue"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation.</span> We released the task on Prolific<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.prolific.com/\" title=\"\">https://www.prolific.com/</a></span></span></span>, requesting 20 native English-speaking participants from the US/UK. We set the filter rules as: 1) Over 90% of LQ samples must be marked as the worst, as the synthesized samples from eSpeak are obviously robotic and unnatural. 2) Over 50% of HQ samples must rank in the top-2 best. While it is possible for other systems to achieve a better score than the real podcast, the evaluation of the real podcast should also remain above average. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T3\" title=\"Table 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the two statistical metrics for the submission results. Based on these rules, Judger-7, 18 and 20 can be excluded. We also provide the box plot for each Judger in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F9\" title=\"Figure 9 &#8227; A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix for more advanced analysis. For instance, apart from the LQ samples, Judger-20 assigns similar scores to all other systems, further confirming the invalidity of this submission.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">To address these challenges and establish a clear evaluation framework, we decompose podcast-like audio into three dimensions: <span class=\"ltx_text ltx_font_bold\">text</span> (conversation transcripts), <span class=\"ltx_text ltx_font_bold\">speech</span> (spoken dialogue), and <span class=\"ltx_text ltx_font_bold\">audio</span> (speech, music, sound effects, and their interaction). While these dimensions inherently overlap, they offers a structured framework for evaluation focus. Specifically, the conversation transcripts in podcasts are primarily used for <span class=\"ltx_text ltx_font_bold\">content</span> (the message being conveyed) evaluation, whereas speech, music and sound effects primarily contribute to <span class=\"ltx_text ltx_font_bold\">format</span> (how the message is presented) evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">General speech evaluation focuses on individual sentences, while podcasts require natural and interactive dialogue, emphasizing dialogue-level naturalness. Additionally, voice presentation in multi-speaker scenarios is critical to ensuring role distinction and overall listener engagement.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "naturalness",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each modality, we design tailored metrics to address diversity considerations. For text, we combine quantitative metrics with LLM-based evaluations to assess conversation scripts. For speech and audio, we design objective metrics and subjective listening tests to evaluate spoken dialogue and overall audio performance. All evaluation methods are organized into open-source tools for ease of use. Subjective tests are enhanced by spammer detection to improve data validity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dialogue content in podcasts is extracted in text format for evaluation, representing the core message the podcast aims to convey. Podcast dialogues often center around specific topics, showcasing participants&#8217; unique perspectives and insights, which makes reference-correlation-based methods infeasible. Instead, the richness of perspectives conveyed (to provide informative takeaways for the listener) and the presentation style of the dialogue (to enhance listener comprehension) should be the primary focus of evaluation. Therefore, we follow the dialogue script-based evaluation methods proposed in PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite>, which adopt a two-fold approach: (1) <span class=\"ltx_text ltx_font_bold\">Quantitative Metrics</span> such as Distinct-N, Semantic-Div, MATTR, and Info-Dens to assess lexical diversity, semantic richness, vocabulary richness, and information density, respectively. These metrics operate independently of reference texts and focus on intrinsic text characteristics; (2) <span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge</span>, leveraging GPT-4 to replace human evaluators for complex and comprehensive assessments. Evaluation criteria include coherence, engagingness, diversity, informativeness and speaker diversity. It incorporates comparative evaluations to reduce bias and evidence-based scoring for robust and reliable results.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "information",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Objective metrics can be calculated efficiently at a low cost without human involvement. However, the <span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> remains a necessary indicator of human perception. Unlike general speech synthesis, which emphasizes sentence-level pronunciation accuracy and naturalness, podcast speech focuses on achieving human-like natural dialogue. Subjective tests for such long-form speech present several key <span class=\"ltx_text ltx_font_bold\">challenges</span>: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> the length of dialogue in podcasts ranges from a few minutes to over an hour, making it impractical to evaluate the entire speech directly; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> the difficulty of comparing more than two systems simultaneously; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> guiding user focus toward dialogue naturalness, rather than on factors like content; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">4)</span> balancing topic diversity within a fixed testing capacity; and <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">5)</span> ensuring that crowdsourced evaluators remain focused and provide reliable feedback.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In PodEval, we design the <span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation</span> based on the MUSHRA framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Schoeffler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib36\" title=\"\">2018</a>)</cite>. The key insight from this framework, <span class=\"ltx_text ltx_font_italic\">incorporating both high-quality and low-quality anchors</span>, helps evaluators establish a reliable reference of quality range. For researchers, analyzing scores for these anchors helps identify inattentive evaluators, enabling the <span class=\"ltx_text ltx_font_italic\">filtering of invalid submissions</span> and improving the data vadility. In our task, we use real podcast segments from the <span class=\"ltx_text ltx_font_italic\">Real-Pod dataset as the high-quality anchor</span> and synthesized dialogue segments from <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib16\" title=\"\">eSpeak Contributors </a></cite><span class=\"ltx_text ltx_font_italic\"> as the low-quality anchor</span>. For podcast samples from different systems, we provide an automatic toolkit to extract dialogue segments featuring <span class=\"ltx_text ltx_font_italic\">turn-taking</span> between speakers, representing a typical dialogue flow. Each dialogue segment is extracted with a <span class=\"ltx_text ltx_font_italic\">preset length</span> (e.g. 15&#8211;25 seconds) to ensure the speech samples are of similar duration. We select dialogue segments from <span class=\"ltx_text ltx_font_italic\">all 17 categories</span> in the Real-Pod dataset to ensure content diversity while keeping the total listening test duration <span class=\"ltx_text ltx_font_italic\">within 30 minutes</span>. In each test group, samples from different systems are presented <span class=\"ltx_text ltx_font_italic\">on the same page</span>, along with a <span class=\"ltx_text ltx_font_italic\">reference Real-Pod sample</span> to guide evaluators on what a natural dialogue sounds like. The scoring is adjusted using a slider ranging from 0 to 100, divided into <span class=\"ltx_text ltx_font_italic\">five stages with a clear definition</span>. Detailed instructions and website design can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS3\" title=\"A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The demo website is hosted at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml\" title=\"\">https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml</a>. Everyone is welcome to try it out and view the results at the end.</span></span></span></p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "naturalness",
                    "filtering",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implement two strategies to enhance the validity of the collected data. <span class=\"ltx_text ltx_font_bold\">1) Attention-check questions:</span> These include questions like <span class=\"ltx_text ltx_font_italic\">Q1. How many speakers are there in the podcast?</span> and <span class=\"ltx_text ltx_font_italic\">Q7. If music or sound effects&#8230; (Select Neutral if none are present)</span>. These questions have standard answers, allowing us to determine whether users are actively listening to the audio. <span class=\"ltx_text ltx_font_bold\">2) Justification for answers:</span> Users have to provide justifications for their responses to each question, which can be short but are required. This requirement significantly increases users&#8217; focus and we can collect more detailed information from their justification. By employing these two strategies, data validity is enhanced by promoting attentiveness and filtering out unreliable responses.</p>\n\n",
                "matched_terms": [
                    "filtering",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Result Analysis.</span> After excluding unqualified submissions, we analyzed system performance based on the remaining 17 valid submissions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F4\" title=\"Figure 4 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the final results. We can observe that dialogue segments from real podcasts (HQ) achieved the highest scores, which aligns with expectations. NotebookLM, a closed-source product, ranked second, reflecting the high naturalness of its synthesized dialogue speech. Among the three open-source podcast generation systems, PodAgent scored the lowest, which is reasonable since its backend TTS system, CosyVoice2, is limited to single-sentence synthesis. In contrast, MoonCast and MOSS-TTSD, which support direct dialogue synthesis, performed better in dialogue naturalness evaluations. Overall, the evaluation results align with expectations, validating the rationality and effectiveness of our evaluation method design.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "naturalness",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Speech (naturalness and authenticity) is the most dominant factor affecting the listener&#8217;s experience</span>. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, PodAgent scored low in dialogue naturalness due to using a single-sentence synthesis TTS system, leading to consistently poor results in this MOS test. This outcome is expected, as dialogue speech is the core component of podcast-like audio programs. Although PodAgent&#8217;s Music/Sound (harmony) score is below Real-Pod (consistent with the results of objective metric - CASP), it is significantly higher than its scores in other metrics, indicating that <span class=\"ltx_text ltx_font_italic\">the gap between PodAgent and Real-Pod in music harmony is smaller than in speech naturalness</span>.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tools for <span class=\"ltx_text ltx_font_italic\">text-based evaluation</span> of dialogue scripts. Includes both <span class=\"ltx_text ltx_font_italic\">Quantitative Metrics</span> and <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> methods.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Framework for <span class=\"ltx_text ltx_font_italic\">subjective human evaluations</span> of podcast speech and audio. One is <span class=\"ltx_text ltx_font_italic\">Dialogue Naturalness Evaluation</span> and the other one is <span class=\"ltx_text ltx_font_italic\">Questionnaire-based MOS Test</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "naturalness",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Experiment Settings:</span> Lengthy listening tests can be exhausting and may lead to inaccurate feedback. It is essential to ensure the overall test duration does not exceed 30 minutes. In the Questionnaire-based MOS Test, each audio sample is around 3 minutes and requires answering 10 questions with corresponding justifications. Based on the Dialogue Naturalness Test results shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, we selected 4 representative systems. Each test group included four podcast samples from different systems but within the same podcast category. According to actual test results, each group took an average of 24 minutes to complete. The 4 representative systems are:</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "dialogue"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MOSS-TTSD:</span> Achieved the highest score among the open-source systems utilized in the Dialogue Naturalness Evaluation (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "naturalness",
                    "dialogue"
                ]
            }
        ]
    },
    "A1.T4": {
        "source_file": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
        "caption": "Table 4: GPT-4: Quantitative metrics in text-based evaluation.",
        "body": "Metrics\nOverall\nFiction\nEducation\nBusiness\nTrue Crime\nHealth & Fitness\n\n\n\n\nDistinct_2\n0.9619\n0.9643\n0.9588\n0.9567\n0.9689\n0.9638\n\n\nInfo-Dens\n6.4507\n6.5865\n6.4569\n6.3213\n6.6541\n6.3880\n\n\nSem-Div\n0.1293\n0.1204\n0.1115\n0.1214\n0.1443\n0.1106\n\n\nMATTR\n0.6914\n0.7027\n0.6989\n0.6933\n0.6831\n0.6870\n\n\nMetrics\nSports\nComedy\nHistory\nNews\nTV & Film\nSociety & Culture\n\n\nDistinct_2\n0.9536\n0.9633\n0.9471\n0.9486\n0.9678\n0.9659\n\n\nInfo-Dens\n6.4228\n6.2256\n6.3792\n6.3225\n6.7614\n6.6473\n\n\nSem-Div\n0.1248\n0.1356\n0.1451\n0.1208\n0.1553\n0.1507\n\n\nMATTR\n0.6973\n0.6922\n0.6905\n0.6756\n0.6903\n0.6901\n\n\nMetrics\nArts\nLeisure\nMusic\nKids\nMental Health\nScience & Tech\n\n\nDistinct_2\n0.9675\n0.9729\n0.9555\n0.9559\n0.9699\n0.9710\n\n\nInfo-Dens\n6.5054\n6.5233\n6.4119\n6.2310\n6.4787\n6.3454\n\n\nSem-Div\n0.1374\n0.1117\n0.1320\n0.1229\n0.1247\n0.1286\n\n\nMATTR\n0.6885\n0.7136\n0.6677\n0.6884\n0.6994\n0.6960",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"--ltx-bg-color:#FFFF00;padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFFF00;\">Overall</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Fiction</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Education</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Business</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">True Crime</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Health &amp; Fitness</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Distinct_2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.9619</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9643</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9588</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9567</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9689</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9638</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Info-Dens</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">6.4507</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.5865</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.4569</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.3213</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.6541</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.3880</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Sem-Div</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.1293</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1204</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1115</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1214</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1443</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1106</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">MATTR</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.6914</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.7027</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6989</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6933</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6831</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6870</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Sports</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Comedy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">History</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">News</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">TV &amp; Film</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Society &amp; Culture</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Distinct_2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9536</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9633</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9471</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9486</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9678</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9659</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Info-Dens</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.4228</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.2256</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.3792</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.3225</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.7614</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.6473</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Sem-Div</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1248</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1356</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1451</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1208</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1553</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1507</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">MATTR</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6973</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6922</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6905</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6756</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6903</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6901</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Arts</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Leisure</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Music</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Kids</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Mental Health</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Science &amp; Tech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Distinct_2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9675</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9729</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9555</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9559</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9699</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9710</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Info-Dens</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.5054</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.5233</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.4119</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.2310</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.4787</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.3454</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Sem-Div</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1374</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1117</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1320</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1229</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1247</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1286</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">MATTR</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6885</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.7136</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6677</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6884</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6994</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6960</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "arts",
            "true",
            "comedy",
            "business",
            "crime",
            "society",
            "kids",
            "distinct2",
            "tech",
            "semdiv",
            "fiction",
            "evaluation",
            "overall",
            "mental",
            "metrics",
            "infodens",
            "gpt4",
            "music",
            "sports",
            "film",
            "textbased",
            "culture",
            "fitness",
            "health",
            "education",
            "news",
            "quantitative",
            "mattr",
            "leisure",
            "science",
            "history"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recently, an increasing number of multimodal (text and audio) benchmarks have emerged, primarily focusing on evaluating models&#8217; understanding capability. However, exploration into assessing generative capabilities remains limited, especially for open-ended long-form content generation. Significant challenges lie in no reference standard answer, no unified evaluation metrics and uncontrollable human judgments. In this work, we take podcast-like audio generation as a starting point and propose PodEval, a comprehensive and well-designed open-source evaluation framework. In this framework: 1) We construct a real-world podcast dataset spanning diverse topics, serving as a reference for human-level creative quality. 2) We introduce a multimodal evaluation strategy and decompose the complex task into three dimensions: text, speech and audio, with different evaluation emphasis on &#8220;Content&#8221; and &#8220;Format&#8221;. 3) For each modality, we design corresponding evaluation methods, involving both objective metrics and subjective listening test. We leverage representative podcast generation systems (including open-source, close-source, and human-made) in our experiments. The results offer in-depth analysis and insights into podcast generation, demonstrating the effectiveness of PodEval in evaluating open-ended long-form audio. This project is open-source to facilitate public use: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/yujxx/PodEval\" title=\"\">https://github.com/yujxx/PodEval</a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid development of AIGC (AI-Generated Content) in recent years, many innovative applications have emerged. AI Podcast represents a key application scenario for audio-based generative models <cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib17\" title=\"\">2023</a>; ByteDance, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib10\" title=\"\">2025</a>)</cite>. However, evaluating podcast-like audio is challenging due to: 1) it is an open-ended task, which means there is no reference standard answer; 2) the evaluation of long-form speech/audio is particularly difficult, as longer formats introduce more variability. Objective metrics often fail to capture human perceptions accurately, while subjective listening tests face issues like user inattention, which reduces the validity of results; and 3) podcasts often incorporate additional elements, like music and sound effects, making the evaluation more complicated.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges and establish a clear evaluation framework, we decompose podcast-like audio into three dimensions: <span class=\"ltx_text ltx_font_bold\">text</span> (conversation transcripts), <span class=\"ltx_text ltx_font_bold\">speech</span> (spoken dialogue), and <span class=\"ltx_text ltx_font_bold\">audio</span> (speech, music, sound effects, and their interaction). While these dimensions inherently overlap, they offers a structured framework for evaluation focus. Specifically, the conversation transcripts in podcasts are primarily used for <span class=\"ltx_text ltx_font_bold\">content</span> (the message being conveyed) evaluation, whereas speech, music and sound effects primarily contribute to <span class=\"ltx_text ltx_font_bold\">format</span> (how the message is presented) evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Different modalities have their own commonly used evaluation methods. For text, metrics such as BLEU <cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib30\" title=\"\">2002</a>)</cite>, ROUGE <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib24\" title=\"\">2004</a>)</cite>, and METEOR <cite class=\"ltx_cite ltx_citemacro_citep\">(Banerjee &amp; Lavie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib7\" title=\"\">2005</a>)</cite> focus on fluency and relevance, while newer approaches like BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib46\" title=\"\">2019</a>)</cite> utilize pre-trained language models to capture semantic alignment. For speech, objective metrics like Mel Cepstral Distortion (MCD) and Perceptual Evaluation of Speech Quality (PESQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib34\" title=\"\">2001</a>)</cite> are widely used, alongside subjective evaluations like Mean Opinion Score (MOS) <cite class=\"ltx_cite ltx_citemacro_citep\">(Sector, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib37\" title=\"\">1996</a>)</cite>. For audio, metrics like Frechet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib22\" title=\"\">2018</a>)</cite> and Kullback-Leibler Divergence (KL) are employed to evaluate audio quality, while listener surveys provide subjective insights. However, these evaluation methods are not directly applicable to podcast evaluation since:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">General speech evaluation focuses on individual sentences, while podcasts require natural and interactive dialogue, emphasizing dialogue-level naturalness. Additionally, voice presentation in multi-speaker scenarios is critical to ensuring role distinction and overall listener engagement.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While music and sound effects are not essential to every podcast, their evaluation, when present, should go beyond the quality of individual audio events. Instead, it should focus on their overall harmony and seamless integration with the speech content to enhance the listener&#8217;s experience.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each modality, we design tailored metrics to address diversity considerations. For text, we combine quantitative metrics with LLM-based evaluations to assess conversation scripts. For speech and audio, we design objective metrics and subjective listening tests to evaluate spoken dialogue and overall audio performance. All evaluation methods are organized into open-source tools for ease of use. Subjective tests are enhanced by spammer detection to improve data validity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "quantitative",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Various evaluation works have emerged along with the development of LLMs and multimodal generative models. <span class=\"ltx_text ltx_font_bold\">Text-related Evaluation</span>, such as SuperGLUE, MMLU, and BIG-bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib41\" title=\"\">2019</a>; Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib18\" title=\"\">2020</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib39\" title=\"\">2022</a>)</cite>, assesses the capabilities of LLMs across diverse tasks with preset ground truth. Subsequently, MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib47\" title=\"\">2023</a>)</cite> explores the potential of LLMs as evaluators, and Chatbot Arena <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib12\" title=\"\">2024</a>)</cite> provides an open platform for assessing LLMs based on human preferences. <span class=\"ltx_text ltx_font_bold\">Speech-related Evaluation</span>, such as SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib45\" title=\"\">2021</a>)</cite>, is designed for discriminative tasks like speech recognition and speaker identification. However, evaluations for generative tasks are scarce due to their inherent diversity and subjectivity, making subjective evaluation essential for speech generation tasks. For instance, VOCBENCH <cite class=\"ltx_cite ltx_citemacro_citep\">(AlBadawy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib3\" title=\"\">2022</a>)</cite> incorporates both subjective and objective evaluations to assess vocoder performance in speech synthesis. Similarly, numerous <span class=\"ltx_text ltx_font_bold\">Audio-related Evaluation</span> work, such as AIR-Bench, Audiobench, MMAU, and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib44\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib42\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib35\" title=\"\">2024</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib26\" title=\"\">2025</a>)</cite>, focus on audio understanding and reasoning. Subjective evaluation remains crucial for assessing audio generation systems and is typically tailored to specific generation tasks. Unlike existing evaluation works, <span class=\"ltx_text ltx_font_bold\">PodEval</span> introduces a comprehensive framework specifically designed for podcast-like audio generation. It emphasizes both subjective and objective evaluations across text, speech, and audio, with all metrics closely aligned with real-world user experience.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dialogue content in podcasts is extracted in text format for evaluation, representing the core message the podcast aims to convey. Podcast dialogues often center around specific topics, showcasing participants&#8217; unique perspectives and insights, which makes reference-correlation-based methods infeasible. Instead, the richness of perspectives conveyed (to provide informative takeaways for the listener) and the presentation style of the dialogue (to enhance listener comprehension) should be the primary focus of evaluation. Therefore, we follow the dialogue script-based evaluation methods proposed in PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite>, which adopt a two-fold approach: (1) <span class=\"ltx_text ltx_font_bold\">Quantitative Metrics</span> such as Distinct-N, Semantic-Div, MATTR, and Info-Dens to assess lexical diversity, semantic richness, vocabulary richness, and information density, respectively. These metrics operate independently of reference texts and focus on intrinsic text characteristics; (2) <span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge</span>, leveraging GPT-4 to replace human evaluators for complex and comprehensive assessments. Evaluation criteria include coherence, engagingness, diversity, informativeness and speaker diversity. It incorporates comparative evaluations to reduce bias and evidence-based scoring for robust and reliable results.</p>\n\n",
                "matched_terms": [
                    "gpt4",
                    "evaluation",
                    "quantitative",
                    "mattr",
                    "metrics",
                    "infodens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech is the core component of a podcast, serving as the medium for content delivery, and how the message delivered plays a crucial role in shaping the listening experience. To ensure a multidimensional evaluation, we first integrate the following <span class=\"ltx_text ltx_font_bold\">Objective Metrics</span>:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce the audio-based evaluation for podcasts, which treats speech as one component and assesses the overall audio performance, including speech, music and sound effects (MSE), and their interactions. Similarly, we first introduce the following <span class=\"ltx_text ltx_font_bold\">Objective metrics</span>:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness</span>: Loudness ensures audio falls within an acceptable volume range. The ITU-R BS.1770-4 standard <cite class=\"ltx_cite ltx_citemacro_citep\">(BS Series, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib9\" title=\"\">2011</a>)</cite> is widely recognized for measuring audio loudness and true-peak levels. Based on this, the <cite class=\"ltx_cite ltx_citemacro_citep\">(EBU R128, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib15\" title=\"\">2011</a>)</cite> standard has been broadly adopted by broadcast and streaming platforms, recommend a target Integrated Loudness (LOUD-IT) of -23 LUFS (&#177;1 LUFS), True Peak (LOUD-TP) <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP and Loudness Range (LOUD-RA) <math alttext=\"&lt;20\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">&lt;20</annotation></semantics></math> LU. For podcast-like streaming, adjustments are made for typical listening environments, such as mobile devices where headphones are commonly used. In these cases, the LOUD-IT is recommended as -18, -16 (&#177;1) or -14 LUFS <cite class=\"ltx_cite ltx_citemacro_citep\">(AES, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib2\" title=\"\">2021</a>; Apple, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib4\" title=\"\">2023</a>; Spotify, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib38\" title=\"\">2023</a>)</cite>. Netflix recommends keeping LOUD-RA between 4 and 18 LU <cite class=\"ltx_cite ltx_citemacro_citep\">(Netflix, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib28\" title=\"\">2024</a>)</cite>. There is no &#8220;absolute right&#8221; reference for loudness metrics. We propose the following reference standards considering all above guidelines: <span class=\"ltx_text ltx_font_bold\">LOUD-IT:</span> <math alttext=\"-18\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>18</mn></mrow><annotation encoding=\"application/x-tex\">-18</annotation></semantics></math> to <math alttext=\"-14\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>14</mn></mrow><annotation encoding=\"application/x-tex\">-14</annotation></semantics></math> LUFS; <span class=\"ltx_text ltx_font_bold\">LOUD-TP:</span> <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP; <span class=\"ltx_text ltx_font_bold\">LOUD-RA:</span> 4 to 18 LU. Based on this &#8220;relatively correct&#8221; reference, we can analyze the distribution of loudness metrics across different systems. We also provide a quantitative scoring strategy in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS4\" title=\"A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "quantitative",
                    "true",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> is primarily designed based on the perceptions of real users. A key challenge lies in how to evaluate extra-long audios. As we mentioned above, podcasts in the real world range from a few minutes to over an hour in length. Conducting listening tests on full-length podcast episodes is impractical due to the time, effort, and financial resources required. Moreover, it is hard to judge podcasts of vastly different lengths in a fair and consistent manner. Research on long-form audio evaluation is limited. <cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib13\" title=\"\">2019</a>)</cite> did investigation on long-form speech evaluation and found that multiple evaluations are necessary due to the low correlation observed across different experimental settings. <cite class=\"ltx_cite ltx_citemacro_cite\">Cambre et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib11\" title=\"\">2020</a>)</cite> conducted a comprehensive evaluation of voice selection for long-form content; however, the minimum required listening time was only 10 seconds. A podcast-related evaluation study <cite class=\"ltx_cite ltx_citemacro_citep\">(Austria, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib6\" title=\"\">2007</a>)</cite> designed a questionnaire with carefully crafted questions in terms of both content and presentation to assess domain-specific podcasts. Different from that, PodEval does not constrain the domain of podcasts, and open-ended content evaluation is separately conducted in the text-based evaluation section. In this audio-based evaluation, we focus on assessing the overall performance of the audios. The design approach is as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text-based evaluation is conducted among GPT-4, PodAgent, MoonCast, and Real-Pod. Other systems in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Podcast generation &#8227; 2 Related work &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> are excluded as they do not provide conversation scripts. PodAgent, with its Host-Guest-Writer multi-agent system, can directly generate podcast scripts based on a given topic. While MoonCast functions similarly to NotebookLM, requiring external knowledge sources but providing prompt template for spontaneous script generation. For this evaluation, the MoonCast system uses the podcast scripts generated by PodAgent as input and transforms them into a spontaneous version.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "textbased",
                    "gpt4"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Metrics.</span> Detailed scores calculated across 17 podcast categories for each system are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS1\" title=\"A.2.1 Quantitative metrics &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.1</span></a>. For a concise and clearer comparison, we present the overall performance (averaged across all 17 categories) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F2\" title=\"Figure 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where we can observe that: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> For each quantitative metric, PodAgent outperforms directly prompting GPT-4; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> When comparing LLM-based methods (GPT-4, PodAgent) with human-created podcasts (Real-Pod), Real-Pod scores lower on lexical diversity (Distinct-2 and MATTR) but higher on information density and semantic diversity (Info-Dens and Sem-Div). This is reasonable for: i) real human interactions often include filler words and use simpler language; ii) most real podcasts are significantly longer (30 minutes to an hour), leading to higher information richness compared to generated podcasts, which are usually only a few minutes long; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> As a spontaneous version of PodAgent, MoonCast shows reduced lexical diversity and information density. While its semantic diversity remains comparable to PodAgent.</p>\n\n",
                "matched_terms": [
                    "gpt4",
                    "overall",
                    "quantitative",
                    "mattr",
                    "metrics",
                    "distinct2",
                    "semdiv",
                    "infodens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> This evaluation compares PodAgent (scored from -3 to 3) with GPT-4 (reference score as 0), both of which generate conversation scripts without external knowledge resources. Detailed scores for each category are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS2\" title=\"A.2.2 LLM-as-a-Judge &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.2</span></a>. We present the overall performance and results for five specific categories in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T2\" title=\"Table 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for analysis. We can see that scores across all metrics and all categories are positive, demonstrating that PodAgent significantly outperforms directly prompting GPT-4 in generating podcast scripts across all evaluated dimensions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "metrics",
                    "gpt4"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation.</span> We released the task on Prolific<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.prolific.com/\" title=\"\">https://www.prolific.com/</a></span></span></span>, requesting 20 native English-speaking participants from the US/UK. We set the filter rules as: 1) Over 90% of LQ samples must be marked as the worst, as the synthesized samples from eSpeak are obviously robotic and unnatural. 2) Over 50% of HQ samples must rank in the top-2 best. While it is possible for other systems to achieve a better score than the real podcast, the evaluation of the real podcast should also remain above average. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T3\" title=\"Table 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the two statistical metrics for the submission results. Based on these rules, Judger-7, 18 and 20 can be excluded. We also provide the box plot for each Judger in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F9\" title=\"Figure 9 &#8227; A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix for more advanced analysis. For instance, apart from the LQ samples, Judger-20 assigns similar scores to all other systems, further confirming the invalidity of this submission.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Result Analysis.</span> After excluding unqualified submissions, we analyzed system performance based on the remaining 17 valid submissions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F4\" title=\"Figure 4 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the final results. We can observe that dialogue segments from real podcasts (HQ) achieved the highest scores, which aligns with expectations. NotebookLM, a closed-source product, ranked second, reflecting the high naturalness of its synthesized dialogue speech. Among the three open-source podcast generation systems, PodAgent scored the lowest, which is reasonable since its backend TTS system, CosyVoice2, is limited to single-sentence synthesis. In contrast, MoonCast and MOSS-TTSD, which support direct dialogue synthesis, performed better in dialogue naturalness evaluations. Overall, the evaluation results align with expectations, validating the rationality and effectiveness of our evaluation method design.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the density distribution of loudness-related metrics, enabling a comparative analysis with the reference range. All seven systems are included. For <span class=\"ltx_text ltx_font_bold\">LOUD_IT</span>, Dia and MoonCast align well with the reference range, while NotebookLM&#8217;s loudness centers around -25. Real-Pod, as manually produced audio, shows a highly scattered loudness distribution. For <span class=\"ltx_text ltx_font_bold\">LOUD_TP</span>, Muyan-TTS performs best, with all samples maintaining a true peak loudness below -1. In contrast, MoonCast, Dia and MOSS-TTSD perform poorly, while Real-Pod continues to exhibit scattered results. For <span class=\"ltx_text ltx_font_bold\">LOUD_RA</span>, MoonCast has a relatively narrow loudness variation range, while PodAgent and MOSS-TTSD display richer variance. Quantitative scores are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "quantitative",
                    "true",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SMR and CASP.</span> PodAgent and Real-Pod are evaluated for these MSE-related metrics. From the density distribution in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, PodAgent exhibits a more concentrated distribution compared to Real-Pod. For <span class=\"ltx_text ltx_font_bold\">SMR</span>, the SMR_SCORE in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows that all PodAgent samples achieve an SMR greater than 0, whereas some Real-Pod cases fail to meet this requirement. For <span class=\"ltx_text ltx_font_bold\">CASP</span>, a higher score indicates better MSE-Speech harmony. Real-Pod demonstrates a higher upper limit, which is expected as exceptional human artistic creations naturally surpass AI-generated outputs. However, PodAgent delivers more consistent performance, and the overall gap between the two systems is not significant, making it an alternative way to enhance creative efficiency.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Speech (naturalness and authenticity) is the most dominant factor affecting the listener&#8217;s experience</span>. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, PodAgent scored low in dialogue naturalness due to using a single-sentence synthesis TTS system, leading to consistently poor results in this MOS test. This outcome is expected, as dialogue speech is the core component of podcast-like audio programs. Although PodAgent&#8217;s Music/Sound (harmony) score is below Real-Pod (consistent with the results of objective metric - CASP), it is significantly higher than its scores in other metrics, indicating that <span class=\"ltx_text ltx_font_italic\">the gap between PodAgent and Real-Pod in music harmony is smaller than in speech naturalness</span>.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PodEval is the first comprehensive evaluation framework for podcast-like audio generation, tackling the challenges of assessing open-ended, long-form content. We constructed a real-world podcast dataset as a benchmark for human-level creative quality across diverse topics and formats. By decomposing evaluation into text, speech, and audio, PodEval introduced multidimensional methods combining objective metrics and well-designed subjective listening tests. Experiments with various podcast generation systems, including open-source, closed-source, and human-made examples, validated the framework&#8217;s effectiveness. The results offer insights into the strengths and weaknesses of different systems (e.g. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F14\" title=\"Figure 14 &#8227; A.6 System analysis report &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>), highlighting PodEval&#8217;s role in advancing podcast generation research and inspiring future work on evaluating open-ended, long-form content generation task.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tools for <span class=\"ltx_text ltx_font_italic\">text-based evaluation</span> of dialogue scripts. Includes both <span class=\"ltx_text ltx_font_italic\">Quantitative Metrics</span> and <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> methods.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quantitative",
                    "metrics",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large Language Models (LLMs) utilized in this work are as follows:(1) <span class=\"ltx_text ltx_font_italic\">Topics Initiation</span> during data processing of the Real-Pod dataset, which is elaborated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S3\" title=\"3 Real-Pod: Real-world podcast dataset &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. (2) <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> method in text-based evaluation, which is illustrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S4\" title=\"4 Text-based evaluation &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. (3) <span class=\"ltx_text ltx_font_italic\">Summarized Users&#8217; Justifications</span> in the Questionnaire-based MOS Test, which is described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS3\" title=\"7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.3</span></a> (Questionnaire-based MOS Test).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "textbased"
                ]
            }
        ]
    },
    "A1.T5": {
        "source_file": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
        "caption": "Table 5: PodAgent: Quantitative metrics in text-based evaluation.",
        "body": "Metrics\nOverall\nFiction\nEducation\nBusiness\nTrue Crime\nHealth & Fitness\n\n\n\n\nDistinct_2\n0.9741\n0.9743\n0.9730\n0.9758\n0.9796\n0.9825\n\n\nInfo-Dens\n7.1767\n7.3791\n7.2163\n7.1126\n7.1810\n7.2927\n\n\nSem-Div\n0.1372\n0.1384\n0.1210\n0.1254\n0.1514\n0.1171\n\n\nMATTR\n0.7216\n0.7399\n0.7291\n0.7258\n0.7263\n0.7386\n\n\nMetrics\nSports\nComedy\nHistory\nNews\nTV & Film\nSociety & Culture\n\n\nDistinct_2\n0.9678\n0.9808\n0.9483\n0.9735\n0.9782\n0.9744\n\n\nInfo-Dens\n7.1239\n7.1600\n7.1004\n7.1282\n7.3311\n6.9568\n\n\nSem-Div\n0.1487\n0.1236\n0.1543\n0.1379\n0.1690\n0.1344\n\n\nMATTR\n0.7183\n0.7248\n0.6752\n0.7156\n0.7274\n0.7119\n\n\nMetrics\nArts\nLeisure\nMusic\nKids\nMental Health\nScience & Tech\n\n\nDistinct_2\n0.9701\n0.9790\n0.9739\n0.9747\n0.9815\n0.9725\n\n\nInfo-Dens\n7.1977\n7.3227\n7.0558\n7.0930\n7.1822\n7.1711\n\n\nSem-Div\n0.1283\n0.1445\n0.1440\n0.1353\n0.1259\n0.1331\n\n\nMATTR\n0.7101\n0.7275\n0.7114\n0.7279\n0.7328\n0.7249",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"--ltx-bg-color:#FFFF00;padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFFF00;\">Overall</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Fiction</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Education</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Business</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">True Crime</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Health &amp; Fitness</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Distinct_2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.9741</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9743</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9730</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9758</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9796</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9825</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Info-Dens</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">7.1767</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.3791</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.2163</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.1126</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.1810</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.2927</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Sem-Div</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.1372</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1384</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1210</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1254</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1514</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1171</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">MATTR</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.7216</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.7399</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.7291</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.7258</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.7263</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.7386</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Sports</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Comedy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">History</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">News</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">TV &amp; Film</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Society &amp; Culture</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Distinct_2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9678</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9808</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9483</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9735</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9782</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9744</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Info-Dens</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.1239</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.1600</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.1004</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.1282</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.3311</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.9568</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Sem-Div</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1487</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1236</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1543</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1379</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1690</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1344</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">MATTR</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.7183</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.7248</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6752</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.7156</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.7274</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.7119</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Arts</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Leisure</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Music</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Kids</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Mental Health</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Science &amp; Tech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Distinct_2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9701</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9790</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9739</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9747</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9815</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9725</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Info-Dens</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.1977</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.3227</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.0558</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.0930</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.1822</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.1711</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Sem-Div</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1283</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1445</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1440</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1353</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1259</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1331</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">MATTR</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.7101</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.7275</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.7114</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.7279</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.7328</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.7249</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "arts",
            "true",
            "comedy",
            "business",
            "crime",
            "society",
            "kids",
            "distinct2",
            "tech",
            "semdiv",
            "fiction",
            "evaluation",
            "overall",
            "mental",
            "metrics",
            "infodens",
            "music",
            "sports",
            "film",
            "podagent",
            "textbased",
            "culture",
            "fitness",
            "health",
            "education",
            "news",
            "quantitative",
            "mattr",
            "leisure",
            "science",
            "history"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recently, an increasing number of multimodal (text and audio) benchmarks have emerged, primarily focusing on evaluating models&#8217; understanding capability. However, exploration into assessing generative capabilities remains limited, especially for open-ended long-form content generation. Significant challenges lie in no reference standard answer, no unified evaluation metrics and uncontrollable human judgments. In this work, we take podcast-like audio generation as a starting point and propose PodEval, a comprehensive and well-designed open-source evaluation framework. In this framework: 1) We construct a real-world podcast dataset spanning diverse topics, serving as a reference for human-level creative quality. 2) We introduce a multimodal evaluation strategy and decompose the complex task into three dimensions: text, speech and audio, with different evaluation emphasis on &#8220;Content&#8221; and &#8220;Format&#8221;. 3) For each modality, we design corresponding evaluation methods, involving both objective metrics and subjective listening test. We leverage representative podcast generation systems (including open-source, close-source, and human-made) in our experiments. The results offer in-depth analysis and insights into podcast generation, demonstrating the effectiveness of PodEval in evaluating open-ended long-form audio. This project is open-source to facilitate public use: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/yujxx/PodEval\" title=\"\">https://github.com/yujxx/PodEval</a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid development of AIGC (AI-Generated Content) in recent years, many innovative applications have emerged. AI Podcast represents a key application scenario for audio-based generative models <cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib17\" title=\"\">2023</a>; ByteDance, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib10\" title=\"\">2025</a>)</cite>. However, evaluating podcast-like audio is challenging due to: 1) it is an open-ended task, which means there is no reference standard answer; 2) the evaluation of long-form speech/audio is particularly difficult, as longer formats introduce more variability. Objective metrics often fail to capture human perceptions accurately, while subjective listening tests face issues like user inattention, which reduces the validity of results; and 3) podcasts often incorporate additional elements, like music and sound effects, making the evaluation more complicated.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges and establish a clear evaluation framework, we decompose podcast-like audio into three dimensions: <span class=\"ltx_text ltx_font_bold\">text</span> (conversation transcripts), <span class=\"ltx_text ltx_font_bold\">speech</span> (spoken dialogue), and <span class=\"ltx_text ltx_font_bold\">audio</span> (speech, music, sound effects, and their interaction). While these dimensions inherently overlap, they offers a structured framework for evaluation focus. Specifically, the conversation transcripts in podcasts are primarily used for <span class=\"ltx_text ltx_font_bold\">content</span> (the message being conveyed) evaluation, whereas speech, music and sound effects primarily contribute to <span class=\"ltx_text ltx_font_bold\">format</span> (how the message is presented) evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Different modalities have their own commonly used evaluation methods. For text, metrics such as BLEU <cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib30\" title=\"\">2002</a>)</cite>, ROUGE <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib24\" title=\"\">2004</a>)</cite>, and METEOR <cite class=\"ltx_cite ltx_citemacro_citep\">(Banerjee &amp; Lavie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib7\" title=\"\">2005</a>)</cite> focus on fluency and relevance, while newer approaches like BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib46\" title=\"\">2019</a>)</cite> utilize pre-trained language models to capture semantic alignment. For speech, objective metrics like Mel Cepstral Distortion (MCD) and Perceptual Evaluation of Speech Quality (PESQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib34\" title=\"\">2001</a>)</cite> are widely used, alongside subjective evaluations like Mean Opinion Score (MOS) <cite class=\"ltx_cite ltx_citemacro_citep\">(Sector, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib37\" title=\"\">1996</a>)</cite>. For audio, metrics like Frechet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib22\" title=\"\">2018</a>)</cite> and Kullback-Leibler Divergence (KL) are employed to evaluate audio quality, while listener surveys provide subjective insights. However, these evaluation methods are not directly applicable to podcast evaluation since:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">General speech evaluation focuses on individual sentences, while podcasts require natural and interactive dialogue, emphasizing dialogue-level naturalness. Additionally, voice presentation in multi-speaker scenarios is critical to ensuring role distinction and overall listener engagement.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While music and sound effects are not essential to every podcast, their evaluation, when present, should go beyond the quality of individual audio events. Instead, it should focus on their overall harmony and seamless integration with the speech content to enhance the listener&#8217;s experience.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each modality, we design tailored metrics to address diversity considerations. For text, we combine quantitative metrics with LLM-based evaluations to assess conversation scripts. For speech and audio, we design objective metrics and subjective listening tests to evaluate spoken dialogue and overall audio performance. All evaluation methods are organized into open-source tools for ease of use. Subjective tests are enhanced by spammer detection to improve data validity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "quantitative",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Various evaluation works have emerged along with the development of LLMs and multimodal generative models. <span class=\"ltx_text ltx_font_bold\">Text-related Evaluation</span>, such as SuperGLUE, MMLU, and BIG-bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib41\" title=\"\">2019</a>; Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib18\" title=\"\">2020</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib39\" title=\"\">2022</a>)</cite>, assesses the capabilities of LLMs across diverse tasks with preset ground truth. Subsequently, MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib47\" title=\"\">2023</a>)</cite> explores the potential of LLMs as evaluators, and Chatbot Arena <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib12\" title=\"\">2024</a>)</cite> provides an open platform for assessing LLMs based on human preferences. <span class=\"ltx_text ltx_font_bold\">Speech-related Evaluation</span>, such as SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib45\" title=\"\">2021</a>)</cite>, is designed for discriminative tasks like speech recognition and speaker identification. However, evaluations for generative tasks are scarce due to their inherent diversity and subjectivity, making subjective evaluation essential for speech generation tasks. For instance, VOCBENCH <cite class=\"ltx_cite ltx_citemacro_citep\">(AlBadawy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib3\" title=\"\">2022</a>)</cite> incorporates both subjective and objective evaluations to assess vocoder performance in speech synthesis. Similarly, numerous <span class=\"ltx_text ltx_font_bold\">Audio-related Evaluation</span> work, such as AIR-Bench, Audiobench, MMAU, and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib44\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib42\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib35\" title=\"\">2024</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib26\" title=\"\">2025</a>)</cite>, focus on audio understanding and reasoning. Subjective evaluation remains crucial for assessing audio generation systems and is typically tailored to specific generation tasks. Unlike existing evaluation works, <span class=\"ltx_text ltx_font_bold\">PodEval</span> introduces a comprehensive framework specifically designed for podcast-like audio generation. It emphasizes both subjective and objective evaluations across text, speech, and audio, with all metrics closely aligned with real-world user experience.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dialogue content in podcasts is extracted in text format for evaluation, representing the core message the podcast aims to convey. Podcast dialogues often center around specific topics, showcasing participants&#8217; unique perspectives and insights, which makes reference-correlation-based methods infeasible. Instead, the richness of perspectives conveyed (to provide informative takeaways for the listener) and the presentation style of the dialogue (to enhance listener comprehension) should be the primary focus of evaluation. Therefore, we follow the dialogue script-based evaluation methods proposed in PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite>, which adopt a two-fold approach: (1) <span class=\"ltx_text ltx_font_bold\">Quantitative Metrics</span> such as Distinct-N, Semantic-Div, MATTR, and Info-Dens to assess lexical diversity, semantic richness, vocabulary richness, and information density, respectively. These metrics operate independently of reference texts and focus on intrinsic text characteristics; (2) <span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge</span>, leveraging GPT-4 to replace human evaluators for complex and comprehensive assessments. Evaluation criteria include coherence, engagingness, diversity, informativeness and speaker diversity. It incorporates comparative evaluations to reduce bias and evidence-based scoring for robust and reliable results.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "evaluation",
                    "quantitative",
                    "mattr",
                    "metrics",
                    "infodens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech is the core component of a podcast, serving as the medium for content delivery, and how the message delivered plays a crucial role in shaping the listening experience. To ensure a multidimensional evaluation, we first integrate the following <span class=\"ltx_text ltx_font_bold\">Objective Metrics</span>:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce the audio-based evaluation for podcasts, which treats speech as one component and assesses the overall audio performance, including speech, music and sound effects (MSE), and their interactions. Similarly, we first introduce the following <span class=\"ltx_text ltx_font_bold\">Objective metrics</span>:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness</span>: Loudness ensures audio falls within an acceptable volume range. The ITU-R BS.1770-4 standard <cite class=\"ltx_cite ltx_citemacro_citep\">(BS Series, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib9\" title=\"\">2011</a>)</cite> is widely recognized for measuring audio loudness and true-peak levels. Based on this, the <cite class=\"ltx_cite ltx_citemacro_citep\">(EBU R128, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib15\" title=\"\">2011</a>)</cite> standard has been broadly adopted by broadcast and streaming platforms, recommend a target Integrated Loudness (LOUD-IT) of -23 LUFS (&#177;1 LUFS), True Peak (LOUD-TP) <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP and Loudness Range (LOUD-RA) <math alttext=\"&lt;20\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">&lt;20</annotation></semantics></math> LU. For podcast-like streaming, adjustments are made for typical listening environments, such as mobile devices where headphones are commonly used. In these cases, the LOUD-IT is recommended as -18, -16 (&#177;1) or -14 LUFS <cite class=\"ltx_cite ltx_citemacro_citep\">(AES, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib2\" title=\"\">2021</a>; Apple, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib4\" title=\"\">2023</a>; Spotify, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib38\" title=\"\">2023</a>)</cite>. Netflix recommends keeping LOUD-RA between 4 and 18 LU <cite class=\"ltx_cite ltx_citemacro_citep\">(Netflix, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib28\" title=\"\">2024</a>)</cite>. There is no &#8220;absolute right&#8221; reference for loudness metrics. We propose the following reference standards considering all above guidelines: <span class=\"ltx_text ltx_font_bold\">LOUD-IT:</span> <math alttext=\"-18\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>18</mn></mrow><annotation encoding=\"application/x-tex\">-18</annotation></semantics></math> to <math alttext=\"-14\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>14</mn></mrow><annotation encoding=\"application/x-tex\">-14</annotation></semantics></math> LUFS; <span class=\"ltx_text ltx_font_bold\">LOUD-TP:</span> <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP; <span class=\"ltx_text ltx_font_bold\">LOUD-RA:</span> 4 to 18 LU. Based on this &#8220;relatively correct&#8221; reference, we can analyze the distribution of loudness metrics across different systems. We also provide a quantitative scoring strategy in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS4\" title=\"A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "quantitative",
                    "true",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> is primarily designed based on the perceptions of real users. A key challenge lies in how to evaluate extra-long audios. As we mentioned above, podcasts in the real world range from a few minutes to over an hour in length. Conducting listening tests on full-length podcast episodes is impractical due to the time, effort, and financial resources required. Moreover, it is hard to judge podcasts of vastly different lengths in a fair and consistent manner. Research on long-form audio evaluation is limited. <cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib13\" title=\"\">2019</a>)</cite> did investigation on long-form speech evaluation and found that multiple evaluations are necessary due to the low correlation observed across different experimental settings. <cite class=\"ltx_cite ltx_citemacro_cite\">Cambre et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib11\" title=\"\">2020</a>)</cite> conducted a comprehensive evaluation of voice selection for long-form content; however, the minimum required listening time was only 10 seconds. A podcast-related evaluation study <cite class=\"ltx_cite ltx_citemacro_citep\">(Austria, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib6\" title=\"\">2007</a>)</cite> designed a questionnaire with carefully crafted questions in terms of both content and presentation to assess domain-specific podcasts. Different from that, PodEval does not constrain the domain of podcasts, and open-ended content evaluation is separately conducted in the text-based evaluation section. In this audio-based evaluation, we focus on assessing the overall performance of the audios. The design approach is as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text-based evaluation is conducted among GPT-4, PodAgent, MoonCast, and Real-Pod. Other systems in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Podcast generation &#8227; 2 Related work &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> are excluded as they do not provide conversation scripts. PodAgent, with its Host-Guest-Writer multi-agent system, can directly generate podcast scripts based on a given topic. While MoonCast functions similarly to NotebookLM, requiring external knowledge sources but providing prompt template for spontaneous script generation. For this evaluation, the MoonCast system uses the podcast scripts generated by PodAgent as input and transforms them into a spontaneous version.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "podagent",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Metrics.</span> Detailed scores calculated across 17 podcast categories for each system are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS1\" title=\"A.2.1 Quantitative metrics &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.1</span></a>. For a concise and clearer comparison, we present the overall performance (averaged across all 17 categories) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F2\" title=\"Figure 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where we can observe that: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> For each quantitative metric, PodAgent outperforms directly prompting GPT-4; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> When comparing LLM-based methods (GPT-4, PodAgent) with human-created podcasts (Real-Pod), Real-Pod scores lower on lexical diversity (Distinct-2 and MATTR) but higher on information density and semantic diversity (Info-Dens and Sem-Div). This is reasonable for: i) real human interactions often include filler words and use simpler language; ii) most real podcasts are significantly longer (30 minutes to an hour), leading to higher information richness compared to generated podcasts, which are usually only a few minutes long; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> As a spontaneous version of PodAgent, MoonCast shows reduced lexical diversity and information density. While its semantic diversity remains comparable to PodAgent.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "podagent",
                    "quantitative",
                    "mattr",
                    "metrics",
                    "distinct2",
                    "semdiv",
                    "infodens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> This evaluation compares PodAgent (scored from -3 to 3) with GPT-4 (reference score as 0), both of which generate conversation scripts without external knowledge resources. Detailed scores for each category are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS2\" title=\"A.2.2 LLM-as-a-Judge &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.2</span></a>. We present the overall performance and results for five specific categories in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T2\" title=\"Table 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for analysis. We can see that scores across all metrics and all categories are positive, demonstrating that PodAgent significantly outperforms directly prompting GPT-4 in generating podcast scripts across all evaluated dimensions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "podagent",
                    "overall",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure fairness, all open-source TTS systems use the same PodAgent-generated scripts. Subjective tests use a spontaneous version from MoonCast, while objective evaluations use the original PodAgent scripts, as filler words in the spontaneous version challenge metrics like WER.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SIM.</span> The SIM metric evaluates zero-shot TTS systems&#8217; ability to replicate the timbre of a reference voice. PodAgent, MoonCast, MuyanTTS, Dia, and MOSS-TTSD&#8212;are assessed as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(2). Each system uses the reference voice selected by PodAgent for the topic. The performance rankings are: MuyanTTS, MOSS-TTSD, Dia, MoonCast, and PodAgent. PodAgent&#8217;s relatively low score in this metric likely stems from its instruction-following style control strategy. While this approach enhances overall conversational expressiveness, it can reduce speaker similarity.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "podagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DNSMOS.</span> The DNSMOS metric was applied to all systems to evaluate speech quality as in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(4). PodAgent, MoonCast, MuyanTTS, MOSS-TTSD, and NotebookLM achieve similar scores, while Real-Pod and Dia show noticeable declines in speech quality. For Real-Pod, the lower scores are due to: (1) real podcasts often use MSE for enhancement, requiring speech-MSE separation before evaluation, which may leave residual MSE artifacts, and (2) human-created podcasts involve recording, editing, or post-processing that introduce noise or instability. Dia struggles with long-form speech synthesis. Its outputs for lengthy podcast scripts frequently feature overly fast speaking speeds and occasional sentence truncations, leading to its relatively low DNSMOS performance.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "podagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation.</span> We released the task on Prolific<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.prolific.com/\" title=\"\">https://www.prolific.com/</a></span></span></span>, requesting 20 native English-speaking participants from the US/UK. We set the filter rules as: 1) Over 90% of LQ samples must be marked as the worst, as the synthesized samples from eSpeak are obviously robotic and unnatural. 2) Over 50% of HQ samples must rank in the top-2 best. While it is possible for other systems to achieve a better score than the real podcast, the evaluation of the real podcast should also remain above average. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T3\" title=\"Table 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the two statistical metrics for the submission results. Based on these rules, Judger-7, 18 and 20 can be excluded. We also provide the box plot for each Judger in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F9\" title=\"Figure 9 &#8227; A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix for more advanced analysis. For instance, apart from the LQ samples, Judger-20 assigns similar scores to all other systems, further confirming the invalidity of this submission.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Result Analysis.</span> After excluding unqualified submissions, we analyzed system performance based on the remaining 17 valid submissions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F4\" title=\"Figure 4 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the final results. We can observe that dialogue segments from real podcasts (HQ) achieved the highest scores, which aligns with expectations. NotebookLM, a closed-source product, ranked second, reflecting the high naturalness of its synthesized dialogue speech. Among the three open-source podcast generation systems, PodAgent scored the lowest, which is reasonable since its backend TTS system, CosyVoice2, is limited to single-sentence synthesis. In contrast, MoonCast and MOSS-TTSD, which support direct dialogue synthesis, performed better in dialogue naturalness evaluations. Overall, the evaluation results align with expectations, validating the rationality and effectiveness of our evaluation method design.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "podagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the density distribution of loudness-related metrics, enabling a comparative analysis with the reference range. All seven systems are included. For <span class=\"ltx_text ltx_font_bold\">LOUD_IT</span>, Dia and MoonCast align well with the reference range, while NotebookLM&#8217;s loudness centers around -25. Real-Pod, as manually produced audio, shows a highly scattered loudness distribution. For <span class=\"ltx_text ltx_font_bold\">LOUD_TP</span>, Muyan-TTS performs best, with all samples maintaining a true peak loudness below -1. In contrast, MoonCast, Dia and MOSS-TTSD perform poorly, while Real-Pod continues to exhibit scattered results. For <span class=\"ltx_text ltx_font_bold\">LOUD_RA</span>, MoonCast has a relatively narrow loudness variation range, while PodAgent and MOSS-TTSD display richer variance. Quantitative scores are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "true",
                    "quantitative",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SMR and CASP.</span> PodAgent and Real-Pod are evaluated for these MSE-related metrics. From the density distribution in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, PodAgent exhibits a more concentrated distribution compared to Real-Pod. For <span class=\"ltx_text ltx_font_bold\">SMR</span>, the SMR_SCORE in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows that all PodAgent samples achieve an SMR greater than 0, whereas some Real-Pod cases fail to meet this requirement. For <span class=\"ltx_text ltx_font_bold\">CASP</span>, a higher score indicates better MSE-Speech harmony. Real-Pod demonstrates a higher upper limit, which is expected as exceptional human artistic creations naturally surpass AI-generated outputs. However, PodAgent delivers more consistent performance, and the overall gap between the two systems is not significant, making it an alternative way to enhance creative efficiency.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "overall",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Speech (naturalness and authenticity) is the most dominant factor affecting the listener&#8217;s experience</span>. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, PodAgent scored low in dialogue naturalness due to using a single-sentence synthesis TTS system, leading to consistently poor results in this MOS test. This outcome is expected, as dialogue speech is the core component of podcast-like audio programs. Although PodAgent&#8217;s Music/Sound (harmony) score is below Real-Pod (consistent with the results of objective metric - CASP), it is significantly higher than its scores in other metrics, indicating that <span class=\"ltx_text ltx_font_italic\">the gap between PodAgent and Real-Pod in music harmony is smaller than in speech naturalness</span>.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Audio Quality metric, while PodAgent and MOSS-TTSD score lower than Real-Pod, PodAgent performs better here than in other metrics, and NotebookLM slightly surpasses Real-Pod. As noted, human-made podcasts often exhibit inconsistent audio quality due to complex production. User feedback, like &#8220;Little bit of mic hiss/bloom but otherwise fine,&#8221; supports this observation. This highlights that <span class=\"ltx_text ltx_font_italic\">when conversational realism approaches that of real speech, AI-based methods offers an advantage in their controllability and consistency in producing high-quality audio</span>.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PodEval is the first comprehensive evaluation framework for podcast-like audio generation, tackling the challenges of assessing open-ended, long-form content. We constructed a real-world podcast dataset as a benchmark for human-level creative quality across diverse topics and formats. By decomposing evaluation into text, speech, and audio, PodEval introduced multidimensional methods combining objective metrics and well-designed subjective listening tests. Experiments with various podcast generation systems, including open-source, closed-source, and human-made examples, validated the framework&#8217;s effectiveness. The results offer insights into the strengths and weaknesses of different systems (e.g. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F14\" title=\"Figure 14 &#8227; A.6 System analysis report &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>), highlighting PodEval&#8217;s role in advancing podcast generation research and inspiring future work on evaluating open-ended, long-form content generation task.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tools for <span class=\"ltx_text ltx_font_italic\">text-based evaluation</span> of dialogue scripts. Includes both <span class=\"ltx_text ltx_font_italic\">Quantitative Metrics</span> and <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> methods.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quantitative",
                    "metrics",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large Language Models (LLMs) utilized in this work are as follows:(1) <span class=\"ltx_text ltx_font_italic\">Topics Initiation</span> during data processing of the Real-Pod dataset, which is elaborated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S3\" title=\"3 Real-Pod: Real-world podcast dataset &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. (2) <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> method in text-based evaluation, which is illustrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S4\" title=\"4 Text-based evaluation &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. (3) <span class=\"ltx_text ltx_font_italic\">Summarized Users&#8217; Justifications</span> in the Questionnaire-based MOS Test, which is described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS3\" title=\"7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.3</span></a> (Questionnaire-based MOS Test).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "textbased"
                ]
            }
        ]
    },
    "A1.T6": {
        "source_file": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
        "caption": "Table 6: MoonCast: Quantitative metrics in text-based evaluation.",
        "body": "Metrics\nOverall\nFiction\nEducation\nBusiness\nTrue Crime\nHealth & Fitness\n\n\n\n\nDistinct_2\n0.9128\n0.9219\n0.8998\n0.8952\n0.9132\n0.9478\n\n\nInfo-Dens\n6.0935\n6.3613\n5.9779\n5.9931\n6.0388\n6.4230\n\n\nSem-Div\n0.1326\n0.1515\n0.1079\n0.1326\n0.1405\n0.1324\n\n\nMATTR\n0.6323\n0.6598\n0.6237\n0.6310\n0.6391\n0.6698\n\n\nMetrics\nSports\nComedy\nHistory\nNews\nTV & Film\nSociety & Culture\n\n\nDistinct_2\n0.9159\n0.9232\n0.9169\n0.9047\n0.9408\n0.8959\n\n\nInfo-Dens\n6.1933\n6.1729\n6.2229\n5.9672\n6.2855\n5.8031\n\n\nSem-Div\n0.1451\n0.1311\n0.1460\n0.1176\n0.1318\n0.1282\n\n\nMATTR\n0.6402\n0.6435\n0.6276\n0.6111\n0.6595\n0.6121\n\n\nMetrics\nArts\nLeisure\nMusic\nKids\nMental Health\nScience & Tech\n\n\nDistinct_2\n0.9252\n0.8889\n0.8957\n0.9039\n0.9222\n0.9073\n\n\nInfo-Dens\n6.2335\n5.9713\n5.9411\n5.9291\n6.0298\n6.0459\n\n\nSem-Div\n0.1444\n0.1309\n0.1227\n0.1291\n0.1187\n0.1432\n\n\nMATTR\n0.6370\n0.6124\n0.6035\n0.6183\n0.6321\n0.6277",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"--ltx-bg-color:#FFFF00;padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFFF00;\">Overall</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Fiction</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Education</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Business</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">True Crime</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Health &amp; Fitness</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Distinct_2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.9128</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9219</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.8998</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.8952</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9132</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9478</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Info-Dens</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">6.0935</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.3613</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">5.9779</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">5.9931</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.0388</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.4230</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Sem-Div</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.1326</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1515</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1079</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1326</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1405</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1324</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">MATTR</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.6323</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6598</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6237</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6310</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6391</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6698</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Sports</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Comedy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">History</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">News</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">TV &amp; Film</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Society &amp; Culture</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Distinct_2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9159</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9232</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9169</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9047</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9408</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.8959</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Info-Dens</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.1933</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.1729</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.2229</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">5.9672</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.2855</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">5.8031</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Sem-Div</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1451</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1311</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1460</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1176</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1318</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1282</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">MATTR</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6402</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6435</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6276</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6111</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6595</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6121</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Arts</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Leisure</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Music</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Kids</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Mental Health</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Science &amp; Tech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Distinct_2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9252</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.8889</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.8957</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9039</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9222</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9073</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Info-Dens</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.2335</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">5.9713</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">5.9411</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">5.9291</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.0298</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">6.0459</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Sem-Div</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1444</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1309</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1227</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1291</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1187</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1432</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">MATTR</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6370</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6124</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6035</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6183</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6321</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6277</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "arts",
            "true",
            "comedy",
            "business",
            "crime",
            "society",
            "kids",
            "distinct2",
            "tech",
            "semdiv",
            "fiction",
            "evaluation",
            "overall",
            "mental",
            "metrics",
            "infodens",
            "music",
            "sports",
            "film",
            "textbased",
            "culture",
            "fitness",
            "mooncast",
            "health",
            "education",
            "news",
            "quantitative",
            "mattr",
            "leisure",
            "science",
            "history"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recently, an increasing number of multimodal (text and audio) benchmarks have emerged, primarily focusing on evaluating models&#8217; understanding capability. However, exploration into assessing generative capabilities remains limited, especially for open-ended long-form content generation. Significant challenges lie in no reference standard answer, no unified evaluation metrics and uncontrollable human judgments. In this work, we take podcast-like audio generation as a starting point and propose PodEval, a comprehensive and well-designed open-source evaluation framework. In this framework: 1) We construct a real-world podcast dataset spanning diverse topics, serving as a reference for human-level creative quality. 2) We introduce a multimodal evaluation strategy and decompose the complex task into three dimensions: text, speech and audio, with different evaluation emphasis on &#8220;Content&#8221; and &#8220;Format&#8221;. 3) For each modality, we design corresponding evaluation methods, involving both objective metrics and subjective listening test. We leverage representative podcast generation systems (including open-source, close-source, and human-made) in our experiments. The results offer in-depth analysis and insights into podcast generation, demonstrating the effectiveness of PodEval in evaluating open-ended long-form audio. This project is open-source to facilitate public use: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/yujxx/PodEval\" title=\"\">https://github.com/yujxx/PodEval</a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid development of AIGC (AI-Generated Content) in recent years, many innovative applications have emerged. AI Podcast represents a key application scenario for audio-based generative models <cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib17\" title=\"\">2023</a>; ByteDance, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib10\" title=\"\">2025</a>)</cite>. However, evaluating podcast-like audio is challenging due to: 1) it is an open-ended task, which means there is no reference standard answer; 2) the evaluation of long-form speech/audio is particularly difficult, as longer formats introduce more variability. Objective metrics often fail to capture human perceptions accurately, while subjective listening tests face issues like user inattention, which reduces the validity of results; and 3) podcasts often incorporate additional elements, like music and sound effects, making the evaluation more complicated.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges and establish a clear evaluation framework, we decompose podcast-like audio into three dimensions: <span class=\"ltx_text ltx_font_bold\">text</span> (conversation transcripts), <span class=\"ltx_text ltx_font_bold\">speech</span> (spoken dialogue), and <span class=\"ltx_text ltx_font_bold\">audio</span> (speech, music, sound effects, and their interaction). While these dimensions inherently overlap, they offers a structured framework for evaluation focus. Specifically, the conversation transcripts in podcasts are primarily used for <span class=\"ltx_text ltx_font_bold\">content</span> (the message being conveyed) evaluation, whereas speech, music and sound effects primarily contribute to <span class=\"ltx_text ltx_font_bold\">format</span> (how the message is presented) evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Different modalities have their own commonly used evaluation methods. For text, metrics such as BLEU <cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib30\" title=\"\">2002</a>)</cite>, ROUGE <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib24\" title=\"\">2004</a>)</cite>, and METEOR <cite class=\"ltx_cite ltx_citemacro_citep\">(Banerjee &amp; Lavie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib7\" title=\"\">2005</a>)</cite> focus on fluency and relevance, while newer approaches like BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib46\" title=\"\">2019</a>)</cite> utilize pre-trained language models to capture semantic alignment. For speech, objective metrics like Mel Cepstral Distortion (MCD) and Perceptual Evaluation of Speech Quality (PESQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib34\" title=\"\">2001</a>)</cite> are widely used, alongside subjective evaluations like Mean Opinion Score (MOS) <cite class=\"ltx_cite ltx_citemacro_citep\">(Sector, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib37\" title=\"\">1996</a>)</cite>. For audio, metrics like Frechet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib22\" title=\"\">2018</a>)</cite> and Kullback-Leibler Divergence (KL) are employed to evaluate audio quality, while listener surveys provide subjective insights. However, these evaluation methods are not directly applicable to podcast evaluation since:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">General speech evaluation focuses on individual sentences, while podcasts require natural and interactive dialogue, emphasizing dialogue-level naturalness. Additionally, voice presentation in multi-speaker scenarios is critical to ensuring role distinction and overall listener engagement.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While music and sound effects are not essential to every podcast, their evaluation, when present, should go beyond the quality of individual audio events. Instead, it should focus on their overall harmony and seamless integration with the speech content to enhance the listener&#8217;s experience.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each modality, we design tailored metrics to address diversity considerations. For text, we combine quantitative metrics with LLM-based evaluations to assess conversation scripts. For speech and audio, we design objective metrics and subjective listening tests to evaluate spoken dialogue and overall audio performance. All evaluation methods are organized into open-source tools for ease of use. Subjective tests are enhanced by spammer detection to improve data validity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "quantitative",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Various evaluation works have emerged along with the development of LLMs and multimodal generative models. <span class=\"ltx_text ltx_font_bold\">Text-related Evaluation</span>, such as SuperGLUE, MMLU, and BIG-bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib41\" title=\"\">2019</a>; Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib18\" title=\"\">2020</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib39\" title=\"\">2022</a>)</cite>, assesses the capabilities of LLMs across diverse tasks with preset ground truth. Subsequently, MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib47\" title=\"\">2023</a>)</cite> explores the potential of LLMs as evaluators, and Chatbot Arena <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib12\" title=\"\">2024</a>)</cite> provides an open platform for assessing LLMs based on human preferences. <span class=\"ltx_text ltx_font_bold\">Speech-related Evaluation</span>, such as SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib45\" title=\"\">2021</a>)</cite>, is designed for discriminative tasks like speech recognition and speaker identification. However, evaluations for generative tasks are scarce due to their inherent diversity and subjectivity, making subjective evaluation essential for speech generation tasks. For instance, VOCBENCH <cite class=\"ltx_cite ltx_citemacro_citep\">(AlBadawy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib3\" title=\"\">2022</a>)</cite> incorporates both subjective and objective evaluations to assess vocoder performance in speech synthesis. Similarly, numerous <span class=\"ltx_text ltx_font_bold\">Audio-related Evaluation</span> work, such as AIR-Bench, Audiobench, MMAU, and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib44\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib42\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib35\" title=\"\">2024</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib26\" title=\"\">2025</a>)</cite>, focus on audio understanding and reasoning. Subjective evaluation remains crucial for assessing audio generation systems and is typically tailored to specific generation tasks. Unlike existing evaluation works, <span class=\"ltx_text ltx_font_bold\">PodEval</span> introduces a comprehensive framework specifically designed for podcast-like audio generation. It emphasizes both subjective and objective evaluations across text, speech, and audio, with all metrics closely aligned with real-world user experience.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dialogue content in podcasts is extracted in text format for evaluation, representing the core message the podcast aims to convey. Podcast dialogues often center around specific topics, showcasing participants&#8217; unique perspectives and insights, which makes reference-correlation-based methods infeasible. Instead, the richness of perspectives conveyed (to provide informative takeaways for the listener) and the presentation style of the dialogue (to enhance listener comprehension) should be the primary focus of evaluation. Therefore, we follow the dialogue script-based evaluation methods proposed in PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite>, which adopt a two-fold approach: (1) <span class=\"ltx_text ltx_font_bold\">Quantitative Metrics</span> such as Distinct-N, Semantic-Div, MATTR, and Info-Dens to assess lexical diversity, semantic richness, vocabulary richness, and information density, respectively. These metrics operate independently of reference texts and focus on intrinsic text characteristics; (2) <span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge</span>, leveraging GPT-4 to replace human evaluators for complex and comprehensive assessments. Evaluation criteria include coherence, engagingness, diversity, informativeness and speaker diversity. It incorporates comparative evaluations to reduce bias and evidence-based scoring for robust and reliable results.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quantitative",
                    "mattr",
                    "metrics",
                    "infodens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech is the core component of a podcast, serving as the medium for content delivery, and how the message delivered plays a crucial role in shaping the listening experience. To ensure a multidimensional evaluation, we first integrate the following <span class=\"ltx_text ltx_font_bold\">Objective Metrics</span>:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce the audio-based evaluation for podcasts, which treats speech as one component and assesses the overall audio performance, including speech, music and sound effects (MSE), and their interactions. Similarly, we first introduce the following <span class=\"ltx_text ltx_font_bold\">Objective metrics</span>:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness</span>: Loudness ensures audio falls within an acceptable volume range. The ITU-R BS.1770-4 standard <cite class=\"ltx_cite ltx_citemacro_citep\">(BS Series, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib9\" title=\"\">2011</a>)</cite> is widely recognized for measuring audio loudness and true-peak levels. Based on this, the <cite class=\"ltx_cite ltx_citemacro_citep\">(EBU R128, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib15\" title=\"\">2011</a>)</cite> standard has been broadly adopted by broadcast and streaming platforms, recommend a target Integrated Loudness (LOUD-IT) of -23 LUFS (&#177;1 LUFS), True Peak (LOUD-TP) <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP and Loudness Range (LOUD-RA) <math alttext=\"&lt;20\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">&lt;20</annotation></semantics></math> LU. For podcast-like streaming, adjustments are made for typical listening environments, such as mobile devices where headphones are commonly used. In these cases, the LOUD-IT is recommended as -18, -16 (&#177;1) or -14 LUFS <cite class=\"ltx_cite ltx_citemacro_citep\">(AES, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib2\" title=\"\">2021</a>; Apple, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib4\" title=\"\">2023</a>; Spotify, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib38\" title=\"\">2023</a>)</cite>. Netflix recommends keeping LOUD-RA between 4 and 18 LU <cite class=\"ltx_cite ltx_citemacro_citep\">(Netflix, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib28\" title=\"\">2024</a>)</cite>. There is no &#8220;absolute right&#8221; reference for loudness metrics. We propose the following reference standards considering all above guidelines: <span class=\"ltx_text ltx_font_bold\">LOUD-IT:</span> <math alttext=\"-18\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>18</mn></mrow><annotation encoding=\"application/x-tex\">-18</annotation></semantics></math> to <math alttext=\"-14\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>14</mn></mrow><annotation encoding=\"application/x-tex\">-14</annotation></semantics></math> LUFS; <span class=\"ltx_text ltx_font_bold\">LOUD-TP:</span> <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP; <span class=\"ltx_text ltx_font_bold\">LOUD-RA:</span> 4 to 18 LU. Based on this &#8220;relatively correct&#8221; reference, we can analyze the distribution of loudness metrics across different systems. We also provide a quantitative scoring strategy in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS4\" title=\"A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "quantitative",
                    "true",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> is primarily designed based on the perceptions of real users. A key challenge lies in how to evaluate extra-long audios. As we mentioned above, podcasts in the real world range from a few minutes to over an hour in length. Conducting listening tests on full-length podcast episodes is impractical due to the time, effort, and financial resources required. Moreover, it is hard to judge podcasts of vastly different lengths in a fair and consistent manner. Research on long-form audio evaluation is limited. <cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib13\" title=\"\">2019</a>)</cite> did investigation on long-form speech evaluation and found that multiple evaluations are necessary due to the low correlation observed across different experimental settings. <cite class=\"ltx_cite ltx_citemacro_cite\">Cambre et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib11\" title=\"\">2020</a>)</cite> conducted a comprehensive evaluation of voice selection for long-form content; however, the minimum required listening time was only 10 seconds. A podcast-related evaluation study <cite class=\"ltx_cite ltx_citemacro_citep\">(Austria, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib6\" title=\"\">2007</a>)</cite> designed a questionnaire with carefully crafted questions in terms of both content and presentation to assess domain-specific podcasts. Different from that, PodEval does not constrain the domain of podcasts, and open-ended content evaluation is separately conducted in the text-based evaluation section. In this audio-based evaluation, we focus on assessing the overall performance of the audios. The design approach is as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text-based evaluation is conducted among GPT-4, PodAgent, MoonCast, and Real-Pod. Other systems in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Podcast generation &#8227; 2 Related work &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> are excluded as they do not provide conversation scripts. PodAgent, with its Host-Guest-Writer multi-agent system, can directly generate podcast scripts based on a given topic. While MoonCast functions similarly to NotebookLM, requiring external knowledge sources but providing prompt template for spontaneous script generation. For this evaluation, the MoonCast system uses the podcast scripts generated by PodAgent as input and transforms them into a spontaneous version.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mooncast",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Metrics.</span> Detailed scores calculated across 17 podcast categories for each system are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS1\" title=\"A.2.1 Quantitative metrics &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.1</span></a>. For a concise and clearer comparison, we present the overall performance (averaged across all 17 categories) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F2\" title=\"Figure 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where we can observe that: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> For each quantitative metric, PodAgent outperforms directly prompting GPT-4; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> When comparing LLM-based methods (GPT-4, PodAgent) with human-created podcasts (Real-Pod), Real-Pod scores lower on lexical diversity (Distinct-2 and MATTR) but higher on information density and semantic diversity (Info-Dens and Sem-Div). This is reasonable for: i) real human interactions often include filler words and use simpler language; ii) most real podcasts are significantly longer (30 minutes to an hour), leading to higher information richness compared to generated podcasts, which are usually only a few minutes long; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> As a spontaneous version of PodAgent, MoonCast shows reduced lexical diversity and information density. While its semantic diversity remains comparable to PodAgent.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "quantitative",
                    "mattr",
                    "mooncast",
                    "metrics",
                    "distinct2",
                    "semdiv",
                    "infodens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> This evaluation compares PodAgent (scored from -3 to 3) with GPT-4 (reference score as 0), both of which generate conversation scripts without external knowledge resources. Detailed scores for each category are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS2\" title=\"A.2.2 LLM-as-a-Judge &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.2</span></a>. We present the overall performance and results for five specific categories in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T2\" title=\"Table 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for analysis. We can see that scores across all metrics and all categories are positive, demonstrating that PodAgent significantly outperforms directly prompting GPT-4 in generating podcast scripts across all evaluated dimensions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure fairness, all open-source TTS systems use the same PodAgent-generated scripts. Subjective tests use a spontaneous version from MoonCast, while objective evaluations use the original PodAgent scripts, as filler words in the spontaneous version challenge metrics like WER.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "mooncast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SIM.</span> The SIM metric evaluates zero-shot TTS systems&#8217; ability to replicate the timbre of a reference voice. PodAgent, MoonCast, MuyanTTS, Dia, and MOSS-TTSD&#8212;are assessed as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(2). Each system uses the reference voice selected by PodAgent for the topic. The performance rankings are: MuyanTTS, MOSS-TTSD, Dia, MoonCast, and PodAgent. PodAgent&#8217;s relatively low score in this metric likely stems from its instruction-following style control strategy. While this approach enhances overall conversational expressiveness, it can reduce speaker similarity.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "mooncast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DNSMOS.</span> The DNSMOS metric was applied to all systems to evaluate speech quality as in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(4). PodAgent, MoonCast, MuyanTTS, MOSS-TTSD, and NotebookLM achieve similar scores, while Real-Pod and Dia show noticeable declines in speech quality. For Real-Pod, the lower scores are due to: (1) real podcasts often use MSE for enhancement, requiring speech-MSE separation before evaluation, which may leave residual MSE artifacts, and (2) human-created podcasts involve recording, editing, or post-processing that introduce noise or instability. Dia struggles with long-form speech synthesis. Its outputs for lengthy podcast scripts frequently feature overly fast speaking speeds and occasional sentence truncations, leading to its relatively low DNSMOS performance.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "mooncast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation.</span> We released the task on Prolific<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.prolific.com/\" title=\"\">https://www.prolific.com/</a></span></span></span>, requesting 20 native English-speaking participants from the US/UK. We set the filter rules as: 1) Over 90% of LQ samples must be marked as the worst, as the synthesized samples from eSpeak are obviously robotic and unnatural. 2) Over 50% of HQ samples must rank in the top-2 best. While it is possible for other systems to achieve a better score than the real podcast, the evaluation of the real podcast should also remain above average. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T3\" title=\"Table 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the two statistical metrics for the submission results. Based on these rules, Judger-7, 18 and 20 can be excluded. We also provide the box plot for each Judger in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F9\" title=\"Figure 9 &#8227; A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix for more advanced analysis. For instance, apart from the LQ samples, Judger-20 assigns similar scores to all other systems, further confirming the invalidity of this submission.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Result Analysis.</span> After excluding unqualified submissions, we analyzed system performance based on the remaining 17 valid submissions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F4\" title=\"Figure 4 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the final results. We can observe that dialogue segments from real podcasts (HQ) achieved the highest scores, which aligns with expectations. NotebookLM, a closed-source product, ranked second, reflecting the high naturalness of its synthesized dialogue speech. Among the three open-source podcast generation systems, PodAgent scored the lowest, which is reasonable since its backend TTS system, CosyVoice2, is limited to single-sentence synthesis. In contrast, MoonCast and MOSS-TTSD, which support direct dialogue synthesis, performed better in dialogue naturalness evaluations. Overall, the evaluation results align with expectations, validating the rationality and effectiveness of our evaluation method design.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "mooncast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the density distribution of loudness-related metrics, enabling a comparative analysis with the reference range. All seven systems are included. For <span class=\"ltx_text ltx_font_bold\">LOUD_IT</span>, Dia and MoonCast align well with the reference range, while NotebookLM&#8217;s loudness centers around -25. Real-Pod, as manually produced audio, shows a highly scattered loudness distribution. For <span class=\"ltx_text ltx_font_bold\">LOUD_TP</span>, Muyan-TTS performs best, with all samples maintaining a true peak loudness below -1. In contrast, MoonCast, Dia and MOSS-TTSD perform poorly, while Real-Pod continues to exhibit scattered results. For <span class=\"ltx_text ltx_font_bold\">LOUD_RA</span>, MoonCast has a relatively narrow loudness variation range, while PodAgent and MOSS-TTSD display richer variance. Quantitative scores are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "quantitative",
                    "true",
                    "metrics",
                    "mooncast"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SMR and CASP.</span> PodAgent and Real-Pod are evaluated for these MSE-related metrics. From the density distribution in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, PodAgent exhibits a more concentrated distribution compared to Real-Pod. For <span class=\"ltx_text ltx_font_bold\">SMR</span>, the SMR_SCORE in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows that all PodAgent samples achieve an SMR greater than 0, whereas some Real-Pod cases fail to meet this requirement. For <span class=\"ltx_text ltx_font_bold\">CASP</span>, a higher score indicates better MSE-Speech harmony. Real-Pod demonstrates a higher upper limit, which is expected as exceptional human artistic creations naturally surpass AI-generated outputs. However, PodAgent delivers more consistent performance, and the overall gap between the two systems is not significant, making it an alternative way to enhance creative efficiency.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Speech (naturalness and authenticity) is the most dominant factor affecting the listener&#8217;s experience</span>. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, PodAgent scored low in dialogue naturalness due to using a single-sentence synthesis TTS system, leading to consistently poor results in this MOS test. This outcome is expected, as dialogue speech is the core component of podcast-like audio programs. Although PodAgent&#8217;s Music/Sound (harmony) score is below Real-Pod (consistent with the results of objective metric - CASP), it is significantly higher than its scores in other metrics, indicating that <span class=\"ltx_text ltx_font_italic\">the gap between PodAgent and Real-Pod in music harmony is smaller than in speech naturalness</span>.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PodEval is the first comprehensive evaluation framework for podcast-like audio generation, tackling the challenges of assessing open-ended, long-form content. We constructed a real-world podcast dataset as a benchmark for human-level creative quality across diverse topics and formats. By decomposing evaluation into text, speech, and audio, PodEval introduced multidimensional methods combining objective metrics and well-designed subjective listening tests. Experiments with various podcast generation systems, including open-source, closed-source, and human-made examples, validated the framework&#8217;s effectiveness. The results offer insights into the strengths and weaknesses of different systems (e.g. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F14\" title=\"Figure 14 &#8227; A.6 System analysis report &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>), highlighting PodEval&#8217;s role in advancing podcast generation research and inspiring future work on evaluating open-ended, long-form content generation task.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tools for <span class=\"ltx_text ltx_font_italic\">text-based evaluation</span> of dialogue scripts. Includes both <span class=\"ltx_text ltx_font_italic\">Quantitative Metrics</span> and <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> methods.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quantitative",
                    "metrics",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large Language Models (LLMs) utilized in this work are as follows:(1) <span class=\"ltx_text ltx_font_italic\">Topics Initiation</span> during data processing of the Real-Pod dataset, which is elaborated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S3\" title=\"3 Real-Pod: Real-world podcast dataset &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. (2) <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> method in text-based evaluation, which is illustrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S4\" title=\"4 Text-based evaluation &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. (3) <span class=\"ltx_text ltx_font_italic\">Summarized Users&#8217; Justifications</span> in the Questionnaire-based MOS Test, which is described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS3\" title=\"7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.3</span></a> (Questionnaire-based MOS Test).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "textbased"
                ]
            }
        ]
    },
    "A1.T7": {
        "source_file": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
        "caption": "Table 7: Real-Pod: Quantitative metrics in text-based evaluation.",
        "body": "Metrics\nOverall\nFiction\nEducation\nBusiness\nTrue Crime\nHealth & Fitness\n\n\n\n\nDistinct_2\n0.9200\n0.9292\n0.9275\n0.9049\n0.9169\n0.9273\n\n\nInfo-Dens\n8.1168\n8.2849\n8.1160\n7.7755\n8.5675\n7.9301\n\n\nSem-Div\n0.1677\n0.1776\n0.1579\n0.1433\n0.1906\n0.1646\n\n\nMATTR\n0.6251\n0.6313\n0.6346\n0.6041\n0.6261\n0.6380\n\n\nMetrics\nSports\nComedy\nHistory\nNews\nTV & Film\nSociety & Culture\n\n\nDistinct_2\n0.9244\n0.8994\n0.9272\n0.9100\n0.9201\n0.8932\n\n\nInfo-Dens\n8.0993\n8.2755\n8.8282\n7.7886\n8.4005\n7.7375\n\n\nSem-Div\n0.1919\n0.1660\n0.1845\n0.1618\n0.1784\n0.1701\n\n\nMATTR\n0.6434\n0.5999\n0.6304\n0.6102\n0.6363\n0.5823\n\n\nMetrics\nArts\nLeisure\nMusic\nKids\nMental Health\nScience & Tech\n\n\nDistinct_2\n0.9111\n0.9242\n0.9420\n0.9092\n0.9298\n0.9439\n\n\nInfo-Dens\n8.1093\n7.6949\n8.0925\n7.7708\n8.2119\n8.3031\n\n\nSem-Div\n0.1653\n0.1591\n0.1761\n0.1492\n0.1668\n0.1485\n\n\nMATTR\n0.6063\n0.6176\n0.6513\n0.6200\n0.6373\n0.6582",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"--ltx-bg-color:#FFFF00;padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFFF00;\">Overall</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Fiction</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Education</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Business</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">True Crime</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Health &amp; Fitness</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Distinct_2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.9200</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9292</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9275</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9049</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9169</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9273</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Info-Dens</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">8.1168</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">8.2849</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">8.1160</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.7755</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">8.5675</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.9301</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Sem-Div</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.1677</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1776</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1579</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1433</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1906</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1646</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">MATTR</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.6251</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6313</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6346</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6041</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6261</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6380</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Sports</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Comedy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">History</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">News</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">TV &amp; Film</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Society &amp; Culture</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Distinct_2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9244</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.8994</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9272</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9100</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9201</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.8932</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Info-Dens</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">8.0993</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">8.2755</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">8.8282</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.7886</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">8.4005</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.7375</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Sem-Div</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1919</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1660</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1845</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1618</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1784</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1701</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">MATTR</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6434</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.5999</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6304</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6102</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6363</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.5823</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Arts</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Leisure</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Music</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Kids</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Mental Health</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Science &amp; Tech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Distinct_2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9111</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9242</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9420</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9092</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9298</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.9439</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Info-Dens</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">8.1093</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.6949</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">8.0925</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">7.7708</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">8.2119</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">8.3031</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Sem-Div</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1653</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1591</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1761</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1492</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1668</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1485</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">MATTR</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6063</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6176</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6513</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6200</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6373</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.6582</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "arts",
            "true",
            "comedy",
            "business",
            "crime",
            "society",
            "kids",
            "realpod",
            "distinct2",
            "tech",
            "semdiv",
            "fiction",
            "evaluation",
            "overall",
            "mental",
            "metrics",
            "infodens",
            "music",
            "sports",
            "film",
            "textbased",
            "culture",
            "fitness",
            "health",
            "education",
            "news",
            "quantitative",
            "mattr",
            "leisure",
            "science",
            "history"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recently, an increasing number of multimodal (text and audio) benchmarks have emerged, primarily focusing on evaluating models&#8217; understanding capability. However, exploration into assessing generative capabilities remains limited, especially for open-ended long-form content generation. Significant challenges lie in no reference standard answer, no unified evaluation metrics and uncontrollable human judgments. In this work, we take podcast-like audio generation as a starting point and propose PodEval, a comprehensive and well-designed open-source evaluation framework. In this framework: 1) We construct a real-world podcast dataset spanning diverse topics, serving as a reference for human-level creative quality. 2) We introduce a multimodal evaluation strategy and decompose the complex task into three dimensions: text, speech and audio, with different evaluation emphasis on &#8220;Content&#8221; and &#8220;Format&#8221;. 3) For each modality, we design corresponding evaluation methods, involving both objective metrics and subjective listening test. We leverage representative podcast generation systems (including open-source, close-source, and human-made) in our experiments. The results offer in-depth analysis and insights into podcast generation, demonstrating the effectiveness of PodEval in evaluating open-ended long-form audio. This project is open-source to facilitate public use: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/yujxx/PodEval\" title=\"\">https://github.com/yujxx/PodEval</a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid development of AIGC (AI-Generated Content) in recent years, many innovative applications have emerged. AI Podcast represents a key application scenario for audio-based generative models <cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib17\" title=\"\">2023</a>; ByteDance, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib10\" title=\"\">2025</a>)</cite>. However, evaluating podcast-like audio is challenging due to: 1) it is an open-ended task, which means there is no reference standard answer; 2) the evaluation of long-form speech/audio is particularly difficult, as longer formats introduce more variability. Objective metrics often fail to capture human perceptions accurately, while subjective listening tests face issues like user inattention, which reduces the validity of results; and 3) podcasts often incorporate additional elements, like music and sound effects, making the evaluation more complicated.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges and establish a clear evaluation framework, we decompose podcast-like audio into three dimensions: <span class=\"ltx_text ltx_font_bold\">text</span> (conversation transcripts), <span class=\"ltx_text ltx_font_bold\">speech</span> (spoken dialogue), and <span class=\"ltx_text ltx_font_bold\">audio</span> (speech, music, sound effects, and their interaction). While these dimensions inherently overlap, they offers a structured framework for evaluation focus. Specifically, the conversation transcripts in podcasts are primarily used for <span class=\"ltx_text ltx_font_bold\">content</span> (the message being conveyed) evaluation, whereas speech, music and sound effects primarily contribute to <span class=\"ltx_text ltx_font_bold\">format</span> (how the message is presented) evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Different modalities have their own commonly used evaluation methods. For text, metrics such as BLEU <cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib30\" title=\"\">2002</a>)</cite>, ROUGE <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib24\" title=\"\">2004</a>)</cite>, and METEOR <cite class=\"ltx_cite ltx_citemacro_citep\">(Banerjee &amp; Lavie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib7\" title=\"\">2005</a>)</cite> focus on fluency and relevance, while newer approaches like BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib46\" title=\"\">2019</a>)</cite> utilize pre-trained language models to capture semantic alignment. For speech, objective metrics like Mel Cepstral Distortion (MCD) and Perceptual Evaluation of Speech Quality (PESQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib34\" title=\"\">2001</a>)</cite> are widely used, alongside subjective evaluations like Mean Opinion Score (MOS) <cite class=\"ltx_cite ltx_citemacro_citep\">(Sector, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib37\" title=\"\">1996</a>)</cite>. For audio, metrics like Frechet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib22\" title=\"\">2018</a>)</cite> and Kullback-Leibler Divergence (KL) are employed to evaluate audio quality, while listener surveys provide subjective insights. However, these evaluation methods are not directly applicable to podcast evaluation since:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">General speech evaluation focuses on individual sentences, while podcasts require natural and interactive dialogue, emphasizing dialogue-level naturalness. Additionally, voice presentation in multi-speaker scenarios is critical to ensuring role distinction and overall listener engagement.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While music and sound effects are not essential to every podcast, their evaluation, when present, should go beyond the quality of individual audio events. Instead, it should focus on their overall harmony and seamless integration with the speech content to enhance the listener&#8217;s experience.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each modality, we design tailored metrics to address diversity considerations. For text, we combine quantitative metrics with LLM-based evaluations to assess conversation scripts. For speech and audio, we design objective metrics and subjective listening tests to evaluate spoken dialogue and overall audio performance. All evaluation methods are organized into open-source tools for ease of use. Subjective tests are enhanced by spammer detection to improve data validity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "quantitative",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Various evaluation works have emerged along with the development of LLMs and multimodal generative models. <span class=\"ltx_text ltx_font_bold\">Text-related Evaluation</span>, such as SuperGLUE, MMLU, and BIG-bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib41\" title=\"\">2019</a>; Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib18\" title=\"\">2020</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib39\" title=\"\">2022</a>)</cite>, assesses the capabilities of LLMs across diverse tasks with preset ground truth. Subsequently, MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib47\" title=\"\">2023</a>)</cite> explores the potential of LLMs as evaluators, and Chatbot Arena <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib12\" title=\"\">2024</a>)</cite> provides an open platform for assessing LLMs based on human preferences. <span class=\"ltx_text ltx_font_bold\">Speech-related Evaluation</span>, such as SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib45\" title=\"\">2021</a>)</cite>, is designed for discriminative tasks like speech recognition and speaker identification. However, evaluations for generative tasks are scarce due to their inherent diversity and subjectivity, making subjective evaluation essential for speech generation tasks. For instance, VOCBENCH <cite class=\"ltx_cite ltx_citemacro_citep\">(AlBadawy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib3\" title=\"\">2022</a>)</cite> incorporates both subjective and objective evaluations to assess vocoder performance in speech synthesis. Similarly, numerous <span class=\"ltx_text ltx_font_bold\">Audio-related Evaluation</span> work, such as AIR-Bench, Audiobench, MMAU, and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib44\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib42\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib35\" title=\"\">2024</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib26\" title=\"\">2025</a>)</cite>, focus on audio understanding and reasoning. Subjective evaluation remains crucial for assessing audio generation systems and is typically tailored to specific generation tasks. Unlike existing evaluation works, <span class=\"ltx_text ltx_font_bold\">PodEval</span> introduces a comprehensive framework specifically designed for podcast-like audio generation. It emphasizes both subjective and objective evaluations across text, speech, and audio, with all metrics closely aligned with real-world user experience.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There is no unified standard for defining &#8220;what makes a good podcast episode.&#8221; Unlike textbooks or official TV programs, podcasts can be created by anyone to share their unique ideas or insights. We do not make direct comparisons between generated podcasts and real podcasts&#8212;such comparisons are inherently unfeasible, especially when they approach topics from entirely different perspectives. Instead, we construct a real-world podcast dataset, called <span class=\"ltx_text ltx_font_bold\">Real-Pod</span> dataset, to serve as a reference for human-level creative quality. It is important to note that this dataset acts as a &#8220;reference&#8221; rather than an absolute &#8220;answer&#8221;. The design principles of the Real-Pod dataset are <span class=\"ltx_text ltx_font_bold\">real</span> (consists of human-made podcasts), <span class=\"ltx_text ltx_font_bold\">broad</span> (diverse topic coverage) and <span class=\"ltx_text ltx_font_bold\">rich</span> (varied formats, like multi-speaker, music and sound). The workflow for constructing the Real-Pod dataset is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S3.F1\" title=\"Figure 1 &#8227; 3 Real-Pod: Real-world podcast dataset &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>:</p>\n\n",
                "matched_terms": [
                    "realpod",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dialogue content in podcasts is extracted in text format for evaluation, representing the core message the podcast aims to convey. Podcast dialogues often center around specific topics, showcasing participants&#8217; unique perspectives and insights, which makes reference-correlation-based methods infeasible. Instead, the richness of perspectives conveyed (to provide informative takeaways for the listener) and the presentation style of the dialogue (to enhance listener comprehension) should be the primary focus of evaluation. Therefore, we follow the dialogue script-based evaluation methods proposed in PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite>, which adopt a two-fold approach: (1) <span class=\"ltx_text ltx_font_bold\">Quantitative Metrics</span> such as Distinct-N, Semantic-Div, MATTR, and Info-Dens to assess lexical diversity, semantic richness, vocabulary richness, and information density, respectively. These metrics operate independently of reference texts and focus on intrinsic text characteristics; (2) <span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge</span>, leveraging GPT-4 to replace human evaluators for complex and comprehensive assessments. Evaluation criteria include coherence, engagingness, diversity, informativeness and speaker diversity. It incorporates comparative evaluations to reduce bias and evidence-based scoring for robust and reliable results.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quantitative",
                    "mattr",
                    "metrics",
                    "infodens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech is the core component of a podcast, serving as the medium for content delivery, and how the message delivered plays a crucial role in shaping the listening experience. To ensure a multidimensional evaluation, we first integrate the following <span class=\"ltx_text ltx_font_bold\">Objective Metrics</span>:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In PodEval, we design the <span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation</span> based on the MUSHRA framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Schoeffler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib36\" title=\"\">2018</a>)</cite>. The key insight from this framework, <span class=\"ltx_text ltx_font_italic\">incorporating both high-quality and low-quality anchors</span>, helps evaluators establish a reliable reference of quality range. For researchers, analyzing scores for these anchors helps identify inattentive evaluators, enabling the <span class=\"ltx_text ltx_font_italic\">filtering of invalid submissions</span> and improving the data vadility. In our task, we use real podcast segments from the <span class=\"ltx_text ltx_font_italic\">Real-Pod dataset as the high-quality anchor</span> and synthesized dialogue segments from <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib16\" title=\"\">eSpeak Contributors </a></cite><span class=\"ltx_text ltx_font_italic\"> as the low-quality anchor</span>. For podcast samples from different systems, we provide an automatic toolkit to extract dialogue segments featuring <span class=\"ltx_text ltx_font_italic\">turn-taking</span> between speakers, representing a typical dialogue flow. Each dialogue segment is extracted with a <span class=\"ltx_text ltx_font_italic\">preset length</span> (e.g. 15&#8211;25 seconds) to ensure the speech samples are of similar duration. We select dialogue segments from <span class=\"ltx_text ltx_font_italic\">all 17 categories</span> in the Real-Pod dataset to ensure content diversity while keeping the total listening test duration <span class=\"ltx_text ltx_font_italic\">within 30 minutes</span>. In each test group, samples from different systems are presented <span class=\"ltx_text ltx_font_italic\">on the same page</span>, along with a <span class=\"ltx_text ltx_font_italic\">reference Real-Pod sample</span> to guide evaluators on what a natural dialogue sounds like. The scoring is adjusted using a slider ranging from 0 to 100, divided into <span class=\"ltx_text ltx_font_italic\">five stages with a clear definition</span>. Detailed instructions and website design can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS3\" title=\"A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The demo website is hosted at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml\" title=\"\">https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml</a>. Everyone is welcome to try it out and view the results at the end.</span></span></span></p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realpod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce the audio-based evaluation for podcasts, which treats speech as one component and assesses the overall audio performance, including speech, music and sound effects (MSE), and their interactions. Similarly, we first introduce the following <span class=\"ltx_text ltx_font_bold\">Objective metrics</span>:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness</span>: Loudness ensures audio falls within an acceptable volume range. The ITU-R BS.1770-4 standard <cite class=\"ltx_cite ltx_citemacro_citep\">(BS Series, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib9\" title=\"\">2011</a>)</cite> is widely recognized for measuring audio loudness and true-peak levels. Based on this, the <cite class=\"ltx_cite ltx_citemacro_citep\">(EBU R128, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib15\" title=\"\">2011</a>)</cite> standard has been broadly adopted by broadcast and streaming platforms, recommend a target Integrated Loudness (LOUD-IT) of -23 LUFS (&#177;1 LUFS), True Peak (LOUD-TP) <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP and Loudness Range (LOUD-RA) <math alttext=\"&lt;20\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">&lt;20</annotation></semantics></math> LU. For podcast-like streaming, adjustments are made for typical listening environments, such as mobile devices where headphones are commonly used. In these cases, the LOUD-IT is recommended as -18, -16 (&#177;1) or -14 LUFS <cite class=\"ltx_cite ltx_citemacro_citep\">(AES, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib2\" title=\"\">2021</a>; Apple, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib4\" title=\"\">2023</a>; Spotify, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib38\" title=\"\">2023</a>)</cite>. Netflix recommends keeping LOUD-RA between 4 and 18 LU <cite class=\"ltx_cite ltx_citemacro_citep\">(Netflix, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib28\" title=\"\">2024</a>)</cite>. There is no &#8220;absolute right&#8221; reference for loudness metrics. We propose the following reference standards considering all above guidelines: <span class=\"ltx_text ltx_font_bold\">LOUD-IT:</span> <math alttext=\"-18\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>18</mn></mrow><annotation encoding=\"application/x-tex\">-18</annotation></semantics></math> to <math alttext=\"-14\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>14</mn></mrow><annotation encoding=\"application/x-tex\">-14</annotation></semantics></math> LUFS; <span class=\"ltx_text ltx_font_bold\">LOUD-TP:</span> <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP; <span class=\"ltx_text ltx_font_bold\">LOUD-RA:</span> 4 to 18 LU. Based on this &#8220;relatively correct&#8221; reference, we can analyze the distribution of loudness metrics across different systems. We also provide a quantitative scoring strategy in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS4\" title=\"A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "quantitative",
                    "true",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> is primarily designed based on the perceptions of real users. A key challenge lies in how to evaluate extra-long audios. As we mentioned above, podcasts in the real world range from a few minutes to over an hour in length. Conducting listening tests on full-length podcast episodes is impractical due to the time, effort, and financial resources required. Moreover, it is hard to judge podcasts of vastly different lengths in a fair and consistent manner. Research on long-form audio evaluation is limited. <cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib13\" title=\"\">2019</a>)</cite> did investigation on long-form speech evaluation and found that multiple evaluations are necessary due to the low correlation observed across different experimental settings. <cite class=\"ltx_cite ltx_citemacro_cite\">Cambre et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib11\" title=\"\">2020</a>)</cite> conducted a comprehensive evaluation of voice selection for long-form content; however, the minimum required listening time was only 10 seconds. A podcast-related evaluation study <cite class=\"ltx_cite ltx_citemacro_citep\">(Austria, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib6\" title=\"\">2007</a>)</cite> designed a questionnaire with carefully crafted questions in terms of both content and presentation to assess domain-specific podcasts. Different from that, PodEval does not constrain the domain of podcasts, and open-ended content evaluation is separately conducted in the text-based evaluation section. In this audio-based evaluation, we focus on assessing the overall performance of the audios. The design approach is as follows:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text-based evaluation is conducted among GPT-4, PodAgent, MoonCast, and Real-Pod. Other systems in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Podcast generation &#8227; 2 Related work &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> are excluded as they do not provide conversation scripts. PodAgent, with its Host-Guest-Writer multi-agent system, can directly generate podcast scripts based on a given topic. While MoonCast functions similarly to NotebookLM, requiring external knowledge sources but providing prompt template for spontaneous script generation. For this evaluation, the MoonCast system uses the podcast scripts generated by PodAgent as input and transforms them into a spontaneous version.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "textbased",
                    "realpod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Metrics.</span> Detailed scores calculated across 17 podcast categories for each system are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS1\" title=\"A.2.1 Quantitative metrics &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.1</span></a>. For a concise and clearer comparison, we present the overall performance (averaged across all 17 categories) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F2\" title=\"Figure 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where we can observe that: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> For each quantitative metric, PodAgent outperforms directly prompting GPT-4; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> When comparing LLM-based methods (GPT-4, PodAgent) with human-created podcasts (Real-Pod), Real-Pod scores lower on lexical diversity (Distinct-2 and MATTR) but higher on information density and semantic diversity (Info-Dens and Sem-Div). This is reasonable for: i) real human interactions often include filler words and use simpler language; ii) most real podcasts are significantly longer (30 minutes to an hour), leading to higher information richness compared to generated podcasts, which are usually only a few minutes long; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> As a spontaneous version of PodAgent, MoonCast shows reduced lexical diversity and information density. While its semantic diversity remains comparable to PodAgent.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "quantitative",
                    "mattr",
                    "realpod",
                    "metrics",
                    "distinct2",
                    "semdiv",
                    "infodens"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> This evaluation compares PodAgent (scored from -3 to 3) with GPT-4 (reference score as 0), both of which generate conversation scripts without external knowledge resources. Detailed scores for each category are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS2\" title=\"A.2.2 LLM-as-a-Judge &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.2</span></a>. We present the overall performance and results for five specific categories in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T2\" title=\"Table 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for analysis. We can see that scores across all metrics and all categories are positive, demonstrating that PodAgent significantly outperforms directly prompting GPT-4 in generating podcast scripts across all evaluated dimensions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DNSMOS.</span> The DNSMOS metric was applied to all systems to evaluate speech quality as in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(4). PodAgent, MoonCast, MuyanTTS, MOSS-TTSD, and NotebookLM achieve similar scores, while Real-Pod and Dia show noticeable declines in speech quality. For Real-Pod, the lower scores are due to: (1) real podcasts often use MSE for enhancement, requiring speech-MSE separation before evaluation, which may leave residual MSE artifacts, and (2) human-created podcasts involve recording, editing, or post-processing that introduce noise or instability. Dia struggles with long-form speech synthesis. Its outputs for lengthy podcast scripts frequently feature overly fast speaking speeds and occasional sentence truncations, leading to its relatively low DNSMOS performance.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realpod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation.</span> We released the task on Prolific<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.prolific.com/\" title=\"\">https://www.prolific.com/</a></span></span></span>, requesting 20 native English-speaking participants from the US/UK. We set the filter rules as: 1) Over 90% of LQ samples must be marked as the worst, as the synthesized samples from eSpeak are obviously robotic and unnatural. 2) Over 50% of HQ samples must rank in the top-2 best. While it is possible for other systems to achieve a better score than the real podcast, the evaluation of the real podcast should also remain above average. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T3\" title=\"Table 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the two statistical metrics for the submission results. Based on these rules, Judger-7, 18 and 20 can be excluded. We also provide the box plot for each Judger in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F9\" title=\"Figure 9 &#8227; A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix for more advanced analysis. For instance, apart from the LQ samples, Judger-20 assigns similar scores to all other systems, further confirming the invalidity of this submission.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Result Analysis.</span> After excluding unqualified submissions, we analyzed system performance based on the remaining 17 valid submissions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F4\" title=\"Figure 4 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the final results. We can observe that dialogue segments from real podcasts (HQ) achieved the highest scores, which aligns with expectations. NotebookLM, a closed-source product, ranked second, reflecting the high naturalness of its synthesized dialogue speech. Among the three open-source podcast generation systems, PodAgent scored the lowest, which is reasonable since its backend TTS system, CosyVoice2, is limited to single-sentence synthesis. In contrast, MoonCast and MOSS-TTSD, which support direct dialogue synthesis, performed better in dialogue naturalness evaluations. Overall, the evaluation results align with expectations, validating the rationality and effectiveness of our evaluation method design.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the density distribution of loudness-related metrics, enabling a comparative analysis with the reference range. All seven systems are included. For <span class=\"ltx_text ltx_font_bold\">LOUD_IT</span>, Dia and MoonCast align well with the reference range, while NotebookLM&#8217;s loudness centers around -25. Real-Pod, as manually produced audio, shows a highly scattered loudness distribution. For <span class=\"ltx_text ltx_font_bold\">LOUD_TP</span>, Muyan-TTS performs best, with all samples maintaining a true peak loudness below -1. In contrast, MoonCast, Dia and MOSS-TTSD perform poorly, while Real-Pod continues to exhibit scattered results. For <span class=\"ltx_text ltx_font_bold\">LOUD_RA</span>, MoonCast has a relatively narrow loudness variation range, while PodAgent and MOSS-TTSD display richer variance. Quantitative scores are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "quantitative",
                    "true",
                    "metrics",
                    "realpod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SMR and CASP.</span> PodAgent and Real-Pod are evaluated for these MSE-related metrics. From the density distribution in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, PodAgent exhibits a more concentrated distribution compared to Real-Pod. For <span class=\"ltx_text ltx_font_bold\">SMR</span>, the SMR_SCORE in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows that all PodAgent samples achieve an SMR greater than 0, whereas some Real-Pod cases fail to meet this requirement. For <span class=\"ltx_text ltx_font_bold\">CASP</span>, a higher score indicates better MSE-Speech harmony. Real-Pod demonstrates a higher upper limit, which is expected as exceptional human artistic creations naturally surpass AI-generated outputs. However, PodAgent delivers more consistent performance, and the overall gap between the two systems is not significant, making it an alternative way to enhance creative efficiency.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "metrics",
                    "realpod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Speech (naturalness and authenticity) is the most dominant factor affecting the listener&#8217;s experience</span>. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, PodAgent scored low in dialogue naturalness due to using a single-sentence synthesis TTS system, leading to consistently poor results in this MOS test. This outcome is expected, as dialogue speech is the core component of podcast-like audio programs. Although PodAgent&#8217;s Music/Sound (harmony) score is below Real-Pod (consistent with the results of objective metric - CASP), it is significantly higher than its scores in other metrics, indicating that <span class=\"ltx_text ltx_font_italic\">the gap between PodAgent and Real-Pod in music harmony is smaller than in speech naturalness</span>.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "realpod",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Real podcasts perform best in most metrics (5/7).</span> Real-Pod significantly outperforms other systems on holistic metrics like Engagement Level (EL) and Human Likelihood (HL). However, Full Episode Willingness (FEW) scores are low across all systems, with NotebookLM and Real-Pod scoring similarly. <span class=\"ltx_text ltx_font_italic\">This highlights the value of perceptual and preference-based question design in the test.</span> FEW, a preference-based question, garnered justifications like &#8220;the topic is not of interest to me&#8221; for lower scores. In contrast, higher scores for EL and HL indicate that users tend to exclude subjective factors (e.g., personal topic interest) when rating audio performance. A similar pattern is observed in Information Delivery (effectiveness) and Speaker Expression (preference).</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "realpod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Audio Quality metric, while PodAgent and MOSS-TTSD score lower than Real-Pod, PodAgent performs better here than in other metrics, and NotebookLM slightly surpasses Real-Pod. As noted, human-made podcasts often exhibit inconsistent audio quality due to complex production. User feedback, like &#8220;Little bit of mic hiss/bloom but otherwise fine,&#8221; supports this observation. This highlights that <span class=\"ltx_text ltx_font_italic\">when conversational realism approaches that of real speech, AI-based methods offers an advantage in their controllability and consistency in producing high-quality audio</span>.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "realpod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PodEval is the first comprehensive evaluation framework for podcast-like audio generation, tackling the challenges of assessing open-ended, long-form content. We constructed a real-world podcast dataset as a benchmark for human-level creative quality across diverse topics and formats. By decomposing evaluation into text, speech, and audio, PodEval introduced multidimensional methods combining objective metrics and well-designed subjective listening tests. Experiments with various podcast generation systems, including open-source, closed-source, and human-made examples, validated the framework&#8217;s effectiveness. The results offer insights into the strengths and weaknesses of different systems (e.g. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F14\" title=\"Figure 14 &#8227; A.6 System analysis report &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>), highlighting PodEval&#8217;s role in advancing podcast generation research and inspiring future work on evaluating open-ended, long-form content generation task.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tools for <span class=\"ltx_text ltx_font_italic\">text-based evaluation</span> of dialogue scripts. Includes both <span class=\"ltx_text ltx_font_italic\">Quantitative Metrics</span> and <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> methods.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quantitative",
                    "metrics",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large Language Models (LLMs) utilized in this work are as follows:(1) <span class=\"ltx_text ltx_font_italic\">Topics Initiation</span> during data processing of the Real-Pod dataset, which is elaborated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S3\" title=\"3 Real-Pod: Real-world podcast dataset &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. (2) <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> method in text-based evaluation, which is illustrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S4\" title=\"4 Text-based evaluation &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. (3) <span class=\"ltx_text ltx_font_italic\">Summarized Users&#8217; Justifications</span> in the Questionnaire-based MOS Test, which is described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS3\" title=\"7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.3</span></a> (Questionnaire-based MOS Test).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "textbased",
                    "realpod"
                ]
            }
        ]
    },
    "A1.T8": {
        "source_file": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
        "caption": "Table 8:  LLM-as-a-Judge: comparison between GPT-4 and PodAgent. Scores range from -3 to 3. Positive values indicate that PodAgent outperforms GPT-4; Negative values suggest the opposite.",
        "body": "Metrics\nOverall\nFiction\nEducation\nBusiness\nTrue Crime\nHealth & Fitness\n\n\n\n\nCoherence\n0.7059\n0.5000\n0.8333\n1.0000\n1.0000\n0.6667\n\n\nEngagingness\n1.0294\n1.1667\n1.0000\n1.1667\n0.6667\n1.1667\n\n\nDiversity\n1.1765\n1.3333\n1.0000\n1.3333\n0.8333\n1.5000\n\n\nInformativeness\n1.6078\n1.5000\n1.6667\n2.0000\n1.1667\n1.6667\n\n\nSpeaker Difference\n1.0637\n0.9167\n1.0000\n1.1667\n0.6667\n1.0000\n\n\nOverall\n1.3064\n1.2500\n1.3333\n1.6667\n0.8333\n1.2500\n\n\nMetrics\nSports\nComedy\nHistory\nNews\nTV & Film\nSociety & Culture\n\n\nCoherence\n0.5000\n0.8333\n1.1667\n0.6667\n0.8333\n0.1667\n\n\nEngagingness\n1.1667\n1.5000\n1.5000\n0.6667\n0.1667\n0.6667\n\n\nDiversity\n1.1667\n1.8333\n1.5000\n1.3333\n1.3333\n0.8333\n\n\nInformativeness\n1.5000\n2.1667\n1.5000\n2.0000\n1.3333\n0.8333\n\n\nSpeaker Difference\n1.1667\n1.5000\n1.1667\n1.3333\n1.1667\n1.3333\n\n\nOverall\n1.5000\n1.8333\n1.5000\n1.5000\n0.8333\n0.5000\n\n\nMetrics\nArts\nLeisure\nMusic\nKids\nMental Health\nScience & Tech\n\n\nCoherence\n0.6667\n0.5000\n0.6667\n0.5000\n0.3333\n1.1667\n\n\nEngagingness\n1.1667\n1.1667\n1.1667\n1.0000\n0.8333\n1.3333\n\n\nDiversity\n1.1667\n1.1667\n1.0000\n1.0000\n0.3333\n1.3333\n\n\nInformativeness\n1.8333\n2.0000\n1.8333\n1.3333\n1.1667\n1.8333\n\n\nSpeaker Difference\n1.3333\n1.1667\n0.8333\n0.8333\n0.6667\n0.8333\n\n\nOverall\n1.5000\n1.6667\n1.5000\n1.1667\n0.8333\n1.5417",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"--ltx-bg-color:#FFFF00;padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFFF00;\">Overall</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Fiction</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Education</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Business</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">True Crime</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Health &amp; Fitness</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Coherence</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">0.7059</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.5000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.6667</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Engagingness</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">1.0294</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Diversity</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">1.1765</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.3333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.3333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Informativeness</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">1.6078</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.6667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">2.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.6667</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Speaker Difference</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">1.0637</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.9167</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.0000</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Overall</th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFFFE6;padding:1.5pt 5.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFFFE6;\">1.3064</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.2500</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.3333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.6667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.2500</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Sports</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Comedy</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">History</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">News</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">TV &amp; Film</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Society &amp; Culture</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Coherence</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.5000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.6667</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.1667</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Engagingness</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6667</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Diversity</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.8333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.3333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.3333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Informativeness</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">2.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">2.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.3333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Speaker Difference</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.3333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.3333</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Overall</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.8333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.5000</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Metrics</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Arts</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Leisure</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Music</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Kids</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Mental Health</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">Science &amp; Tech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">Coherence</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.6667</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.5000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.6667</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.5000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">0.3333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Engagingness</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.3333</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Diversity</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.3333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.3333</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Informativeness</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.8333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">2.0000</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.8333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.3333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.8333</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 5.0pt;\">Speaker Difference</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.3333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.6667</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">Overall</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">1.6667</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">1.5000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">1.1667</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">0.8333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 5.0pt;\">1.5417</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "arts",
            "true",
            "difference",
            "comedy",
            "business",
            "crime",
            "society",
            "coherence",
            "kids",
            "tech",
            "range",
            "fiction",
            "suggest",
            "overall",
            "scores",
            "metrics",
            "mental",
            "music",
            "gpt4",
            "sports",
            "film",
            "podagent",
            "culture",
            "opposite",
            "comparison",
            "positive",
            "outperforms",
            "fitness",
            "negative",
            "from",
            "engagingness",
            "health",
            "llmasajudge",
            "indicate",
            "education",
            "informativeness",
            "diversity",
            "speaker",
            "news",
            "leisure",
            "science",
            "values",
            "between",
            "history"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">With the rapid development of AIGC (AI-Generated Content) in recent years, many innovative applications have emerged. AI Podcast represents a key application scenario for audio-based generative models <cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib17\" title=\"\">2023</a>; ByteDance, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib10\" title=\"\">2025</a>)</cite>. However, evaluating podcast-like audio is challenging due to: 1) it is an open-ended task, which means there is no reference standard answer; 2) the evaluation of long-form speech/audio is particularly difficult, as longer formats introduce more variability. Objective metrics often fail to capture human perceptions accurately, while subjective listening tests face issues like user inattention, which reduces the validity of results; and 3) podcasts often incorporate additional elements, like music and sound effects, making the evaluation more complicated.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most content-related objective metrics rely on reference scripts to measure quality and relevance. However, podcast generation lacks standardized references as it is an open-ended generation task. Moreover, relying on such references limits the diversity and creativity of the generated content.</p>\n\n",
                "matched_terms": [
                    "diversity",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While music and sound effects are not essential to every podcast, their evaluation, when present, should go beyond the quality of individual audio events. Instead, it should focus on their overall harmony and seamless integration with the speech content to enhance the listener&#8217;s experience.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each modality, we design tailored metrics to address diversity considerations. For text, we combine quantitative metrics with LLM-based evaluations to assess conversation scripts. For speech and audio, we design objective metrics and subjective listening tests to evaluate spoken dialogue and overall audio performance. All evaluation methods are organized into open-source tools for ease of use. Subjective tests are enhanced by spammer detection to improve data validity.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "diversity",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Various evaluation works have emerged along with the development of LLMs and multimodal generative models. <span class=\"ltx_text ltx_font_bold\">Text-related Evaluation</span>, such as SuperGLUE, MMLU, and BIG-bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib41\" title=\"\">2019</a>; Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib18\" title=\"\">2020</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib39\" title=\"\">2022</a>)</cite>, assesses the capabilities of LLMs across diverse tasks with preset ground truth. Subsequently, MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib47\" title=\"\">2023</a>)</cite> explores the potential of LLMs as evaluators, and Chatbot Arena <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib12\" title=\"\">2024</a>)</cite> provides an open platform for assessing LLMs based on human preferences. <span class=\"ltx_text ltx_font_bold\">Speech-related Evaluation</span>, such as SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib45\" title=\"\">2021</a>)</cite>, is designed for discriminative tasks like speech recognition and speaker identification. However, evaluations for generative tasks are scarce due to their inherent diversity and subjectivity, making subjective evaluation essential for speech generation tasks. For instance, VOCBENCH <cite class=\"ltx_cite ltx_citemacro_citep\">(AlBadawy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib3\" title=\"\">2022</a>)</cite> incorporates both subjective and objective evaluations to assess vocoder performance in speech synthesis. Similarly, numerous <span class=\"ltx_text ltx_font_bold\">Audio-related Evaluation</span> work, such as AIR-Bench, Audiobench, MMAU, and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib44\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib42\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib35\" title=\"\">2024</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib26\" title=\"\">2025</a>)</cite>, focus on audio understanding and reasoning. Subjective evaluation remains crucial for assessing audio generation systems and is typically tailored to specific generation tasks. Unlike existing evaluation works, <span class=\"ltx_text ltx_font_bold\">PodEval</span> introduces a comprehensive framework specifically designed for podcast-like audio generation. It emphasizes both subjective and objective evaluations across text, speech, and audio, with all metrics closely aligned with real-world user experience.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "diversity",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There is no unified standard for defining &#8220;what makes a good podcast episode.&#8221; Unlike textbooks or official TV programs, podcasts can be created by anyone to share their unique ideas or insights. We do not make direct comparisons between generated podcasts and real podcasts&#8212;such comparisons are inherently unfeasible, especially when they approach topics from entirely different perspectives. Instead, we construct a real-world podcast dataset, called <span class=\"ltx_text ltx_font_bold\">Real-Pod</span> dataset, to serve as a reference for human-level creative quality. It is important to note that this dataset acts as a &#8220;reference&#8221; rather than an absolute &#8220;answer&#8221;. The design principles of the Real-Pod dataset are <span class=\"ltx_text ltx_font_bold\">real</span> (consists of human-made podcasts), <span class=\"ltx_text ltx_font_bold\">broad</span> (diverse topic coverage) and <span class=\"ltx_text ltx_font_bold\">rich</span> (varied formats, like multi-speaker, music and sound). The workflow for constructing the Real-Pod dataset is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S3.F1\" title=\"Figure 1 &#8227; 3 Real-Pod: Real-world podcast dataset &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>:</p>\n\n",
                "matched_terms": [
                    "between",
                    "from",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dialogue content in podcasts is extracted in text format for evaluation, representing the core message the podcast aims to convey. Podcast dialogues often center around specific topics, showcasing participants&#8217; unique perspectives and insights, which makes reference-correlation-based methods infeasible. Instead, the richness of perspectives conveyed (to provide informative takeaways for the listener) and the presentation style of the dialogue (to enhance listener comprehension) should be the primary focus of evaluation. Therefore, we follow the dialogue script-based evaluation methods proposed in PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite>, which adopt a two-fold approach: (1) <span class=\"ltx_text ltx_font_bold\">Quantitative Metrics</span> such as Distinct-N, Semantic-Div, MATTR, and Info-Dens to assess lexical diversity, semantic richness, vocabulary richness, and information density, respectively. These metrics operate independently of reference texts and focus on intrinsic text characteristics; (2) <span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge</span>, leveraging GPT-4 to replace human evaluators for complex and comprehensive assessments. Evaluation criteria include coherence, engagingness, diversity, informativeness and speaker diversity. It incorporates comparative evaluations to reduce bias and evidence-based scoring for robust and reliable results.</p>\n\n",
                "matched_terms": [
                    "gpt4",
                    "podagent",
                    "llmasajudge",
                    "informativeness",
                    "diversity",
                    "speaker",
                    "coherence",
                    "engagingness",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SIM</span> stands for Speaker Similarity. In podcast generation systems, zero-shot TTS is often employed to replicate the voice of a preset speaker. The SIM between the synthesized voice and the reference voice serves as a crucial metric about vocal fidelity. In PodEval, SIM is quantified using the cosine similarity of extracted speaker embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Plaquet &amp; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib31\" title=\"\">2023</a>; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib8\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "between",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SPTD</span> is a brand new metric we proposed, standing for Speaker Timbre Difference. As audio programs, podcasts are accessible only through listening. In multi-speaker conversations, voices with greater timbre differences enhance clarity and make the information easier to follow. SPTD is to assess the overall timbre variation across speakers. Equation <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S5.E1\" title=\"In 5 Speech-based evaluation &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> calculates the SPTD among <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I1.i4.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> distinct speakers.</p>\n\n",
                "matched_terms": [
                    "difference",
                    "speaker",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Objective metrics can be calculated efficiently at a low cost without human involvement. However, the <span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> remains a necessary indicator of human perception. Unlike general speech synthesis, which emphasizes sentence-level pronunciation accuracy and naturalness, podcast speech focuses on achieving human-like natural dialogue. Subjective tests for such long-form speech present several key <span class=\"ltx_text ltx_font_bold\">challenges</span>: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> the length of dialogue in podcasts ranges from a few minutes to over an hour, making it impractical to evaluate the entire speech directly; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> the difficulty of comparing more than two systems simultaneously; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> guiding user focus toward dialogue naturalness, rather than on factors like content; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">4)</span> balancing topic diversity within a fixed testing capacity; and <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">5)</span> ensuring that crowdsourced evaluators remain focused and provide reliable feedback.</p>\n\n",
                "matched_terms": [
                    "from",
                    "diversity",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In PodEval, we design the <span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation</span> based on the MUSHRA framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Schoeffler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib36\" title=\"\">2018</a>)</cite>. The key insight from this framework, <span class=\"ltx_text ltx_font_italic\">incorporating both high-quality and low-quality anchors</span>, helps evaluators establish a reliable reference of quality range. For researchers, analyzing scores for these anchors helps identify inattentive evaluators, enabling the <span class=\"ltx_text ltx_font_italic\">filtering of invalid submissions</span> and improving the data vadility. In our task, we use real podcast segments from the <span class=\"ltx_text ltx_font_italic\">Real-Pod dataset as the high-quality anchor</span> and synthesized dialogue segments from <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib16\" title=\"\">eSpeak Contributors </a></cite><span class=\"ltx_text ltx_font_italic\"> as the low-quality anchor</span>. For podcast samples from different systems, we provide an automatic toolkit to extract dialogue segments featuring <span class=\"ltx_text ltx_font_italic\">turn-taking</span> between speakers, representing a typical dialogue flow. Each dialogue segment is extracted with a <span class=\"ltx_text ltx_font_italic\">preset length</span> (e.g. 15&#8211;25 seconds) to ensure the speech samples are of similar duration. We select dialogue segments from <span class=\"ltx_text ltx_font_italic\">all 17 categories</span> in the Real-Pod dataset to ensure content diversity while keeping the total listening test duration <span class=\"ltx_text ltx_font_italic\">within 30 minutes</span>. In each test group, samples from different systems are presented <span class=\"ltx_text ltx_font_italic\">on the same page</span>, along with a <span class=\"ltx_text ltx_font_italic\">reference Real-Pod sample</span> to guide evaluators on what a natural dialogue sounds like. The scoring is adjusted using a slider ranging from 0 to 100, divided into <span class=\"ltx_text ltx_font_italic\">five stages with a clear definition</span>. Detailed instructions and website design can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS3\" title=\"A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The demo website is hosted at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml\" title=\"\">https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml</a>. Everyone is welcome to try it out and view the results at the end.</span></span></span></p>\n\n",
                "matched_terms": [
                    "diversity",
                    "between",
                    "from",
                    "scores",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce the audio-based evaluation for podcasts, which treats speech as one component and assesses the overall audio performance, including speech, music and sound effects (MSE), and their interactions. Similarly, we first introduce the following <span class=\"ltx_text ltx_font_bold\">Objective metrics</span>:</p>\n\n",
                "matched_terms": [
                    "overall",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness</span>: Loudness ensures audio falls within an acceptable volume range. The ITU-R BS.1770-4 standard <cite class=\"ltx_cite ltx_citemacro_citep\">(BS Series, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib9\" title=\"\">2011</a>)</cite> is widely recognized for measuring audio loudness and true-peak levels. Based on this, the <cite class=\"ltx_cite ltx_citemacro_citep\">(EBU R128, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib15\" title=\"\">2011</a>)</cite> standard has been broadly adopted by broadcast and streaming platforms, recommend a target Integrated Loudness (LOUD-IT) of -23 LUFS (&#177;1 LUFS), True Peak (LOUD-TP) <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP and Loudness Range (LOUD-RA) <math alttext=\"&lt;20\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">&lt;20</annotation></semantics></math> LU. For podcast-like streaming, adjustments are made for typical listening environments, such as mobile devices where headphones are commonly used. In these cases, the LOUD-IT is recommended as -18, -16 (&#177;1) or -14 LUFS <cite class=\"ltx_cite ltx_citemacro_citep\">(AES, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib2\" title=\"\">2021</a>; Apple, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib4\" title=\"\">2023</a>; Spotify, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib38\" title=\"\">2023</a>)</cite>. Netflix recommends keeping LOUD-RA between 4 and 18 LU <cite class=\"ltx_cite ltx_citemacro_citep\">(Netflix, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib28\" title=\"\">2024</a>)</cite>. There is no &#8220;absolute right&#8221; reference for loudness metrics. We propose the following reference standards considering all above guidelines: <span class=\"ltx_text ltx_font_bold\">LOUD-IT:</span> <math alttext=\"-18\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>18</mn></mrow><annotation encoding=\"application/x-tex\">-18</annotation></semantics></math> to <math alttext=\"-14\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>14</mn></mrow><annotation encoding=\"application/x-tex\">-14</annotation></semantics></math> LUFS; <span class=\"ltx_text ltx_font_bold\">LOUD-TP:</span> <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP; <span class=\"ltx_text ltx_font_bold\">LOUD-RA:</span> 4 to 18 LU. Based on this &#8220;relatively correct&#8221; reference, we can analyze the distribution of loudness metrics across different systems. We also provide a quantitative scoring strategy in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS4\" title=\"A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "between",
                    "true",
                    "metrics",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SMR</span> (Speech-to-Music Ratio): MSE are typically integrated into podcast audio to enhance the overall listening experience. Since speech is the primary focus in podcasts, it is essential to ensure that MSE dose not overpower or mask the speech, maintaining clarity and intelligibility of the dialogue. SMR measures the balance between speech and MSE, with a minimum requirement of being greater than 0. SMR_SCORE is the proportion of cases where SMR exceeds 0.</p>\n\n",
                "matched_terms": [
                    "between",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> is primarily designed based on the perceptions of real users. A key challenge lies in how to evaluate extra-long audios. As we mentioned above, podcasts in the real world range from a few minutes to over an hour in length. Conducting listening tests on full-length podcast episodes is impractical due to the time, effort, and financial resources required. Moreover, it is hard to judge podcasts of vastly different lengths in a fair and consistent manner. Research on long-form audio evaluation is limited. <cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib13\" title=\"\">2019</a>)</cite> did investigation on long-form speech evaluation and found that multiple evaluations are necessary due to the low correlation observed across different experimental settings. <cite class=\"ltx_cite ltx_citemacro_cite\">Cambre et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib11\" title=\"\">2020</a>)</cite> conducted a comprehensive evaluation of voice selection for long-form content; however, the minimum required listening time was only 10 seconds. A podcast-related evaluation study <cite class=\"ltx_cite ltx_citemacro_citep\">(Austria, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib6\" title=\"\">2007</a>)</cite> designed a questionnaire with carefully crafted questions in terms of both content and presentation to assess domain-specific podcasts. Different from that, PodEval does not constrain the domain of podcasts, and open-ended content evaluation is separately conducted in the text-based evaluation section. In this audio-based evaluation, we focus on assessing the overall performance of the audios. The design approach is as follows:</p>\n\n",
                "matched_terms": [
                    "from",
                    "overall",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The test data are preprocessed by extracting <span class=\"ltx_text ltx_font_bold\">the first / middle / final minute</span>. These segments are concatenated into a single audio, separated by a beep signal. This method unifies podcast length, captures overall performance from diverse positions, and minimizes content-related biases.</p>\n\n",
                "matched_terms": [
                    "from",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implement two strategies to enhance the validity of the collected data. <span class=\"ltx_text ltx_font_bold\">1) Attention-check questions:</span> These include questions like <span class=\"ltx_text ltx_font_italic\">Q1. How many speakers are there in the podcast?</span> and <span class=\"ltx_text ltx_font_italic\">Q7. If music or sound effects&#8230; (Select Neutral if none are present)</span>. These questions have standard answers, allowing us to determine whether users are actively listening to the audio. <span class=\"ltx_text ltx_font_bold\">2) Justification for answers:</span> Users have to provide justifications for their responses to each question, which can be short but are required. This requirement significantly increases users&#8217; focus and we can collect more detailed information from their justification. By employing these two strategies, data validity is enhanced by promoting attentiveness and filtering out unreliable responses.</p>\n\n",
                "matched_terms": [
                    "from",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text-based evaluation is conducted among GPT-4, PodAgent, MoonCast, and Real-Pod. Other systems in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Podcast generation &#8227; 2 Related work &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> are excluded as they do not provide conversation scripts. PodAgent, with its Host-Guest-Writer multi-agent system, can directly generate podcast scripts based on a given topic. While MoonCast functions similarly to NotebookLM, requiring external knowledge sources but providing prompt template for spontaneous script generation. For this evaluation, the MoonCast system uses the podcast scripts generated by PodAgent as input and transforms them into a spontaneous version.</p>\n\n",
                "matched_terms": [
                    "gpt4",
                    "podagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Metrics.</span> Detailed scores calculated across 17 podcast categories for each system are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS1\" title=\"A.2.1 Quantitative metrics &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.1</span></a>. For a concise and clearer comparison, we present the overall performance (averaged across all 17 categories) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F2\" title=\"Figure 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where we can observe that: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> For each quantitative metric, PodAgent outperforms directly prompting GPT-4; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> When comparing LLM-based methods (GPT-4, PodAgent) with human-created podcasts (Real-Pod), Real-Pod scores lower on lexical diversity (Distinct-2 and MATTR) but higher on information density and semantic diversity (Info-Dens and Sem-Div). This is reasonable for: i) real human interactions often include filler words and use simpler language; ii) most real podcasts are significantly longer (30 minutes to an hour), leading to higher information richness compared to generated podcasts, which are usually only a few minutes long; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> As a spontaneous version of PodAgent, MoonCast shows reduced lexical diversity and information density. While its semantic diversity remains comparable to PodAgent.</p>\n\n",
                "matched_terms": [
                    "gpt4",
                    "podagent",
                    "diversity",
                    "comparison",
                    "outperforms",
                    "overall",
                    "scores",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> This evaluation compares PodAgent (scored from -3 to 3) with GPT-4 (reference score as 0), both of which generate conversation scripts without external knowledge resources. Detailed scores for each category are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS2\" title=\"A.2.2 LLM-as-a-Judge &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.2</span></a>. We present the overall performance and results for five specific categories in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T2\" title=\"Table 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for analysis. We can see that scores across all metrics and all categories are positive, demonstrating that PodAgent significantly outperforms directly prompting GPT-4 in generating podcast scripts across all evaluated dimensions.</p>\n\n",
                "matched_terms": [
                    "gpt4",
                    "podagent",
                    "llmasajudge",
                    "positive",
                    "outperforms",
                    "from",
                    "overall",
                    "scores",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure fairness, all open-source TTS systems use the same PodAgent-generated scripts. Subjective tests use a spontaneous version from MoonCast, while objective evaluations use the original PodAgent scripts, as filler words in the spontaneous version challenge metrics like WER.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "from",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SIM.</span> The SIM metric evaluates zero-shot TTS systems&#8217; ability to replicate the timbre of a reference voice. PodAgent, MoonCast, MuyanTTS, Dia, and MOSS-TTSD&#8212;are assessed as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(2). Each system uses the reference voice selected by PodAgent for the topic. The performance rankings are: MuyanTTS, MOSS-TTSD, Dia, MoonCast, and PodAgent. PodAgent&#8217;s relatively low score in this metric likely stems from its instruction-following style control strategy. While this approach enhances overall conversational expressiveness, it can reduce speaker similarity.</p>\n\n",
                "matched_terms": [
                    "from",
                    "overall",
                    "speaker",
                    "podagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SPTD.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(3) shows timbre variation across speakers in the conversation for three systems: Real-Pod, PodAgent, and NotebookLM. Real-Pod reflects real-world podcasts, PodAgent uses a voice selection mechanism for distinct voices, and NotebookLM fixed voices (one male, one female). The SPTD scores rank as follows: PodAgent, NotebookLM, and Real-Pod. This likely reflects that real-world podcasts prioritize guest expertise and availability over timbre differences. PodAgent demonstrates an effective automated voice selection process for podcast creation.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "podagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DNSMOS.</span> The DNSMOS metric was applied to all systems to evaluate speech quality as in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(4). PodAgent, MoonCast, MuyanTTS, MOSS-TTSD, and NotebookLM achieve similar scores, while Real-Pod and Dia show noticeable declines in speech quality. For Real-Pod, the lower scores are due to: (1) real podcasts often use MSE for enhancement, requiring speech-MSE separation before evaluation, which may leave residual MSE artifacts, and (2) human-created podcasts involve recording, editing, or post-processing that introduce noise or instability. Dia struggles with long-form speech synthesis. Its outputs for lengthy podcast scripts frequently feature overly fast speaking speeds and occasional sentence truncations, leading to its relatively low DNSMOS performance.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "podagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation.</span> We released the task on Prolific<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.prolific.com/\" title=\"\">https://www.prolific.com/</a></span></span></span>, requesting 20 native English-speaking participants from the US/UK. We set the filter rules as: 1) Over 90% of LQ samples must be marked as the worst, as the synthesized samples from eSpeak are obviously robotic and unnatural. 2) Over 50% of HQ samples must rank in the top-2 best. While it is possible for other systems to achieve a better score than the real podcast, the evaluation of the real podcast should also remain above average. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T3\" title=\"Table 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the two statistical metrics for the submission results. Based on these rules, Judger-7, 18 and 20 can be excluded. We also provide the box plot for each Judger in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F9\" title=\"Figure 9 &#8227; A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix for more advanced analysis. For instance, apart from the LQ samples, Judger-20 assigns similar scores to all other systems, further confirming the invalidity of this submission.</p>\n\n",
                "matched_terms": [
                    "from",
                    "scores",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Result Analysis.</span> After excluding unqualified submissions, we analyzed system performance based on the remaining 17 valid submissions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F4\" title=\"Figure 4 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the final results. We can observe that dialogue segments from real podcasts (HQ) achieved the highest scores, which aligns with expectations. NotebookLM, a closed-source product, ranked second, reflecting the high naturalness of its synthesized dialogue speech. Among the three open-source podcast generation systems, PodAgent scored the lowest, which is reasonable since its backend TTS system, CosyVoice2, is limited to single-sentence synthesis. In contrast, MoonCast and MOSS-TTSD, which support direct dialogue synthesis, performed better in dialogue naturalness evaluations. Overall, the evaluation results align with expectations, validating the rationality and effectiveness of our evaluation method design.</p>\n\n",
                "matched_terms": [
                    "from",
                    "overall",
                    "scores",
                    "podagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the density distribution of loudness-related metrics, enabling a comparative analysis with the reference range. All seven systems are included. For <span class=\"ltx_text ltx_font_bold\">LOUD_IT</span>, Dia and MoonCast align well with the reference range, while NotebookLM&#8217;s loudness centers around -25. Real-Pod, as manually produced audio, shows a highly scattered loudness distribution. For <span class=\"ltx_text ltx_font_bold\">LOUD_TP</span>, Muyan-TTS performs best, with all samples maintaining a true peak loudness below -1. In contrast, MoonCast, Dia and MOSS-TTSD perform poorly, while Real-Pod continues to exhibit scattered results. For <span class=\"ltx_text ltx_font_bold\">LOUD_RA</span>, MoonCast has a relatively narrow loudness variation range, while PodAgent and MOSS-TTSD display richer variance. Quantitative scores are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "true",
                    "podagent",
                    "scores",
                    "metrics",
                    "range"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SMR and CASP.</span> PodAgent and Real-Pod are evaluated for these MSE-related metrics. From the density distribution in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, PodAgent exhibits a more concentrated distribution compared to Real-Pod. For <span class=\"ltx_text ltx_font_bold\">SMR</span>, the SMR_SCORE in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows that all PodAgent samples achieve an SMR greater than 0, whereas some Real-Pod cases fail to meet this requirement. For <span class=\"ltx_text ltx_font_bold\">CASP</span>, a higher score indicates better MSE-Speech harmony. Real-Pod demonstrates a higher upper limit, which is expected as exceptional human artistic creations naturally surpass AI-generated outputs. However, PodAgent delivers more consistent performance, and the overall gap between the two systems is not significant, making it an alternative way to enhance creative efficiency.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "between",
                    "from",
                    "overall",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to direct scores, we also derive a corresponding score based on users&#8217; justifications. Specifically, given justification texts from multiple systems for the same question, GPT-4 uses the following prompt to score: <span class=\"ltx_text ltx_font_italic\">&#8221;For each system, summarize the corresponding comments into one sentence and assign a score between 1 and 5.&#8221;</span> A detailed experiment setup is provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS5.SSS2\" title=\"A.5.2 Questionnaire-based MOS test &#8227; A.5 Audio-based evaluation (subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.5.2</span></a>, and separate scores are listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T10\" title=\"Table 10 &#8227; A.5.2 Questionnaire-based MOS test &#8227; A.5 Audio-based evaluation (subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F6\" title=\"Figure 6 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the final results, averaging the direct score and the justification-based score. From the result, we can observe that:</p>\n\n",
                "matched_terms": [
                    "gpt4",
                    "between",
                    "from",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Speech (naturalness and authenticity) is the most dominant factor affecting the listener&#8217;s experience</span>. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, PodAgent scored low in dialogue naturalness due to using a single-sentence synthesis TTS system, leading to consistently poor results in this MOS test. This outcome is expected, as dialogue speech is the core component of podcast-like audio programs. Although PodAgent&#8217;s Music/Sound (harmony) score is below Real-Pod (consistent with the results of objective metric - CASP), it is significantly higher than its scores in other metrics, indicating that <span class=\"ltx_text ltx_font_italic\">the gap between PodAgent and Real-Pod in music harmony is smaller than in speech naturalness</span>.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "between",
                    "scores",
                    "metrics",
                    "music"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Real podcasts perform best in most metrics (5/7).</span> Real-Pod significantly outperforms other systems on holistic metrics like Engagement Level (EL) and Human Likelihood (HL). However, Full Episode Willingness (FEW) scores are low across all systems, with NotebookLM and Real-Pod scoring similarly. <span class=\"ltx_text ltx_font_italic\">This highlights the value of perceptual and preference-based question design in the test.</span> FEW, a preference-based question, garnered justifications like &#8220;the topic is not of interest to me&#8221; for lower scores. In contrast, higher scores for EL and HL indicate that users tend to exclude subjective factors (e.g., personal topic interest) when rating audio performance. A similar pattern is observed in Information Delivery (effectiveness) and Speaker Expression (preference).</p>\n\n",
                "matched_terms": [
                    "indicate",
                    "speaker",
                    "outperforms",
                    "scores",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Audio Quality metric, while PodAgent and MOSS-TTSD score lower than Real-Pod, PodAgent performs better here than in other metrics, and NotebookLM slightly surpasses Real-Pod. As noted, human-made podcasts often exhibit inconsistent audio quality due to complex production. User feedback, like &#8220;Little bit of mic hiss/bloom but otherwise fine,&#8221; supports this observation. This highlights that <span class=\"ltx_text ltx_font_italic\">when conversational realism approaches that of real speech, AI-based methods offers an advantage in their controllability and consistency in producing high-quality audio</span>.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tools for <span class=\"ltx_text ltx_font_italic\">text-based evaluation</span> of dialogue scripts. Includes both <span class=\"ltx_text ltx_font_italic\">Quantitative Metrics</span> and <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> methods.</p>\n\n",
                "matched_terms": [
                    "llmasajudge",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Experiment Settings:</span> Lengthy listening tests can be exhausting and may lead to inaccurate feedback. It is essential to ensure the overall test duration does not exceed 30 minutes. In the Questionnaire-based MOS Test, each audio sample is around 3 minutes and requires answering 10 questions with corresponding justifications. Based on the Dialogue Naturalness Test results shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, we selected 4 representative systems. Each test group included four podcast samples from different systems but within the same podcast category. According to actual test results, each group took an average of 24 minutes to complete. The 4 representative systems are:</p>\n\n",
                "matched_terms": [
                    "from",
                    "overall"
                ]
            }
        ]
    },
    "A1.T9": {
        "source_file": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
        "caption": "Table 9: Audio-based objective metrics - Quantitative scores.",
        "body": "System\nLOUD_IT_SCORE\nLOUD_TP_SCORE\nLOUD_LRA_SCORE\nSMR_BASIC_SCORE\nCASP\n\n\n\n\nReal-Pod\n0.72\n0.53\n0.82\n0.99\n0.58\n\n\nPodAgent\n0.80\n0.32\n1.00\n1.00\n0.56\n\n\nMoonCast\n1.00\n0.01\n0.68\n-\n-\n\n\nMuyan-TTS\n0.88\n1.00\n0.83\n-\n-\n\n\nDia\n0.98\n0.01\n0.95\n-\n-\n\n\nMOSS-TTSD\n0.88\n0.02\n0.99\n-\n-\n\n\nNotebookLM\n0.51\n0.56\n1.00\n-\n-",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">System</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">LOUD_IT_SCORE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">LOUD_TP_SCORE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">LOUD_LRA_SCORE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">SMR_BASIC_SCORE</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.5pt 4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F2F2F2;\">CASP</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.5pt 4.0pt;\">Real-Pod</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 4.0pt;\">0.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 4.0pt;\">0.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 4.0pt;\">0.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 4.0pt;\">0.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.5pt 4.0pt;\">0.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 4.0pt;\">PodAgent</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.80</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.32</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 4.0pt;\">MoonCast</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.68</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 4.0pt;\">Muyan-TTS</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.88</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">1.00</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 4.0pt;\">Dia</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.98</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.01</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.95</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.5pt 4.0pt;\">MOSS-TTSD</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.88</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.02</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">0.99</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:1.5pt 4.0pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:1.5pt 4.0pt;\">NotebookLM</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 4.0pt;\">0.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 4.0pt;\">0.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 4.0pt;\">1.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 4.0pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.5pt 4.0pt;\">-</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "audiobased",
            "system",
            "smrbasicscore",
            "dia",
            "loudtpscore",
            "podagent",
            "louditscore",
            "mossttsd",
            "notebooklm",
            "objective",
            "quantitative",
            "muyantts",
            "realpod",
            "mooncast",
            "casp",
            "loudlrascore",
            "scores",
            "metrics"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the density distribution of loudness-related metrics, enabling a comparative analysis with the reference range. All seven systems are included. For <span class=\"ltx_text ltx_font_bold\">LOUD_IT</span>, Dia and MoonCast align well with the reference range, while NotebookLM&#8217;s loudness centers around -25. Real-Pod, as manually produced audio, shows a highly scattered loudness distribution. For <span class=\"ltx_text ltx_font_bold\">LOUD_TP</span>, Muyan-TTS performs best, with all samples maintaining a true peak loudness below -1. In contrast, MoonCast, Dia and MOSS-TTSD perform poorly, while Real-Pod continues to exhibit scattered results. For <span class=\"ltx_text ltx_font_bold\">LOUD_RA</span>, MoonCast has a relatively narrow loudness variation range, while PodAgent and MOSS-TTSD display richer variance. Quantitative scores are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SMR and CASP.</span> PodAgent and Real-Pod are evaluated for these MSE-related metrics. From the density distribution in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, PodAgent exhibits a more concentrated distribution compared to Real-Pod. For <span class=\"ltx_text ltx_font_bold\">SMR</span>, the SMR_SCORE in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows that all PodAgent samples achieve an SMR greater than 0, whereas some Real-Pod cases fail to meet this requirement. For <span class=\"ltx_text ltx_font_bold\">CASP</span>, a higher score indicates better MSE-Speech harmony. Real-Pod demonstrates a higher upper limit, which is expected as exceptional human artistic creations naturally surpass AI-generated outputs. However, PodAgent delivers more consistent performance, and the overall gap between the two systems is not significant, making it an alternative way to enhance creative efficiency.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recently, an increasing number of multimodal (text and audio) benchmarks have emerged, primarily focusing on evaluating models&#8217; understanding capability. However, exploration into assessing generative capabilities remains limited, especially for open-ended long-form content generation. Significant challenges lie in no reference standard answer, no unified evaluation metrics and uncontrollable human judgments. In this work, we take podcast-like audio generation as a starting point and propose PodEval, a comprehensive and well-designed open-source evaluation framework. In this framework: 1) We construct a real-world podcast dataset spanning diverse topics, serving as a reference for human-level creative quality. 2) We introduce a multimodal evaluation strategy and decompose the complex task into three dimensions: text, speech and audio, with different evaluation emphasis on &#8220;Content&#8221; and &#8220;Format&#8221;. 3) For each modality, we design corresponding evaluation methods, involving both objective metrics and subjective listening test. We leverage representative podcast generation systems (including open-source, close-source, and human-made) in our experiments. The results offer in-depth analysis and insights into podcast generation, demonstrating the effectiveness of PodEval in evaluating open-ended long-form audio. This project is open-source to facilitate public use: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/yujxx/PodEval\" title=\"\">https://github.com/yujxx/PodEval</a>.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid development of AIGC (AI-Generated Content) in recent years, many innovative applications have emerged. AI Podcast represents a key application scenario for audio-based generative models <cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib17\" title=\"\">2023</a>; ByteDance, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib10\" title=\"\">2025</a>)</cite>. However, evaluating podcast-like audio is challenging due to: 1) it is an open-ended task, which means there is no reference standard answer; 2) the evaluation of long-form speech/audio is particularly difficult, as longer formats introduce more variability. Objective metrics often fail to capture human perceptions accurately, while subjective listening tests face issues like user inattention, which reduces the validity of results; and 3) podcasts often incorporate additional elements, like music and sound effects, making the evaluation more complicated.</p>\n\n",
                "matched_terms": [
                    "audiobased",
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Different modalities have their own commonly used evaluation methods. For text, metrics such as BLEU <cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib30\" title=\"\">2002</a>)</cite>, ROUGE <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib24\" title=\"\">2004</a>)</cite>, and METEOR <cite class=\"ltx_cite ltx_citemacro_citep\">(Banerjee &amp; Lavie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib7\" title=\"\">2005</a>)</cite> focus on fluency and relevance, while newer approaches like BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib46\" title=\"\">2019</a>)</cite> utilize pre-trained language models to capture semantic alignment. For speech, objective metrics like Mel Cepstral Distortion (MCD) and Perceptual Evaluation of Speech Quality (PESQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib34\" title=\"\">2001</a>)</cite> are widely used, alongside subjective evaluations like Mean Opinion Score (MOS) <cite class=\"ltx_cite ltx_citemacro_citep\">(Sector, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib37\" title=\"\">1996</a>)</cite>. For audio, metrics like Frechet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib22\" title=\"\">2018</a>)</cite> and Kullback-Leibler Divergence (KL) are employed to evaluate audio quality, while listener surveys provide subjective insights. However, these evaluation methods are not directly applicable to podcast evaluation since:</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most content-related objective metrics rely on reference scripts to measure quality and relevance. However, podcast generation lacks standardized references as it is an open-ended generation task. Moreover, relying on such references limits the diversity and creativity of the generated content.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each modality, we design tailored metrics to address diversity considerations. For text, we combine quantitative metrics with LLM-based evaluations to assess conversation scripts. For speech and audio, we design objective metrics and subjective listening tests to evaluate spoken dialogue and overall audio performance. All evaluation methods are organized into open-source tools for ease of use. Subjective tests are enhanced by spammer detection to improve data validity.</p>\n\n",
                "matched_terms": [
                    "quantitative",
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Podcasts are a popular audio format, with platforms like Apple Podcasts and Spotify leading the way. The rise of the AI podcast began with Google&#8217;s NotebookLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib17\" title=\"\">2023</a>)</cite>, which gained popularity in late 2024 for its &#8220;Audio Overviews&#8221; feature. This feature converts materials into conversational, two-person podcasts, praised for its highly natural dialogue speech. Similarly, most open-source podcast generation systems focus on dialogue speech synthesis, like Dia <cite class=\"ltx_cite ltx_citemacro_citep\">(Nari Labs, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib27\" title=\"\">2025</a>)</cite>, Muyan-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib23\" title=\"\">2025</a>)</cite>, MoonCast <cite class=\"ltx_cite ltx_citemacro_citep\">(Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib21\" title=\"\">2025</a>)</cite> and MOSS-TTSD <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenMOSS Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib29\" title=\"\">2025</a>)</cite>. These systems function primarily as dialogue Text-to-Speech (TTS) engines for text-given scenarios. Another type of podcast generation system takes a more holistic approach, incorporating elements beyond speech, such as text and music/sound. For example, WavJourney <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib25\" title=\"\">2023</a>)</cite> leverages LLMs to connect components like TTS and Text-to-Audio (TTA), generating element-rich audio programs. Upon this, PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite> introduces a &#8220;Host-Guest-Writer&#8221; multi-agent system to create informative conversation scripts and builds a voice pool for appropriate voice selection. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Podcast generation &#8227; 2 Related work &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> compares the systems leveraged in subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "system",
                    "podagent",
                    "notebooklm",
                    "mossttsd",
                    "muyantts",
                    "mooncast",
                    "dia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PodAgent uses CosyVoice2<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib14\" title=\"\">2024</a>)</cite> as its backend TTS model, which is a single-sentence TTS system.</p>\n\n",
                "matched_terms": [
                    "system",
                    "podagent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Various evaluation works have emerged along with the development of LLMs and multimodal generative models. <span class=\"ltx_text ltx_font_bold\">Text-related Evaluation</span>, such as SuperGLUE, MMLU, and BIG-bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib41\" title=\"\">2019</a>; Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib18\" title=\"\">2020</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib39\" title=\"\">2022</a>)</cite>, assesses the capabilities of LLMs across diverse tasks with preset ground truth. Subsequently, MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib47\" title=\"\">2023</a>)</cite> explores the potential of LLMs as evaluators, and Chatbot Arena <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib12\" title=\"\">2024</a>)</cite> provides an open platform for assessing LLMs based on human preferences. <span class=\"ltx_text ltx_font_bold\">Speech-related Evaluation</span>, such as SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib45\" title=\"\">2021</a>)</cite>, is designed for discriminative tasks like speech recognition and speaker identification. However, evaluations for generative tasks are scarce due to their inherent diversity and subjectivity, making subjective evaluation essential for speech generation tasks. For instance, VOCBENCH <cite class=\"ltx_cite ltx_citemacro_citep\">(AlBadawy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib3\" title=\"\">2022</a>)</cite> incorporates both subjective and objective evaluations to assess vocoder performance in speech synthesis. Similarly, numerous <span class=\"ltx_text ltx_font_bold\">Audio-related Evaluation</span> work, such as AIR-Bench, Audiobench, MMAU, and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib44\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib42\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib35\" title=\"\">2024</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib26\" title=\"\">2025</a>)</cite>, focus on audio understanding and reasoning. Subjective evaluation remains crucial for assessing audio generation systems and is typically tailored to specific generation tasks. Unlike existing evaluation works, <span class=\"ltx_text ltx_font_bold\">PodEval</span> introduces a comprehensive framework specifically designed for podcast-like audio generation. It emphasizes both subjective and objective evaluations across text, speech, and audio, with all metrics closely aligned with real-world user experience.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dialogue content in podcasts is extracted in text format for evaluation, representing the core message the podcast aims to convey. Podcast dialogues often center around specific topics, showcasing participants&#8217; unique perspectives and insights, which makes reference-correlation-based methods infeasible. Instead, the richness of perspectives conveyed (to provide informative takeaways for the listener) and the presentation style of the dialogue (to enhance listener comprehension) should be the primary focus of evaluation. Therefore, we follow the dialogue script-based evaluation methods proposed in PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite>, which adopt a two-fold approach: (1) <span class=\"ltx_text ltx_font_bold\">Quantitative Metrics</span> such as Distinct-N, Semantic-Div, MATTR, and Info-Dens to assess lexical diversity, semantic richness, vocabulary richness, and information density, respectively. These metrics operate independently of reference texts and focus on intrinsic text characteristics; (2) <span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge</span>, leveraging GPT-4 to replace human evaluators for complex and comprehensive assessments. Evaluation criteria include coherence, engagingness, diversity, informativeness and speaker diversity. It incorporates comparative evaluations to reduce bias and evidence-based scoring for robust and reliable results.</p>\n\n",
                "matched_terms": [
                    "quantitative",
                    "podagent",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech is the core component of a podcast, serving as the medium for content delivery, and how the message delivered plays a crucial role in shaping the listening experience. To ensure a multidimensional evaluation, we first integrate the following <span class=\"ltx_text ltx_font_bold\">Objective Metrics</span>:</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Objective metrics can be calculated efficiently at a low cost without human involvement. However, the <span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> remains a necessary indicator of human perception. Unlike general speech synthesis, which emphasizes sentence-level pronunciation accuracy and naturalness, podcast speech focuses on achieving human-like natural dialogue. Subjective tests for such long-form speech present several key <span class=\"ltx_text ltx_font_bold\">challenges</span>: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> the length of dialogue in podcasts ranges from a few minutes to over an hour, making it impractical to evaluate the entire speech directly; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> the difficulty of comparing more than two systems simultaneously; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> guiding user focus toward dialogue naturalness, rather than on factors like content; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">4)</span> balancing topic diversity within a fixed testing capacity; and <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">5)</span> ensuring that crowdsourced evaluators remain focused and provide reliable feedback.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In PodEval, we design the <span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation</span> based on the MUSHRA framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Schoeffler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib36\" title=\"\">2018</a>)</cite>. The key insight from this framework, <span class=\"ltx_text ltx_font_italic\">incorporating both high-quality and low-quality anchors</span>, helps evaluators establish a reliable reference of quality range. For researchers, analyzing scores for these anchors helps identify inattentive evaluators, enabling the <span class=\"ltx_text ltx_font_italic\">filtering of invalid submissions</span> and improving the data vadility. In our task, we use real podcast segments from the <span class=\"ltx_text ltx_font_italic\">Real-Pod dataset as the high-quality anchor</span> and synthesized dialogue segments from <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib16\" title=\"\">eSpeak Contributors </a></cite><span class=\"ltx_text ltx_font_italic\"> as the low-quality anchor</span>. For podcast samples from different systems, we provide an automatic toolkit to extract dialogue segments featuring <span class=\"ltx_text ltx_font_italic\">turn-taking</span> between speakers, representing a typical dialogue flow. Each dialogue segment is extracted with a <span class=\"ltx_text ltx_font_italic\">preset length</span> (e.g. 15&#8211;25 seconds) to ensure the speech samples are of similar duration. We select dialogue segments from <span class=\"ltx_text ltx_font_italic\">all 17 categories</span> in the Real-Pod dataset to ensure content diversity while keeping the total listening test duration <span class=\"ltx_text ltx_font_italic\">within 30 minutes</span>. In each test group, samples from different systems are presented <span class=\"ltx_text ltx_font_italic\">on the same page</span>, along with a <span class=\"ltx_text ltx_font_italic\">reference Real-Pod sample</span> to guide evaluators on what a natural dialogue sounds like. The scoring is adjusted using a slider ranging from 0 to 100, divided into <span class=\"ltx_text ltx_font_italic\">five stages with a clear definition</span>. Detailed instructions and website design can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS3\" title=\"A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The demo website is hosted at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml\" title=\"\">https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml</a>. Everyone is welcome to try it out and view the results at the end.</span></span></span></p>\n\n",
                "matched_terms": [
                    "scores",
                    "realpod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce the audio-based evaluation for podcasts, which treats speech as one component and assesses the overall audio performance, including speech, music and sound effects (MSE), and their interactions. Similarly, we first introduce the following <span class=\"ltx_text ltx_font_bold\">Objective metrics</span>:</p>\n\n",
                "matched_terms": [
                    "audiobased",
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness</span>: Loudness ensures audio falls within an acceptable volume range. The ITU-R BS.1770-4 standard <cite class=\"ltx_cite ltx_citemacro_citep\">(BS Series, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib9\" title=\"\">2011</a>)</cite> is widely recognized for measuring audio loudness and true-peak levels. Based on this, the <cite class=\"ltx_cite ltx_citemacro_citep\">(EBU R128, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib15\" title=\"\">2011</a>)</cite> standard has been broadly adopted by broadcast and streaming platforms, recommend a target Integrated Loudness (LOUD-IT) of -23 LUFS (&#177;1 LUFS), True Peak (LOUD-TP) <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP and Loudness Range (LOUD-RA) <math alttext=\"&lt;20\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">&lt;20</annotation></semantics></math> LU. For podcast-like streaming, adjustments are made for typical listening environments, such as mobile devices where headphones are commonly used. In these cases, the LOUD-IT is recommended as -18, -16 (&#177;1) or -14 LUFS <cite class=\"ltx_cite ltx_citemacro_citep\">(AES, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib2\" title=\"\">2021</a>; Apple, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib4\" title=\"\">2023</a>; Spotify, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib38\" title=\"\">2023</a>)</cite>. Netflix recommends keeping LOUD-RA between 4 and 18 LU <cite class=\"ltx_cite ltx_citemacro_citep\">(Netflix, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib28\" title=\"\">2024</a>)</cite>. There is no &#8220;absolute right&#8221; reference for loudness metrics. We propose the following reference standards considering all above guidelines: <span class=\"ltx_text ltx_font_bold\">LOUD-IT:</span> <math alttext=\"-18\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>18</mn></mrow><annotation encoding=\"application/x-tex\">-18</annotation></semantics></math> to <math alttext=\"-14\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>14</mn></mrow><annotation encoding=\"application/x-tex\">-14</annotation></semantics></math> LUFS; <span class=\"ltx_text ltx_font_bold\">LOUD-TP:</span> <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP; <span class=\"ltx_text ltx_font_bold\">LOUD-RA:</span> 4 to 18 LU. Based on this &#8220;relatively correct&#8221; reference, we can analyze the distribution of loudness metrics across different systems. We also provide a quantitative scoring strategy in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS4\" title=\"A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "quantitative",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text-based evaluation is conducted among GPT-4, PodAgent, MoonCast, and Real-Pod. Other systems in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Podcast generation &#8227; 2 Related work &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> are excluded as they do not provide conversation scripts. PodAgent, with its Host-Guest-Writer multi-agent system, can directly generate podcast scripts based on a given topic. While MoonCast functions similarly to NotebookLM, requiring external knowledge sources but providing prompt template for spontaneous script generation. For this evaluation, the MoonCast system uses the podcast scripts generated by PodAgent as input and transforms them into a spontaneous version.</p>\n\n",
                "matched_terms": [
                    "system",
                    "podagent",
                    "notebooklm",
                    "mooncast",
                    "realpod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Metrics.</span> Detailed scores calculated across 17 podcast categories for each system are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS1\" title=\"A.2.1 Quantitative metrics &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.1</span></a>. For a concise and clearer comparison, we present the overall performance (averaged across all 17 categories) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F2\" title=\"Figure 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where we can observe that: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> For each quantitative metric, PodAgent outperforms directly prompting GPT-4; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> When comparing LLM-based methods (GPT-4, PodAgent) with human-created podcasts (Real-Pod), Real-Pod scores lower on lexical diversity (Distinct-2 and MATTR) but higher on information density and semantic diversity (Info-Dens and Sem-Div). This is reasonable for: i) real human interactions often include filler words and use simpler language; ii) most real podcasts are significantly longer (30 minutes to an hour), leading to higher information richness compared to generated podcasts, which are usually only a few minutes long; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> As a spontaneous version of PodAgent, MoonCast shows reduced lexical diversity and information density. While its semantic diversity remains comparable to PodAgent.</p>\n\n",
                "matched_terms": [
                    "system",
                    "podagent",
                    "quantitative",
                    "mooncast",
                    "realpod",
                    "scores",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> This evaluation compares PodAgent (scored from -3 to 3) with GPT-4 (reference score as 0), both of which generate conversation scripts without external knowledge resources. Detailed scores for each category are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS2\" title=\"A.2.2 LLM-as-a-Judge &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.2</span></a>. We present the overall performance and results for five specific categories in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T2\" title=\"Table 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for analysis. We can see that scores across all metrics and all categories are positive, demonstrating that PodAgent significantly outperforms directly prompting GPT-4 in generating podcast scripts across all evaluated dimensions.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "podagent",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure fairness, all open-source TTS systems use the same PodAgent-generated scripts. Subjective tests use a spontaneous version from MoonCast, while objective evaluations use the original PodAgent scripts, as filler words in the spontaneous version challenge metrics like WER.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "podagent",
                    "mooncast",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(1) shows the WER results calculated for the entire conversation script. All systems, except MuyanTTS, achieve WER scores below 20%. Analysis of sampled MuyanTTS outputs reveals robustness issues like repeated sentences and the insertion of unknown content.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "muyantts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SIM.</span> The SIM metric evaluates zero-shot TTS systems&#8217; ability to replicate the timbre of a reference voice. PodAgent, MoonCast, MuyanTTS, Dia, and MOSS-TTSD&#8212;are assessed as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(2). Each system uses the reference voice selected by PodAgent for the topic. The performance rankings are: MuyanTTS, MOSS-TTSD, Dia, MoonCast, and PodAgent. PodAgent&#8217;s relatively low score in this metric likely stems from its instruction-following style control strategy. While this approach enhances overall conversational expressiveness, it can reduce speaker similarity.</p>\n\n",
                "matched_terms": [
                    "system",
                    "podagent",
                    "mossttsd",
                    "muyantts",
                    "mooncast",
                    "dia"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SPTD.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(3) shows timbre variation across speakers in the conversation for three systems: Real-Pod, PodAgent, and NotebookLM. Real-Pod reflects real-world podcasts, PodAgent uses a voice selection mechanism for distinct voices, and NotebookLM fixed voices (one male, one female). The SPTD scores rank as follows: PodAgent, NotebookLM, and Real-Pod. This likely reflects that real-world podcasts prioritize guest expertise and availability over timbre differences. PodAgent demonstrates an effective automated voice selection process for podcast creation.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "podagent",
                    "realpod",
                    "notebooklm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DNSMOS.</span> The DNSMOS metric was applied to all systems to evaluate speech quality as in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(4). PodAgent, MoonCast, MuyanTTS, MOSS-TTSD, and NotebookLM achieve similar scores, while Real-Pod and Dia show noticeable declines in speech quality. For Real-Pod, the lower scores are due to: (1) real podcasts often use MSE for enhancement, requiring speech-MSE separation before evaluation, which may leave residual MSE artifacts, and (2) human-created podcasts involve recording, editing, or post-processing that introduce noise or instability. Dia struggles with long-form speech synthesis. Its outputs for lengthy podcast scripts frequently feature overly fast speaking speeds and occasional sentence truncations, leading to its relatively low DNSMOS performance.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "notebooklm",
                    "mossttsd",
                    "muyantts",
                    "realpod",
                    "mooncast",
                    "dia",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation.</span> We released the task on Prolific<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.prolific.com/\" title=\"\">https://www.prolific.com/</a></span></span></span>, requesting 20 native English-speaking participants from the US/UK. We set the filter rules as: 1) Over 90% of LQ samples must be marked as the worst, as the synthesized samples from eSpeak are obviously robotic and unnatural. 2) Over 50% of HQ samples must rank in the top-2 best. While it is possible for other systems to achieve a better score than the real podcast, the evaluation of the real podcast should also remain above average. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T3\" title=\"Table 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the two statistical metrics for the submission results. Based on these rules, Judger-7, 18 and 20 can be excluded. We also provide the box plot for each Judger in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F9\" title=\"Figure 9 &#8227; A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix for more advanced analysis. For instance, apart from the LQ samples, Judger-20 assigns similar scores to all other systems, further confirming the invalidity of this submission.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Result Analysis.</span> After excluding unqualified submissions, we analyzed system performance based on the remaining 17 valid submissions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F4\" title=\"Figure 4 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the final results. We can observe that dialogue segments from real podcasts (HQ) achieved the highest scores, which aligns with expectations. NotebookLM, a closed-source product, ranked second, reflecting the high naturalness of its synthesized dialogue speech. Among the three open-source podcast generation systems, PodAgent scored the lowest, which is reasonable since its backend TTS system, CosyVoice2, is limited to single-sentence synthesis. In contrast, MoonCast and MOSS-TTSD, which support direct dialogue synthesis, performed better in dialogue naturalness evaluations. Overall, the evaluation results align with expectations, validating the rationality and effectiveness of our evaluation method design.</p>\n\n",
                "matched_terms": [
                    "system",
                    "podagent",
                    "notebooklm",
                    "mossttsd",
                    "mooncast",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to direct scores, we also derive a corresponding score based on users&#8217; justifications. Specifically, given justification texts from multiple systems for the same question, GPT-4 uses the following prompt to score: <span class=\"ltx_text ltx_font_italic\">&#8221;For each system, summarize the corresponding comments into one sentence and assign a score between 1 and 5.&#8221;</span> A detailed experiment setup is provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS5.SSS2\" title=\"A.5.2 Questionnaire-based MOS test &#8227; A.5 Audio-based evaluation (subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.5.2</span></a>, and separate scores are listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T10\" title=\"Table 10 &#8227; A.5.2 Questionnaire-based MOS test &#8227; A.5 Audio-based evaluation (subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F6\" title=\"Figure 6 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the final results, averaging the direct score and the justification-based score. From the result, we can observe that:</p>\n\n",
                "matched_terms": [
                    "system",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Speech (naturalness and authenticity) is the most dominant factor affecting the listener&#8217;s experience</span>. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, PodAgent scored low in dialogue naturalness due to using a single-sentence synthesis TTS system, leading to consistently poor results in this MOS test. This outcome is expected, as dialogue speech is the core component of podcast-like audio programs. Although PodAgent&#8217;s Music/Sound (harmony) score is below Real-Pod (consistent with the results of objective metric - CASP), it is significantly higher than its scores in other metrics, indicating that <span class=\"ltx_text ltx_font_italic\">the gap between PodAgent and Real-Pod in music harmony is smaller than in speech naturalness</span>.</p>\n\n",
                "matched_terms": [
                    "system",
                    "podagent",
                    "objective",
                    "realpod",
                    "casp",
                    "scores",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Real podcasts perform best in most metrics (5/7).</span> Real-Pod significantly outperforms other systems on holistic metrics like Engagement Level (EL) and Human Likelihood (HL). However, Full Episode Willingness (FEW) scores are low across all systems, with NotebookLM and Real-Pod scoring similarly. <span class=\"ltx_text ltx_font_italic\">This highlights the value of perceptual and preference-based question design in the test.</span> FEW, a preference-based question, garnered justifications like &#8220;the topic is not of interest to me&#8221; for lower scores. In contrast, higher scores for EL and HL indicate that users tend to exclude subjective factors (e.g., personal topic interest) when rating audio performance. A similar pattern is observed in Information Delivery (effectiveness) and Speaker Expression (preference).</p>\n\n",
                "matched_terms": [
                    "scores",
                    "metrics",
                    "realpod",
                    "notebooklm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Audio Quality metric, while PodAgent and MOSS-TTSD score lower than Real-Pod, PodAgent performs better here than in other metrics, and NotebookLM slightly surpasses Real-Pod. As noted, human-made podcasts often exhibit inconsistent audio quality due to complex production. User feedback, like &#8220;Little bit of mic hiss/bloom but otherwise fine,&#8221; supports this observation. This highlights that <span class=\"ltx_text ltx_font_italic\">when conversational realism approaches that of real speech, AI-based methods offers an advantage in their controllability and consistency in producing high-quality audio</span>.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "notebooklm",
                    "mossttsd",
                    "realpod",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PodEval is the first comprehensive evaluation framework for podcast-like audio generation, tackling the challenges of assessing open-ended, long-form content. We constructed a real-world podcast dataset as a benchmark for human-level creative quality across diverse topics and formats. By decomposing evaluation into text, speech, and audio, PodEval introduced multidimensional methods combining objective metrics and well-designed subjective listening tests. Experiments with various podcast generation systems, including open-source, closed-source, and human-made examples, validated the framework&#8217;s effectiveness. The results offer insights into the strengths and weaknesses of different systems (e.g. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F14\" title=\"Figure 14 &#8227; A.6 System analysis report &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>), highlighting PodEval&#8217;s role in advancing podcast generation research and inspiring future work on evaluating open-ended, long-form content generation task.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tools for <span class=\"ltx_text ltx_font_italic\">text-based evaluation</span> of dialogue scripts. Includes both <span class=\"ltx_text ltx_font_italic\">Quantitative Metrics</span> and <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> methods.</p>\n\n",
                "matched_terms": [
                    "quantitative",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Toolkit for <span class=\"ltx_text ltx_font_italic\">objective evaluation</span> of podcast audio and speech quality. Includes DNSMOS, WER, SIM, SPTD, Loudness, SMR, and CASP.</p>\n\n",
                "matched_terms": [
                    "casp",
                    "objective"
                ]
            }
        ]
    },
    "A1.T10": {
        "source_file": "PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation",
        "caption": "Table 10: Questionnaire-based MOS test - (Q.) represents the average score from the direct scoring answers, and (J.) represents the score derived from the justifications.",
        "body": "Metrics\n\n\n\n\nSystems\n\n\nMOSS-TTSD\nNotebookLM\nPodAgent\nReal-Pod\n\n\n\n\n\nQ.\nJ.\nQ.\nJ.\nQ.\nJ.\nQ.\nJ.\n\n\nInformation Delivery\n4.0\n3.0\n4.2\n4.0\n1.6\n1.0\n4.2\n4.0\n\n\nMusic/Sound Effects\nN/A\nN/A\nN/A\nN/A\n2.4\n2.0\n3.3\n3.0\n\n\nEngagement Level\n2.2\n3.0\n3.1\n3.0\n1.1\n1.0\n3.6\n4.0\n\n\nFull Episode Likelihood\n2.1\n2.0\n2.1\n3.0\n1.0\n1.0\n2.3\n3.0\n\n\nHuman Likelihood\n3.0\n3.0\n3.3\n3.5\n1.1\n1.0\n4.2\n4.0\n\n\nAudio Quality\n3.5\n3.0\n4.2\n4.0\n3.0\n2.0\n3.9\n4.0\n\n\nSpeaker Expression\n3.3\n3.0\n4.0\n3.0\n1.5\n1.0\n3.4\n4.0",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><svg height=\"21.61\" overflow=\"visible\" version=\"1.1\" width=\"97.94\"><g transform=\"translate(0,21.61) scale(1,-1)\"><path d=\"M 0,21.61 97.94,0\" stroke=\"#000000\" stroke-width=\"0.4\" style=\"--ltx-stroke-color:#000000;\"/><g class=\"ltx_svg_fog\" transform=\"translate(0,0)\"><g transform=\"translate(0,9.46) scale(1, -1)\"><foreignobject height=\"9.46\" overflow=\"visible\" width=\"45.09\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_inline-block ltx_align_left\">\n<span class=\"ltx_p\">Metrics</span>\n</span>\n</span></span></span></foreignobject></g></g><g class=\"ltx_svg_fog\" transform=\"translate(48.97,9.46)\"><g transform=\"translate(0,12.15) scale(1, -1)\"><foreignobject height=\"12.15\" overflow=\"visible\" width=\"48.97\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_inline-block ltx_align_right\">\n<span class=\"ltx_p\">Systems</span>\n</span>\n</span></span></span></foreignobject></g></g></g></svg></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">MOSS-TTSD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">NotebookLM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">PodAgent</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">Real-Pod</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">Q.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">J.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">Q.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">J.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">Q.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">J.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">Q.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"><span class=\"ltx_text ltx_font_bold\">J.</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Information Delivery</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Music/Sound Effects</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">N/A</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">N/A</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">N/A</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">N/A</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Engagement Level</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Full Episode Likelihood</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Human Likelihood</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Audio Quality</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Speaker Expression</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "musicsound",
            "information",
            "likelihood",
            "realpod",
            "expression",
            "questionnairebased",
            "mossttsd",
            "delivery",
            "audio",
            "direct",
            "metrics",
            "episode",
            "score",
            "systems",
            "notebooklm",
            "podagent",
            "engagement",
            "level",
            "mos",
            "average",
            "from",
            "answers",
            "test",
            "justifications",
            "scoring",
            "speaker",
            "human",
            "quality",
            "full",
            "effects",
            "represents",
            "derived"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In addition to direct scores, we also derive a corresponding score based on users&#8217; justifications. Specifically, given justification texts from multiple systems for the same question, GPT-4 uses the following prompt to score: <span class=\"ltx_text ltx_font_italic\">&#8221;For each system, summarize the corresponding comments into one sentence and assign a score between 1 and 5.&#8221;</span> A detailed experiment setup is provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS5.SSS2\" title=\"A.5.2 Questionnaire-based MOS test &#8227; A.5 Audio-based evaluation (subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.5.2</span></a>, and separate scores are listed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T10\" title=\"Table 10 &#8227; A.5.2 Questionnaire-based MOS test &#8227; A.5 Audio-based evaluation (subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F6\" title=\"Figure 6 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows the final results, averaging the direct score and the justification-based score. From the result, we can observe that:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recently, an increasing number of multimodal (text and audio) benchmarks have emerged, primarily focusing on evaluating models&#8217; understanding capability. However, exploration into assessing generative capabilities remains limited, especially for open-ended long-form content generation. Significant challenges lie in no reference standard answer, no unified evaluation metrics and uncontrollable human judgments. In this work, we take podcast-like audio generation as a starting point and propose PodEval, a comprehensive and well-designed open-source evaluation framework. In this framework: 1) We construct a real-world podcast dataset spanning diverse topics, serving as a reference for human-level creative quality. 2) We introduce a multimodal evaluation strategy and decompose the complex task into three dimensions: text, speech and audio, with different evaluation emphasis on &#8220;Content&#8221; and &#8220;Format&#8221;. 3) For each modality, we design corresponding evaluation methods, involving both objective metrics and subjective listening test. We leverage representative podcast generation systems (including open-source, close-source, and human-made) in our experiments. The results offer in-depth analysis and insights into podcast generation, demonstrating the effectiveness of PodEval in evaluating open-ended long-form audio. This project is open-source to facilitate public use: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/yujxx/PodEval\" title=\"\">https://github.com/yujxx/PodEval</a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "human",
                    "quality",
                    "audio",
                    "metrics",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the rapid development of AIGC (AI-Generated Content) in recent years, many innovative applications have emerged. AI Podcast represents a key application scenario for audio-based generative models <cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib17\" title=\"\">2023</a>; ByteDance, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib10\" title=\"\">2025</a>)</cite>. However, evaluating podcast-like audio is challenging due to: 1) it is an open-ended task, which means there is no reference standard answer; 2) the evaluation of long-form speech/audio is particularly difficult, as longer formats introduce more variability. Objective metrics often fail to capture human perceptions accurately, while subjective listening tests face issues like user inattention, which reduces the validity of results; and 3) podcasts often incorporate additional elements, like music and sound effects, making the evaluation more complicated.</p>\n\n",
                "matched_terms": [
                    "human",
                    "metrics",
                    "audio",
                    "effects",
                    "represents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these challenges and establish a clear evaluation framework, we decompose podcast-like audio into three dimensions: <span class=\"ltx_text ltx_font_bold\">text</span> (conversation transcripts), <span class=\"ltx_text ltx_font_bold\">speech</span> (spoken dialogue), and <span class=\"ltx_text ltx_font_bold\">audio</span> (speech, music, sound effects, and their interaction). While these dimensions inherently overlap, they offers a structured framework for evaluation focus. Specifically, the conversation transcripts in podcasts are primarily used for <span class=\"ltx_text ltx_font_bold\">content</span> (the message being conveyed) evaluation, whereas speech, music and sound effects primarily contribute to <span class=\"ltx_text ltx_font_bold\">format</span> (how the message is presented) evaluation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "effects"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Different modalities have their own commonly used evaluation methods. For text, metrics such as BLEU <cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib30\" title=\"\">2002</a>)</cite>, ROUGE <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib24\" title=\"\">2004</a>)</cite>, and METEOR <cite class=\"ltx_cite ltx_citemacro_citep\">(Banerjee &amp; Lavie, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib7\" title=\"\">2005</a>)</cite> focus on fluency and relevance, while newer approaches like BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib46\" title=\"\">2019</a>)</cite> utilize pre-trained language models to capture semantic alignment. For speech, objective metrics like Mel Cepstral Distortion (MCD) and Perceptual Evaluation of Speech Quality (PESQ) <cite class=\"ltx_cite ltx_citemacro_citep\">(Rix et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib34\" title=\"\">2001</a>)</cite> are widely used, alongside subjective evaluations like Mean Opinion Score (MOS) <cite class=\"ltx_cite ltx_citemacro_citep\">(Sector, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib37\" title=\"\">1996</a>)</cite>. For audio, metrics like Frechet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">(Kilgour et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib22\" title=\"\">2018</a>)</cite> and Kullback-Leibler Divergence (KL) are employed to evaluate audio quality, while listener surveys provide subjective insights. However, these evaluation methods are not directly applicable to podcast evaluation since:</p>\n\n",
                "matched_terms": [
                    "score",
                    "quality",
                    "mos",
                    "audio",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most content-related objective metrics rely on reference scripts to measure quality and relevance. However, podcast generation lacks standardized references as it is an open-ended generation task. Moreover, relying on such references limits the diversity and creativity of the generated content.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While music and sound effects are not essential to every podcast, their evaluation, when present, should go beyond the quality of individual audio events. Instead, it should focus on their overall harmony and seamless integration with the speech content to enhance the listener&#8217;s experience.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "effects",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We decompose podcast-like audio evaluation from a multimodal viewpoint&#8212;text, speech, and audio&#8212;to establish a clear evaluation framework, with distinct focuses on &#8220;Content&#8221; and &#8220;Format&#8221;.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each modality, we design tailored metrics to address diversity considerations. For text, we combine quantitative metrics with LLM-based evaluations to assess conversation scripts. For speech and audio, we design objective metrics and subjective listening tests to evaluate spoken dialogue and overall audio performance. All evaluation methods are organized into open-source tools for ease of use. Subjective tests are enhanced by spammer detection to improve data validity.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Podcasts are a popular audio format, with platforms like Apple Podcasts and Spotify leading the way. The rise of the AI podcast began with Google&#8217;s NotebookLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Google, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib17\" title=\"\">2023</a>)</cite>, which gained popularity in late 2024 for its &#8220;Audio Overviews&#8221; feature. This feature converts materials into conversational, two-person podcasts, praised for its highly natural dialogue speech. Similarly, most open-source podcast generation systems focus on dialogue speech synthesis, like Dia <cite class=\"ltx_cite ltx_citemacro_citep\">(Nari Labs, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib27\" title=\"\">2025</a>)</cite>, Muyan-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib23\" title=\"\">2025</a>)</cite>, MoonCast <cite class=\"ltx_cite ltx_citemacro_citep\">(Ju et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib21\" title=\"\">2025</a>)</cite> and MOSS-TTSD <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenMOSS Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib29\" title=\"\">2025</a>)</cite>. These systems function primarily as dialogue Text-to-Speech (TTS) engines for text-given scenarios. Another type of podcast generation system takes a more holistic approach, incorporating elements beyond speech, such as text and music/sound. For example, WavJourney <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib25\" title=\"\">2023</a>)</cite> leverages LLMs to connect components like TTS and Text-to-Audio (TTA), generating element-rich audio programs. Upon this, PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite> introduces a &#8220;Host-Guest-Writer&#8221; multi-agent system to create informative conversation scripts and builds a voice pool for appropriate voice selection. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Podcast generation &#8227; 2 Related work &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> compares the systems leveraged in subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "musicsound",
                    "systems",
                    "podagent",
                    "notebooklm",
                    "mossttsd",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Various evaluation works have emerged along with the development of LLMs and multimodal generative models. <span class=\"ltx_text ltx_font_bold\">Text-related Evaluation</span>, such as SuperGLUE, MMLU, and BIG-bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib41\" title=\"\">2019</a>; Hendrycks et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib18\" title=\"\">2020</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib39\" title=\"\">2022</a>)</cite>, assesses the capabilities of LLMs across diverse tasks with preset ground truth. Subsequently, MT-Bench <cite class=\"ltx_cite ltx_citemacro_citep\">(Zheng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib47\" title=\"\">2023</a>)</cite> explores the potential of LLMs as evaluators, and Chatbot Arena <cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib12\" title=\"\">2024</a>)</cite> provides an open platform for assessing LLMs based on human preferences. <span class=\"ltx_text ltx_font_bold\">Speech-related Evaluation</span>, such as SUPERB <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib45\" title=\"\">2021</a>)</cite>, is designed for discriminative tasks like speech recognition and speaker identification. However, evaluations for generative tasks are scarce due to their inherent diversity and subjectivity, making subjective evaluation essential for speech generation tasks. For instance, VOCBENCH <cite class=\"ltx_cite ltx_citemacro_citep\">(AlBadawy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib3\" title=\"\">2022</a>)</cite> incorporates both subjective and objective evaluations to assess vocoder performance in speech synthesis. Similarly, numerous <span class=\"ltx_text ltx_font_bold\">Audio-related Evaluation</span> work, such as AIR-Bench, Audiobench, MMAU, and MMAR <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib44\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib42\" title=\"\">2024</a>; Sakshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib35\" title=\"\">2024</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib26\" title=\"\">2025</a>)</cite>, focus on audio understanding and reasoning. Subjective evaluation remains crucial for assessing audio generation systems and is typically tailored to specific generation tasks. Unlike existing evaluation works, <span class=\"ltx_text ltx_font_bold\">PodEval</span> introduces a comprehensive framework specifically designed for podcast-like audio generation. It emphasizes both subjective and objective evaluations across text, speech, and audio, with all metrics closely aligned with real-world user experience.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "speaker",
                    "human",
                    "audio",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There is no unified standard for defining &#8220;what makes a good podcast episode.&#8221; Unlike textbooks or official TV programs, podcasts can be created by anyone to share their unique ideas or insights. We do not make direct comparisons between generated podcasts and real podcasts&#8212;such comparisons are inherently unfeasible, especially when they approach topics from entirely different perspectives. Instead, we construct a real-world podcast dataset, called <span class=\"ltx_text ltx_font_bold\">Real-Pod</span> dataset, to serve as a reference for human-level creative quality. It is important to note that this dataset acts as a &#8220;reference&#8221; rather than an absolute &#8220;answer&#8221;. The design principles of the Real-Pod dataset are <span class=\"ltx_text ltx_font_bold\">real</span> (consists of human-made podcasts), <span class=\"ltx_text ltx_font_bold\">broad</span> (diverse topic coverage) and <span class=\"ltx_text ltx_font_bold\">rich</span> (varied formats, like multi-speaker, music and sound). The workflow for constructing the Real-Pod dataset is illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S3.F1\" title=\"Figure 1 &#8227; 3 Real-Pod: Real-world podcast dataset &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>:</p>\n\n",
                "matched_terms": [
                    "from",
                    "direct",
                    "quality",
                    "realpod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Podcast Episode</span>. After finalizing the topic collection, we manually searched and screened podcast episodes to identify those most relevant to the selected topics. The selection process was guided by: (1) Topic Relevance: Episodes were selected based on their alignment with the predefined topics. (2) Rich Format: Preference was given to episodes that featured multi-speaker conversations, included background music and sound effects, and exhibited high audio quality.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "episode",
                    "effects",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The dialogue content in podcasts is extracted in text format for evaluation, representing the core message the podcast aims to convey. Podcast dialogues often center around specific topics, showcasing participants&#8217; unique perspectives and insights, which makes reference-correlation-based methods infeasible. Instead, the richness of perspectives conveyed (to provide informative takeaways for the listener) and the presentation style of the dialogue (to enhance listener comprehension) should be the primary focus of evaluation. Therefore, we follow the dialogue script-based evaluation methods proposed in PodAgent <cite class=\"ltx_cite ltx_citemacro_citep\">(Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib43\" title=\"\">2025</a>)</cite>, which adopt a two-fold approach: (1) <span class=\"ltx_text ltx_font_bold\">Quantitative Metrics</span> such as Distinct-N, Semantic-Div, MATTR, and Info-Dens to assess lexical diversity, semantic richness, vocabulary richness, and information density, respectively. These metrics operate independently of reference texts and focus on intrinsic text characteristics; (2) <span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge</span>, leveraging GPT-4 to replace human evaluators for complex and comprehensive assessments. Evaluation criteria include coherence, engagingness, diversity, informativeness and speaker diversity. It incorporates comparative evaluations to reduce bias and evidence-based scoring for robust and reliable results.</p>\n\n",
                "matched_terms": [
                    "scoring",
                    "podagent",
                    "information",
                    "speaker",
                    "human",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech is the core component of a podcast, serving as the medium for content delivery, and how the message delivered plays a crucial role in shaping the listening experience. To ensure a multidimensional evaluation, we first integrate the following <span class=\"ltx_text ltx_font_bold\">Objective Metrics</span>:</p>\n\n",
                "matched_terms": [
                    "delivery",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SIM</span> stands for Speaker Similarity. In podcast generation systems, zero-shot TTS is often employed to replicate the voice of a preset speaker. The SIM between the synthesized voice and the reference voice serves as a crucial metric about vocal fidelity. In PodEval, SIM is quantified using the cosine similarity of extracted speaker embeddings <cite class=\"ltx_cite ltx_citemacro_citep\">(Plaquet &amp; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib31\" title=\"\">2023</a>; Bredin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib8\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SPTD</span> is a brand new metric we proposed, standing for Speaker Timbre Difference. As audio programs, podcasts are accessible only through listening. In multi-speaker conversations, voices with greater timbre differences enhance clarity and make the information easier to follow. SPTD is to assess the overall timbre variation across speakers. Equation <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S5.E1\" title=\"In 5 Speech-based evaluation &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> calculates the SPTD among <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I1.i4.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> distinct speakers.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speaker",
                    "information"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Objective metrics can be calculated efficiently at a low cost without human involvement. However, the <span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> remains a necessary indicator of human perception. Unlike general speech synthesis, which emphasizes sentence-level pronunciation accuracy and naturalness, podcast speech focuses on achieving human-like natural dialogue. Subjective tests for such long-form speech present several key <span class=\"ltx_text ltx_font_bold\">challenges</span>: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> the length of dialogue in podcasts ranges from a few minutes to over an hour, making it impractical to evaluate the entire speech directly; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> the difficulty of comparing more than two systems simultaneously; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> guiding user focus toward dialogue naturalness, rather than on factors like content; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">4)</span> balancing topic diversity within a fixed testing capacity; and <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">5)</span> ensuring that crowdsourced evaluators remain focused and provide reliable feedback.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "human",
                    "from",
                    "metrics",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In PodEval, we design the <span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation</span> based on the MUSHRA framework <cite class=\"ltx_cite ltx_citemacro_citep\">(Schoeffler et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib36\" title=\"\">2018</a>)</cite>. The key insight from this framework, <span class=\"ltx_text ltx_font_italic\">incorporating both high-quality and low-quality anchors</span>, helps evaluators establish a reliable reference of quality range. For researchers, analyzing scores for these anchors helps identify inattentive evaluators, enabling the <span class=\"ltx_text ltx_font_italic\">filtering of invalid submissions</span> and improving the data vadility. In our task, we use real podcast segments from the <span class=\"ltx_text ltx_font_italic\">Real-Pod dataset as the high-quality anchor</span> and synthesized dialogue segments from <cite class=\"ltx_cite ltx_citemacro_cite\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib16\" title=\"\">eSpeak Contributors </a></cite><span class=\"ltx_text ltx_font_italic\"> as the low-quality anchor</span>. For podcast samples from different systems, we provide an automatic toolkit to extract dialogue segments featuring <span class=\"ltx_text ltx_font_italic\">turn-taking</span> between speakers, representing a typical dialogue flow. Each dialogue segment is extracted with a <span class=\"ltx_text ltx_font_italic\">preset length</span> (e.g. 15&#8211;25 seconds) to ensure the speech samples are of similar duration. We select dialogue segments from <span class=\"ltx_text ltx_font_italic\">all 17 categories</span> in the Real-Pod dataset to ensure content diversity while keeping the total listening test duration <span class=\"ltx_text ltx_font_italic\">within 30 minutes</span>. In each test group, samples from different systems are presented <span class=\"ltx_text ltx_font_italic\">on the same page</span>, along with a <span class=\"ltx_text ltx_font_italic\">reference Real-Pod sample</span> to guide evaluators on what a natural dialogue sounds like. The scoring is adjusted using a slider ranging from 0 to 100, divided into <span class=\"ltx_text ltx_font_italic\">five stages with a clear definition</span>. Detailed instructions and website design can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS3\" title=\"A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.3</span></a>. <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The demo website is hosted at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml\" title=\"\">https://podeval.github.io/PodEval-Subjective/?config=dialogue.yaml</a>. Everyone is welcome to try it out and view the results at the end.</span></span></span></p>\n\n",
                "matched_terms": [
                    "scoring",
                    "systems",
                    "quality",
                    "realpod",
                    "from",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we introduce the audio-based evaluation for podcasts, which treats speech as one component and assesses the overall audio performance, including speech, music and sound effects (MSE), and their interactions. Similarly, we first introduce the following <span class=\"ltx_text ltx_font_bold\">Objective metrics</span>:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "effects",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness</span>: Loudness ensures audio falls within an acceptable volume range. The ITU-R BS.1770-4 standard <cite class=\"ltx_cite ltx_citemacro_citep\">(BS Series, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib9\" title=\"\">2011</a>)</cite> is widely recognized for measuring audio loudness and true-peak levels. Based on this, the <cite class=\"ltx_cite ltx_citemacro_citep\">(EBU R128, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib15\" title=\"\">2011</a>)</cite> standard has been broadly adopted by broadcast and streaming platforms, recommend a target Integrated Loudness (LOUD-IT) of -23 LUFS (&#177;1 LUFS), True Peak (LOUD-TP) <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP and Loudness Range (LOUD-RA) <math alttext=\"&lt;20\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">&lt;20</annotation></semantics></math> LU. For podcast-like streaming, adjustments are made for typical listening environments, such as mobile devices where headphones are commonly used. In these cases, the LOUD-IT is recommended as -18, -16 (&#177;1) or -14 LUFS <cite class=\"ltx_cite ltx_citemacro_citep\">(AES, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib2\" title=\"\">2021</a>; Apple, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib4\" title=\"\">2023</a>; Spotify, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib38\" title=\"\">2023</a>)</cite>. Netflix recommends keeping LOUD-RA between 4 and 18 LU <cite class=\"ltx_cite ltx_citemacro_citep\">(Netflix, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib28\" title=\"\">2024</a>)</cite>. There is no &#8220;absolute right&#8221; reference for loudness metrics. We propose the following reference standards considering all above guidelines: <span class=\"ltx_text ltx_font_bold\">LOUD-IT:</span> <math alttext=\"-18\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>18</mn></mrow><annotation encoding=\"application/x-tex\">-18</annotation></semantics></math> to <math alttext=\"-14\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>14</mn></mrow><annotation encoding=\"application/x-tex\">-14</annotation></semantics></math> LUFS; <span class=\"ltx_text ltx_font_bold\">LOUD-TP:</span> <math alttext=\"\\leq-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\leq-1</annotation></semantics></math> dBTP; <span class=\"ltx_text ltx_font_bold\">LOUD-RA:</span> 4 to 18 LU. Based on this &#8220;relatively correct&#8221; reference, we can analyze the distribution of loudness metrics across different systems. We also provide a quantitative scoring strategy in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS4\" title=\"A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "audio",
                    "scoring",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CASP</span> (MSE-Speech Harmony): Harmony between speech and MSE is an advanced requirement. Appropriate MSE can enhance audio engagement, while discordant MSE distracts and negatively impacts the experience. The DualScore, calculated by the CASP framework proposed in <cite class=\"ltx_cite ltx_citemacro_cite\">Tian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib40\" title=\"\">2025</a>)</cite>, measures the correlation between audio and speech. In PodEval, we employ the CASP model, pretrained on <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S6.I1.i3.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>1,000 hours of podcast data, to assess MSE-Speech Harmony.</p>\n\n",
                "matched_terms": [
                    "engagement",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective Listening Test</span> is primarily designed based on the perceptions of real users. A key challenge lies in how to evaluate extra-long audios. As we mentioned above, podcasts in the real world range from a few minutes to over an hour in length. Conducting listening tests on full-length podcast episodes is impractical due to the time, effort, and financial resources required. Moreover, it is hard to judge podcasts of vastly different lengths in a fair and consistent manner. Research on long-form audio evaluation is limited. <cite class=\"ltx_cite ltx_citemacro_cite\">Clark et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib13\" title=\"\">2019</a>)</cite> did investigation on long-form speech evaluation and found that multiple evaluations are necessary due to the low correlation observed across different experimental settings. <cite class=\"ltx_cite ltx_citemacro_cite\">Cambre et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib11\" title=\"\">2020</a>)</cite> conducted a comprehensive evaluation of voice selection for long-form content; however, the minimum required listening time was only 10 seconds. A podcast-related evaluation study <cite class=\"ltx_cite ltx_citemacro_citep\">(Austria, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#bib.bib6\" title=\"\">2007</a>)</cite> designed a questionnaire with carefully crafted questions in terms of both content and presentation to assess domain-specific podcasts. Different from that, PodEval does not constrain the domain of podcasts, and open-ended content evaluation is separately conducted in the text-based evaluation section. In this audio-based evaluation, we focus on assessing the overall performance of the audios. The design approach is as follows:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design it as a <span class=\"ltx_text ltx_font_bold\">MOS test</span>, where evaluators listen to one audio sample at a time and provide judgments based on predefined criteria. Compared to comparative methods, this approach is more suitable for long-form content by avoiding attention overload and consistency compromising.</p>\n\n",
                "matched_terms": [
                    "mos",
                    "audio",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The test data are preprocessed by extracting <span class=\"ltx_text ltx_font_bold\">the first / middle / final minute</span>. These segments are concatenated into a single audio, separated by a beep signal. This method unifies podcast length, captures overall performance from diverse positions, and minimizes content-related biases.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "from",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The judgment session consists of a <span class=\"ltx_text ltx_font_bold\">questionnaire</span> with 8 questions covering multiple dimensions, integrating both perceptual (e.g., &#8220;Information Delivery Effectiveness&#8221;) and preference-based (e.g., &#8220;Speaker Expression Preference&#8221;) questions. This distinction helps clarify whether the ratings are rooted in objective perception or subjective preference. Users are also asked about their willingness to listen to the full episode and the perceived human likelihood, offering insights into interest levels and audio naturalness. The detailed content can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS5.SSS2\" title=\"A.5.2 Questionnaire-based MOS test &#8227; A.5 Audio-based evaluation (subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.5.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "episode",
                    "delivery",
                    "full",
                    "human",
                    "expression",
                    "audio",
                    "likelihood"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We implement two strategies to enhance the validity of the collected data. <span class=\"ltx_text ltx_font_bold\">1) Attention-check questions:</span> These include questions like <span class=\"ltx_text ltx_font_italic\">Q1. How many speakers are there in the podcast?</span> and <span class=\"ltx_text ltx_font_italic\">Q7. If music or sound effects&#8230; (Select Neutral if none are present)</span>. These questions have standard answers, allowing us to determine whether users are actively listening to the audio. <span class=\"ltx_text ltx_font_bold\">2) Justification for answers:</span> Users have to provide justifications for their responses to each question, which can be short but are required. This requirement significantly increases users&#8217; focus and we can collect more detailed information from their justification. By employing these two strategies, data validity is enhanced by promoting attentiveness and filtering out unreliable responses.</p>\n\n",
                "matched_terms": [
                    "information",
                    "from",
                    "audio",
                    "answers",
                    "justifications"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The text-based evaluation is conducted among GPT-4, PodAgent, MoonCast, and Real-Pod. Other systems in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S2.T1\" title=\"Table 1 &#8227; 2.1 Podcast generation &#8227; 2 Related work &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> are excluded as they do not provide conversation scripts. PodAgent, with its Host-Guest-Writer multi-agent system, can directly generate podcast scripts based on a given topic. While MoonCast functions similarly to NotebookLM, requiring external knowledge sources but providing prompt template for spontaneous script generation. For this evaluation, the MoonCast system uses the podcast scripts generated by PodAgent as input and transforms them into a spontaneous version.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "systems",
                    "realpod",
                    "notebooklm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Quantitative Metrics.</span> Detailed scores calculated across 17 podcast categories for each system are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS1\" title=\"A.2.1 Quantitative metrics &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.1</span></a>. For a concise and clearer comparison, we present the overall performance (averaged across all 17 categories) in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F2\" title=\"Figure 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where we can observe that: <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">1)</span> For each quantitative metric, PodAgent outperforms directly prompting GPT-4; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">2)</span> When comparing LLM-based methods (GPT-4, PodAgent) with human-created podcasts (Real-Pod), Real-Pod scores lower on lexical diversity (Distinct-2 and MATTR) but higher on information density and semantic diversity (Info-Dens and Sem-Div). This is reasonable for: i) real human interactions often include filler words and use simpler language; ii) most real podcasts are significantly longer (30 minutes to an hour), leading to higher information richness compared to generated podcasts, which are usually only a few minutes long; <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">3)</span> As a spontaneous version of PodAgent, MoonCast shows reduced lexical diversity and information density. While its semantic diversity remains comparable to PodAgent.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "information",
                    "human",
                    "realpod",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-as-a-Judge.</span> This evaluation compares PodAgent (scored from -3 to 3) with GPT-4 (reference score as 0), both of which generate conversation scripts without external knowledge resources. Detailed scores for each category are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.SS2.SSS2\" title=\"A.2.2 LLM-as-a-Judge &#8227; A.2 Text-based evaluation &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">A.2.2</span></a>. We present the overall performance and results for five specific categories in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T2\" title=\"Table 2 &#8227; 7.1 Text-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> for analysis. We can see that scores across all metrics and all categories are positive, demonstrating that PodAgent significantly outperforms directly prompting GPT-4 in generating podcast scripts across all evaluated dimensions.</p>\n\n",
                "matched_terms": [
                    "score",
                    "podagent",
                    "from",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure fairness, all open-source TTS systems use the same PodAgent-generated scripts. Subjective tests use a spontaneous version from MoonCast, while objective evaluations use the original PodAgent scripts, as filler words in the spontaneous version challenge metrics like WER.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "podagent",
                    "from",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SIM.</span> The SIM metric evaluates zero-shot TTS systems&#8217; ability to replicate the timbre of a reference voice. PodAgent, MoonCast, MuyanTTS, Dia, and MOSS-TTSD&#8212;are assessed as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(2). Each system uses the reference voice selected by PodAgent for the topic. The performance rankings are: MuyanTTS, MOSS-TTSD, Dia, MoonCast, and PodAgent. PodAgent&#8217;s relatively low score in this metric likely stems from its instruction-following style control strategy. While this approach enhances overall conversational expressiveness, it can reduce speaker similarity.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "score",
                    "mossttsd",
                    "speaker",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SPTD.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(3) shows timbre variation across speakers in the conversation for three systems: Real-Pod, PodAgent, and NotebookLM. Real-Pod reflects real-world podcasts, PodAgent uses a voice selection mechanism for distinct voices, and NotebookLM fixed voices (one male, one female). The SPTD scores rank as follows: PodAgent, NotebookLM, and Real-Pod. This likely reflects that real-world podcasts prioritize guest expertise and availability over timbre differences. PodAgent demonstrates an effective automated voice selection process for podcast creation.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "systems",
                    "realpod",
                    "notebooklm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DNSMOS.</span> The DNSMOS metric was applied to all systems to evaluate speech quality as in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F3\" title=\"Figure 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>-(4). PodAgent, MoonCast, MuyanTTS, MOSS-TTSD, and NotebookLM achieve similar scores, while Real-Pod and Dia show noticeable declines in speech quality. For Real-Pod, the lower scores are due to: (1) real podcasts often use MSE for enhancement, requiring speech-MSE separation before evaluation, which may leave residual MSE artifacts, and (2) human-created podcasts involve recording, editing, or post-processing that introduce noise or instability. Dia struggles with long-form speech synthesis. Its outputs for lengthy podcast scripts frequently feature overly fast speaking speeds and occasional sentence truncations, leading to its relatively low DNSMOS performance.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "systems",
                    "notebooklm",
                    "mossttsd",
                    "quality",
                    "realpod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialogue Naturalness Evaluation.</span> We released the task on Prolific<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.prolific.com/\" title=\"\">https://www.prolific.com/</a></span></span></span>, requesting 20 native English-speaking participants from the US/UK. We set the filter rules as: 1) Over 90% of LQ samples must be marked as the worst, as the synthesized samples from eSpeak are obviously robotic and unnatural. 2) Over 50% of HQ samples must rank in the top-2 best. While it is possible for other systems to achieve a better score than the real podcast, the evaluation of the real podcast should also remain above average. Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.T3\" title=\"Table 3 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the two statistical metrics for the submission results. Based on these rules, Judger-7, 18 and 20 can be excluded. We also provide the box plot for each Judger in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F9\" title=\"Figure 9 &#8227; A.3 Speech-based evaluation (Subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> in the appendix for more advanced analysis. For instance, apart from the LQ samples, Judger-20 assigns similar scores to all other systems, further confirming the invalidity of this submission.</p>\n\n",
                "matched_terms": [
                    "score",
                    "systems",
                    "average",
                    "from",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Result Analysis.</span> After excluding unqualified submissions, we analyzed system performance based on the remaining 17 valid submissions. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F4\" title=\"Figure 4 &#8227; 7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the final results. We can observe that dialogue segments from real podcasts (HQ) achieved the highest scores, which aligns with expectations. NotebookLM, a closed-source product, ranked second, reflecting the high naturalness of its synthesized dialogue speech. Among the three open-source podcast generation systems, PodAgent scored the lowest, which is reasonable since its backend TTS system, CosyVoice2, is limited to single-sentence synthesis. In contrast, MoonCast and MOSS-TTSD, which support direct dialogue synthesis, performed better in dialogue naturalness evaluations. Overall, the evaluation results align with expectations, validating the rationality and effectiveness of our evaluation method design.</p>\n\n",
                "matched_terms": [
                    "podagent",
                    "systems",
                    "notebooklm",
                    "mossttsd",
                    "from",
                    "direct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Loudness.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> presents the density distribution of loudness-related metrics, enabling a comparative analysis with the reference range. All seven systems are included. For <span class=\"ltx_text ltx_font_bold\">LOUD_IT</span>, Dia and MoonCast align well with the reference range, while NotebookLM&#8217;s loudness centers around -25. Real-Pod, as manually produced audio, shows a highly scattered loudness distribution. For <span class=\"ltx_text ltx_font_bold\">LOUD_TP</span>, Muyan-TTS performs best, with all samples maintaining a true peak loudness below -1. In contrast, MoonCast, Dia and MOSS-TTSD perform poorly, while Real-Pod continues to exhibit scattered results. For <span class=\"ltx_text ltx_font_bold\">LOUD_RA</span>, MoonCast has a relatively narrow loudness variation range, while PodAgent and MOSS-TTSD display richer variance. Quantitative scores are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "podagent",
                    "mossttsd",
                    "realpod",
                    "audio",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SMR and CASP.</span> PodAgent and Real-Pod are evaluated for these MSE-related metrics. From the density distribution in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.F5\" title=\"Figure 5 &#8227; 7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, PodAgent exhibits a more concentrated distribution compared to Real-Pod. For <span class=\"ltx_text ltx_font_bold\">SMR</span>, the SMR_SCORE in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.T9\" title=\"Table 9 &#8227; A.4 Audio-based evaluation (objective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows that all PodAgent samples achieve an SMR greater than 0, whereas some Real-Pod cases fail to meet this requirement. For <span class=\"ltx_text ltx_font_bold\">CASP</span>, a higher score indicates better MSE-Speech harmony. Real-Pod demonstrates a higher upper limit, which is expected as exceptional human artistic creations naturally surpass AI-generated outputs. However, PodAgent delivers more consistent performance, and the overall gap between the two systems is not significant, making it an alternative way to enhance creative efficiency.</p>\n\n",
                "matched_terms": [
                    "score",
                    "podagent",
                    "systems",
                    "human",
                    "realpod",
                    "from",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Questionnaire-based MOS Test.</span> We recruited native English speakers from Prolific for this test. Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S6\" title=\"6 Audio-based evaluation &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> describes our final test design. Prior to this, we conducted a <span class=\"ltx_text ltx_font_italic\">Pilot Test</span> using the questionnaire design in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F10\" title=\"Figure 10 &#8227; A.5.1 Pilot test &#8227; A.5 Audio-based evaluation (subjective) &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Based on feedback, we made the following improvements: 1) Reduced the scoring scale from 10 to 5 with clear definitions to reduce ambiguity and improve consistency. 2) Refined the questions to introduce perceptual and preference-based considerations. 3) Added a justification requirement for each question. These changes increased the pass rate from 75% to 90%.</p>\n\n",
                "matched_terms": [
                    "scoring",
                    "questionnairebased",
                    "mos",
                    "from",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Speech (naturalness and authenticity) is the most dominant factor affecting the listener&#8217;s experience</span>. In Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, PodAgent scored low in dialogue naturalness due to using a single-sentence synthesis TTS system, leading to consistently poor results in this MOS test. This outcome is expected, as dialogue speech is the core component of podcast-like audio programs. Although PodAgent&#8217;s Music/Sound (harmony) score is below Real-Pod (consistent with the results of objective metric - CASP), it is significantly higher than its scores in other metrics, indicating that <span class=\"ltx_text ltx_font_italic\">the gap between PodAgent and Real-Pod in music harmony is smaller than in speech naturalness</span>.</p>\n\n",
                "matched_terms": [
                    "musicsound",
                    "score",
                    "podagent",
                    "mos",
                    "realpod",
                    "audio",
                    "metrics",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Real podcasts perform best in most metrics (5/7).</span> Real-Pod significantly outperforms other systems on holistic metrics like Engagement Level (EL) and Human Likelihood (HL). However, Full Episode Willingness (FEW) scores are low across all systems, with NotebookLM and Real-Pod scoring similarly. <span class=\"ltx_text ltx_font_italic\">This highlights the value of perceptual and preference-based question design in the test.</span> FEW, a preference-based question, garnered justifications like &#8220;the topic is not of interest to me&#8221; for lower scores. In contrast, higher scores for EL and HL indicate that users tend to exclude subjective factors (e.g., personal topic interest) when rating audio performance. A similar pattern is observed in Information Delivery (effectiveness) and Speaker Expression (preference).</p>\n\n",
                "matched_terms": [
                    "episode",
                    "scoring",
                    "information",
                    "systems",
                    "notebooklm",
                    "delivery",
                    "engagement",
                    "full",
                    "speaker",
                    "human",
                    "level",
                    "realpod",
                    "metrics",
                    "expression",
                    "audio",
                    "likelihood",
                    "test",
                    "justifications"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the Audio Quality metric, while PodAgent and MOSS-TTSD score lower than Real-Pod, PodAgent performs better here than in other metrics, and NotebookLM slightly surpasses Real-Pod. As noted, human-made podcasts often exhibit inconsistent audio quality due to complex production. User feedback, like &#8220;Little bit of mic hiss/bloom but otherwise fine,&#8221; supports this observation. This highlights that <span class=\"ltx_text ltx_font_italic\">when conversational realism approaches that of real speech, AI-based methods offers an advantage in their controllability and consistency in producing high-quality audio</span>.</p>\n\n",
                "matched_terms": [
                    "score",
                    "podagent",
                    "notebooklm",
                    "mossttsd",
                    "quality",
                    "realpod",
                    "audio",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">PodEval is the first comprehensive evaluation framework for podcast-like audio generation, tackling the challenges of assessing open-ended, long-form content. We constructed a real-world podcast dataset as a benchmark for human-level creative quality across diverse topics and formats. By decomposing evaluation into text, speech, and audio, PodEval introduced multidimensional methods combining objective metrics and well-designed subjective listening tests. Experiments with various podcast generation systems, including open-source, closed-source, and human-made examples, validated the framework&#8217;s effectiveness. The results offer insights into the strengths and weaknesses of different systems (e.g. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#A1.F14\" title=\"Figure 14 &#8227; A.6 System analysis report &#8227; Appendix A Appendix &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>), highlighting PodEval&#8217;s role in advancing podcast generation research and inspiring future work on evaluating open-ended, long-form content generation task.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "audio",
                    "metrics",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work introduces PodEval, a comprehensive framework for evaluating podcast-like audio generation, with careful consideration of ethical implications. The <span class=\"ltx_text ltx_font_italic\">Real-Pod dataset</span> was constructed using publicly available podcasts in alignment with fair use, avoiding sensitive or private data. Instead of directly providing audio files, the dataset offers publicly accessible download links and download toolkit to reduce the risk of misuse and ensure proper attribution. <span class=\"ltx_text ltx_font_italic\">Subjective evaluations</span> were conducted using crowdsourced workers recruited through the Prolific platform, with compensation exceeding the platform&#8217;s minimum wage requirements. Reliability was ensured through attention-check questions and clear instructions for participants. To mitigate bias, the framework incorporates <span class=\"ltx_text ltx_font_italic\">diverse topics and evaluators</span>, promoting inclusivity and fairness. While PodEval aims to advance AI-assisted podcast generation, we emphasize its role as a tool to enhance, not replace, human creativity. PodEval is designed to foster innovation while adhering to principles of transparency, fairness, and ethical AI development.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "human",
                    "realpod"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Toolkit for <span class=\"ltx_text ltx_font_italic\">objective evaluation</span> of podcast audio and speech quality. Includes DNSMOS, WER, SIM, SPTD, Loudness, SMR, and CASP.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Framework for <span class=\"ltx_text ltx_font_italic\">subjective human evaluations</span> of podcast speech and audio. One is <span class=\"ltx_text ltx_font_italic\">Dialogue Naturalness Evaluation</span> and the other one is <span class=\"ltx_text ltx_font_italic\">Questionnaire-based MOS Test</span>.</p>\n\n",
                "matched_terms": [
                    "questionnairebased",
                    "human",
                    "mos",
                    "audio",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large Language Models (LLMs) utilized in this work are as follows:(1) <span class=\"ltx_text ltx_font_italic\">Topics Initiation</span> during data processing of the Real-Pod dataset, which is elaborated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S3\" title=\"3 Real-Pod: Real-world podcast dataset &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. (2) <span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge</span> method in text-based evaluation, which is illustrated in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S4\" title=\"4 Text-based evaluation &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. (3) <span class=\"ltx_text ltx_font_italic\">Summarized Users&#8217; Justifications</span> in the Questionnaire-based MOS Test, which is described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS3\" title=\"7.3 Audio-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.3</span></a> (Questionnaire-based MOS Test).</p>\n\n",
                "matched_terms": [
                    "questionnairebased",
                    "mos",
                    "realpod",
                    "test",
                    "justifications"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Experiment Settings:</span> Lengthy listening tests can be exhausting and may lead to inaccurate feedback. It is essential to ensure the overall test duration does not exceed 30 minutes. In the Questionnaire-based MOS Test, each audio sample is around 3 minutes and requires answering 10 questions with corresponding justifications. Based on the Dialogue Naturalness Test results shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>, we selected 4 representative systems. Each test group included four podcast samples from different systems but within the same podcast category. According to actual test results, each group took an average of 24 minutes to complete. The 4 representative systems are:</p>\n\n",
                "matched_terms": [
                    "systems",
                    "questionnairebased",
                    "from",
                    "mos",
                    "average",
                    "audio",
                    "test",
                    "justifications"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MOSS-TTSD:</span> Achieved the highest score among the open-source systems utilized in the Dialogue Naturalness Evaluation (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00485v1#S7.SS2\" title=\"7.2 Speech-based evaluation &#8227; 7 Experiments &#8227; PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "systems",
                    "score",
                    "mossttsd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">NotebookLM:</span> A pioneering podcast generation product, widely recognized for its exceptional performance, is nearly indistinguishable from real podcasts.</p>\n\n",
                "matched_terms": [
                    "from",
                    "notebooklm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-Pod:</span> A collection of podcasts sourced from the real world.</p>\n\n",
                "matched_terms": [
                    "from",
                    "realpod"
                ]
            }
        ]
    }
}