{
    "S3.T1": {
        "caption": "Table 1:  Comparison of EU (FRA and Eur-Lex) and UN documents by keyword.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Keyword</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt\">EU #</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">EU wds</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">UN #</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">UN wds</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Disabled</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">20</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">17,980</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">1,718</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">9,705</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Human rights</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">72</th>\n<td class=\"ltx_td ltx_align_right\">35,070</td>\n<td class=\"ltx_td ltx_align_right\">13,396</td>\n<td class=\"ltx_td ltx_align_right\">10,048</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Jews</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">29</th>\n<td class=\"ltx_td ltx_align_right\">25,111</td>\n<td class=\"ltx_td ltx_align_right\">178</td>\n<td class=\"ltx_td ltx_align_right\">5,707</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LGBT</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">22</th>\n<td class=\"ltx_td ltx_align_right\">26,953</td>\n<td class=\"ltx_td ltx_align_right\">164</td>\n<td class=\"ltx_td ltx_align_right\">5,998</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Migrants</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">61</th>\n<td class=\"ltx_td ltx_align_right\">25,948</td>\n<td class=\"ltx_td ltx_align_right\">3,784</td>\n<td class=\"ltx_td ltx_align_right\">8,748</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Muslim</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">11</th>\n<td class=\"ltx_td ltx_align_right\">21,340</td>\n<td class=\"ltx_td ltx_align_right\">406</td>\n<td class=\"ltx_td ltx_align_right\">3,272</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">POC</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">38</th>\n<td class=\"ltx_td ltx_align_right\">27,897</td>\n<td class=\"ltx_td ltx_align_right\">4,840</td>\n<td class=\"ltx_td ltx_align_right\">8,365</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Women</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">13</th>\n<td class=\"ltx_td ltx_align_right\">23,122</td>\n<td class=\"ltx_td ltx_align_right\">8,040</td>\n<td class=\"ltx_td ltx_align_right\">8,006</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total / Avg.</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">266</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">25,928</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">32,526</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">7,669</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "muslim",
            "avg",
            "documents",
            "jews",
            "total",
            "wds",
            "rights",
            "keyword",
            "human",
            "poc",
            "migrants",
            "eurlex",
            "women",
            "comparison",
            "lgbt",
            "disabled"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S3.T1\" title=\"Table 1 &#8227; 3 Knowledge Base Construction &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports descriptive statistics of the UN and EU corpora by target group keyword. While the UN collection is considerably larger in terms of the number of documents (over 32k), EU reports are more detailed, with substantially higher average word counts per document (<math alttext=\"\\approx 26k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p5.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>26</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 26k</annotation></semantics></math> vs. <math alttext=\"\\approx 7.7k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p5.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>7.7</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 7.7k</annotation></semantics></math>). Coverage also varies by group: for example, the UN corpus contains a large volume of material on human rights and women, while EU provides more in-depth analyses on migrants, LGBT+ individuals, and people with disabilities. Together, these complementary sources balance breadth (UN) with depth (EU), creating a diverse and representative foundation for counter-speech generation. This ensures that our knowledge base is both comprehensive and adaptable to different CS scenarios.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Counter-speech generation is at the core of many expert activities, such as fact-checking and hate speech, to counter harmful content. Yet, existing work treats counter-speech generation as pure text generation task, mainly based on Large Language Models or NGO experts. These approaches show severe drawbacks due to the limited reliability and coherence in the generated countering text, and in scalability, respectively. To close this gap, we introduce a novel framework to model counter-speech generation as knowledge-wise text generation process. Our framework integrates advanced Retrieval-Augmented Generation (RAG) pipelines to ensure the generation of trustworthy counter-speech for 8 main target groups identified in the hate speech literature, including women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. We built a knowledge base over the United Nations Digital Library, EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792 texts. We use the MultiTarget-CONAN dataset to empirically assess the quality of the generated counter-speech, both through standard metrics (i.e., JudgeLM) and a human evaluation. Results show that our framework outperforms standard LLM baselines and competitive approach, on both assessments. The resulting framework and the knowledge base pave the way for studying trustworthy and sound counter-speech generation, in hate speech and beyond. <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">Warning<span class=\"ltx_text ltx_font_medium\">: this paper contains explicit examples some readers may find offensive.</span></span></p>\n\n",
                "matched_terms": [
                    "jews",
                    "total",
                    "rights",
                    "human",
                    "migrants",
                    "eurlex",
                    "women",
                    "lgbt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tackle this challenging task by proposing a novel knowledge-grounded framework for automatic counter-speech generation that integrates advanced retrieval-augmented generation (RAG) pipelines. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows two examples of CS, with and without RAG augmentation. Our contributions are fourfold: <span class=\"ltx_text ltx_font_bold\">(1)</span> we provide a systematic comparison of multiple retrievers and LLMs for CS generation, combining three retrieval methods with four language models; <span class=\"ltx_text ltx_font_bold\">(2)</span> we enforce concise, two-sentence outputs tailored for social media deployment, ensuring responses remain natural, relatable, and effective in real-world contexts; <span class=\"ltx_text ltx_font_bold\">(3)</span> we conduct a pairwise evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, adapted to balance factual grounding with conciseness and pragmatic suitability, together with an extensive human evaluation, showing that our framework steadily outperforms standard baselines and state-of-the-art competitors; and <span class=\"ltx_text ltx_font_bold\">(4)</span> we will release all generated CS along with their top-3 retrieved evidence sentences, offering a reusable resource for the research community<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code will be made available upon acceptance.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "human",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step, we built a comprehensive Knowledge Base (KB) designed to ensure maximal coverage of documents addressing social groups commonly targeted by hate speech. The goal of this KB is to gather all relevant materials &#8212; such as reports, resolutions, and legal texts &#8212; that provide evidence or context regarding these topics.\nSpecifically, we focused on the following 8 target groups: women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. These categories align with the classic targets in the literature, including the ones of our baseline MultiTarget-CONAN <cite class=\"ltx_cite ltx_citemacro_cite\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib14\" title=\"\">2019</a>)</cite>.\nWe used GPT-based prompting to generate synonyms and semantically related keywords, ensuring that queries captured diverse terminology across cultural and policy contexts (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for the prompt). To guarantee relevance, we performed a keyword-based search so that retrieved knowledge was directly aligned with the hateful messages targeting these groups. To ensure reliability in the generated CS, we relied exclusively on institutional publicly available sources, i.e., the United Nations Digital Library, EUR-Lex, and the European Union Agency for Fundamental Rights (FRA). To construct our knowledge base, we follow three main steps: document retrieval, PDF-to-text conversion, and knowledge base integration.</p>\n\n",
                "matched_terms": [
                    "documents",
                    "jews",
                    "rights",
                    "migrants",
                    "eurlex",
                    "women",
                    "lgbt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Document Retrieval.</span> The <span class=\"ltx_text ltx_font_bold\">United Nations Digital Library</span> serves as the central repository for official UN documents on human rights, equality, and anti-discrimination. We developed a custom crawler using <span class=\"ltx_text ltx_font_typewriter\">requests</span> and <span class=\"ltx_text ltx_font_typewriter\">BeautifulSoup</span> to systematically combine three query dimensions&#8212;target keywords, document types (e.g., resolutions, treaties, NGO statements), and years (2000&#8211;2025). The crawler paginated through results, extracted and normalized documents in English, and organized downloads by target group. A metadata file recorded <span class=\"ltx_text ltx_font_typewriter\">id, fname, target, type, year, url</span>. Error handling covered duplicate checks, retries, and skipped completed downloads. At the European level, <span class=\"ltx_text ltx_font_bold\">EUR-Lex</span> provides access to EU law, treaties, and legal acts, while the <span class=\"ltx_text ltx_font_bold\">EU Agency for Fundamental Rights (FRA)</span> publishes reports on human rights within the EU. Using the same target keywords, we retrieved documents in English (2000&#8211;2025) from both sources, following the same metadata structure as the UN corpus. These sources complement the UN materials, forming a multi-level knowledge base&#8212;global and European&#8212;that ensures comprehensive and reliable grounding for CS generation.</p>\n\n",
                "matched_terms": [
                    "rights",
                    "documents",
                    "human",
                    "eurlex"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experimented on the Multi-Target CONAN (MTCo) dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib9\" title=\"\">2022</a>)</cite>, which contains 5,003 HS/CS pairs in English covering multiple target groups, including people with disabilities, Jews, LGBT+ individuals, Muslims, migrants, people of color (POC), and women. Collected through a human-in-the-loop process, MTCo provides high-quality, contextually relevant CS, and serves as a first baseline for our experiments. Additionally, we compare our pipeline with the four instruction-tuned LLMs of comparable size without RAG. Finally, we compare our results with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>We cannot compare to <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> due to lack of publicly available code and data, even upon request.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "jews",
                    "poc",
                    "migrants",
                    "women",
                    "lgbt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted a human evaluation with 26 participants to assess the quality and effectiveness of the generated CS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Participants&#8217; age is between 18 and 50, there is a balance between genders, and their level of education spans high school diploma to PhD. Detailed statistics are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5\" title=\"Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span>. Our evaluation setup consisted of 10 HS examples, extracted randomly from MT-Co keeping the target distribution, with 9 CS candidates for each HS. We selected the CS from MT-Co and the CS generated by our four LLMs without the RAG pipeline, and the corresponding CS with RAG using BMG-M3 as retriever, since it has shown the strongest performance in the automatic and LLM-based metrics. Participants rated all 90 CS candidates along four dimensions, i.e., Relevance, Factuality, Cogency, and Correctness (1&#8211;3 scale), judged whether each CS was effective (Yes/No) and selected the best CS per HS. We collected a total of 2340 evaluations.</p>\n\n",
                "matched_terms": [
                    "total",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S9.T6\" title=\"Table 6 &#8227; 9 Comparison with competitors &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> also reports the per-target breakdown of our pairwise comparison.\nAgainst Baseline 1, our system achieves overall consistent improvements. Gains are observed across all HS targets, with particularly strong performance for <span class=\"ltx_text ltx_font_italic\">DISABLED</span> (69.4%), <span class=\"ltx_text ltx_font_italic\">LGBT+</span> (64.0%), and <span class=\"ltx_text ltx_font_italic\">WOMEN</span> (61.2%). For larger target categories such as <span class=\"ltx_text ltx_font_italic\">MUSLIMS</span> and <span class=\"ltx_text ltx_font_italic\">MIGRANTS</span>, our model still maintains a clear advantage with 55.2% and 56.8% wins respectively, despite the higher difficulty and variability in these groups. The most balanced outcome is seen for <span class=\"ltx_text ltx_font_italic\">JEWS</span>, where results are split evenly (50%). Against Baseline 2, our system achieves consistently strong results across all targets, outperforming it in 93% of cases. The largest margins are observed for <span class=\"ltx_text ltx_font_italic\">MIGRANTS</span> (100% wins) and <span class=\"ltx_text ltx_font_italic\">JEWS</span> (95.7%).</p>\n\n",
                "matched_terms": [
                    "jews",
                    "migrants",
                    "women",
                    "comparison",
                    "lgbt",
                    "disabled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a RAG-based framework for automatic counter-speech generation. We systematically compared three retrieval methods and four LLMs for CS generation targeting height groups (women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, other), relying on a novel and unique knowledge base, built over three institutional sources. We conducted an extensive experimental evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, and a human evaluation. Our results show that our approach outperforms competitive approaches and standard baselines on both of them. Our experiments demonstrated the versatility and soundness of our framework for counter-speech generation to fight online abusive content.</p>\n\n",
                "matched_terms": [
                    "jews",
                    "human",
                    "migrants",
                    "women",
                    "lgbt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The following keywords were used to retrieve documents from the United Nations Digital Library, the FRA and EUR-Lex portals, covering the period 2000&#8211;2025 and targeting multiple groups and thematic areas relevant to our study:</p>\n\n",
                "matched_terms": [
                    "eurlex",
                    "documents"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LGBT</span>: LGBT rights, Homophobia, Transphobia, Biphobia, Gender identity, Conversion therapy, Same-sex marriage, Stonewall riots, LGBT, LGBTQIA+, Gay, Lesbian, Transgender, Non-binary, LGBT hate speech, discrimination against LGBT people, gay rights movement, LGBT discrimination, LGBT hate crimes, sexual orientation, HIV/AIDS &amp; gay/lesbian</p>\n\n",
                "matched_terms": [
                    "rights",
                    "lgbt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Disabled</span>: Disability rights, Accessibility, Social model of disability, disability, disabled, down syndrome, autism, mental disability, physical disability, neurodiversity, ableism, inclusive design, Discrimination against people with disabilities</p>\n\n",
                "matched_terms": [
                    "rights",
                    "disabled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Women</span>: Sexism, Misogyny, Feminism, Gender inequality, Women&#8217;s rights, Me Too movement, Gender-based violence, women, women hate speech, feminism, violence against women, gender inequality, glass ceiling, discrimination against women</p>\n\n",
                "matched_terms": [
                    "rights",
                    "women"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Migrants</span>: Xenophobia, Anti-immigration, Refugee crisis, Asylum seekers, Undocumented immigrants, Immigration law, refugee, migrants, immigrants, immigration, migration, immigration hate speech, migrants rights, illegal aliens, immigration and crime, immigration and unemployment, discrimination against migrants, aliens</p>\n\n",
                "matched_terms": [
                    "migrants",
                    "rights"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Rights and Law</span>: human rights, human rights treaties, universal declaration of human rights, international human rights law, civil rights, social justice, equality before the law, international court of justice, refugee rights, minority rights, gender equality law, european charter of human rigths</p>\n\n",
                "matched_terms": [
                    "rights",
                    "human"
                ]
            }
        ]
    },
    "S6.T2": {
        "caption": "Table 2:  Automatic evaluation results for CS generation across different LLMs and retrieval strategies (averages).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BLEU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">METEOR</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ROUGE-L</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BERTScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">F1</span></sub></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Distinct-1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Distinct-2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Repetition Rate</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Safety</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_bold\">No Retrieval (No-RAG)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LLaMA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0115</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1638</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.1164</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8575</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0196</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1238</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0034</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.987</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CommandR</th>\n<td class=\"ltx_td ltx_align_center\">0.0118</td>\n<td class=\"ltx_td ltx_align_center\">0.1432</td>\n<td class=\"ltx_td ltx_align_center\">0.1163</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.8590</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0233</td>\n<td class=\"ltx_td ltx_align_center\">0.1462</td>\n<td class=\"ltx_td ltx_align_center\">0.0002</td>\n<td class=\"ltx_td ltx_align_center\">0.988</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mistral</th>\n<td class=\"ltx_td ltx_align_center\">0.0116</td>\n<td class=\"ltx_td ltx_align_center\">0.1435</td>\n<td class=\"ltx_td ltx_align_center\">0.1121</td>\n<td class=\"ltx_td ltx_align_center\">0.8575</td>\n<td class=\"ltx_td ltx_align_center\">0.0210</td>\n<td class=\"ltx_td ltx_align_center\">0.1059</td>\n<td class=\"ltx_td ltx_align_center\">0.2668</td>\n<td class=\"ltx_td ltx_align_center\">0.992</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GPT</th>\n<td class=\"ltx_td ltx_align_center\">0.0105</td>\n<td class=\"ltx_td ltx_align_center\">0.1494</td>\n<td class=\"ltx_td ltx_align_center\">0.1124</td>\n<td class=\"ltx_td ltx_align_center\">0.8580</td>\n<td class=\"ltx_td ltx_align_center\">0.0156</td>\n<td class=\"ltx_td ltx_align_center\">0.1037</td>\n<td class=\"ltx_td ltx_align_center\">0.0018</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.993</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_bold\">BM25 Retrieval</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LLaMA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0079</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1639</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1059</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8491</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0291</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0088</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.974</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CommandR</th>\n<td class=\"ltx_td ltx_align_center\">0.0080</td>\n<td class=\"ltx_td ltx_align_center\">0.1527</td>\n<td class=\"ltx_td ltx_align_center\">0.1059</td>\n<td class=\"ltx_td ltx_align_center\">0.8484</td>\n<td class=\"ltx_td ltx_align_center\">0.0304</td>\n<td class=\"ltx_td ltx_align_center\">0.1963</td>\n<td class=\"ltx_td ltx_align_center\">0.0010</td>\n<td class=\"ltx_td ltx_align_center\">0.978</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mistral</th>\n<td class=\"ltx_td ltx_align_center\">0.0079</td>\n<td class=\"ltx_td ltx_align_center\">0.1442</td>\n<td class=\"ltx_td ltx_align_center\">0.0986</td>\n<td class=\"ltx_td ltx_align_center\">0.8490</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0371</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.2173</span></td>\n<td class=\"ltx_td ltx_align_center\">0.0004</td>\n<td class=\"ltx_td ltx_align_center\">0.981</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GPT</th>\n<td class=\"ltx_td ltx_align_center\">0.0070</td>\n<td class=\"ltx_td ltx_align_center\">0.1580</td>\n<td class=\"ltx_td ltx_align_center\">0.1015</td>\n<td class=\"ltx_td ltx_align_center\">0.8507</td>\n<td class=\"ltx_td ltx_align_center\">0.0231</td>\n<td class=\"ltx_td ltx_align_center\">0.1648</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0000</span></td>\n<td class=\"ltx_td ltx_align_center\">0.983</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_bold\">Sentence-BERT Retrieval</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LLaMA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0087</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1683</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1090</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8508</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0272</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1835</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0106</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.971</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CommandR</th>\n<td class=\"ltx_td ltx_align_center\">0.0090</td>\n<td class=\"ltx_td ltx_align_center\">0.1614</td>\n<td class=\"ltx_td ltx_align_center\">0.1115</td>\n<td class=\"ltx_td ltx_align_center\">0.8504</td>\n<td class=\"ltx_td ltx_align_center\">0.0281</td>\n<td class=\"ltx_td ltx_align_center\">0.1780</td>\n<td class=\"ltx_td ltx_align_center\">0.0054</td>\n<td class=\"ltx_td ltx_align_center\">0.969</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mistral</th>\n<td class=\"ltx_td ltx_align_center\">0.0085</td>\n<td class=\"ltx_td ltx_align_center\">0.1469</td>\n<td class=\"ltx_td ltx_align_center\">0.1011</td>\n<td class=\"ltx_td ltx_align_center\">0.8500</td>\n<td class=\"ltx_td ltx_align_center\">0.0347</td>\n<td class=\"ltx_td ltx_align_center\">0.2009</td>\n<td class=\"ltx_td ltx_align_center\">0.0010</td>\n<td class=\"ltx_td ltx_align_center\">0.976</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GPT</th>\n<td class=\"ltx_td ltx_align_center\">0.0075</td>\n<td class=\"ltx_td ltx_align_center\">0.1616</td>\n<td class=\"ltx_td ltx_align_center\">0.1036</td>\n<td class=\"ltx_td ltx_align_center\">0.8519</td>\n<td class=\"ltx_td ltx_align_center\">0.0213</td>\n<td class=\"ltx_td ltx_align_center\">0.1531</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0000</span></td>\n<td class=\"ltx_td ltx_align_center\">0.979</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"9\"><span class=\"ltx_text ltx_font_bold\">BGE-M3 Retrieval</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LLaMA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0089</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.1744</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1119</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.8511</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0237</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1667</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0042</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.972</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CommandR</th>\n<td class=\"ltx_td ltx_align_center\">0.0091</td>\n<td class=\"ltx_td ltx_align_center\">0.1597</td>\n<td class=\"ltx_td ltx_align_center\">0.1143</td>\n<td class=\"ltx_td ltx_align_center\">0.8523</td>\n<td class=\"ltx_td ltx_align_center\">0.0247</td>\n<td class=\"ltx_td ltx_align_center\">0.1713</td>\n<td class=\"ltx_td ltx_align_center\">0.0002</td>\n<td class=\"ltx_td ltx_align_center\">0.974</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mistral</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.0094</span></td>\n<td class=\"ltx_td ltx_align_center\">0.1505</td>\n<td class=\"ltx_td ltx_align_center\">0.1063</td>\n<td class=\"ltx_td ltx_align_center\">0.8537</td>\n<td class=\"ltx_td ltx_align_center\">0.0315</td>\n<td class=\"ltx_td ltx_align_center\">0.1939</td>\n<td class=\"ltx_td ltx_align_center\">0.0008</td>\n<td class=\"ltx_td ltx_align_center\">0.979</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">GPT</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0079</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.1658</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.1086</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.8538</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0203</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.1533</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.0000</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.982</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "automatic",
            "meteor",
            "evaluation",
            "distinct1",
            "rougel",
            "rate",
            "strategies",
            "generation",
            "bgem3",
            "retrieval",
            "bm25",
            "across",
            "llama",
            "sentencebert",
            "results",
            "safety",
            "gpt",
            "averages",
            "llms",
            "commandr",
            "repetition",
            "model",
            "bertscoref1",
            "norag",
            "distinct2",
            "mistral",
            "different",
            "bleu"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S6.T2\" title=\"Table 2 &#8227; Human evaluation metrics. &#8227; 6 Metrics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports average results, that can be interpreted along three main dimensions: content quality, diversity, and safety.\n<span class=\"ltx_text ltx_font_bold\">Content Quality:</span> scores on BLEU, METEOR, and ROUGE-L remain relatively low, as expected for open-ended generation, while BERTScore values are consistently high (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.85), indicating strong semantic similarity to MT-Co references. Non-RAG outputs achieve slightly higher lexical and semantic overlap, suggesting that retrieval introduces content that, while factually richer, diverges from exact reference phrasing. RAG outputs show less words overlapping,\nlikely reflecting the inclusion of factual information from the knowledge base.\n<span class=\"ltx_text ltx_font_bold\">Diversity:</span> incorporating retrieval substantially improves lexical diversity and reduces repetition. Mistral with BGE-M3 shows the largest gains, while No-RAG models, particularly Mistral, exhibit high repetition and lower diversity. <span class=\"ltx_text ltx_font_bold\">Safety:</span> No-RAG configurations achieve the highest safety, with GPT leading (0.993). RAG outputs show slightly lower safety, likely due to occasional noise introduced from the retrieval process. Nevertheless, safety remains high overall, indicating that fact-grounded augmentation does not substantially compromise non-harmfulness.\n<span class=\"ltx_text ltx_font_bold\">Model-level Trends:</span> GPT consistently delivers the safest outputs and competitive BERTScore, though with lower lexical diversity than Mistral. Mistral excels in diversity with RAG but performs poorly in No-RAG due to high repetition. CommandR and LLaMA provide stable but moderate performance across metrics. Overall, RAG with BGE-M3 achieves the best balance between quality, diversity, and informativeness. All results differ significantly (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A4\" title=\"Appendix D Statistical Significance of Retrieval Effects &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> for more details).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Counter-speech generation is at the core of many expert activities, such as fact-checking and hate speech, to counter harmful content. Yet, existing work treats counter-speech generation as pure text generation task, mainly based on Large Language Models or NGO experts. These approaches show severe drawbacks due to the limited reliability and coherence in the generated countering text, and in scalability, respectively. To close this gap, we introduce a novel framework to model counter-speech generation as knowledge-wise text generation process. Our framework integrates advanced Retrieval-Augmented Generation (RAG) pipelines to ensure the generation of trustworthy counter-speech for 8 main target groups identified in the hate speech literature, including women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. We built a knowledge base over the United Nations Digital Library, EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792 texts. We use the MultiTarget-CONAN dataset to empirically assess the quality of the generated counter-speech, both through standard metrics (i.e., JudgeLM) and a human evaluation. Results show that our framework outperforms standard LLM baselines and competitive approach, on both assessments. The resulting framework and the knowledge base pave the way for studying trustworthy and sound counter-speech generation, in hate speech and beyond. <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">Warning<span class=\"ltx_text ltx_font_medium\">: this paper contains explicit examples some readers may find offensive.</span></span></p>\n\n",
                "matched_terms": [
                    "generation",
                    "evaluation",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">NGOs and experts successfully deployed CS campaigns, but the scale of HS makes manual CS unsustainable <cite class=\"ltx_cite ltx_citemacro_cite\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib16\" title=\"\">2021b</a>)</cite>.\nThis challenge has motivated research on automatic CS generation, an emerging NLP task. Early approaches relied on curated datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib14\" title=\"\">2019</a>); Fanton et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib20\" title=\"\">2021</a>)</cite> and fine-tuning of pre-trained language models (PLMs) <cite class=\"ltx_cite ltx_citemacro_cite\">Tekiro&#287;lu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib48\" title=\"\">2020</a>); Zhu and Bhat (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib56\" title=\"\">2021</a>); Tekiro&#287;lu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib47\" title=\"\">2022</a>)</cite>. While effective in-domain, such models often produce generic or off-topic responses on unseen HS.\nTo achieve factuality and informativeness in the generated CS, it is therefore essential to embed knowledge on the target HS groups in the generation process.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "automatic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tackle this challenging task by proposing a novel knowledge-grounded framework for automatic counter-speech generation that integrates advanced retrieval-augmented generation (RAG) pipelines. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows two examples of CS, with and without RAG augmentation. Our contributions are fourfold: <span class=\"ltx_text ltx_font_bold\">(1)</span> we provide a systematic comparison of multiple retrievers and LLMs for CS generation, combining three retrieval methods with four language models; <span class=\"ltx_text ltx_font_bold\">(2)</span> we enforce concise, two-sentence outputs tailored for social media deployment, ensuring responses remain natural, relatable, and effective in real-world contexts; <span class=\"ltx_text ltx_font_bold\">(3)</span> we conduct a pairwise evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, adapted to balance factual grounding with conciseness and pragmatic suitability, together with an extensive human evaluation, showing that our framework steadily outperforms standard baselines and state-of-the-art competitors; and <span class=\"ltx_text ltx_font_bold\">(4)</span> we will release all generated CS along with their top-3 retrieved evidence sentences, offering a reusable resource for the research community<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code will be made available upon acceptance.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "automatic",
                    "generation",
                    "evaluation",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial sequence-to-sequence models produced generic outputs <cite class=\"ltx_cite ltx_citemacro_cite\">Qian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib36\" title=\"\">2019</a>)</cite>. Subsequent pipelines improved quality via retrieval and selection <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu and Bhat (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib56\" title=\"\">2021</a>); Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib15\" title=\"\">2021a</a>); Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>)</cite>. Other studies focused on style and control <cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib7\" title=\"\">2023</a>); Saha et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib42\" title=\"\">2022</a>); Gupta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib22\" title=\"\">2023</a>)</cite>, leveraging argumentative and emotional cues. With LLMs, few-shot and zero-shot prompting became popular <cite class=\"ltx_cite ltx_citemacro_cite\">Ashida and Komachi (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib1\" title=\"\">2022</a>); Tekiro&#287;lu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib47\" title=\"\">2022</a>); Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib53\" title=\"\">2023</a>)</cite>, reducing annotation needs but often lacking factual grounding.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic metrics (BLEU, ROUGE, BERTScore) correlate poorly with human judgments, prompting exploration of novelty- and repetition-based scores <cite class=\"ltx_cite ltx_citemacro_cite\">Wang and Wan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib49\" title=\"\">2018</a>); Bertoldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib6\" title=\"\">2013</a>)</cite>, and LLM-based frameworks (e.g., GPT-4, PandaLM, JudgeLM, UniEval) <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib55\" title=\"\">2023</a>); Zhong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib54\" title=\"\">2022</a>)</cite>, which show improved reliability for multi-aspect CS assessment <cite class=\"ltx_cite ltx_citemacro_cite\">Jones et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib25\" title=\"\">2024</a>); Damo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib18\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "automatic",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG addresses a key limitation of prior approaches by grounding outputs in verifiable evidence, thereby enhancing factuality and persuasiveness in countering HS.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib15\" title=\"\">2021a</a>)</cite> proposed a retrieval-augmented pipeline that first generates queries from HS using keyword extraction, then employs BM25 <cite class=\"ltx_cite ltx_citemacro_cite\">Robertson and Zaragoza (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib39\" title=\"\">2009</a>)</cite> to retrieve relevant articles from Newsroom <cite class=\"ltx_cite ltx_citemacro_cite\">Grusky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib21\" title=\"\">2018</a>)</cite> and WikiText-103 <cite class=\"ltx_cite ltx_citemacro_cite\">Merity et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib32\" title=\"\">2016</a>)</cite>. From these, the most relevant sentences are selected using the ROUGE-L metric <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib29\" title=\"\">2004</a>)</cite>, before being passed to GPT-2 <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib37\" title=\"\">2019</a>)</cite> and XNLG <cite class=\"ltx_cite ltx_citemacro_cite\">Chi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib13\" title=\"\">2020</a>)</cite>.\nSimilarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>)</cite> introduced RAUCG, which retrieves counter-arguments from the ChangeMyView subreddit, selecting them based on stance consistency, semantic overlap, and a custom perplexity-based fitness function. The final generation step employs energy-based decoding to preserve factual knowledge while countering HS fluently.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> proposed ReZG, a retrieval-augmented zero-shot approach that integrates multi-dimensional hierarchical retrieval with constrained decoding, enabling the generation of more specific CS for unseen HS targets. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> leverage both curated background knowledge and web search to improve factuality.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> evaluated reranker-based pipelines, showing that fine-grained retrieval significantly improves factuality and relevance.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "retrieval",
                    "rougel",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step, we built a comprehensive Knowledge Base (KB) designed to ensure maximal coverage of documents addressing social groups commonly targeted by hate speech. The goal of this KB is to gather all relevant materials &#8212; such as reports, resolutions, and legal texts &#8212; that provide evidence or context regarding these topics.\nSpecifically, we focused on the following 8 target groups: women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. These categories align with the classic targets in the literature, including the ones of our baseline MultiTarget-CONAN <cite class=\"ltx_cite ltx_citemacro_cite\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib14\" title=\"\">2019</a>)</cite>.\nWe used GPT-based prompting to generate synonyms and semantically related keywords, ensuring that queries captured diverse terminology across cultural and policy contexts (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for the prompt). To guarantee relevance, we performed a keyword-based search so that retrieved knowledge was directly aligned with the hateful messages targeting these groups. To ensure reliability in the generated CS, we relied exclusively on institutional publicly available sources, i.e., the United Nations Digital Library, EUR-Lex, and the European Union Agency for Fundamental Rights (FRA). To construct our knowledge base, we follow three main steps: document retrieval, PDF-to-text conversion, and knowledge base integration.</p>\n\n",
                "matched_terms": [
                    "across",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Document Retrieval.</span> The <span class=\"ltx_text ltx_font_bold\">United Nations Digital Library</span> serves as the central repository for official UN documents on human rights, equality, and anti-discrimination. We developed a custom crawler using <span class=\"ltx_text ltx_font_typewriter\">requests</span> and <span class=\"ltx_text ltx_font_typewriter\">BeautifulSoup</span> to systematically combine three query dimensions&#8212;target keywords, document types (e.g., resolutions, treaties, NGO statements), and years (2000&#8211;2025). The crawler paginated through results, extracted and normalized documents in English, and organized downloads by target group. A metadata file recorded <span class=\"ltx_text ltx_font_typewriter\">id, fname, target, type, year, url</span>. Error handling covered duplicate checks, retries, and skipped completed downloads. At the European level, <span class=\"ltx_text ltx_font_bold\">EUR-Lex</span> provides access to EU law, treaties, and legal acts, while the <span class=\"ltx_text ltx_font_bold\">EU Agency for Fundamental Rights (FRA)</span> publishes reports on human rights within the EU. Using the same target keywords, we retrieved documents in English (2000&#8211;2025) from both sources, following the same metadata structure as the UN corpus. These sources complement the UN materials, forming a multi-level knowledge base&#8212;global and European&#8212;that ensures comprehensive and reliable grounding for CS generation.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "retrieval",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S3.T1\" title=\"Table 1 &#8227; 3 Knowledge Base Construction &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports descriptive statistics of the UN and EU corpora by target group keyword. While the UN collection is considerably larger in terms of the number of documents (over 32k), EU reports are more detailed, with substantially higher average word counts per document (<math alttext=\"\\approx 26k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p5.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>26</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 26k</annotation></semantics></math> vs. <math alttext=\"\\approx 7.7k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p5.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>7.7</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 7.7k</annotation></semantics></math>). Coverage also varies by group: for example, the UN corpus contains a large volume of material on human rights and women, while EU provides more in-depth analyses on migrants, LGBT+ individuals, and people with disabilities. Together, these complementary sources balance breadth (UN) with depth (EU), creating a diverse and representative foundation for counter-speech generation. This ensures that our knowledge base is both comprehensive and adaptable to different CS scenarios.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our RAG-based framework integrates three key components: (i) paragraph retrieval, (ii) paragraph summarization, and (iii) counter-speech generation. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S4.F2\" title=\"Figure 2 &#8227; 4 Pipeline &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides an overview of the pipeline.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a HS message <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, we aim to retrieve a small set of relevant paragraphs from the KB (i.e., the most similar to the target HS).\nFor this, we employ three complementary retrieval models: BM25, Sentence-BERT (SBERT), and BGE-M3.\nLet the KB consist of paragraphs <math alttext=\"\\mathcal{P}=\\{p_{1},p_{2},\\ldots,p_{N}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mi>N</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}=\\{p_{1},p_{2},\\ldots,p_{N}\\}</annotation></semantics></math>. For each retriever\n<math alttext=\"r\\in\\{\\text{BM25},\\text{SBERT},\\text{BGE-M3}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>BM25</mtext><mo>,</mo><mtext>SBERT</mtext><mo>,</mo><mtext>BGE-M3</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in\\{\\text{BM25},\\text{SBERT},\\text{BGE-M3}\\}</annotation></semantics></math>, we compute a similarity score:\n<math alttext=\"s_{r}(h,p_{i})=\\text{sim}_{r}(\\phi(h),\\phi(p_{i}))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>s</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mtext>sim</mtext><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">s_{r}(h,p_{i})=\\text{sim}_{r}(\\phi(h),\\phi(p_{i}))</annotation></semantics></math>\nwhere <math alttext=\"\\phi(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(\\cdot)</annotation></semantics></math> is the embedding (or term-weight) function induced by the retriever. For BM25, the similarity score is computed based on TF-IDF, and document length normalization. For SBERT and BGE-M3, similarity is measured as the cosine similarity between dense vector embeddings of the HS and candidate paragraph, capturing semantic relatedness even in the absence of exact lexical overlap.\nFor each retriever, we obtain the top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> ranked paragraphs:\n<math alttext=\"R_{r}(h)=\\{p_{r,1},p_{r,2},\\ldots,p_{r,k}\\},\\quad k=3.\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mrow><mrow><mrow><msub><mi>R</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mn>2</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo rspace=\"1.167em\">,</mo><mrow><mi>k</mi><mo>=</mo><mn>3</mn></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">R_{r}(h)=\\{p_{r,1},p_{r,2},\\ldots,p_{r,k}\\},\\quad k=3.</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "sentencebert",
                    "bgem3",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although retrieval provides relevant context, LLMs have limited context windows, making direct use of full paragraphs infeasible. To address this, we summarize retrieved paragraphs before passing them to the generation stage.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "generation",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Summaries are stored in CSV format along with their source IDs, ensuring alignment between retrieval and generation stages. The summarization prompts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the final stage, the summaries are used as external knowledge to generate CS. For each HS message <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, retriever <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>, and model <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, the generation function is defined as:</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since we use three retrievers and four LLMs, the system produces: <math alttext=\"|C(h)|=|\\{c_{r,m}(h)\\}|=3\\times 4=12\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>c</mi><mrow><mi>r</mi><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow><mo>=</mo><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">|C(h)|=|\\{c_{r,m}(h)\\}|=3\\times 4=12</annotation></semantics></math> CS outputs for each HS instance. This enables systematic comparison across retrieval methods and summarization strategies.\nTo ensure that generated CS is deployable in (online) real-world contexts, we restrict outputs to a maximum of two sentences. This reflects the communicative norms of social media platforms, where posts are typically 1&#8211;2 sentences long <cite class=\"ltx_cite ltx_citemacro_citep\">(&#350;ahinu&#231; and Toraman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib43\" title=\"\">2021</a>)</cite>. Prior work shows that overly verbose responses are less engaging and less effective in countering harmful narratives <cite class=\"ltx_cite ltx_citemacro_citep\">(Russo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib41\" title=\"\">2023</a>)</cite>. Concise and relatable CS has been repeatedly identified as key to user engagement and effectiveness <cite class=\"ltx_cite ltx_citemacro_citep\">(Bonaldi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib8\" title=\"\">2024</a>; Benesch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib4\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "strategies",
                    "across",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ Meta-Llama-3.1-8B-Instruct<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct</a></span></span></span>, Cohere&#8217;s Command-R-7B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>We included CommandR, optimized for grounding outputs in retrieved context.</span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Cohere et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib17\" title=\"\">2025</a>)</cite>, Mistral-7B-Instruct-v0.3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\" title=\"\">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3</a></span></span></span>, and gpt-4o-mini-2024-07-18. These models were selected as LLMs of similar size, balancing efficiency and accuracy, allowing comparison across open-weight and proprietary settings.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "across",
                    "commandr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BM25</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Robertson and Zaragoza (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib39\" title=\"\">2009</a>)</cite> is a sparse lexical retriever based on TF-IDF with document length normalization. It remains an effective baseline for keyword-sensitive domains where exact term overlap is important. <span class=\"ltx_text ltx_font_bold\">Sentence-BERT (SBERT)</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib38\" title=\"\">2019</a>)</cite> encodes queries and passages into dense vector embeddings optimized for semantic similarity, with ranking performed via cosine similarity.\n<span class=\"ltx_text ltx_font_bold\">BGE-M3 (BAAI General Embeddings).</span> BGE-M3 <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib12\" title=\"\">2024</a>)</cite> is a recent dense embedding model trained for multilingual and multi-task semantic retrieval. These retrievers capture complementary dimensions of relevance, from exact term matching (BM25) to semantic similarity (SBERT, BGE-M3), ensuring robust retrieval across different types of hateful content and knowledge sources.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "sentencebert",
                    "bgem3",
                    "different",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experimented on the Multi-Target CONAN (MTCo) dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib9\" title=\"\">2022</a>)</cite>, which contains 5,003 HS/CS pairs in English covering multiple target groups, including people with disabilities, Jews, LGBT+ individuals, Muslims, migrants, people of color (POC), and women. Collected through a human-in-the-loop process, MTCo provides high-quality, contextually relevant CS, and serves as a first baseline for our experiments. Additionally, we compare our pipeline with the four instruction-tuned LLMs of comparable size without RAG. Finally, we compare our results with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>We cannot compare to <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> due to lack of publicly available code and data, even upon request.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Reference-based Metrics.</span> To evaluate alignment with human-written CS in MT-Co, we report: <span class=\"ltx_text ltx_font_bold\">BLEU-4</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib35\" title=\"\">2002</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">ROUGE-L</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib29\" title=\"\">2004</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">METEOR</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Banerjee and Lavie (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib2\" title=\"\">2005</a>)</cite>, and <span class=\"ltx_text ltx_font_bold\">BERTScore</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib52\" title=\"\">2019</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "meteor",
                    "rougel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Reference-less Metrics.</span> To complement reference-based evaluation, we assess intrinsic qualities of generated CS. <span class=\"ltx_text ltx_font_bold\">Distinct-1/2</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib28\" title=\"\">2015</a>)</cite> measures the proportion of unique unigrams and bigrams, reflecting lexical diversity. <span class=\"ltx_text ltx_font_bold\">Repetition Rate (RR)</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Cettolo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib11\" title=\"\">2014</a>)</cite> represents the fraction of repeated n-grams within a generation. <span class=\"ltx_text ltx_font_bold\">Safety</span>: the OpenAI&#8217;s content moderation API scores each output across categories of potential harm (e.g., hate, sexual, violence), with higher values indicate safer counter-speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib8\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "rate",
                    "repetition",
                    "evaluation",
                    "generation",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge Evaluation.</span>\nWe use an adaptation of <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib55\" title=\"\">2023</a>)</cite> tailored for CS evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib10\" title=\"\">2025</a>); Zubiaga et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib58\" title=\"\">2024</a>)</cite>. While traditional metrics capture surface and semantic alignment, JudgeLM provides a more holistic evaluation of CS quality along different dimensions. We use the version of <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> with fine-tuned Llama-instruct-7B.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic metrics cannot fully account for pragmatic qualities of CS. To address this, we conduct a human evaluation focusing on dimensions directly relevant to the effectiveness and quality of CS <cite class=\"ltx_cite ltx_citemacro_cite\">Stapleton and Wu (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib46\" title=\"\">2015</a>); Bengoetxea et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib5\" title=\"\">2024</a>); Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib8\" title=\"\">2024</a>)</cite>. Participants assess each CS along the following criteria, using a Likert scale from 1 to 3 (3 being the highest score)(see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A6\" title=\"Appendix F Guidelines for Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> for full guidelines):</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "automatic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate CS quality using <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span>, focusing on four dimensions: factuality, number of relevant facts, relevance to the HS, and specificity. This prompt (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>) guides pairwise comparisons between model outputs, emphasizing informative and targeted CS over surface-level similarity.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T3\" title=\"Table 3 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> results for RAG vs. No-RAG models across retrieval strategies. BGE-M3 yields the strongest results, followed by SentBERT and BM25, confirming the advantage of semantically rich retrieval. Among generators, GPT wins most comparisons, showing effective integration of retrieved content. Mistral also benefits notably from RAG, while CommandR performs less consistently.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T4\" title=\"Table 4 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the best RAG setups (LLM + BGE-M3) against human-written MT-Co CS. All RAG systems perform strongly, with GPT + BGE-M3 approaching human-level quality and Mistral + BGE-M3 remaining competitive, whereas CommandR trails slightly but still surpasses baselines.</p>\n\n",
                "matched_terms": [
                    "across",
                    "commandr",
                    "strategies",
                    "model",
                    "bgem3",
                    "results",
                    "norag",
                    "gpt",
                    "retrieval",
                    "bm25",
                    "mistral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations reveal clear trends: <span class=\"ltx_text ltx_font_bold\">(1)</span> RAG outputs are less repetitive, and more diverse\nthan No-RAG, reflecting the addition of factual content. Lexical and semantic similarity remains high, ensuring CS remains aligned with the original intent.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> Safety is only slightly reduced with RAG, likely due to occasional noisy content.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> RAG consistently outperforms No-RAG and MT-Co CS in both automatic and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "norag",
                    "automatic",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings demonstrate that RAG &#8212;particularly with semantically rich BGE-M3 embeddings&#8212; enhances CS quality, diversity, and factual grounding, while maintaining strong safety and overall effectiveness.</p>\n\n",
                "matched_terms": [
                    "bgem3",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted a human evaluation with 26 participants to assess the quality and effectiveness of the generated CS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Participants&#8217; age is between 18 and 50, there is a balance between genders, and their level of education spans high school diploma to PhD. Detailed statistics are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5\" title=\"Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span>. Our evaluation setup consisted of 10 HS examples, extracted randomly from MT-Co keeping the target distribution, with 9 CS candidates for each HS. We selected the CS from MT-Co and the CS generated by our four LLMs without the RAG pipeline, and the corresponding CS with RAG using BMG-M3 as retriever, since it has shown the strongest performance in the automatic and LLM-based metrics. Participants rated all 90 CS candidates along four dimensions, i.e., Relevance, Factuality, Cogency, and Correctness (1&#8211;3 scale), judged whether each CS was effective (Yes/No) and selected the best CS per HS. We collected a total of 2340 evaluations.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "evaluation",
                    "automatic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metric Scores.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S8.T5\" title=\"Table 5 &#8227; 8 Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports average scores for Relevance, Factuality, Cogency, and Correctness across CS methods. GPT RAG achieved the highest scores overall, particularly in Factuality (2.75) and Cogency (2.41). RAG-based methods generally outperformed their non-RAG counterparts, while MT-Co scored lowest across all metrics, indicating lower relevance, factual accuracy, and overall effectiveness.</p>\n\n",
                "matched_terms": [
                    "across",
                    "gpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effectiveness.</span> Out of the 2340 evaluations collected, 1312 were marked as effective, corresponding to 56% of the cases. More specifically, Llama RAG was considered effective most frequently (with 197 votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effectiveness votes (48) (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A3\" title=\"Appendix C Additional Results from the Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for full results). This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. The non-RAG models and MTCo were less effective overall. This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline, suggesting that both model choice and augmentation strategy strongly influence performance.</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "llama",
                    "model",
                    "results",
                    "gpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, RAG-based methods outperform non-RAG counterparts across all the four metrics, especially in Factuality, Cogency and Effectiveness, and are preferred by annotators in best-choice votes. The baseline method MT-Co consistently scores lowest across all metrics and effectiveness. Overall, <span class=\"ltx_text ltx_font_bold\">GPT RAG</span> is the best CS method across all HS and annotators. Results from the human evaluation are aligned with both automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "automatic",
                    "evaluation",
                    "results",
                    "gpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To contextualize our results, we compared our RAG-based CS with publicly available samples from two recent studies. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> (Baseline 1) released 1,697 GPT-4o-generated CS responses from their RAG pipeline addressing MT-Co hate speech, while <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> (Baseline 2) submitted 400 CS samples to the COLING 2025 shared task using LLaMA-EUS-8B with retrieved knowledge. Since the samples were multilingual, we retained only English CS aligned with MT-Co, totaling 124 samples.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> produced 100 English CS, 24 HS instances overlapped with MT-CONAN and were retained with multiple CS variants.</span></span></span>\nFor fair comparison, we reproduced settings closest to the original models: GPT-4o-mini + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite>, and LLaMA-8B + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>. This alignment minimizes differences due to model size, isolating the effects of retrieval and pipeline design. Pairwise performance was then assessed with <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> (prompt available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>). It is fine-tuned to rate helpfulness, relevance, accuracy, and level of detail of CS responses, which already accounts for evidence and factuality in the original prompt. To additionally emphasize conciseness, and suitability for real-world deployment on social media, we limit the CS to a maximum of two sentences.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model",
                    "bgem3",
                    "results",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S9.T6\" title=\"Table 6 &#8227; 9 Comparison with competitors &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that our RAG-augmented CS consistently outperforms the baseline samples from both studies. GPT-4o-mini + BGE-M3 outperforms Baseline 1 in 962 out of 1,697 pairwise battles, while LLaMA-8B + BGE-M3 outperforms Baseline 2 in 114 out of 124 battles. These results demonstrate that our RAG pipeline produces CS that is not only factually richer but also concise and suitable for social media, aligning with the intended communicative goals of real-world deployment.</p>\n\n",
                "matched_terms": [
                    "bgem3",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S9.T6\" title=\"Table 6 &#8227; 9 Comparison with competitors &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> also reports the per-target breakdown of our pairwise comparison.\nAgainst Baseline 1, our system achieves overall consistent improvements. Gains are observed across all HS targets, with particularly strong performance for <span class=\"ltx_text ltx_font_italic\">DISABLED</span> (69.4%), <span class=\"ltx_text ltx_font_italic\">LGBT+</span> (64.0%), and <span class=\"ltx_text ltx_font_italic\">WOMEN</span> (61.2%). For larger target categories such as <span class=\"ltx_text ltx_font_italic\">MUSLIMS</span> and <span class=\"ltx_text ltx_font_italic\">MIGRANTS</span>, our model still maintains a clear advantage with 55.2% and 56.8% wins respectively, despite the higher difficulty and variability in these groups. The most balanced outcome is seen for <span class=\"ltx_text ltx_font_italic\">JEWS</span>, where results are split evenly (50%). Against Baseline 2, our system achieves consistently strong results across all targets, outperforming it in 93% of cases. The largest margins are observed for <span class=\"ltx_text ltx_font_italic\">MIGRANTS</span> (100% wins) and <span class=\"ltx_text ltx_font_italic\">JEWS</span> (95.7%).</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results indicate that while our models consistently outperform prior baselines, the improvement varies by target group. In particular, our LLaMA-based system demonstrates decisive superiority over Baseline 2 comparable setup, whereas the GPT-based comparison with Baseline 1 highlights more incremental but robust gains across diverse HS categories, highlighting the effectiveness of our RAG approach in improving both informativeness and practical usability of the CS.</p>\n\n",
                "matched_terms": [
                    "across",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a RAG-based framework for automatic counter-speech generation. We systematically compared three retrieval methods and four LLMs for CS generation targeting height groups (women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, other), relying on a novel and unique knowledge base, built over three institutional sources. We conducted an extensive experimental evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, and a human evaluation. Our results show that our approach outperforms competitive approaches and standard baselines on both of them. Our experiments demonstrated the versatility and soundness of our framework for counter-speech generation to fight online abusive content.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "automatic",
                    "generation",
                    "evaluation",
                    "results",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our retrieval process operates at the paragraph level, meaning documents in the knowledge base are split into shorter segments rather than used in full. This improves efficiency but may potentially fragment contextual information, omitting relevant background or nuance. Moreover, we restrict the retrieved context to the top-<math alttext=\"k=3\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">k=3</annotation></semantics></math> most similar paragraphs to control input length and maintain conciseness in generation. Although this design balances informativeness and computational efficiency, varying <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.p1.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> could influence the factual richness and diversity of the generated counter-speech.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, despite employing strong retrievers (BM25, SBERT, and BGE-M3), retrieval quality depends on the coverage and relevance of the knowledge base. Gaps or biases in external sources can propagate into the generated responses, particularly for emerging or culturally specific hate topics, even if we mitigated this by choosing recognized authoritative sources.</p>\n\n",
                "matched_terms": [
                    "bgem3",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study involves the use of hate speech examples for the development and evaluation of counter-speech generation systems. We acknowledge that the inclusion of HS content poses potential risks of exposure to harmful language and emotional distress for researchers and annotators. All individuals involved in data handling were informed of these risks and participated voluntarily, following institutional ethical guidelines.\nAlthough our goal is to promote positive and factual discourse, automatic CS generation can inadvertently reinforce biases, produce factually incorrect content, or convey unintended tones. To mitigate these risks, we rely on institutional sources (UN, EU, FRA) for retrieval, explicitly evaluate factuality and correctness, and use human oversight in all analyses. The system is presented for research purposes only and is not intended for unsupervised deployment.\nWe further recognize that the perceived effectiveness and appropriateness of CS depend on social and cultural context. Our methods and findings should therefore not be generalized without careful adaptation and ethical review. No personal or private data were used; all retrieved materials come from publicly available institutional sources.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "evaluation",
                    "retrieval",
                    "automatic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Step 3 of our pipeline (CS generation), we prompt all LLMs with the following:</p>\n\n",
                "matched_terms": [
                    "llms",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the total number of times each CS method was rated effective, and how many times it has been selected as the best one across all HS and annotators. RAG-based methods are clearly preferred compared to their No RAG counter-parts. Concerning effectiveness, Llama RAG was considered effective most frequently (197 &#8220;Yes&#8221; votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effective votes (48). This indicates that RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. Furthermore, RAG methods are selected more often as best ones compared to their No RAG counterparts, with GPT RAG and Llama RAG achieving the best results.</p>\n\n",
                "matched_terms": [
                    "across",
                    "commandr",
                    "llama",
                    "results",
                    "gpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted non-parametric Friedman tests followed by Bonferroni-corrected Wilcoxon signed-rank tests to evaluate whether RAG retrieval strategies (BM25, Sentence-BERT, BGE) significantly affected model outputs compared to the No-RAG baseline for the per-sample automatic metrics (BLEU, METEOR, ROUGE-L, BERTScore), which allow direct pairwise comparison across systems. In contrast, diversity metrics such as Distinct-1, Distinct-2, and Repetition Rate are computed at the corpus level, producing a single value per model.\nBecause these measures do not yield per-sample scores and thus lack within-system variance, statistical testing is not applicable.\nFor all models, Friedman tests revealed significant overall effects (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), and pairwise comparisons confirmed that retrieval methods consistently induced statistically significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>) across quality and safety metrics.</p>\n\n",
                "matched_terms": [
                    "across",
                    "rougel",
                    "automatic",
                    "rate",
                    "strategies",
                    "repetition",
                    "model",
                    "meteor",
                    "sentencebert",
                    "distinct1",
                    "safety",
                    "distinct2",
                    "norag",
                    "retrieval",
                    "bm25",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we provide different examples of counter-speech addressing the same hateful message from MT-CONAN. We show CS from GPT No RAG and GPT RAG with BGE-M3.</p>\n\n",
                "matched_terms": [
                    "different",
                    "gpt",
                    "bgem3"
                ]
            }
        ]
    },
    "S7.T3": {
        "caption": "Table 3: JudgeLM wins (out of 5003 pairwise comparisons) comparing corresponding LLMs w/ and w/out RAG with different retrieval strategies. Lost indicates that the model was outperformed.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BM25</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SentBERT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BGE-M3</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Mistral</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3011</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3103</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3557</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LLaMA</th>\n<td class=\"ltx_td ltx_align_center\">2576</td>\n<td class=\"ltx_td ltx_align_center\">2805</td>\n<td class=\"ltx_td ltx_align_center\">3532</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GPT</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3524</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3941</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4223</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">CommandR</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2490 (lost)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2620</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3125</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sentbert",
            "out",
            "outperformed",
            "corresponding",
            "pairwise",
            "strategies",
            "rag",
            "bgem3",
            "judgelm",
            "retrieval",
            "bm25",
            "comparing",
            "llama",
            "lost",
            "comparisons",
            "indicates",
            "wout",
            "gpt",
            "llms",
            "wins",
            "commandr",
            "model",
            "mistral",
            "different",
            "lost"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate CS quality using <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span>, focusing on four dimensions: factuality, number of relevant facts, relevance to the HS, and specificity. This prompt (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>) guides pairwise comparisons between model outputs, emphasizing informative and targeted CS over surface-level similarity.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T3\" title=\"Table 3 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> results for RAG vs. No-RAG models across retrieval strategies. BGE-M3 yields the strongest results, followed by SentBERT and BM25, confirming the advantage of semantically rich retrieval. Among generators, GPT wins most comparisons, showing effective integration of retrieved content. Mistral also benefits notably from RAG, while CommandR performs less consistently.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T4\" title=\"Table 4 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the best RAG setups (LLM + BGE-M3) against human-written MT-Co CS. All RAG systems perform strongly, with GPT + BGE-M3 approaching human-level quality and Mistral + BGE-M3 remaining competitive, whereas CommandR trails slightly but still surpasses baselines.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Counter-speech generation is at the core of many expert activities, such as fact-checking and hate speech, to counter harmful content. Yet, existing work treats counter-speech generation as pure text generation task, mainly based on Large Language Models or NGO experts. These approaches show severe drawbacks due to the limited reliability and coherence in the generated countering text, and in scalability, respectively. To close this gap, we introduce a novel framework to model counter-speech generation as knowledge-wise text generation process. Our framework integrates advanced Retrieval-Augmented Generation (RAG) pipelines to ensure the generation of trustworthy counter-speech for 8 main target groups identified in the hate speech literature, including women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. We built a knowledge base over the United Nations Digital Library, EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792 texts. We use the MultiTarget-CONAN dataset to empirically assess the quality of the generated counter-speech, both through standard metrics (i.e., JudgeLM) and a human evaluation. Results show that our framework outperforms standard LLM baselines and competitive approach, on both assessments. The resulting framework and the knowledge base pave the way for studying trustworthy and sound counter-speech generation, in hate speech and beyond. <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">Warning<span class=\"ltx_text ltx_font_medium\">: this paper contains explicit examples some readers may find offensive.</span></span></p>\n\n",
                "matched_terms": [
                    "judgelm",
                    "rag",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tackle this challenging task by proposing a novel knowledge-grounded framework for automatic counter-speech generation that integrates advanced retrieval-augmented generation (RAG) pipelines. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows two examples of CS, with and without RAG augmentation. Our contributions are fourfold: <span class=\"ltx_text ltx_font_bold\">(1)</span> we provide a systematic comparison of multiple retrievers and LLMs for CS generation, combining three retrieval methods with four language models; <span class=\"ltx_text ltx_font_bold\">(2)</span> we enforce concise, two-sentence outputs tailored for social media deployment, ensuring responses remain natural, relatable, and effective in real-world contexts; <span class=\"ltx_text ltx_font_bold\">(3)</span> we conduct a pairwise evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, adapted to balance factual grounding with conciseness and pragmatic suitability, together with an extensive human evaluation, showing that our framework steadily outperforms standard baselines and state-of-the-art competitors; and <span class=\"ltx_text ltx_font_bold\">(4)</span> we will release all generated CS along with their top-3 retrieved evidence sentences, offering a reusable resource for the research community<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code will be made available upon acceptance.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "pairwise",
                    "rag",
                    "judgelm",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Initial sequence-to-sequence models produced generic outputs <cite class=\"ltx_cite ltx_citemacro_cite\">Qian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib36\" title=\"\">2019</a>)</cite>. Subsequent pipelines improved quality via retrieval and selection <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu and Bhat (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib56\" title=\"\">2021</a>); Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib15\" title=\"\">2021a</a>); Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>)</cite>. Other studies focused on style and control <cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib7\" title=\"\">2023</a>); Saha et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib42\" title=\"\">2022</a>); Gupta et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib22\" title=\"\">2023</a>)</cite>, leveraging argumentative and emotional cues. With LLMs, few-shot and zero-shot prompting became popular <cite class=\"ltx_cite ltx_citemacro_cite\">Ashida and Komachi (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib1\" title=\"\">2022</a>); Tekiro&#287;lu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib47\" title=\"\">2022</a>); Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib53\" title=\"\">2023</a>)</cite>, reducing annotation needs but often lacking factual grounding.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG addresses a key limitation of prior approaches by grounding outputs in verifiable evidence, thereby enhancing factuality and persuasiveness in countering HS.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib15\" title=\"\">2021a</a>)</cite> proposed a retrieval-augmented pipeline that first generates queries from HS using keyword extraction, then employs BM25 <cite class=\"ltx_cite ltx_citemacro_cite\">Robertson and Zaragoza (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib39\" title=\"\">2009</a>)</cite> to retrieve relevant articles from Newsroom <cite class=\"ltx_cite ltx_citemacro_cite\">Grusky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib21\" title=\"\">2018</a>)</cite> and WikiText-103 <cite class=\"ltx_cite ltx_citemacro_cite\">Merity et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib32\" title=\"\">2016</a>)</cite>. From these, the most relevant sentences are selected using the ROUGE-L metric <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib29\" title=\"\">2004</a>)</cite>, before being passed to GPT-2 <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib37\" title=\"\">2019</a>)</cite> and XNLG <cite class=\"ltx_cite ltx_citemacro_cite\">Chi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib13\" title=\"\">2020</a>)</cite>.\nSimilarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>)</cite> introduced RAUCG, which retrieves counter-arguments from the ChangeMyView subreddit, selecting them based on stance consistency, semantic overlap, and a custom perplexity-based fitness function. The final generation step employs energy-based decoding to preserve factual knowledge while countering HS fluently.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> proposed ReZG, a retrieval-augmented zero-shot approach that integrates multi-dimensional hierarchical retrieval with constrained decoding, enabling the generation of more specific CS for unseen HS targets. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> leverage both curated background knowledge and web search to improve factuality.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> evaluated reranker-based pipelines, showing that fine-grained retrieval significantly improves factuality and relevance.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a HS message <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, we aim to retrieve a small set of relevant paragraphs from the KB (i.e., the most similar to the target HS).\nFor this, we employ three complementary retrieval models: BM25, Sentence-BERT (SBERT), and BGE-M3.\nLet the KB consist of paragraphs <math alttext=\"\\mathcal{P}=\\{p_{1},p_{2},\\ldots,p_{N}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mi>N</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}=\\{p_{1},p_{2},\\ldots,p_{N}\\}</annotation></semantics></math>. For each retriever\n<math alttext=\"r\\in\\{\\text{BM25},\\text{SBERT},\\text{BGE-M3}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>BM25</mtext><mo>,</mo><mtext>SBERT</mtext><mo>,</mo><mtext>BGE-M3</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in\\{\\text{BM25},\\text{SBERT},\\text{BGE-M3}\\}</annotation></semantics></math>, we compute a similarity score:\n<math alttext=\"s_{r}(h,p_{i})=\\text{sim}_{r}(\\phi(h),\\phi(p_{i}))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>s</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mtext>sim</mtext><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">s_{r}(h,p_{i})=\\text{sim}_{r}(\\phi(h),\\phi(p_{i}))</annotation></semantics></math>\nwhere <math alttext=\"\\phi(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(\\cdot)</annotation></semantics></math> is the embedding (or term-weight) function induced by the retriever. For BM25, the similarity score is computed based on TF-IDF, and document length normalization. For SBERT and BGE-M3, similarity is measured as the cosine similarity between dense vector embeddings of the HS and candidate paragraph, capturing semantic relatedness even in the absence of exact lexical overlap.\nFor each retriever, we obtain the top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> ranked paragraphs:\n<math alttext=\"R_{r}(h)=\\{p_{r,1},p_{r,2},\\ldots,p_{r,k}\\},\\quad k=3.\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mrow><mrow><mrow><msub><mi>R</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mn>2</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo rspace=\"1.167em\">,</mo><mrow><mi>k</mi><mo>=</mo><mn>3</mn></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">R_{r}(h)=\\{p_{r,1},p_{r,2},\\ldots,p_{r,k}\\},\\quad k=3.</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "bgem3",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although retrieval provides relevant context, LLMs have limited context windows, making direct use of full paragraphs infeasible. To address this, we summarize retrieved paragraphs before passing them to the generation stage.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each retriever <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> and each paragraph <math alttext=\"p_{r,j}\\in R_{r}(h)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>&#8712;</mo><mrow><msub><mi>R</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">p_{r,j}\\in R_{r}(h)</annotation></semantics></math>, we obtain a summary using one out of four LLMs:\n<math alttext=\"m\\in\\{\\text{GPT-4o-mini},\\text{Llama},\\text{Command-R},\\text{Mistral}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>GPT-4o-mini</mtext><mo>,</mo><mtext>Llama</mtext><mo>,</mo><mtext>Command-R</mtext><mo>,</mo><mtext>Mistral</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">m\\in\\{\\text{GPT-4o-mini},\\text{Llama},\\text{Command-R},\\text{Mistral}\\}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "out"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since we use three retrievers and four LLMs, the system produces: <math alttext=\"|C(h)|=|\\{c_{r,m}(h)\\}|=3\\times 4=12\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>c</mi><mrow><mi>r</mi><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow><mo>=</mo><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">|C(h)|=|\\{c_{r,m}(h)\\}|=3\\times 4=12</annotation></semantics></math> CS outputs for each HS instance. This enables systematic comparison across retrieval methods and summarization strategies.\nTo ensure that generated CS is deployable in (online) real-world contexts, we restrict outputs to a maximum of two sentences. This reflects the communicative norms of social media platforms, where posts are typically 1&#8211;2 sentences long <cite class=\"ltx_cite ltx_citemacro_citep\">(&#350;ahinu&#231; and Toraman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib43\" title=\"\">2021</a>)</cite>. Prior work shows that overly verbose responses are less engaging and less effective in countering harmful narratives <cite class=\"ltx_cite ltx_citemacro_citep\">(Russo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib41\" title=\"\">2023</a>)</cite>. Concise and relatable CS has been repeatedly identified as key to user engagement and effectiveness <cite class=\"ltx_cite ltx_citemacro_citep\">(Bonaldi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib8\" title=\"\">2024</a>; Benesch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib4\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "strategies",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ Meta-Llama-3.1-8B-Instruct<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct</a></span></span></span>, Cohere&#8217;s Command-R-7B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>We included CommandR, optimized for grounding outputs in retrieved context.</span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Cohere et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib17\" title=\"\">2025</a>)</cite>, Mistral-7B-Instruct-v0.3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\" title=\"\">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3</a></span></span></span>, and gpt-4o-mini-2024-07-18. These models were selected as LLMs of similar size, balancing efficiency and accuracy, allowing comparison across open-weight and proprietary settings.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "commandr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BM25</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Robertson and Zaragoza (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib39\" title=\"\">2009</a>)</cite> is a sparse lexical retriever based on TF-IDF with document length normalization. It remains an effective baseline for keyword-sensitive domains where exact term overlap is important. <span class=\"ltx_text ltx_font_bold\">Sentence-BERT (SBERT)</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib38\" title=\"\">2019</a>)</cite> encodes queries and passages into dense vector embeddings optimized for semantic similarity, with ranking performed via cosine similarity.\n<span class=\"ltx_text ltx_font_bold\">BGE-M3 (BAAI General Embeddings).</span> BGE-M3 <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib12\" title=\"\">2024</a>)</cite> is a recent dense embedding model trained for multilingual and multi-task semantic retrieval. These retrievers capture complementary dimensions of relevance, from exact term matching (BM25) to semantic similarity (SBERT, BGE-M3), ensuring robust retrieval across different types of hateful content and knowledge sources.</p>\n\n",
                "matched_terms": [
                    "model",
                    "bgem3",
                    "different",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experimented on the Multi-Target CONAN (MTCo) dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib9\" title=\"\">2022</a>)</cite>, which contains 5,003 HS/CS pairs in English covering multiple target groups, including people with disabilities, Jews, LGBT+ individuals, Muslims, migrants, people of color (POC), and women. Collected through a human-in-the-loop process, MTCo provides high-quality, contextually relevant CS, and serves as a first baseline for our experiments. Additionally, we compare our pipeline with the four instruction-tuned LLMs of comparable size without RAG. Finally, we compare our results with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>We cannot compare to <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> due to lack of publicly available code and data, even upon request.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">LLM-as-a-Judge Evaluation.</span>\nWe use an adaptation of <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib55\" title=\"\">2023</a>)</cite> tailored for CS evaluation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib10\" title=\"\">2025</a>); Zubiaga et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib58\" title=\"\">2024</a>)</cite>. While traditional metrics capture surface and semantic alignment, JudgeLM provides a more holistic evaluation of CS quality along different dimensions. We use the version of <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> with fine-tuned Llama-instruct-7B.</p>\n\n",
                "matched_terms": [
                    "judgelm",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S6.T2\" title=\"Table 2 &#8227; Human evaluation metrics. &#8227; 6 Metrics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports average results, that can be interpreted along three main dimensions: content quality, diversity, and safety.\n<span class=\"ltx_text ltx_font_bold\">Content Quality:</span> scores on BLEU, METEOR, and ROUGE-L remain relatively low, as expected for open-ended generation, while BERTScore values are consistently high (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.85), indicating strong semantic similarity to MT-Co references. Non-RAG outputs achieve slightly higher lexical and semantic overlap, suggesting that retrieval introduces content that, while factually richer, diverges from exact reference phrasing. RAG outputs show less words overlapping,\nlikely reflecting the inclusion of factual information from the knowledge base.\n<span class=\"ltx_text ltx_font_bold\">Diversity:</span> incorporating retrieval substantially improves lexical diversity and reduces repetition. Mistral with BGE-M3 shows the largest gains, while No-RAG models, particularly Mistral, exhibit high repetition and lower diversity. <span class=\"ltx_text ltx_font_bold\">Safety:</span> No-RAG configurations achieve the highest safety, with GPT leading (0.993). RAG outputs show slightly lower safety, likely due to occasional noise introduced from the retrieval process. Nevertheless, safety remains high overall, indicating that fact-grounded augmentation does not substantially compromise non-harmfulness.\n<span class=\"ltx_text ltx_font_bold\">Model-level Trends:</span> GPT consistently delivers the safest outputs and competitive BERTScore, though with lower lexical diversity than Mistral. Mistral excels in diversity with RAG but performs poorly in No-RAG due to high repetition. CommandR and LLaMA provide stable but moderate performance across metrics. Overall, RAG with BGE-M3 achieves the best balance between quality, diversity, and informativeness. All results differ significantly (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A4\" title=\"Appendix D Statistical Significance of Retrieval Effects &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> for more details).</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "llama",
                    "rag",
                    "bgem3",
                    "gpt",
                    "retrieval",
                    "mistral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations reveal clear trends: <span class=\"ltx_text ltx_font_bold\">(1)</span> RAG outputs are less repetitive, and more diverse\nthan No-RAG, reflecting the addition of factual content. Lexical and semantic similarity remains high, ensuring CS remains aligned with the original intent.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> Safety is only slightly reduced with RAG, likely due to occasional noisy content.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> RAG consistently outperforms No-RAG and MT-Co CS in both automatic and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "judgelm",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings demonstrate that RAG &#8212;particularly with semantically rich BGE-M3 embeddings&#8212; enhances CS quality, diversity, and factual grounding, while maintaining strong safety and overall effectiveness.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "bgem3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted a human evaluation with 26 participants to assess the quality and effectiveness of the generated CS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Participants&#8217; age is between 18 and 50, there is a balance between genders, and their level of education spans high school diploma to PhD. Detailed statistics are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5\" title=\"Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span>. Our evaluation setup consisted of 10 HS examples, extracted randomly from MT-Co keeping the target distribution, with 9 CS candidates for each HS. We selected the CS from MT-Co and the CS generated by our four LLMs without the RAG pipeline, and the corresponding CS with RAG using BMG-M3 as retriever, since it has shown the strongest performance in the automatic and LLM-based metrics. Participants rated all 90 CS candidates along four dimensions, i.e., Relevance, Factuality, Cogency, and Correctness (1&#8211;3 scale), judged whether each CS was effective (Yes/No) and selected the best CS per HS. We collected a total of 2340 evaluations.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "rag",
                    "corresponding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metric Scores.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S8.T5\" title=\"Table 5 &#8227; 8 Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports average scores for Relevance, Factuality, Cogency, and Correctness across CS methods. GPT RAG achieved the highest scores overall, particularly in Factuality (2.75) and Cogency (2.41). RAG-based methods generally outperformed their non-RAG counterparts, while MT-Co scored lowest across all metrics, indicating lower relevance, factual accuracy, and overall effectiveness.</p>\n\n",
                "matched_terms": [
                    "outperformed",
                    "gpt",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Best CS.</span> RAG-based methods are chosen more frequently as best CS, implying that they are clearly preferred compared to their No RAG counter-parts, with GPT RAG receiving the highest number of votes (71). Complete statistics for each method are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A3\" title=\"Appendix C Additional Results from the Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "gpt",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effectiveness.</span> Out of the 2340 evaluations collected, 1312 were marked as effective, corresponding to 56% of the cases. More specifically, Llama RAG was considered effective most frequently (with 197 votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effectiveness votes (48) (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A3\" title=\"Appendix C Additional Results from the Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for full results). This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. The non-RAG models and MTCo were less effective overall. This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline, suggesting that both model choice and augmentation strategy strongly influence performance.</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "corresponding",
                    "llama",
                    "out",
                    "model",
                    "rag",
                    "indicates",
                    "gpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, RAG-based methods outperform non-RAG counterparts across all the four metrics, especially in Factuality, Cogency and Effectiveness, and are preferred by annotators in best-choice votes. The baseline method MT-Co consistently scores lowest across all metrics and effectiveness. Overall, <span class=\"ltx_text ltx_font_bold\">GPT RAG</span> is the best CS method across all HS and annotators. Results from the human evaluation are aligned with both automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "judgelm",
                    "gpt",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To contextualize our results, we compared our RAG-based CS with publicly available samples from two recent studies. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> (Baseline 1) released 1,697 GPT-4o-generated CS responses from their RAG pipeline addressing MT-Co hate speech, while <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> (Baseline 2) submitted 400 CS samples to the COLING 2025 shared task using LLaMA-EUS-8B with retrieved knowledge. Since the samples were multilingual, we retained only English CS aligned with MT-Co, totaling 124 samples.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> produced 100 English CS, 24 HS instances overlapped with MT-CONAN and were retained with multiple CS variants.</span></span></span>\nFor fair comparison, we reproduced settings closest to the original models: GPT-4o-mini + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite>, and LLaMA-8B + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>. This alignment minimizes differences due to model size, isolating the effects of retrieval and pipeline design. Pairwise performance was then assessed with <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> (prompt available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>). It is fine-tuned to rate helpfulness, relevance, accuracy, and level of detail of CS responses, which already accounts for evidence and factuality in the original prompt. To additionally emphasize conciseness, and suitability for real-world deployment on social media, we limit the CS to a maximum of two sentences.</p>\n\n",
                "matched_terms": [
                    "pairwise",
                    "model",
                    "rag",
                    "bgem3",
                    "judgelm",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S9.T6\" title=\"Table 6 &#8227; 9 Comparison with competitors &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that our RAG-augmented CS consistently outperforms the baseline samples from both studies. GPT-4o-mini + BGE-M3 outperforms Baseline 1 in 962 out of 1,697 pairwise battles, while LLaMA-8B + BGE-M3 outperforms Baseline 2 in 114 out of 124 battles. These results demonstrate that our RAG pipeline produces CS that is not only factually richer but also concise and suitable for social media, aligning with the intended communicative goals of real-world deployment.</p>\n\n",
                "matched_terms": [
                    "pairwise",
                    "rag",
                    "out",
                    "bgem3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S9.T6\" title=\"Table 6 &#8227; 9 Comparison with competitors &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> also reports the per-target breakdown of our pairwise comparison.\nAgainst Baseline 1, our system achieves overall consistent improvements. Gains are observed across all HS targets, with particularly strong performance for <span class=\"ltx_text ltx_font_italic\">DISABLED</span> (69.4%), <span class=\"ltx_text ltx_font_italic\">LGBT+</span> (64.0%), and <span class=\"ltx_text ltx_font_italic\">WOMEN</span> (61.2%). For larger target categories such as <span class=\"ltx_text ltx_font_italic\">MUSLIMS</span> and <span class=\"ltx_text ltx_font_italic\">MIGRANTS</span>, our model still maintains a clear advantage with 55.2% and 56.8% wins respectively, despite the higher difficulty and variability in these groups. The most balanced outcome is seen for <span class=\"ltx_text ltx_font_italic\">JEWS</span>, where results are split evenly (50%). Against Baseline 2, our system achieves consistently strong results across all targets, outperforming it in 93% of cases. The largest margins are observed for <span class=\"ltx_text ltx_font_italic\">MIGRANTS</span> (100% wins) and <span class=\"ltx_text ltx_font_italic\">JEWS</span> (95.7%).</p>\n\n",
                "matched_terms": [
                    "pairwise",
                    "wins",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a RAG-based framework for automatic counter-speech generation. We systematically compared three retrieval methods and four LLMs for CS generation targeting height groups (women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, other), relying on a novel and unique knowledge base, built over three institutional sources. We conducted an extensive experimental evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, and a human evaluation. Our results show that our approach outperforms competitive approaches and standard baselines on both of them. Our experiments demonstrated the versatility and soundness of our framework for counter-speech generation to fight online abusive content.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "judgelm",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, despite employing strong retrievers (BM25, SBERT, and BGE-M3), retrieval quality depends on the coverage and relevance of the knowledge base. Gaps or biases in external sources can propagate into the generated responses, particularly for emerging or culturally specific hate topics, even if we mitigated this by choosing recognized authoritative sources.</p>\n\n",
                "matched_terms": [
                    "bgem3",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to use <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> as a metric, we used the following prompts. The first one is used to compare RAG methods against No RAG methods, and against the MT-Co baseline. The second prompt is used to compare our RAG-based CS against those generated with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>); Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "judgelm",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the total number of times each CS method was rated effective, and how many times it has been selected as the best one across all HS and annotators. RAG-based methods are clearly preferred compared to their No RAG counter-parts. Concerning effectiveness, Llama RAG was considered effective most frequently (197 &#8220;Yes&#8221; votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effective votes (48). This indicates that RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. Furthermore, RAG methods are selected more often as best ones compared to their No RAG counterparts, with GPT RAG and Llama RAG achieving the best results.</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "llama",
                    "rag",
                    "indicates",
                    "gpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted non-parametric Friedman tests followed by Bonferroni-corrected Wilcoxon signed-rank tests to evaluate whether RAG retrieval strategies (BM25, Sentence-BERT, BGE) significantly affected model outputs compared to the No-RAG baseline for the per-sample automatic metrics (BLEU, METEOR, ROUGE-L, BERTScore), which allow direct pairwise comparison across systems. In contrast, diversity metrics such as Distinct-1, Distinct-2, and Repetition Rate are computed at the corpus level, producing a single value per model.\nBecause these measures do not yield per-sample scores and thus lack within-system variance, statistical testing is not applicable.\nFor all models, Friedman tests revealed significant overall effects (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), and pairwise comparisons confirmed that retrieval methods consistently induced statistically significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>) across quality and safety metrics.</p>\n\n",
                "matched_terms": [
                    "pairwise",
                    "strategies",
                    "model",
                    "rag",
                    "comparisons",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we provide different examples of counter-speech addressing the same hateful message from MT-CONAN. We show CS from GPT No RAG and GPT RAG with BGE-M3.</p>\n\n",
                "matched_terms": [
                    "different",
                    "gpt",
                    "rag",
                    "bgem3"
                ]
            }
        ]
    },
    "S7.T4": {
        "caption": "Table 4:  JudgeLM scores comparing the best RAG methods against CS from MT-Co.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BM25</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">SentBERT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BGE-M3</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Mistral</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4721</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4831</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4944</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LLaMA</th>\n<td class=\"ltx_td ltx_align_center\">4701</td>\n<td class=\"ltx_td ltx_align_center\">4757</td>\n<td class=\"ltx_td ltx_align_center\">4893</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GPT</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4866</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4948</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">4976</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">CommandR</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4161</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4351</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">4653</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gpt",
            "comparing",
            "sentbert",
            "commandr",
            "against",
            "llama",
            "model",
            "mtco",
            "best",
            "rag",
            "from",
            "bgem3",
            "methods",
            "judgelm",
            "mistral",
            "bm25",
            "scores"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate CS quality using <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span>, focusing on four dimensions: factuality, number of relevant facts, relevance to the HS, and specificity. This prompt (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>) guides pairwise comparisons between model outputs, emphasizing informative and targeted CS over surface-level similarity.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T3\" title=\"Table 3 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> results for RAG vs. No-RAG models across retrieval strategies. BGE-M3 yields the strongest results, followed by SentBERT and BM25, confirming the advantage of semantically rich retrieval. Among generators, GPT wins most comparisons, showing effective integration of retrieved content. Mistral also benefits notably from RAG, while CommandR performs less consistently.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T4\" title=\"Table 4 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the best RAG setups (LLM + BGE-M3) against human-written MT-Co CS. All RAG systems perform strongly, with GPT + BGE-M3 approaching human-level quality and Mistral + BGE-M3 remaining competitive, whereas CommandR trails slightly but still surpasses baselines.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Counter-speech generation is at the core of many expert activities, such as fact-checking and hate speech, to counter harmful content. Yet, existing work treats counter-speech generation as pure text generation task, mainly based on Large Language Models or NGO experts. These approaches show severe drawbacks due to the limited reliability and coherence in the generated countering text, and in scalability, respectively. To close this gap, we introduce a novel framework to model counter-speech generation as knowledge-wise text generation process. Our framework integrates advanced Retrieval-Augmented Generation (RAG) pipelines to ensure the generation of trustworthy counter-speech for 8 main target groups identified in the hate speech literature, including women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. We built a knowledge base over the United Nations Digital Library, EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792 texts. We use the MultiTarget-CONAN dataset to empirically assess the quality of the generated counter-speech, both through standard metrics (i.e., JudgeLM) and a human evaluation. Results show that our framework outperforms standard LLM baselines and competitive approach, on both assessments. The resulting framework and the knowledge base pave the way for studying trustworthy and sound counter-speech generation, in hate speech and beyond. <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">Warning<span class=\"ltx_text ltx_font_medium\">: this paper contains explicit examples some readers may find offensive.</span></span></p>\n\n",
                "matched_terms": [
                    "judgelm",
                    "rag",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tackle this challenging task by proposing a novel knowledge-grounded framework for automatic counter-speech generation that integrates advanced retrieval-augmented generation (RAG) pipelines. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows two examples of CS, with and without RAG augmentation. Our contributions are fourfold: <span class=\"ltx_text ltx_font_bold\">(1)</span> we provide a systematic comparison of multiple retrievers and LLMs for CS generation, combining three retrieval methods with four language models; <span class=\"ltx_text ltx_font_bold\">(2)</span> we enforce concise, two-sentence outputs tailored for social media deployment, ensuring responses remain natural, relatable, and effective in real-world contexts; <span class=\"ltx_text ltx_font_bold\">(3)</span> we conduct a pairwise evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, adapted to balance factual grounding with conciseness and pragmatic suitability, together with an extensive human evaluation, showing that our framework steadily outperforms standard baselines and state-of-the-art competitors; and <span class=\"ltx_text ltx_font_bold\">(4)</span> we will release all generated CS along with their top-3 retrieved evidence sentences, offering a reusable resource for the research community<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code will be made available upon acceptance.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "against",
                    "rag",
                    "judgelm",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic metrics (BLEU, ROUGE, BERTScore) correlate poorly with human judgments, prompting exploration of novelty- and repetition-based scores <cite class=\"ltx_cite ltx_citemacro_cite\">Wang and Wan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib49\" title=\"\">2018</a>); Bertoldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib6\" title=\"\">2013</a>)</cite>, and LLM-based frameworks (e.g., GPT-4, PandaLM, JudgeLM, UniEval) <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib55\" title=\"\">2023</a>); Zhong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib54\" title=\"\">2022</a>)</cite>, which show improved reliability for multi-aspect CS assessment <cite class=\"ltx_cite ltx_citemacro_cite\">Jones et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib25\" title=\"\">2024</a>); Damo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib18\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "judgelm",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG addresses a key limitation of prior approaches by grounding outputs in verifiable evidence, thereby enhancing factuality and persuasiveness in countering HS.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib15\" title=\"\">2021a</a>)</cite> proposed a retrieval-augmented pipeline that first generates queries from HS using keyword extraction, then employs BM25 <cite class=\"ltx_cite ltx_citemacro_cite\">Robertson and Zaragoza (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib39\" title=\"\">2009</a>)</cite> to retrieve relevant articles from Newsroom <cite class=\"ltx_cite ltx_citemacro_cite\">Grusky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib21\" title=\"\">2018</a>)</cite> and WikiText-103 <cite class=\"ltx_cite ltx_citemacro_cite\">Merity et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib32\" title=\"\">2016</a>)</cite>. From these, the most relevant sentences are selected using the ROUGE-L metric <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib29\" title=\"\">2004</a>)</cite>, before being passed to GPT-2 <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib37\" title=\"\">2019</a>)</cite> and XNLG <cite class=\"ltx_cite ltx_citemacro_cite\">Chi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib13\" title=\"\">2020</a>)</cite>.\nSimilarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>)</cite> introduced RAUCG, which retrieves counter-arguments from the ChangeMyView subreddit, selecting them based on stance consistency, semantic overlap, and a custom perplexity-based fitness function. The final generation step employs energy-based decoding to preserve factual knowledge while countering HS fluently.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> proposed ReZG, a retrieval-augmented zero-shot approach that integrates multi-dimensional hierarchical retrieval with constrained decoding, enabling the generation of more specific CS for unseen HS targets. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> leverage both curated background knowledge and web search to improve factuality.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> evaluated reranker-based pipelines, showing that fine-grained retrieval significantly improves factuality and relevance.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "from",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Differently from previous approaches, in this work we design a RAG pipeline built on a novel large, authoritative knowledge base that minimizes the risk of misinformation. Our approach consistently produces CS that is factually rich, but also concise and well-suited for social media contexts.\n</p>\n\n",
                "matched_terms": [
                    "rag",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge Base Integration.</span> All materials from UN, EU, and FRA were standardized into JSON with metadata (target group, document type, year, URL). The KB spans the years 2000&#8211;2025 and combines factual and policy-oriented resources, organized by target group and document type. To the best of our knowledge, this is the first large-scale, authoritative knowledge base specifically constructed for counter-speech generation.</p>\n\n",
                "matched_terms": [
                    "best",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a HS message <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, we aim to retrieve a small set of relevant paragraphs from the KB (i.e., the most similar to the target HS).\nFor this, we employ three complementary retrieval models: BM25, Sentence-BERT (SBERT), and BGE-M3.\nLet the KB consist of paragraphs <math alttext=\"\\mathcal{P}=\\{p_{1},p_{2},\\ldots,p_{N}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mi>N</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}=\\{p_{1},p_{2},\\ldots,p_{N}\\}</annotation></semantics></math>. For each retriever\n<math alttext=\"r\\in\\{\\text{BM25},\\text{SBERT},\\text{BGE-M3}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>BM25</mtext><mo>,</mo><mtext>SBERT</mtext><mo>,</mo><mtext>BGE-M3</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in\\{\\text{BM25},\\text{SBERT},\\text{BGE-M3}\\}</annotation></semantics></math>, we compute a similarity score:\n<math alttext=\"s_{r}(h,p_{i})=\\text{sim}_{r}(\\phi(h),\\phi(p_{i}))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>s</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mtext>sim</mtext><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">s_{r}(h,p_{i})=\\text{sim}_{r}(\\phi(h),\\phi(p_{i}))</annotation></semantics></math>\nwhere <math alttext=\"\\phi(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(\\cdot)</annotation></semantics></math> is the embedding (or term-weight) function induced by the retriever. For BM25, the similarity score is computed based on TF-IDF, and document length normalization. For SBERT and BGE-M3, similarity is measured as the cosine similarity between dense vector embeddings of the HS and candidate paragraph, capturing semantic relatedness even in the absence of exact lexical overlap.\nFor each retriever, we obtain the top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> ranked paragraphs:\n<math alttext=\"R_{r}(h)=\\{p_{r,1},p_{r,2},\\ldots,p_{r,k}\\},\\quad k=3.\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mrow><mrow><mrow><msub><mi>R</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mn>2</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo rspace=\"1.167em\">,</mo><mrow><mi>k</mi><mo>=</mo><mn>3</mn></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">R_{r}(h)=\\{p_{r,1},p_{r,2},\\ldots,p_{r,k}\\},\\quad k=3.</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "bm25",
                    "from",
                    "bgem3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BM25</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Robertson and Zaragoza (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib39\" title=\"\">2009</a>)</cite> is a sparse lexical retriever based on TF-IDF with document length normalization. It remains an effective baseline for keyword-sensitive domains where exact term overlap is important. <span class=\"ltx_text ltx_font_bold\">Sentence-BERT (SBERT)</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib38\" title=\"\">2019</a>)</cite> encodes queries and passages into dense vector embeddings optimized for semantic similarity, with ranking performed via cosine similarity.\n<span class=\"ltx_text ltx_font_bold\">BGE-M3 (BAAI General Embeddings).</span> BGE-M3 <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib12\" title=\"\">2024</a>)</cite> is a recent dense embedding model trained for multilingual and multi-task semantic retrieval. These retrievers capture complementary dimensions of relevance, from exact term matching (BM25) to semantic similarity (SBERT, BGE-M3), ensuring robust retrieval across different types of hateful content and knowledge sources.</p>\n\n",
                "matched_terms": [
                    "bm25",
                    "bgem3",
                    "model",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experimented on the Multi-Target CONAN (MTCo) dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib9\" title=\"\">2022</a>)</cite>, which contains 5,003 HS/CS pairs in English covering multiple target groups, including people with disabilities, Jews, LGBT+ individuals, Muslims, migrants, people of color (POC), and women. Collected through a human-in-the-loop process, MTCo provides high-quality, contextually relevant CS, and serves as a first baseline for our experiments. Additionally, we compare our pipeline with the four instruction-tuned LLMs of comparable size without RAG. Finally, we compare our results with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>We cannot compare to <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> due to lack of publicly available code and data, even upon request.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "mtco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S6.T2\" title=\"Table 2 &#8227; Human evaluation metrics. &#8227; 6 Metrics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports average results, that can be interpreted along three main dimensions: content quality, diversity, and safety.\n<span class=\"ltx_text ltx_font_bold\">Content Quality:</span> scores on BLEU, METEOR, and ROUGE-L remain relatively low, as expected for open-ended generation, while BERTScore values are consistently high (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.85), indicating strong semantic similarity to MT-Co references. Non-RAG outputs achieve slightly higher lexical and semantic overlap, suggesting that retrieval introduces content that, while factually richer, diverges from exact reference phrasing. RAG outputs show less words overlapping,\nlikely reflecting the inclusion of factual information from the knowledge base.\n<span class=\"ltx_text ltx_font_bold\">Diversity:</span> incorporating retrieval substantially improves lexical diversity and reduces repetition. Mistral with BGE-M3 shows the largest gains, while No-RAG models, particularly Mistral, exhibit high repetition and lower diversity. <span class=\"ltx_text ltx_font_bold\">Safety:</span> No-RAG configurations achieve the highest safety, with GPT leading (0.993). RAG outputs show slightly lower safety, likely due to occasional noise introduced from the retrieval process. Nevertheless, safety remains high overall, indicating that fact-grounded augmentation does not substantially compromise non-harmfulness.\n<span class=\"ltx_text ltx_font_bold\">Model-level Trends:</span> GPT consistently delivers the safest outputs and competitive BERTScore, though with lower lexical diversity than Mistral. Mistral excels in diversity with RAG but performs poorly in No-RAG due to high repetition. CommandR and LLaMA provide stable but moderate performance across metrics. Overall, RAG with BGE-M3 achieves the best balance between quality, diversity, and informativeness. All results differ significantly (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A4\" title=\"Appendix D Statistical Significance of Retrieval Effects &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> for more details).</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "llama",
                    "mtco",
                    "best",
                    "scores",
                    "rag",
                    "from",
                    "bgem3",
                    "gpt",
                    "mistral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations reveal clear trends: <span class=\"ltx_text ltx_font_bold\">(1)</span> RAG outputs are less repetitive, and more diverse\nthan No-RAG, reflecting the addition of factual content. Lexical and semantic similarity remains high, ensuring CS remains aligned with the original intent.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> Safety is only slightly reduced with RAG, likely due to occasional noisy content.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> RAG consistently outperforms No-RAG and MT-Co CS in both automatic and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "judgelm",
                    "rag",
                    "mtco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings demonstrate that RAG &#8212;particularly with semantically rich BGE-M3 embeddings&#8212; enhances CS quality, diversity, and factual grounding, while maintaining strong safety and overall effectiveness.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "bgem3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted a human evaluation with 26 participants to assess the quality and effectiveness of the generated CS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Participants&#8217; age is between 18 and 50, there is a balance between genders, and their level of education spans high school diploma to PhD. Detailed statistics are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5\" title=\"Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span>. Our evaluation setup consisted of 10 HS examples, extracted randomly from MT-Co keeping the target distribution, with 9 CS candidates for each HS. We selected the CS from MT-Co and the CS generated by our four LLMs without the RAG pipeline, and the corresponding CS with RAG using BMG-M3 as retriever, since it has shown the strongest performance in the automatic and LLM-based metrics. Participants rated all 90 CS candidates along four dimensions, i.e., Relevance, Factuality, Cogency, and Correctness (1&#8211;3 scale), judged whether each CS was effective (Yes/No) and selected the best CS per HS. We collected a total of 2340 evaluations.</p>\n\n",
                "matched_terms": [
                    "best",
                    "rag",
                    "from",
                    "mtco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metric Scores.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S8.T5\" title=\"Table 5 &#8227; 8 Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports average scores for Relevance, Factuality, Cogency, and Correctness across CS methods. GPT RAG achieved the highest scores overall, particularly in Factuality (2.75) and Cogency (2.41). RAG-based methods generally outperformed their non-RAG counterparts, while MT-Co scored lowest across all metrics, indicating lower relevance, factual accuracy, and overall effectiveness.</p>\n\n",
                "matched_terms": [
                    "mtco",
                    "rag",
                    "methods",
                    "gpt",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Best CS.</span> RAG-based methods are chosen more frequently as best CS, implying that they are clearly preferred compared to their No RAG counter-parts, with GPT RAG receiving the highest number of votes (71). Complete statistics for each method are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A3\" title=\"Appendix C Additional Results from the Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "best",
                    "rag",
                    "gpt",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effectiveness.</span> Out of the 2340 evaluations collected, 1312 were marked as effective, corresponding to 56% of the cases. More specifically, Llama RAG was considered effective most frequently (with 197 votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effectiveness votes (48) (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A3\" title=\"Appendix C Additional Results from the Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for full results). This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. The non-RAG models and MTCo were less effective overall. This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline, suggesting that both model choice and augmentation strategy strongly influence performance.</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "llama",
                    "model",
                    "mtco",
                    "rag",
                    "methods",
                    "gpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, RAG-based methods outperform non-RAG counterparts across all the four metrics, especially in Factuality, Cogency and Effectiveness, and are preferred by annotators in best-choice votes. The baseline method MT-Co consistently scores lowest across all metrics and effectiveness. Overall, <span class=\"ltx_text ltx_font_bold\">GPT RAG</span> is the best CS method across all HS and annotators. Results from the human evaluation are aligned with both automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "mtco",
                    "best",
                    "rag",
                    "from",
                    "methods",
                    "judgelm",
                    "gpt",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To contextualize our results, we compared our RAG-based CS with publicly available samples from two recent studies. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> (Baseline 1) released 1,697 GPT-4o-generated CS responses from their RAG pipeline addressing MT-Co hate speech, while <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> (Baseline 2) submitted 400 CS samples to the COLING 2025 shared task using LLaMA-EUS-8B with retrieved knowledge. Since the samples were multilingual, we retained only English CS aligned with MT-Co, totaling 124 samples.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> produced 100 English CS, 24 HS instances overlapped with MT-CONAN and were retained with multiple CS variants.</span></span></span>\nFor fair comparison, we reproduced settings closest to the original models: GPT-4o-mini + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite>, and LLaMA-8B + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>. This alignment minimizes differences due to model size, isolating the effects of retrieval and pipeline design. Pairwise performance was then assessed with <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> (prompt available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>). It is fine-tuned to rate helpfulness, relevance, accuracy, and level of detail of CS responses, which already accounts for evidence and factuality in the original prompt. To additionally emphasize conciseness, and suitability for real-world deployment on social media, we limit the CS to a maximum of two sentences.</p>\n\n",
                "matched_terms": [
                    "model",
                    "mtco",
                    "rag",
                    "from",
                    "bgem3",
                    "judgelm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S9.T6\" title=\"Table 6 &#8227; 9 Comparison with competitors &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that our RAG-augmented CS consistently outperforms the baseline samples from both studies. GPT-4o-mini + BGE-M3 outperforms Baseline 1 in 962 out of 1,697 pairwise battles, while LLaMA-8B + BGE-M3 outperforms Baseline 2 in 114 out of 124 battles. These results demonstrate that our RAG pipeline produces CS that is not only factually richer but also concise and suitable for social media, aligning with the intended communicative goals of real-world deployment.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "from",
                    "bgem3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S9.T6\" title=\"Table 6 &#8227; 9 Comparison with competitors &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> also reports the per-target breakdown of our pairwise comparison.\nAgainst Baseline 1, our system achieves overall consistent improvements. Gains are observed across all HS targets, with particularly strong performance for <span class=\"ltx_text ltx_font_italic\">DISABLED</span> (69.4%), <span class=\"ltx_text ltx_font_italic\">LGBT+</span> (64.0%), and <span class=\"ltx_text ltx_font_italic\">WOMEN</span> (61.2%). For larger target categories such as <span class=\"ltx_text ltx_font_italic\">MUSLIMS</span> and <span class=\"ltx_text ltx_font_italic\">MIGRANTS</span>, our model still maintains a clear advantage with 55.2% and 56.8% wins respectively, despite the higher difficulty and variability in these groups. The most balanced outcome is seen for <span class=\"ltx_text ltx_font_italic\">JEWS</span>, where results are split evenly (50%). Against Baseline 2, our system achieves consistently strong results across all targets, outperforming it in 93% of cases. The largest margins are observed for <span class=\"ltx_text ltx_font_italic\">MIGRANTS</span> (100% wins) and <span class=\"ltx_text ltx_font_italic\">JEWS</span> (95.7%).</p>\n\n",
                "matched_terms": [
                    "against",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a RAG-based framework for automatic counter-speech generation. We systematically compared three retrieval methods and four LLMs for CS generation targeting height groups (women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, other), relying on a novel and unique knowledge base, built over three institutional sources. We conducted an extensive experimental evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, and a human evaluation. Our results show that our approach outperforms competitive approaches and standard baselines on both of them. Our experiments demonstrated the versatility and soundness of our framework for counter-speech generation to fight online abusive content.</p>\n\n",
                "matched_terms": [
                    "against",
                    "judgelm",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, despite employing strong retrievers (BM25, SBERT, and BGE-M3), retrieval quality depends on the coverage and relevance of the knowledge base. Gaps or biases in external sources can propagate into the generated responses, particularly for emerging or culturally specific hate topics, even if we mitigated this by choosing recognized authoritative sources.</p>\n\n",
                "matched_terms": [
                    "bgem3",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study involves the use of hate speech examples for the development and evaluation of counter-speech generation systems. We acknowledge that the inclusion of HS content poses potential risks of exposure to harmful language and emotional distress for researchers and annotators. All individuals involved in data handling were informed of these risks and participated voluntarily, following institutional ethical guidelines.\nAlthough our goal is to promote positive and factual discourse, automatic CS generation can inadvertently reinforce biases, produce factually incorrect content, or convey unintended tones. To mitigate these risks, we rely on institutional sources (UN, EU, FRA) for retrieval, explicitly evaluate factuality and correctness, and use human oversight in all analyses. The system is presented for research purposes only and is not intended for unsupervised deployment.\nWe further recognize that the perceived effectiveness and appropriateness of CS depend on social and cultural context. Our methods and findings should therefore not be generalized without careful adaptation and ethical review. No personal or private data were used; all retrieved materials come from publicly available institutional sources.</p>\n\n",
                "matched_terms": [
                    "from",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Disabled</span>: Disability rights, Accessibility, Social model of disability, disability, disabled, down syndrome, autism, mental disability, physical disability, neurodiversity, ableism, inclusive design, Discrimination against people with disabilities</p>\n\n",
                "matched_terms": [
                    "against",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to use <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> as a metric, we used the following prompts. The first one is used to compare RAG methods against No RAG methods, and against the MT-Co baseline. The second prompt is used to compare our RAG-based CS against those generated with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>); Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "against",
                    "mtco",
                    "rag",
                    "methods",
                    "judgelm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the total number of times each CS method was rated effective, and how many times it has been selected as the best one across all HS and annotators. RAG-based methods are clearly preferred compared to their No RAG counter-parts. Concerning effectiveness, Llama RAG was considered effective most frequently (197 &#8220;Yes&#8221; votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effective votes (48). This indicates that RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. Furthermore, RAG methods are selected more often as best ones compared to their No RAG counterparts, with GPT RAG and Llama RAG achieving the best results.</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "llama",
                    "mtco",
                    "best",
                    "rag",
                    "methods",
                    "gpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted non-parametric Friedman tests followed by Bonferroni-corrected Wilcoxon signed-rank tests to evaluate whether RAG retrieval strategies (BM25, Sentence-BERT, BGE) significantly affected model outputs compared to the No-RAG baseline for the per-sample automatic metrics (BLEU, METEOR, ROUGE-L, BERTScore), which allow direct pairwise comparison across systems. In contrast, diversity metrics such as Distinct-1, Distinct-2, and Repetition Rate are computed at the corpus level, producing a single value per model.\nBecause these measures do not yield per-sample scores and thus lack within-system variance, statistical testing is not applicable.\nFor all models, Friedman tests revealed significant overall effects (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), and pairwise comparisons confirmed that retrieval methods consistently induced statistically significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>) across quality and safety metrics.</p>\n\n",
                "matched_terms": [
                    "model",
                    "rag",
                    "methods",
                    "bm25",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S6\" title=\"6 Metrics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we carried out a human evaluation. Participants were voluntarily recruited and could opt out from the study at any time. Precise instructions were given, highlighting the potential risks and distress of the study. We define the following metrics used for the human evaluation: Relevance, Factuality, Cogency, and Correctness use a Likert scale from 1 to 3 (with 3 being the best possible score), while Effectiveness, and Is the Best are binary dimensions to which the participants could reply \"Yes\" or \"No\". They are defined as follows:</p>\n\n",
                "matched_terms": [
                    "best",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Refers to whether the counter-speech is the best one among the ones generated for the same HS. Select the CS that you prefer based on the scores you provided.</p>\n\n",
                "matched_terms": [
                    "best",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we provide different examples of counter-speech addressing the same hateful message from MT-CONAN. We show CS from GPT No RAG and GPT RAG with BGE-M3.</p>\n\n",
                "matched_terms": [
                    "gpt",
                    "rag",
                    "from",
                    "bgem3"
                ]
            }
        ]
    },
    "S8.T5": {
        "caption": "Table 5:  Average scores (13) for Relevance (Rel.), Factuality (Fact.), Cogency (Cog.), and Correctness (Corr.), per CS method.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Rel.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Fact.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Cog.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Corr.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MT-Co</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.07</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Llama No RAG</th>\n<td class=\"ltx_td ltx_align_center\">2.49</td>\n<td class=\"ltx_td ltx_align_center\">2.77</td>\n<td class=\"ltx_td ltx_align_center\">2.14</td>\n<td class=\"ltx_td ltx_align_center\">2.72</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Llama RAG</th>\n<td class=\"ltx_td ltx_align_center\">2.53</td>\n<td class=\"ltx_td ltx_align_center\">2.75</td>\n<td class=\"ltx_td ltx_align_center\">2.50</td>\n<td class=\"ltx_td ltx_align_center\">2.66</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CommandR No RAG</th>\n<td class=\"ltx_td ltx_align_center\">2.25</td>\n<td class=\"ltx_td ltx_align_center\">2.75</td>\n<td class=\"ltx_td ltx_align_center\">1.85</td>\n<td class=\"ltx_td ltx_align_center\">2.63</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CommandR RAG</th>\n<td class=\"ltx_td ltx_align_center\">2.48</td>\n<td class=\"ltx_td ltx_align_center\">2.74</td>\n<td class=\"ltx_td ltx_align_center\">2.25</td>\n<td class=\"ltx_td ltx_align_center\">2.75</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mistral No RAG</th>\n<td class=\"ltx_td ltx_align_center\">2.11</td>\n<td class=\"ltx_td ltx_align_center\">2.69</td>\n<td class=\"ltx_td ltx_align_center\">1.77</td>\n<td class=\"ltx_td ltx_align_center\">2.58</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mistral RAG</th>\n<td class=\"ltx_td ltx_align_center\">2.30</td>\n<td class=\"ltx_td ltx_align_center\">2.69</td>\n<td class=\"ltx_td ltx_align_center\">2.16</td>\n<td class=\"ltx_td ltx_align_center\">2.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GPT No RAG</th>\n<td class=\"ltx_td ltx_align_center\">2.34</td>\n<td class=\"ltx_td ltx_align_center\">2.73</td>\n<td class=\"ltx_td ltx_align_center\">1.91</td>\n<td class=\"ltx_td ltx_align_center\">2.62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span>gray!20 GPT RAG</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">2.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">2.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">2.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">2.68</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "cogency",
            "13",
            "mtco",
            "average",
            "fact",
            "rag",
            "cog",
            "scores",
            "relevance",
            "rel",
            "llama",
            "corr",
            "gpt",
            "commandr",
            "factuality",
            "correctness",
            "method",
            "rowcolorgray20",
            "mistral"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metric Scores.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S8.T5\" title=\"Table 5 &#8227; 8 Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports average scores for Relevance, Factuality, Cogency, and Correctness across CS methods. GPT RAG achieved the highest scores overall, particularly in Factuality (2.75) and Cogency (2.41). RAG-based methods generally outperformed their non-RAG counterparts, while MT-Co scored lowest across all metrics, indicating lower relevance, factual accuracy, and overall effectiveness.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">RAG addresses a key limitation of prior approaches by grounding outputs in verifiable evidence, thereby enhancing factuality and persuasiveness in countering HS.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib15\" title=\"\">2021a</a>)</cite> proposed a retrieval-augmented pipeline that first generates queries from HS using keyword extraction, then employs BM25 <cite class=\"ltx_cite ltx_citemacro_cite\">Robertson and Zaragoza (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib39\" title=\"\">2009</a>)</cite> to retrieve relevant articles from Newsroom <cite class=\"ltx_cite ltx_citemacro_cite\">Grusky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib21\" title=\"\">2018</a>)</cite> and WikiText-103 <cite class=\"ltx_cite ltx_citemacro_cite\">Merity et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib32\" title=\"\">2016</a>)</cite>. From these, the most relevant sentences are selected using the ROUGE-L metric <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib29\" title=\"\">2004</a>)</cite>, before being passed to GPT-2 <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib37\" title=\"\">2019</a>)</cite> and XNLG <cite class=\"ltx_cite ltx_citemacro_cite\">Chi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib13\" title=\"\">2020</a>)</cite>.\nSimilarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>)</cite> introduced RAUCG, which retrieves counter-arguments from the ChangeMyView subreddit, selecting them based on stance consistency, semantic overlap, and a custom perplexity-based fitness function. The final generation step employs energy-based decoding to preserve factual knowledge while countering HS fluently.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> proposed ReZG, a retrieval-augmented zero-shot approach that integrates multi-dimensional hierarchical retrieval with constrained decoding, enabling the generation of more specific CS for unseen HS targets. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> leverage both curated background knowledge and web search to improve factuality.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> evaluated reranker-based pipelines, showing that fine-grained retrieval significantly improves factuality and relevance.</p>\n\n",
                "matched_terms": [
                    "relevance",
                    "rag",
                    "factuality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experimented on the Multi-Target CONAN (MTCo) dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib9\" title=\"\">2022</a>)</cite>, which contains 5,003 HS/CS pairs in English covering multiple target groups, including people with disabilities, Jews, LGBT+ individuals, Muslims, migrants, people of color (POC), and women. Collected through a human-in-the-loop process, MTCo provides high-quality, contextually relevant CS, and serves as a first baseline for our experiments. Additionally, we compare our pipeline with the four instruction-tuned LLMs of comparable size without RAG. Finally, we compare our results with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>We cannot compare to <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> due to lack of publicly available code and data, even upon request.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "mtco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S6.T2\" title=\"Table 2 &#8227; Human evaluation metrics. &#8227; 6 Metrics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports average results, that can be interpreted along three main dimensions: content quality, diversity, and safety.\n<span class=\"ltx_text ltx_font_bold\">Content Quality:</span> scores on BLEU, METEOR, and ROUGE-L remain relatively low, as expected for open-ended generation, while BERTScore values are consistently high (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.85), indicating strong semantic similarity to MT-Co references. Non-RAG outputs achieve slightly higher lexical and semantic overlap, suggesting that retrieval introduces content that, while factually richer, diverges from exact reference phrasing. RAG outputs show less words overlapping,\nlikely reflecting the inclusion of factual information from the knowledge base.\n<span class=\"ltx_text ltx_font_bold\">Diversity:</span> incorporating retrieval substantially improves lexical diversity and reduces repetition. Mistral with BGE-M3 shows the largest gains, while No-RAG models, particularly Mistral, exhibit high repetition and lower diversity. <span class=\"ltx_text ltx_font_bold\">Safety:</span> No-RAG configurations achieve the highest safety, with GPT leading (0.993). RAG outputs show slightly lower safety, likely due to occasional noise introduced from the retrieval process. Nevertheless, safety remains high overall, indicating that fact-grounded augmentation does not substantially compromise non-harmfulness.\n<span class=\"ltx_text ltx_font_bold\">Model-level Trends:</span> GPT consistently delivers the safest outputs and competitive BERTScore, though with lower lexical diversity than Mistral. Mistral excels in diversity with RAG but performs poorly in No-RAG due to high repetition. CommandR and LLaMA provide stable but moderate performance across metrics. Overall, RAG with BGE-M3 achieves the best balance between quality, diversity, and informativeness. All results differ significantly (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A4\" title=\"Appendix D Statistical Significance of Retrieval Effects &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> for more details).</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "llama",
                    "average",
                    "mtco",
                    "rag",
                    "mistral",
                    "gpt",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate CS quality using <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span>, focusing on four dimensions: factuality, number of relevant facts, relevance to the HS, and specificity. This prompt (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>) guides pairwise comparisons between model outputs, emphasizing informative and targeted CS over surface-level similarity.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T3\" title=\"Table 3 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> results for RAG vs. No-RAG models across retrieval strategies. BGE-M3 yields the strongest results, followed by SentBERT and BM25, confirming the advantage of semantically rich retrieval. Among generators, GPT wins most comparisons, showing effective integration of retrieved content. Mistral also benefits notably from RAG, while CommandR performs less consistently.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T4\" title=\"Table 4 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the best RAG setups (LLM + BGE-M3) against human-written MT-Co CS. All RAG systems perform strongly, with GPT + BGE-M3 approaching human-level quality and Mistral + BGE-M3 remaining competitive, whereas CommandR trails slightly but still surpasses baselines.</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "relevance",
                    "mtco",
                    "factuality",
                    "rag",
                    "gpt",
                    "mistral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations reveal clear trends: <span class=\"ltx_text ltx_font_bold\">(1)</span> RAG outputs are less repetitive, and more diverse\nthan No-RAG, reflecting the addition of factual content. Lexical and semantic similarity remains high, ensuring CS remains aligned with the original intent.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> Safety is only slightly reduced with RAG, likely due to occasional noisy content.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> RAG consistently outperforms No-RAG and MT-Co CS in both automatic and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "mtco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted a human evaluation with 26 participants to assess the quality and effectiveness of the generated CS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Participants&#8217; age is between 18 and 50, there is a balance between genders, and their level of education spans high school diploma to PhD. Detailed statistics are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5\" title=\"Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span>. Our evaluation setup consisted of 10 HS examples, extracted randomly from MT-Co keeping the target distribution, with 9 CS candidates for each HS. We selected the CS from MT-Co and the CS generated by our four LLMs without the RAG pipeline, and the corresponding CS with RAG using BMG-M3 as retriever, since it has shown the strongest performance in the automatic and LLM-based metrics. Participants rated all 90 CS candidates along four dimensions, i.e., Relevance, Factuality, Cogency, and Correctness (1&#8211;3 scale), judged whether each CS was effective (Yes/No) and selected the best CS per HS. We collected a total of 2340 evaluations.</p>\n\n",
                "matched_terms": [
                    "relevance",
                    "cogency",
                    "13",
                    "mtco",
                    "factuality",
                    "correctness",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Best CS.</span> RAG-based methods are chosen more frequently as best CS, implying that they are clearly preferred compared to their No RAG counter-parts, with GPT RAG receiving the highest number of votes (71). Complete statistics for each method are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A3\" title=\"Appendix C Additional Results from the Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "gpt",
                    "rag",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effectiveness.</span> Out of the 2340 evaluations collected, 1312 were marked as effective, corresponding to 56% of the cases. More specifically, Llama RAG was considered effective most frequently (with 197 votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effectiveness votes (48) (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A3\" title=\"Appendix C Additional Results from the Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for full results). This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. The non-RAG models and MTCo were less effective overall. This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline, suggesting that both model choice and augmentation strategy strongly influence performance.</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "llama",
                    "mtco",
                    "rag",
                    "method",
                    "gpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, RAG-based methods outperform non-RAG counterparts across all the four metrics, especially in Factuality, Cogency and Effectiveness, and are preferred by annotators in best-choice votes. The baseline method MT-Co consistently scores lowest across all metrics and effectiveness. Overall, <span class=\"ltx_text ltx_font_bold\">GPT RAG</span> is the best CS method across all HS and annotators. Results from the human evaluation are aligned with both automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "cogency",
                    "mtco",
                    "factuality",
                    "rag",
                    "method",
                    "gpt",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To contextualize our results, we compared our RAG-based CS with publicly available samples from two recent studies. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> (Baseline 1) released 1,697 GPT-4o-generated CS responses from their RAG pipeline addressing MT-Co hate speech, while <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> (Baseline 2) submitted 400 CS samples to the COLING 2025 shared task using LLaMA-EUS-8B with retrieved knowledge. Since the samples were multilingual, we retained only English CS aligned with MT-Co, totaling 124 samples.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> produced 100 English CS, 24 HS instances overlapped with MT-CONAN and were retained with multiple CS variants.</span></span></span>\nFor fair comparison, we reproduced settings closest to the original models: GPT-4o-mini + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite>, and LLaMA-8B + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>. This alignment minimizes differences due to model size, isolating the effects of retrieval and pipeline design. Pairwise performance was then assessed with <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> (prompt available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>). It is fine-tuned to rate helpfulness, relevance, accuracy, and level of detail of CS responses, which already accounts for evidence and factuality in the original prompt. To additionally emphasize conciseness, and suitability for real-world deployment on social media, we limit the CS to a maximum of two sentences.</p>\n\n",
                "matched_terms": [
                    "relevance",
                    "rag",
                    "factuality",
                    "mtco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study involves the use of hate speech examples for the development and evaluation of counter-speech generation systems. We acknowledge that the inclusion of HS content poses potential risks of exposure to harmful language and emotional distress for researchers and annotators. All individuals involved in data handling were informed of these risks and participated voluntarily, following institutional ethical guidelines.\nAlthough our goal is to promote positive and factual discourse, automatic CS generation can inadvertently reinforce biases, produce factually incorrect content, or convey unintended tones. To mitigate these risks, we rely on institutional sources (UN, EU, FRA) for retrieval, explicitly evaluate factuality and correctness, and use human oversight in all analyses. The system is presented for research purposes only and is not intended for unsupervised deployment.\nWe further recognize that the perceived effectiveness and appropriateness of CS depend on social and cultural context. Our methods and findings should therefore not be generalized without careful adaptation and ethical review. No personal or private data were used; all retrieved materials come from publicly available institutional sources.</p>\n\n",
                "matched_terms": [
                    "factuality",
                    "correctness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to use <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> as a metric, we used the following prompts. The first one is used to compare RAG methods against No RAG methods, and against the MT-Co baseline. The second prompt is used to compare our RAG-based CS against those generated with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>); Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "mtco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the total number of times each CS method was rated effective, and how many times it has been selected as the best one across all HS and annotators. RAG-based methods are clearly preferred compared to their No RAG counter-parts. Concerning effectiveness, Llama RAG was considered effective most frequently (197 &#8220;Yes&#8221; votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effective votes (48). This indicates that RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. Furthermore, RAG methods are selected more often as best ones compared to their No RAG counterparts, with GPT RAG and Llama RAG achieving the best results.</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "llama",
                    "mtco",
                    "rag",
                    "method",
                    "gpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted non-parametric Friedman tests followed by Bonferroni-corrected Wilcoxon signed-rank tests to evaluate whether RAG retrieval strategies (BM25, Sentence-BERT, BGE) significantly affected model outputs compared to the No-RAG baseline for the per-sample automatic metrics (BLEU, METEOR, ROUGE-L, BERTScore), which allow direct pairwise comparison across systems. In contrast, diversity metrics such as Distinct-1, Distinct-2, and Repetition Rate are computed at the corpus level, producing a single value per model.\nBecause these measures do not yield per-sample scores and thus lack within-system variance, statistical testing is not applicable.\nFor all models, Friedman tests revealed significant overall effects (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), and pairwise comparisons confirmed that retrieval methods consistently induced statistically significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>) across quality and safety metrics.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S6\" title=\"6 Metrics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we carried out a human evaluation. Participants were voluntarily recruited and could opt out from the study at any time. Precise instructions were given, highlighting the potential risks and distress of the study. We define the following metrics used for the human evaluation: Relevance, Factuality, Cogency, and Correctness use a Likert scale from 1 to 3 (with 3 being the best possible score), while Effectiveness, and Is the Best are binary dimensions to which the participants could reply \"Yes\" or \"No\". They are defined as follows:</p>\n\n",
                "matched_terms": [
                    "relevance",
                    "cogency",
                    "factuality",
                    "correctness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we provide different examples of counter-speech addressing the same hateful message from MT-CONAN. We show CS from GPT No RAG and GPT RAG with BGE-M3.</p>\n\n",
                "matched_terms": [
                    "gpt",
                    "rag"
                ]
            }
        ]
    },
    "S9.T6": {
        "caption": "Table 6:  Per-target JudgeLM comparison of our systems against Russo (2025) and Wilk et al. (2025). Results show the number and percentage of pairwise wins across HS target groups, with totals included for each baseline.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model vs Baseline</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Target</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Wins (ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">% Wins</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"5\">vs Russo (2025)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">JEWS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">95.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LGBT+</td>\n<td class=\"ltx_td ltx_align_center\">19</td>\n<td class=\"ltx_td ltx_align_center\">17</td>\n<td class=\"ltx_td ltx_align_center\">89.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MIGRANTS</td>\n<td class=\"ltx_td ltx_align_center\">30</td>\n<td class=\"ltx_td ltx_align_center\">30</td>\n<td class=\"ltx_td ltx_align_center\">100.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">POC</td>\n<td class=\"ltx_td ltx_align_center\">15</td>\n<td class=\"ltx_td ltx_align_center\">11</td>\n<td class=\"ltx_td ltx_align_center\">73.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">WOMEN</td>\n<td class=\"ltx_td ltx_align_center\">37</td>\n<td class=\"ltx_td ltx_align_center\">34</td>\n<td class=\"ltx_td ltx_align_center\">91.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">124</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">114</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">93.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"8\">vs Wilk et al. (2025)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">DISABLED</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">JEWS</td>\n<td class=\"ltx_td ltx_align_center\">204</td>\n<td class=\"ltx_td ltx_align_center\">102</td>\n<td class=\"ltx_td ltx_align_center\">50.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">LGBT+</td>\n<td class=\"ltx_td ltx_align_center\">164</td>\n<td class=\"ltx_td ltx_align_center\">105</td>\n<td class=\"ltx_td ltx_align_center\">64.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MIGRANTS</td>\n<td class=\"ltx_td ltx_align_center\">310</td>\n<td class=\"ltx_td ltx_align_center\">176</td>\n<td class=\"ltx_td ltx_align_center\">56.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">MUSLIMS</td>\n<td class=\"ltx_td ltx_align_center\">518</td>\n<td class=\"ltx_td ltx_align_center\">286</td>\n<td class=\"ltx_td ltx_align_center\">55.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">POC</td>\n<td class=\"ltx_td ltx_align_center\">109</td>\n<td class=\"ltx_td ltx_align_center\">58</td>\n<td class=\"ltx_td ltx_align_center\">53.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">WOMEN</td>\n<td class=\"ltx_td ltx_align_center\">232</td>\n<td class=\"ltx_td ltx_align_center\">142</td>\n<td class=\"ltx_td ltx_align_center\">61.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Other</td>\n<td class=\"ltx_td ltx_align_center\">124</td>\n<td class=\"ltx_td ltx_align_center\">68</td>\n<td class=\"ltx_td ltx_align_center\">54.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_bb\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1697</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">962</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">57.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wilk",
            "each",
            "poc",
            "comparison",
            "lgbt",
            "our",
            "russo",
            "systems",
            "against",
            "pairwise",
            "show",
            "target",
            "percentage",
            "judgelm",
            "totals",
            "disabled",
            "across",
            "jews",
            "results",
            "migrants",
            "groups",
            "number",
            "women",
            "ours",
            "wins",
            "included",
            "total",
            "model",
            "other",
            "muslims",
            "baseline",
            "pertarget"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S9.T6\" title=\"Table 6 &#8227; 9 Comparison with competitors &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that our RAG-augmented CS consistently outperforms the baseline samples from both studies. GPT-4o-mini + BGE-M3 outperforms Baseline 1 in 962 out of 1,697 pairwise battles, while LLaMA-8B + BGE-M3 outperforms Baseline 2 in 114 out of 124 battles. These results demonstrate that our RAG pipeline produces CS that is not only factually richer but also concise and suitable for social media, aligning with the intended communicative goals of real-world deployment.</p>\n\n",
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S9.T6\" title=\"Table 6 &#8227; 9 Comparison with competitors &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> also reports the per-target breakdown of our pairwise comparison.\nAgainst Baseline 1, our system achieves overall consistent improvements. Gains are observed across all HS targets, with particularly strong performance for <span class=\"ltx_text ltx_font_italic\">DISABLED</span> (69.4%), <span class=\"ltx_text ltx_font_italic\">LGBT+</span> (64.0%), and <span class=\"ltx_text ltx_font_italic\">WOMEN</span> (61.2%). For larger target categories such as <span class=\"ltx_text ltx_font_italic\">MUSLIMS</span> and <span class=\"ltx_text ltx_font_italic\">MIGRANTS</span>, our model still maintains a clear advantage with 55.2% and 56.8% wins respectively, despite the higher difficulty and variability in these groups. The most balanced outcome is seen for <span class=\"ltx_text ltx_font_italic\">JEWS</span>, where results are split evenly (50%). Against Baseline 2, our system achieves consistently strong results across all targets, outperforming it in 93% of cases. The largest margins are observed for <span class=\"ltx_text ltx_font_italic\">MIGRANTS</span> (100% wins) and <span class=\"ltx_text ltx_font_italic\">JEWS</span> (95.7%).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Counter-speech generation is at the core of many expert activities, such as fact-checking and hate speech, to counter harmful content. Yet, existing work treats counter-speech generation as pure text generation task, mainly based on Large Language Models or NGO experts. These approaches show severe drawbacks due to the limited reliability and coherence in the generated countering text, and in scalability, respectively. To close this gap, we introduce a novel framework to model counter-speech generation as knowledge-wise text generation process. Our framework integrates advanced Retrieval-Augmented Generation (RAG) pipelines to ensure the generation of trustworthy counter-speech for 8 main target groups identified in the hate speech literature, including women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. We built a knowledge base over the United Nations Digital Library, EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792 texts. We use the MultiTarget-CONAN dataset to empirically assess the quality of the generated counter-speech, both through standard metrics (i.e., JudgeLM) and a human evaluation. Results show that our framework outperforms standard LLM baselines and competitive approach, on both assessments. The resulting framework and the knowledge base pave the way for studying trustworthy and sound counter-speech generation, in hate speech and beyond. <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">Warning<span class=\"ltx_text ltx_font_medium\">: this paper contains explicit examples some readers may find offensive.</span></span></p>\n\n",
                "matched_terms": [
                    "jews",
                    "total",
                    "women",
                    "model",
                    "show",
                    "target",
                    "other",
                    "muslims",
                    "results",
                    "judgelm",
                    "migrants",
                    "groups",
                    "lgbt",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">NGOs and experts successfully deployed CS campaigns, but the scale of HS makes manual CS unsustainable <cite class=\"ltx_cite ltx_citemacro_cite\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib16\" title=\"\">2021b</a>)</cite>.\nThis challenge has motivated research on automatic CS generation, an emerging NLP task. Early approaches relied on curated datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib14\" title=\"\">2019</a>); Fanton et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib20\" title=\"\">2021</a>)</cite> and fine-tuning of pre-trained language models (PLMs) <cite class=\"ltx_cite ltx_citemacro_cite\">Tekiro&#287;lu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib48\" title=\"\">2020</a>); Zhu and Bhat (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib56\" title=\"\">2021</a>); Tekiro&#287;lu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib47\" title=\"\">2022</a>)</cite>. While effective in-domain, such models often produce generic or off-topic responses on unseen HS.\nTo achieve factuality and informativeness in the generated CS, it is therefore essential to embed knowledge on the target HS groups in the generation process.</p>\n\n",
                "matched_terms": [
                    "target",
                    "groups"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tackle this challenging task by proposing a novel knowledge-grounded framework for automatic counter-speech generation that integrates advanced retrieval-augmented generation (RAG) pipelines. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows two examples of CS, with and without RAG augmentation. Our contributions are fourfold: <span class=\"ltx_text ltx_font_bold\">(1)</span> we provide a systematic comparison of multiple retrievers and LLMs for CS generation, combining three retrieval methods with four language models; <span class=\"ltx_text ltx_font_bold\">(2)</span> we enforce concise, two-sentence outputs tailored for social media deployment, ensuring responses remain natural, relatable, and effective in real-world contexts; <span class=\"ltx_text ltx_font_bold\">(3)</span> we conduct a pairwise evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, adapted to balance factual grounding with conciseness and pragmatic suitability, together with an extensive human evaluation, showing that our framework steadily outperforms standard baselines and state-of-the-art competitors; and <span class=\"ltx_text ltx_font_bold\">(4)</span> we will release all generated CS along with their top-3 retrieved evidence sentences, offering a reusable resource for the research community<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code will be made available upon acceptance.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "russo",
                    "systems",
                    "wilk",
                    "against",
                    "pairwise",
                    "judgelm",
                    "comparison",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic metrics (BLEU, ROUGE, BERTScore) correlate poorly with human judgments, prompting exploration of novelty- and repetition-based scores <cite class=\"ltx_cite ltx_citemacro_cite\">Wang and Wan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib49\" title=\"\">2018</a>); Bertoldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib6\" title=\"\">2013</a>)</cite>, and LLM-based frameworks (e.g., GPT-4, PandaLM, JudgeLM, UniEval) <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib55\" title=\"\">2023</a>); Zhong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib54\" title=\"\">2022</a>)</cite>, which show improved reliability for multi-aspect CS assessment <cite class=\"ltx_cite ltx_citemacro_cite\">Jones et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib25\" title=\"\">2024</a>); Damo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib18\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "judgelm",
                    "show"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG addresses a key limitation of prior approaches by grounding outputs in verifiable evidence, thereby enhancing factuality and persuasiveness in countering HS.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib15\" title=\"\">2021a</a>)</cite> proposed a retrieval-augmented pipeline that first generates queries from HS using keyword extraction, then employs BM25 <cite class=\"ltx_cite ltx_citemacro_cite\">Robertson and Zaragoza (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib39\" title=\"\">2009</a>)</cite> to retrieve relevant articles from Newsroom <cite class=\"ltx_cite ltx_citemacro_cite\">Grusky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib21\" title=\"\">2018</a>)</cite> and WikiText-103 <cite class=\"ltx_cite ltx_citemacro_cite\">Merity et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib32\" title=\"\">2016</a>)</cite>. From these, the most relevant sentences are selected using the ROUGE-L metric <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib29\" title=\"\">2004</a>)</cite>, before being passed to GPT-2 <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib37\" title=\"\">2019</a>)</cite> and XNLG <cite class=\"ltx_cite ltx_citemacro_cite\">Chi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib13\" title=\"\">2020</a>)</cite>.\nSimilarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>)</cite> introduced RAUCG, which retrieves counter-arguments from the ChangeMyView subreddit, selecting them based on stance consistency, semantic overlap, and a custom perplexity-based fitness function. The final generation step employs energy-based decoding to preserve factual knowledge while countering HS fluently.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> proposed ReZG, a retrieval-augmented zero-shot approach that integrates multi-dimensional hierarchical retrieval with constrained decoding, enabling the generation of more specific CS for unseen HS targets. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> leverage both curated background knowledge and web search to improve factuality.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> evaluated reranker-based pipelines, showing that fine-grained retrieval significantly improves factuality and relevance.</p>\n\n",
                "matched_terms": [
                    "wilk",
                    "russo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step, we built a comprehensive Knowledge Base (KB) designed to ensure maximal coverage of documents addressing social groups commonly targeted by hate speech. The goal of this KB is to gather all relevant materials &#8212; such as reports, resolutions, and legal texts &#8212; that provide evidence or context regarding these topics.\nSpecifically, we focused on the following 8 target groups: women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. These categories align with the classic targets in the literature, including the ones of our baseline MultiTarget-CONAN <cite class=\"ltx_cite ltx_citemacro_cite\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib14\" title=\"\">2019</a>)</cite>.\nWe used GPT-based prompting to generate synonyms and semantically related keywords, ensuring that queries captured diverse terminology across cultural and policy contexts (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for the prompt). To guarantee relevance, we performed a keyword-based search so that retrieved knowledge was directly aligned with the hateful messages targeting these groups. To ensure reliability in the generated CS, we relied exclusively on institutional publicly available sources, i.e., the United Nations Digital Library, EUR-Lex, and the European Union Agency for Fundamental Rights (FRA). To construct our knowledge base, we follow three main steps: document retrieval, PDF-to-text conversion, and knowledge base integration.</p>\n\n",
                "matched_terms": [
                    "across",
                    "jews",
                    "women",
                    "target",
                    "other",
                    "muslims",
                    "migrants",
                    "baseline",
                    "groups",
                    "lgbt",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Document Retrieval.</span> The <span class=\"ltx_text ltx_font_bold\">United Nations Digital Library</span> serves as the central repository for official UN documents on human rights, equality, and anti-discrimination. We developed a custom crawler using <span class=\"ltx_text ltx_font_typewriter\">requests</span> and <span class=\"ltx_text ltx_font_typewriter\">BeautifulSoup</span> to systematically combine three query dimensions&#8212;target keywords, document types (e.g., resolutions, treaties, NGO statements), and years (2000&#8211;2025). The crawler paginated through results, extracted and normalized documents in English, and organized downloads by target group. A metadata file recorded <span class=\"ltx_text ltx_font_typewriter\">id, fname, target, type, year, url</span>. Error handling covered duplicate checks, retries, and skipped completed downloads. At the European level, <span class=\"ltx_text ltx_font_bold\">EUR-Lex</span> provides access to EU law, treaties, and legal acts, while the <span class=\"ltx_text ltx_font_bold\">EU Agency for Fundamental Rights (FRA)</span> publishes reports on human rights within the EU. Using the same target keywords, we retrieved documents in English (2000&#8211;2025) from both sources, following the same metadata structure as the UN corpus. These sources complement the UN materials, forming a multi-level knowledge base&#8212;global and European&#8212;that ensures comprehensive and reliable grounding for CS generation.</p>\n\n",
                "matched_terms": [
                    "target",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge Base Integration.</span> All materials from UN, EU, and FRA were standardized into JSON with metadata (target group, document type, year, URL). The KB spans the years 2000&#8211;2025 and combines factual and policy-oriented resources, organized by target group and document type. To the best of our knowledge, this is the first large-scale, authoritative knowledge base specifically constructed for counter-speech generation.</p>\n\n",
                "matched_terms": [
                    "target",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S3.T1\" title=\"Table 1 &#8227; 3 Knowledge Base Construction &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports descriptive statistics of the UN and EU corpora by target group keyword. While the UN collection is considerably larger in terms of the number of documents (over 32k), EU reports are more detailed, with substantially higher average word counts per document (<math alttext=\"\\approx 26k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p5.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>26</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 26k</annotation></semantics></math> vs. <math alttext=\"\\approx 7.7k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p5.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>7.7</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 7.7k</annotation></semantics></math>). Coverage also varies by group: for example, the UN corpus contains a large volume of material on human rights and women, while EU provides more in-depth analyses on migrants, LGBT+ individuals, and people with disabilities. Together, these complementary sources balance breadth (UN) with depth (EU), creating a diverse and representative foundation for counter-speech generation. This ensures that our knowledge base is both comprehensive and adaptable to different CS scenarios.</p>\n\n",
                "matched_terms": [
                    "target",
                    "migrants",
                    "women",
                    "number",
                    "lgbt",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our domain-specific knowledge base (KB), as described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S3\" title=\"3 Knowledge Base Construction &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> is segmented into paragraphs and tokenized for fine-grained access. Each paragraph is embedded and stored using FAISS <cite class=\"ltx_cite ltx_citemacro_cite\">Douze et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib19\" title=\"\">2024</a>)</cite> to allow efficient similarity search. We obtain a total of <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 3 billions paragraphs.</p>\n\n",
                "matched_terms": [
                    "total",
                    "each",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a HS message <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, we aim to retrieve a small set of relevant paragraphs from the KB (i.e., the most similar to the target HS).\nFor this, we employ three complementary retrieval models: BM25, Sentence-BERT (SBERT), and BGE-M3.\nLet the KB consist of paragraphs <math alttext=\"\\mathcal{P}=\\{p_{1},p_{2},\\ldots,p_{N}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mi>N</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}=\\{p_{1},p_{2},\\ldots,p_{N}\\}</annotation></semantics></math>. For each retriever\n<math alttext=\"r\\in\\{\\text{BM25},\\text{SBERT},\\text{BGE-M3}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>BM25</mtext><mo>,</mo><mtext>SBERT</mtext><mo>,</mo><mtext>BGE-M3</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in\\{\\text{BM25},\\text{SBERT},\\text{BGE-M3}\\}</annotation></semantics></math>, we compute a similarity score:\n<math alttext=\"s_{r}(h,p_{i})=\\text{sim}_{r}(\\phi(h),\\phi(p_{i}))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>s</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mtext>sim</mtext><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">s_{r}(h,p_{i})=\\text{sim}_{r}(\\phi(h),\\phi(p_{i}))</annotation></semantics></math>\nwhere <math alttext=\"\\phi(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(\\cdot)</annotation></semantics></math> is the embedding (or term-weight) function induced by the retriever. For BM25, the similarity score is computed based on TF-IDF, and document length normalization. For SBERT and BGE-M3, similarity is measured as the cosine similarity between dense vector embeddings of the HS and candidate paragraph, capturing semantic relatedness even in the absence of exact lexical overlap.\nFor each retriever, we obtain the top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> ranked paragraphs:\n<math alttext=\"R_{r}(h)=\\{p_{r,1},p_{r,2},\\ldots,p_{r,k}\\},\\quad k=3.\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mrow><mrow><mrow><msub><mi>R</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mn>2</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo rspace=\"1.167em\">,</mo><mrow><mi>k</mi><mo>=</mo><mn>3</mn></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">R_{r}(h)=\\{p_{r,1},p_{r,2},\\ldots,p_{r,k}\\},\\quad k=3.</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "target",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the final stage, the summaries are used as external knowledge to generate CS. For each HS message <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, retriever <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>, and model <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, the generation function is defined as:</p>\n\n",
                "matched_terms": [
                    "model",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since we use three retrievers and four LLMs, the system produces: <math alttext=\"|C(h)|=|\\{c_{r,m}(h)\\}|=3\\times 4=12\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>c</mi><mrow><mi>r</mi><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow><mo>=</mo><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">|C(h)|=|\\{c_{r,m}(h)\\}|=3\\times 4=12</annotation></semantics></math> CS outputs for each HS instance. This enables systematic comparison across retrieval methods and summarization strategies.\nTo ensure that generated CS is deployable in (online) real-world contexts, we restrict outputs to a maximum of two sentences. This reflects the communicative norms of social media platforms, where posts are typically 1&#8211;2 sentences long <cite class=\"ltx_cite ltx_citemacro_citep\">(&#350;ahinu&#231; and Toraman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib43\" title=\"\">2021</a>)</cite>. Prior work shows that overly verbose responses are less engaging and less effective in countering harmful narratives <cite class=\"ltx_cite ltx_citemacro_citep\">(Russo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib41\" title=\"\">2023</a>)</cite>. Concise and relatable CS has been repeatedly identified as key to user engagement and effectiveness <cite class=\"ltx_cite ltx_citemacro_citep\">(Bonaldi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib8\" title=\"\">2024</a>; Benesch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib4\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "each",
                    "comparison",
                    "russo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ Meta-Llama-3.1-8B-Instruct<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct</a></span></span></span>, Cohere&#8217;s Command-R-7B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>We included CommandR, optimized for grounding outputs in retrieved context.</span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Cohere et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib17\" title=\"\">2025</a>)</cite>, Mistral-7B-Instruct-v0.3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\" title=\"\">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3</a></span></span></span>, and gpt-4o-mini-2024-07-18. These models were selected as LLMs of similar size, balancing efficiency and accuracy, allowing comparison across open-weight and proprietary settings.</p>\n\n",
                "matched_terms": [
                    "included",
                    "across",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BM25</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Robertson and Zaragoza (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib39\" title=\"\">2009</a>)</cite> is a sparse lexical retriever based on TF-IDF with document length normalization. It remains an effective baseline for keyword-sensitive domains where exact term overlap is important. <span class=\"ltx_text ltx_font_bold\">Sentence-BERT (SBERT)</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib38\" title=\"\">2019</a>)</cite> encodes queries and passages into dense vector embeddings optimized for semantic similarity, with ranking performed via cosine similarity.\n<span class=\"ltx_text ltx_font_bold\">BGE-M3 (BAAI General Embeddings).</span> BGE-M3 <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib12\" title=\"\">2024</a>)</cite> is a recent dense embedding model trained for multilingual and multi-task semantic retrieval. These retrievers capture complementary dimensions of relevance, from exact term matching (BM25) to semantic similarity (SBERT, BGE-M3), ensuring robust retrieval across different types of hateful content and knowledge sources.</p>\n\n",
                "matched_terms": [
                    "across",
                    "baseline",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experimented on the Multi-Target CONAN (MTCo) dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib9\" title=\"\">2022</a>)</cite>, which contains 5,003 HS/CS pairs in English covering multiple target groups, including people with disabilities, Jews, LGBT+ individuals, Muslims, migrants, people of color (POC), and women. Collected through a human-in-the-loop process, MTCo provides high-quality, contextually relevant CS, and serves as a first baseline for our experiments. Additionally, we compare our pipeline with the four instruction-tuned LLMs of comparable size without RAG. Finally, we compare our results with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>We cannot compare to <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> due to lack of publicly available code and data, even upon request.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "jews",
                    "russo",
                    "wilk",
                    "women",
                    "target",
                    "muslims",
                    "poc",
                    "results",
                    "migrants",
                    "baseline",
                    "groups",
                    "lgbt",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Reference-less Metrics.</span> To complement reference-based evaluation, we assess intrinsic qualities of generated CS. <span class=\"ltx_text ltx_font_bold\">Distinct-1/2</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib28\" title=\"\">2015</a>)</cite> measures the proportion of unique unigrams and bigrams, reflecting lexical diversity. <span class=\"ltx_text ltx_font_bold\">Repetition Rate (RR)</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Cettolo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib11\" title=\"\">2014</a>)</cite> represents the fraction of repeated n-grams within a generation. <span class=\"ltx_text ltx_font_bold\">Safety</span>: the OpenAI&#8217;s content moderation API scores each output across categories of potential harm (e.g., hate, sexual, violence), with higher values indicate safer counter-speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib8\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S6.T2\" title=\"Table 2 &#8227; Human evaluation metrics. &#8227; 6 Metrics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports average results, that can be interpreted along three main dimensions: content quality, diversity, and safety.\n<span class=\"ltx_text ltx_font_bold\">Content Quality:</span> scores on BLEU, METEOR, and ROUGE-L remain relatively low, as expected for open-ended generation, while BERTScore values are consistently high (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.85), indicating strong semantic similarity to MT-Co references. Non-RAG outputs achieve slightly higher lexical and semantic overlap, suggesting that retrieval introduces content that, while factually richer, diverges from exact reference phrasing. RAG outputs show less words overlapping,\nlikely reflecting the inclusion of factual information from the knowledge base.\n<span class=\"ltx_text ltx_font_bold\">Diversity:</span> incorporating retrieval substantially improves lexical diversity and reduces repetition. Mistral with BGE-M3 shows the largest gains, while No-RAG models, particularly Mistral, exhibit high repetition and lower diversity. <span class=\"ltx_text ltx_font_bold\">Safety:</span> No-RAG configurations achieve the highest safety, with GPT leading (0.993). RAG outputs show slightly lower safety, likely due to occasional noise introduced from the retrieval process. Nevertheless, safety remains high overall, indicating that fact-grounded augmentation does not substantially compromise non-harmfulness.\n<span class=\"ltx_text ltx_font_bold\">Model-level Trends:</span> GPT consistently delivers the safest outputs and competitive BERTScore, though with lower lexical diversity than Mistral. Mistral excels in diversity with RAG but performs poorly in No-RAG due to high repetition. CommandR and LLaMA provide stable but moderate performance across metrics. Overall, RAG with BGE-M3 achieves the best balance between quality, diversity, and informativeness. All results differ significantly (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A4\" title=\"Appendix D Statistical Significance of Retrieval Effects &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> for more details).</p>\n\n",
                "matched_terms": [
                    "across",
                    "show",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate CS quality using <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span>, focusing on four dimensions: factuality, number of relevant facts, relevance to the HS, and specificity. This prompt (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>) guides pairwise comparisons between model outputs, emphasizing informative and targeted CS over surface-level similarity.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T3\" title=\"Table 3 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> results for RAG vs. No-RAG models across retrieval strategies. BGE-M3 yields the strongest results, followed by SentBERT and BM25, confirming the advantage of semantically rich retrieval. Among generators, GPT wins most comparisons, showing effective integration of retrieved content. Mistral also benefits notably from RAG, while CommandR performs less consistently.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T4\" title=\"Table 4 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the best RAG setups (LLM + BGE-M3) against human-written MT-Co CS. All RAG systems perform strongly, with GPT + BGE-M3 approaching human-level quality and Mistral + BGE-M3 remaining competitive, whereas CommandR trails slightly but still surpasses baselines.</p>\n\n",
                "matched_terms": [
                    "across",
                    "wins",
                    "systems",
                    "against",
                    "pairwise",
                    "model",
                    "results",
                    "judgelm",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted a human evaluation with 26 participants to assess the quality and effectiveness of the generated CS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Participants&#8217; age is between 18 and 50, there is a balance between genders, and their level of education spans high school diploma to PhD. Detailed statistics are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5\" title=\"Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span>. Our evaluation setup consisted of 10 HS examples, extracted randomly from MT-Co keeping the target distribution, with 9 CS candidates for each HS. We selected the CS from MT-Co and the CS generated by our four LLMs without the RAG pipeline, and the corresponding CS with RAG using BMG-M3 as retriever, since it has shown the strongest performance in the automatic and LLM-based metrics. Participants rated all 90 CS candidates along four dimensions, i.e., Relevance, Factuality, Cogency, and Correctness (1&#8211;3 scale), judged whether each CS was effective (Yes/No) and selected the best CS per HS. We collected a total of 2340 evaluations.</p>\n\n",
                "matched_terms": [
                    "total",
                    "target",
                    "each",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Best CS.</span> RAG-based methods are chosen more frequently as best CS, implying that they are clearly preferred compared to their No RAG counter-parts, with GPT RAG receiving the highest number of votes (71). Complete statistics for each method are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A3\" title=\"Appendix C Additional Results from the Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effectiveness.</span> Out of the 2340 evaluations collected, 1312 were marked as effective, corresponding to 56% of the cases. More specifically, Llama RAG was considered effective most frequently (with 197 votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effectiveness votes (48) (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A3\" title=\"Appendix C Additional Results from the Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for full results). This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. The non-RAG models and MTCo were less effective overall. This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline, suggesting that both model choice and augmentation strategy strongly influence performance.</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, RAG-based methods outperform non-RAG counterparts across all the four metrics, especially in Factuality, Cogency and Effectiveness, and are preferred by annotators in best-choice votes. The baseline method MT-Co consistently scores lowest across all metrics and effectiveness. Overall, <span class=\"ltx_text ltx_font_bold\">GPT RAG</span> is the best CS method across all HS and annotators. Results from the human evaluation are aligned with both automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "judgelm",
                    "across",
                    "baseline",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To contextualize our results, we compared our RAG-based CS with publicly available samples from two recent studies. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> (Baseline 1) released 1,697 GPT-4o-generated CS responses from their RAG pipeline addressing MT-Co hate speech, while <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> (Baseline 2) submitted 400 CS samples to the COLING 2025 shared task using LLaMA-EUS-8B with retrieved knowledge. Since the samples were multilingual, we retained only English CS aligned with MT-Co, totaling 124 samples.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> produced 100 English CS, 24 HS instances overlapped with MT-CONAN and were retained with multiple CS variants.</span></span></span>\nFor fair comparison, we reproduced settings closest to the original models: GPT-4o-mini + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite>, and LLaMA-8B + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>. This alignment minimizes differences due to model size, isolating the effects of retrieval and pipeline design. Pairwise performance was then assessed with <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> (prompt available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>). It is fine-tuned to rate helpfulness, relevance, accuracy, and level of detail of CS responses, which already accounts for evidence and factuality in the original prompt. To additionally emphasize conciseness, and suitability for real-world deployment on social media, we limit the CS to a maximum of two sentences.</p>\n\n",
                "matched_terms": [
                    "russo",
                    "wilk",
                    "pairwise",
                    "model",
                    "results",
                    "judgelm",
                    "baseline",
                    "comparison",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results indicate that while our models consistently outperform prior baselines, the improvement varies by target group. In particular, our LLaMA-based system demonstrates decisive superiority over Baseline 2 comparable setup, whereas the GPT-based comparison with Baseline 1 highlights more incremental but robust gains across diverse HS categories, highlighting the effectiveness of our RAG approach in improving both informativeness and practical usability of the CS.</p>\n\n",
                "matched_terms": [
                    "across",
                    "target",
                    "results",
                    "baseline",
                    "comparison",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a RAG-based framework for automatic counter-speech generation. We systematically compared three retrieval methods and four LLMs for CS generation targeting height groups (women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, other), relying on a novel and unique knowledge base, built over three institutional sources. We conducted an extensive experimental evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, and a human evaluation. Our results show that our approach outperforms competitive approaches and standard baselines on both of them. Our experiments demonstrated the versatility and soundness of our framework for counter-speech generation to fight online abusive content.</p>\n\n",
                "matched_terms": [
                    "jews",
                    "russo",
                    "systems",
                    "wilk",
                    "against",
                    "women",
                    "show",
                    "other",
                    "muslims",
                    "results",
                    "judgelm",
                    "migrants",
                    "groups",
                    "lgbt",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study involves the use of hate speech examples for the development and evaluation of counter-speech generation systems. We acknowledge that the inclusion of HS content poses potential risks of exposure to harmful language and emotional distress for researchers and annotators. All individuals involved in data handling were informed of these risks and participated voluntarily, following institutional ethical guidelines.\nAlthough our goal is to promote positive and factual discourse, automatic CS generation can inadvertently reinforce biases, produce factually incorrect content, or convey unintended tones. To mitigate these risks, we rely on institutional sources (UN, EU, FRA) for retrieval, explicitly evaluate factuality and correctness, and use human oversight in all analyses. The system is presented for research purposes only and is not intended for unsupervised deployment.\nWe further recognize that the perceived effectiveness and appropriateness of CS depend on social and cultural context. Our methods and findings should therefore not be generalized without careful adaptation and ethical review. No personal or private data were used; all retrieved materials come from publicly available institutional sources.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The following keywords were used to retrieve documents from the United Nations Digital Library, the FRA and EUR-Lex portals, covering the period 2000&#8211;2025 and targeting multiple groups and thematic areas relevant to our study:</p>\n\n",
                "matched_terms": [
                    "groups",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">People of Color</span>: People of color, Racism, Anti-Black racism, Systemic racism, Racial inequality, Racial equality, Racial profiling, White privilege, Black Lives Matter, Colonialism, Racial discrimination, discrimination against black people, blacks, race, black women, black people, blacks hate speech, african descent, ethnic minorities, ethnic inequalities, minority</p>\n\n",
                "matched_terms": [
                    "against",
                    "women"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LGBT</span>: LGBT rights, Homophobia, Transphobia, Biphobia, Gender identity, Conversion therapy, Same-sex marriage, Stonewall riots, LGBT, LGBTQIA+, Gay, Lesbian, Transgender, Non-binary, LGBT hate speech, discrimination against LGBT people, gay rights movement, LGBT discrimination, LGBT hate crimes, sexual orientation, HIV/AIDS &amp; gay/lesbian</p>\n\n",
                "matched_terms": [
                    "against",
                    "lgbt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Disabled</span>: Disability rights, Accessibility, Social model of disability, disability, disabled, down syndrome, autism, mental disability, physical disability, neurodiversity, ableism, inclusive design, Discrimination against people with disabilities</p>\n\n",
                "matched_terms": [
                    "against",
                    "model",
                    "disabled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Muslims</span>: Islamophobia, Discrimination against Muslims, Anti-Muslim hate crimes, Muslim communities, Religious discrimination, islam, muslim, muslim hate speech, religion, discrimination against muslims, muslim communities</p>\n\n",
                "matched_terms": [
                    "against",
                    "muslims"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Women</span>: Sexism, Misogyny, Feminism, Gender inequality, Women&#8217;s rights, Me Too movement, Gender-based violence, women, women hate speech, feminism, violence against women, gender inequality, glass ceiling, discrimination against women</p>\n\n",
                "matched_terms": [
                    "against",
                    "women"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Migrants</span>: Xenophobia, Anti-immigration, Refugee crisis, Asylum seekers, Undocumented immigrants, Immigration law, refugee, migrants, immigrants, immigration, migration, immigration hate speech, migrants rights, illegal aliens, immigration and crime, immigration and unemployment, discrimination against migrants, aliens</p>\n\n",
                "matched_terms": [
                    "against",
                    "migrants"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Step 2 of our pipeline (document summarization), for each of the 3 paragraphs, we prompt the LLMs with the following:</p>\n\n",
                "matched_terms": [
                    "each",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to use <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> as a metric, we used the following prompts. The first one is used to compare RAG methods against No RAG methods, and against the MT-Co baseline. The second prompt is used to compare our RAG-based CS against those generated with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>); Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "russo",
                    "wilk",
                    "against",
                    "judgelm",
                    "baseline",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the total number of times each CS method was rated effective, and how many times it has been selected as the best one across all HS and annotators. RAG-based methods are clearly preferred compared to their No RAG counter-parts. Concerning effectiveness, Llama RAG was considered effective most frequently (197 &#8220;Yes&#8221; votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effective votes (48). This indicates that RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. Furthermore, RAG methods are selected more often as best ones compared to their No RAG counterparts, with GPT RAG and Llama RAG achieving the best results.</p>\n\n",
                "matched_terms": [
                    "across",
                    "total",
                    "each",
                    "results",
                    "baseline",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted non-parametric Friedman tests followed by Bonferroni-corrected Wilcoxon signed-rank tests to evaluate whether RAG retrieval strategies (BM25, Sentence-BERT, BGE) significantly affected model outputs compared to the No-RAG baseline for the per-sample automatic metrics (BLEU, METEOR, ROUGE-L, BERTScore), which allow direct pairwise comparison across systems. In contrast, diversity metrics such as Distinct-1, Distinct-2, and Repetition Rate are computed at the corpus level, producing a single value per model.\nBecause these measures do not yield per-sample scores and thus lack within-system variance, statistical testing is not applicable.\nFor all models, Friedman tests revealed significant overall effects (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), and pairwise comparisons confirmed that retrieval methods consistently induced statistically significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>) across quality and safety metrics.</p>\n\n",
                "matched_terms": [
                    "across",
                    "systems",
                    "pairwise",
                    "model",
                    "baseline",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we report the demographic characteristics of the 26 participants of the human evaluation.\nFigures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5.F3\" title=\"Figure 3 &#8227; Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5.F5\" title=\"Figure 5 &#8227; Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5.F6\" title=\"Figure 6 &#8227; Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> show the distribution of age groups, gender, geographical area of origin, and the highest obtained education level. Age varies between 18 and 50 years, with the majority group being between 18 and 35. Gender is evenly distributed among females and males with one person identifying as non-binary. The geographical area of origin covers all major areas, with a majority of European people. Education level spans from high school diploma to PhD, with the majority of respondents having pursued a PhD. We also asked the respondents their area of expertise, which covers the following fields: Computer Science, Computer Engineering, Data Science, Psychology, Natural Language Processing, Linguistics, and Management.</p>\n\n",
                "matched_terms": [
                    "show",
                    "groups"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5.F7\" title=\"Figure 7 &#8227; Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the target groups in which the respondents identify. They belong to almost all the HS targets we considered in our analysis, with the majority of them identifying as women.</p>\n\n",
                "matched_terms": [
                    "groups",
                    "target",
                    "women",
                    "our"
                ]
            }
        ]
    },
    "A2.T7": {
        "caption": "Table 7: Comparison of counter-speech methods showing both the number of times a CS was voted Yes for Effectiveness and the number of Best Choice votes received.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">CS Method</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Effectiveness &#8220;Yes&#8221; Votes</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Best CS Votes</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Llama RAG</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">197</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">67</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GPT RAG</th>\n<td class=\"ltx_td ltx_align_right\">191</td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\">71</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CommandR RAG</th>\n<td class=\"ltx_td ltx_align_right\">172</td>\n<td class=\"ltx_td ltx_align_right\">26</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Llama No RAG</th>\n<td class=\"ltx_td ltx_align_right\">171</td>\n<td class=\"ltx_td ltx_align_right\">33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mistral RAG</th>\n<td class=\"ltx_td ltx_align_right\">156</td>\n<td class=\"ltx_td ltx_align_right\">24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GPT No RAG</th>\n<td class=\"ltx_td ltx_align_right\">148</td>\n<td class=\"ltx_td ltx_align_right\">14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CommandR No RAG</th>\n<td class=\"ltx_td ltx_align_right\">120</td>\n<td class=\"ltx_td ltx_align_right\">7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mistral No RAG</th>\n<td class=\"ltx_td ltx_align_right\">109</td>\n<td class=\"ltx_td ltx_align_right\">11</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">MTCo</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">48</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">7</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "mtco",
            "comparison",
            "effectiveness",
            "yes",
            "showing",
            "rag",
            "counterspeech",
            "methods",
            "received",
            "llama",
            "choice",
            "both",
            "gpt",
            "number",
            "commandr",
            "votes",
            "voted",
            "best",
            "method",
            "times",
            "mistral"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the total number of times each CS method was rated effective, and how many times it has been selected as the best one across all HS and annotators. RAG-based methods are clearly preferred compared to their No RAG counter-parts. Concerning effectiveness, Llama RAG was considered effective most frequently (197 &#8220;Yes&#8221; votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effective votes (48). This indicates that RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. Furthermore, RAG methods are selected more often as best ones compared to their No RAG counterparts, with GPT RAG and Llama RAG achieving the best results.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Counter-speech generation is at the core of many expert activities, such as fact-checking and hate speech, to counter harmful content. Yet, existing work treats counter-speech generation as pure text generation task, mainly based on Large Language Models or NGO experts. These approaches show severe drawbacks due to the limited reliability and coherence in the generated countering text, and in scalability, respectively. To close this gap, we introduce a novel framework to model counter-speech generation as knowledge-wise text generation process. Our framework integrates advanced Retrieval-Augmented Generation (RAG) pipelines to ensure the generation of trustworthy counter-speech for 8 main target groups identified in the hate speech literature, including women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. We built a knowledge base over the United Nations Digital Library, EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792 texts. We use the MultiTarget-CONAN dataset to empirically assess the quality of the generated counter-speech, both through standard metrics (i.e., JudgeLM) and a human evaluation. Results show that our framework outperforms standard LLM baselines and competitive approach, on both assessments. The resulting framework and the knowledge base pave the way for studying trustworthy and sound counter-speech generation, in hate speech and beyond. <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">Warning<span class=\"ltx_text ltx_font_medium\">: this paper contains explicit examples some readers may find offensive.</span></span></p>\n\n",
                "matched_terms": [
                    "rag",
                    "both",
                    "counterspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tackle this challenging task by proposing a novel knowledge-grounded framework for automatic counter-speech generation that integrates advanced retrieval-augmented generation (RAG) pipelines. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows two examples of CS, with and without RAG augmentation. Our contributions are fourfold: <span class=\"ltx_text ltx_font_bold\">(1)</span> we provide a systematic comparison of multiple retrievers and LLMs for CS generation, combining three retrieval methods with four language models; <span class=\"ltx_text ltx_font_bold\">(2)</span> we enforce concise, two-sentence outputs tailored for social media deployment, ensuring responses remain natural, relatable, and effective in real-world contexts; <span class=\"ltx_text ltx_font_bold\">(3)</span> we conduct a pairwise evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, adapted to balance factual grounding with conciseness and pragmatic suitability, together with an extensive human evaluation, showing that our framework steadily outperforms standard baselines and state-of-the-art competitors; and <span class=\"ltx_text ltx_font_bold\">(4)</span> we will release all generated CS along with their top-3 retrieved evidence sentences, offering a reusable resource for the research community<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code will be made available upon acceptance.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "showing",
                    "rag",
                    "counterspeech",
                    "methods",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG addresses a key limitation of prior approaches by grounding outputs in verifiable evidence, thereby enhancing factuality and persuasiveness in countering HS.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib15\" title=\"\">2021a</a>)</cite> proposed a retrieval-augmented pipeline that first generates queries from HS using keyword extraction, then employs BM25 <cite class=\"ltx_cite ltx_citemacro_cite\">Robertson and Zaragoza (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib39\" title=\"\">2009</a>)</cite> to retrieve relevant articles from Newsroom <cite class=\"ltx_cite ltx_citemacro_cite\">Grusky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib21\" title=\"\">2018</a>)</cite> and WikiText-103 <cite class=\"ltx_cite ltx_citemacro_cite\">Merity et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib32\" title=\"\">2016</a>)</cite>. From these, the most relevant sentences are selected using the ROUGE-L metric <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib29\" title=\"\">2004</a>)</cite>, before being passed to GPT-2 <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib37\" title=\"\">2019</a>)</cite> and XNLG <cite class=\"ltx_cite ltx_citemacro_cite\">Chi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib13\" title=\"\">2020</a>)</cite>.\nSimilarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>)</cite> introduced RAUCG, which retrieves counter-arguments from the ChangeMyView subreddit, selecting them based on stance consistency, semantic overlap, and a custom perplexity-based fitness function. The final generation step employs energy-based decoding to preserve factual knowledge while countering HS fluently.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> proposed ReZG, a retrieval-augmented zero-shot approach that integrates multi-dimensional hierarchical retrieval with constrained decoding, enabling the generation of more specific CS for unseen HS targets. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> leverage both curated background knowledge and web search to improve factuality.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> evaluated reranker-based pipelines, showing that fine-grained retrieval significantly improves factuality and relevance.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "showing",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge Base Integration.</span> All materials from UN, EU, and FRA were standardized into JSON with metadata (target group, document type, year, URL). The KB spans the years 2000&#8211;2025 and combines factual and policy-oriented resources, organized by target group and document type. To the best of our knowledge, this is the first large-scale, authoritative knowledge base specifically constructed for counter-speech generation.</p>\n\n",
                "matched_terms": [
                    "best",
                    "counterspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S3.T1\" title=\"Table 1 &#8227; 3 Knowledge Base Construction &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports descriptive statistics of the UN and EU corpora by target group keyword. While the UN collection is considerably larger in terms of the number of documents (over 32k), EU reports are more detailed, with substantially higher average word counts per document (<math alttext=\"\\approx 26k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p5.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>26</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 26k</annotation></semantics></math> vs. <math alttext=\"\\approx 7.7k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p5.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>7.7</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>k</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 7.7k</annotation></semantics></math>). Coverage also varies by group: for example, the UN corpus contains a large volume of material on human rights and women, while EU provides more in-depth analyses on migrants, LGBT+ individuals, and people with disabilities. Together, these complementary sources balance breadth (UN) with depth (EU), creating a diverse and representative foundation for counter-speech generation. This ensures that our knowledge base is both comprehensive and adaptable to different CS scenarios.</p>\n\n",
                "matched_terms": [
                    "number",
                    "both",
                    "counterspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since we use three retrievers and four LLMs, the system produces: <math alttext=\"|C(h)|=|\\{c_{r,m}(h)\\}|=3\\times 4=12\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>c</mi><mrow><mi>r</mi><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow><mo>=</mo><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">|C(h)|=|\\{c_{r,m}(h)\\}|=3\\times 4=12</annotation></semantics></math> CS outputs for each HS instance. This enables systematic comparison across retrieval methods and summarization strategies.\nTo ensure that generated CS is deployable in (online) real-world contexts, we restrict outputs to a maximum of two sentences. This reflects the communicative norms of social media platforms, where posts are typically 1&#8211;2 sentences long <cite class=\"ltx_cite ltx_citemacro_citep\">(&#350;ahinu&#231; and Toraman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib43\" title=\"\">2021</a>)</cite>. Prior work shows that overly verbose responses are less engaging and less effective in countering harmful narratives <cite class=\"ltx_cite ltx_citemacro_citep\">(Russo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib41\" title=\"\">2023</a>)</cite>. Concise and relatable CS has been repeatedly identified as key to user engagement and effectiveness <cite class=\"ltx_cite ltx_citemacro_citep\">(Bonaldi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib8\" title=\"\">2024</a>; Benesch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib4\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "comparison",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ Meta-Llama-3.1-8B-Instruct<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct</a></span></span></span>, Cohere&#8217;s Command-R-7B<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>We included CommandR, optimized for grounding outputs in retrieved context.</span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Cohere et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib17\" title=\"\">2025</a>)</cite>, Mistral-7B-Instruct-v0.3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\" title=\"\">https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3</a></span></span></span>, and gpt-4o-mini-2024-07-18. These models were selected as LLMs of similar size, balancing efficiency and accuracy, allowing comparison across open-weight and proprietary settings.</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experimented on the Multi-Target CONAN (MTCo) dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib9\" title=\"\">2022</a>)</cite>, which contains 5,003 HS/CS pairs in English covering multiple target groups, including people with disabilities, Jews, LGBT+ individuals, Muslims, migrants, people of color (POC), and women. Collected through a human-in-the-loop process, MTCo provides high-quality, contextually relevant CS, and serves as a first baseline for our experiments. Additionally, we compare our pipeline with the four instruction-tuned LLMs of comparable size without RAG. Finally, we compare our results with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>We cannot compare to <cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> due to lack of publicly available code and data, even upon request.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "mtco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S6.T2\" title=\"Table 2 &#8227; Human evaluation metrics. &#8227; 6 Metrics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports average results, that can be interpreted along three main dimensions: content quality, diversity, and safety.\n<span class=\"ltx_text ltx_font_bold\">Content Quality:</span> scores on BLEU, METEOR, and ROUGE-L remain relatively low, as expected for open-ended generation, while BERTScore values are consistently high (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.85), indicating strong semantic similarity to MT-Co references. Non-RAG outputs achieve slightly higher lexical and semantic overlap, suggesting that retrieval introduces content that, while factually richer, diverges from exact reference phrasing. RAG outputs show less words overlapping,\nlikely reflecting the inclusion of factual information from the knowledge base.\n<span class=\"ltx_text ltx_font_bold\">Diversity:</span> incorporating retrieval substantially improves lexical diversity and reduces repetition. Mistral with BGE-M3 shows the largest gains, while No-RAG models, particularly Mistral, exhibit high repetition and lower diversity. <span class=\"ltx_text ltx_font_bold\">Safety:</span> No-RAG configurations achieve the highest safety, with GPT leading (0.993). RAG outputs show slightly lower safety, likely due to occasional noise introduced from the retrieval process. Nevertheless, safety remains high overall, indicating that fact-grounded augmentation does not substantially compromise non-harmfulness.\n<span class=\"ltx_text ltx_font_bold\">Model-level Trends:</span> GPT consistently delivers the safest outputs and competitive BERTScore, though with lower lexical diversity than Mistral. Mistral excels in diversity with RAG but performs poorly in No-RAG due to high repetition. CommandR and LLaMA provide stable but moderate performance across metrics. Overall, RAG with BGE-M3 achieves the best balance between quality, diversity, and informativeness. All results differ significantly (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A4\" title=\"Appendix D Statistical Significance of Retrieval Effects &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> for more details).</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "llama",
                    "mtco",
                    "best",
                    "rag",
                    "gpt",
                    "mistral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate CS quality using <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span>, focusing on four dimensions: factuality, number of relevant facts, relevance to the HS, and specificity. This prompt (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>) guides pairwise comparisons between model outputs, emphasizing informative and targeted CS over surface-level similarity.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T3\" title=\"Table 3 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> results for RAG vs. No-RAG models across retrieval strategies. BGE-M3 yields the strongest results, followed by SentBERT and BM25, confirming the advantage of semantically rich retrieval. Among generators, GPT wins most comparisons, showing effective integration of retrieved content. Mistral also benefits notably from RAG, while CommandR performs less consistently.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T4\" title=\"Table 4 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the best RAG setups (LLM + BGE-M3) against human-written MT-Co CS. All RAG systems perform strongly, with GPT + BGE-M3 approaching human-level quality and Mistral + BGE-M3 remaining competitive, whereas CommandR trails slightly but still surpasses baselines.</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "showing",
                    "mtco",
                    "best",
                    "rag",
                    "gpt",
                    "number",
                    "mistral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations reveal clear trends: <span class=\"ltx_text ltx_font_bold\">(1)</span> RAG outputs are less repetitive, and more diverse\nthan No-RAG, reflecting the addition of factual content. Lexical and semantic similarity remains high, ensuring CS remains aligned with the original intent.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> Safety is only slightly reduced with RAG, likely due to occasional noisy content.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> RAG consistently outperforms No-RAG and MT-Co CS in both automatic and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "both",
                    "mtco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings demonstrate that RAG &#8212;particularly with semantically rich BGE-M3 embeddings&#8212; enhances CS quality, diversity, and factual grounding, while maintaining strong safety and overall effectiveness.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "rag"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted a human evaluation with 26 participants to assess the quality and effectiveness of the generated CS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Participants&#8217; age is between 18 and 50, there is a balance between genders, and their level of education spans high school diploma to PhD. Detailed statistics are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5\" title=\"Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span>. Our evaluation setup consisted of 10 HS examples, extracted randomly from MT-Co keeping the target distribution, with 9 CS candidates for each HS. We selected the CS from MT-Co and the CS generated by our four LLMs without the RAG pipeline, and the corresponding CS with RAG using BMG-M3 as retriever, since it has shown the strongest performance in the automatic and LLM-based metrics. Participants rated all 90 CS candidates along four dimensions, i.e., Relevance, Factuality, Cogency, and Correctness (1&#8211;3 scale), judged whether each CS was effective (Yes/No) and selected the best CS per HS. We collected a total of 2340 evaluations.</p>\n\n",
                "matched_terms": [
                    "best",
                    "effectiveness",
                    "rag",
                    "mtco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metric Scores.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S8.T5\" title=\"Table 5 &#8227; 8 Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports average scores for Relevance, Factuality, Cogency, and Correctness across CS methods. GPT RAG achieved the highest scores overall, particularly in Factuality (2.75) and Cogency (2.41). RAG-based methods generally outperformed their non-RAG counterparts, while MT-Co scored lowest across all metrics, indicating lower relevance, factual accuracy, and overall effectiveness.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "mtco",
                    "rag",
                    "methods",
                    "gpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Best CS.</span> RAG-based methods are chosen more frequently as best CS, implying that they are clearly preferred compared to their No RAG counter-parts, with GPT RAG receiving the highest number of votes (71). Complete statistics for each method are available in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A3\" title=\"Appendix C Additional Results from the Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "votes",
                    "best",
                    "rag",
                    "method",
                    "methods",
                    "gpt",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effectiveness.</span> Out of the 2340 evaluations collected, 1312 were marked as effective, corresponding to 56% of the cases. More specifically, Llama RAG was considered effective most frequently (with 197 votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effectiveness votes (48) (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A3\" title=\"Appendix C Additional Results from the Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for full results). This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. The non-RAG models and MTCo were less effective overall. This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline, suggesting that both model choice and augmentation strategy strongly influence performance.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "commandr",
                    "llama",
                    "votes",
                    "choice",
                    "mtco",
                    "rag",
                    "both",
                    "method",
                    "methods",
                    "gpt",
                    "received"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, RAG-based methods outperform non-RAG counterparts across all the four metrics, especially in Factuality, Cogency and Effectiveness, and are preferred by annotators in best-choice votes. The baseline method MT-Co consistently scores lowest across all metrics and effectiveness. Overall, <span class=\"ltx_text ltx_font_bold\">GPT RAG</span> is the best CS method across all HS and annotators. Results from the human evaluation are aligned with both automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "votes",
                    "mtco",
                    "best",
                    "rag",
                    "both",
                    "method",
                    "methods",
                    "gpt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To contextualize our results, we compared our RAG-based CS with publicly available samples from two recent studies. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> (Baseline 1) released 1,697 GPT-4o-generated CS responses from their RAG pipeline addressing MT-Co hate speech, while <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> (Baseline 2) submitted 400 CS samples to the COLING 2025 shared task using LLaMA-EUS-8B with retrieved knowledge. Since the samples were multilingual, we retained only English CS aligned with MT-Co, totaling 124 samples.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> produced 100 English CS, 24 HS instances overlapped with MT-CONAN and were retained with multiple CS variants.</span></span></span>\nFor fair comparison, we reproduced settings closest to the original models: GPT-4o-mini + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite>, and LLaMA-8B + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>. This alignment minimizes differences due to model size, isolating the effects of retrieval and pipeline design. Pairwise performance was then assessed with <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> (prompt available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>). It is fine-tuned to rate helpfulness, relevance, accuracy, and level of detail of CS responses, which already accounts for evidence and factuality in the original prompt. To additionally emphasize conciseness, and suitability for real-world deployment on social media, we limit the CS to a maximum of two sentences.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "comparison",
                    "mtco"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S9.T6\" title=\"Table 6 &#8227; 9 Comparison with competitors &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that our RAG-augmented CS consistently outperforms the baseline samples from both studies. GPT-4o-mini + BGE-M3 outperforms Baseline 1 in 962 out of 1,697 pairwise battles, while LLaMA-8B + BGE-M3 outperforms Baseline 2 in 114 out of 124 battles. These results demonstrate that our RAG pipeline produces CS that is not only factually richer but also concise and suitable for social media, aligning with the intended communicative goals of real-world deployment.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results indicate that while our models consistently outperform prior baselines, the improvement varies by target group. In particular, our LLaMA-based system demonstrates decisive superiority over Baseline 2 comparable setup, whereas the GPT-based comparison with Baseline 1 highlights more incremental but robust gains across diverse HS categories, highlighting the effectiveness of our RAG approach in improving both informativeness and practical usability of the CS.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "rag",
                    "both",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a RAG-based framework for automatic counter-speech generation. We systematically compared three retrieval methods and four LLMs for CS generation targeting height groups (women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, other), relying on a novel and unique knowledge base, built over three institutional sources. We conducted an extensive experimental evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, and a human evaluation. Our results show that our approach outperforms competitive approaches and standard baselines on both of them. Our experiments demonstrated the versatility and soundness of our framework for counter-speech generation to fight online abusive content.</p>\n\n",
                "matched_terms": [
                    "both",
                    "methods",
                    "counterspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study involves the use of hate speech examples for the development and evaluation of counter-speech generation systems. We acknowledge that the inclusion of HS content poses potential risks of exposure to harmful language and emotional distress for researchers and annotators. All individuals involved in data handling were informed of these risks and participated voluntarily, following institutional ethical guidelines.\nAlthough our goal is to promote positive and factual discourse, automatic CS generation can inadvertently reinforce biases, produce factually incorrect content, or convey unintended tones. To mitigate these risks, we rely on institutional sources (UN, EU, FRA) for retrieval, explicitly evaluate factuality and correctness, and use human oversight in all analyses. The system is presented for research purposes only and is not intended for unsupervised deployment.\nWe further recognize that the perceived effectiveness and appropriateness of CS depend on social and cultural context. Our methods and findings should therefore not be generalized without careful adaptation and ethical review. No personal or private data were used; all retrieved materials come from publicly available institutional sources.</p>\n\n",
                "matched_terms": [
                    "effectiveness",
                    "methods",
                    "counterspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to use <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> as a metric, we used the following prompts. The first one is used to compare RAG methods against No RAG methods, and against the MT-Co baseline. The second prompt is used to compare our RAG-based CS against those generated with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>); Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "mtco",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted non-parametric Friedman tests followed by Bonferroni-corrected Wilcoxon signed-rank tests to evaluate whether RAG retrieval strategies (BM25, Sentence-BERT, BGE) significantly affected model outputs compared to the No-RAG baseline for the per-sample automatic metrics (BLEU, METEOR, ROUGE-L, BERTScore), which allow direct pairwise comparison across systems. In contrast, diversity metrics such as Distinct-1, Distinct-2, and Repetition Rate are computed at the corpus level, producing a single value per model.\nBecause these measures do not yield per-sample scores and thus lack within-system variance, statistical testing is not applicable.\nFor all models, Friedman tests revealed significant overall effects (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), and pairwise comparisons confirmed that retrieval methods consistently induced statistically significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>) across quality and safety metrics.</p>\n\n",
                "matched_terms": [
                    "rag",
                    "comparison",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S6\" title=\"6 Metrics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we carried out a human evaluation. Participants were voluntarily recruited and could opt out from the study at any time. Precise instructions were given, highlighting the potential risks and distress of the study. We define the following metrics used for the human evaluation: Relevance, Factuality, Cogency, and Correctness use a Likert scale from 1 to 3 (with 3 being the best possible score), while Effectiveness, and Is the Best are binary dimensions to which the participants could reply \"Yes\" or \"No\". They are defined as follows:</p>\n\n",
                "matched_terms": [
                    "best",
                    "effectiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Refers to whether the counter-speech is the best one among the ones generated for the same HS. Select the CS that you prefer based on the scores you provided.</p>\n\n",
                "matched_terms": [
                    "best",
                    "counterspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, we provide different examples of counter-speech addressing the same hateful message from MT-CONAN. We show CS from GPT No RAG and GPT RAG with BGE-M3.</p>\n\n",
                "matched_terms": [
                    "gpt",
                    "rag",
                    "counterspeech"
                ]
            }
        ]
    },
    "A3.T8": {
        "caption": "Table 8: Statistical significance (Friedman and Bonferroni-corrected Wilcoxon tests) comparing No-RAG against retrieval-based setups for all metrics. Extremely small pp-values (<10300<10^{-300}) indicate strong evidence that retrieval methods significantly affect generation outcomes.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Metric</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Friedman <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m1\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">No-RAG vs BM25 <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m2\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">No-RAG vs SentBERT <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m3\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">No-RAG vs BGE <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m4\" intent=\":literal\"><semantics><mi>p</mi><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LLaMA</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">BLEU</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.0\\times 10^{-307}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m5\" intent=\":literal\"><semantics><mrow><mn>2.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>307</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2.0\\times 10^{-307}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1.1\\times 10^{-182}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m6\" intent=\":literal\"><semantics><mrow><mn>1.1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>182</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.1\\times 10^{-182}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.3\\times 10^{-144}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m7\" intent=\":literal\"><semantics><mrow><mn>2.3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>144</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2.3\\times 10^{-144}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"1.4\\times 10^{-132}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m8\" intent=\":literal\"><semantics><mrow><mn>1.4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>132</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.4\\times 10^{-132}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">METEOR</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"4.4\\times 10^{-39}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m9\" intent=\":literal\"><semantics><mrow><mn>4.4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>39</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4.4\\times 10^{-39}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"7.2\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m10\" intent=\":literal\"><semantics><mrow><mn>7.2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">7.2\\times 10^{-3}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"5.5\\times 10^{-11}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m11\" intent=\":literal\"><semantics><mrow><mn>5.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>11</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5.5\\times 10^{-11}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"9.0\\times 10^{-34}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m12\" intent=\":literal\"><semantics><mrow><mn>9.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>34</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">9.0\\times 10^{-34}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ROUGE-L</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.5\\times 10^{-33}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m13\" intent=\":literal\"><semantics><mrow><mn>1.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>33</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.5\\times 10^{-33}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"9.3\\times 10^{-37}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m14\" intent=\":literal\"><semantics><mrow><mn>9.3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>37</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">9.3\\times 10^{-37}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"7.3\\times 10^{-18}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m15\" intent=\":literal\"><semantics><mrow><mn>7.3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>18</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">7.3\\times 10^{-18}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"8.4\\times 10^{-9}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m16\" intent=\":literal\"><semantics><mrow><mn>8.4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>9</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">8.4\\times 10^{-9}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">BERTScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">F1</span></sub>\n</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m18\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m19\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"6.8\\times 10^{-276}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m20\" intent=\":literal\"><semantics><mrow><mn>6.8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>276</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6.8\\times 10^{-276}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.8\\times 10^{-257}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m21\" intent=\":literal\"><semantics><mrow><mn>1.8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>257</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.8\\times 10^{-257}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Safety</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.6\\times 10^{-175}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m22\" intent=\":literal\"><semantics><mrow><mn>3.6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>175</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.6\\times 10^{-175}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-79}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m23\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>79</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-79}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-134}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m24\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>134</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-134}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-173}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m25\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>173</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-173}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">CommandR</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">BLEU</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m26\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.5\\times 10^{-307}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m27\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>307</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2.5\\times 10^{-307}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"9.4\\times 10^{-220}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m28\" intent=\":literal\"><semantics><mrow><mn>9.4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>220</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">9.4\\times 10^{-220}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.5\\times 10^{-177}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m29\" intent=\":literal\"><semantics><mrow><mn>2.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>177</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2.5\\times 10^{-177}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">METEOR</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.2\\times 10^{-102}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m30\" intent=\":literal\"><semantics><mrow><mn>2.2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>102</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2.2\\times 10^{-102}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"4.5\\times 10^{-37}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m31\" intent=\":literal\"><semantics><mrow><mn>4.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>37</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4.5\\times 10^{-37}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.2\\times 10^{-90}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m32\" intent=\":literal\"><semantics><mrow><mn>1.2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>90</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.2\\times 10^{-90}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"8.8\\times 10^{-79}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m33\" intent=\":literal\"><semantics><mrow><mn>8.8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>79</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">8.8\\times 10^{-79}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ROUGE-L</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.5\\times 10^{-31}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m34\" intent=\":literal\"><semantics><mrow><mn>3.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>31</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.5\\times 10^{-31}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"4.9\\times 10^{-30}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m35\" intent=\":literal\"><semantics><mrow><mn>4.9</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>30</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4.9\\times 10^{-30}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.2\\times 10^{-7}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m36\" intent=\":literal\"><semantics><mrow><mn>3.2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>7</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.2\\times 10^{-7}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.5\\times 10^{-1}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m37\" intent=\":literal\"><semantics><mrow><mn>1.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.5\\times 10^{-1}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">BERTScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">F1</span></sub>\n</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m39\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m40\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m41\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m42\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Safety</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m43\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-185}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m44\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>185</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-185}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m45\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m46\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Mistral</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">BLEU</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m47\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"4.9\\times 10^{-243}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m48\" intent=\":literal\"><semantics><mrow><mn>4.9</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>243</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4.9\\times 10^{-243}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.1\\times 10^{-194}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m49\" intent=\":literal\"><semantics><mrow><mn>2.1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>194</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2.1\\times 10^{-194}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"6.6\\times 10^{-124}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m50\" intent=\":literal\"><semantics><mrow><mn>6.6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>124</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6.6\\times 10^{-124}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">METEOR</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"7.1\\times 10^{-18}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m51\" intent=\":literal\"><semantics><mrow><mn>7.1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>18</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">7.1\\times 10^{-18}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.8\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m52\" intent=\":literal\"><semantics><mrow><mn>1.8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.8\\times 10^{-4}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"5.4\\times 10^{-10}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m53\" intent=\":literal\"><semantics><mrow><mn>5.4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>10</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5.4\\times 10^{-10}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.3\\times 10^{-21}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m54\" intent=\":literal\"><semantics><mrow><mn>2.3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>21</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2.3\\times 10^{-21}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ROUGE-L</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.9\\times 10^{-58}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m55\" intent=\":literal\"><semantics><mrow><mn>2.9</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>58</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2.9\\times 10^{-58}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"6.5\\times 10^{-58}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m56\" intent=\":literal\"><semantics><mrow><mn>6.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>58</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6.5\\times 10^{-58}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.9\\times 10^{-37}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m57\" intent=\":literal\"><semantics><mrow><mn>2.9</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>37</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2.9\\times 10^{-37}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"5.4\\times 10^{-11}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m58\" intent=\":literal\"><semantics><mrow><mn>5.4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>11</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5.4\\times 10^{-11}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">BERTScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">F1</span></sub>\n</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m60\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m61\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m62\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"4.4\\times 10^{-127}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m63\" intent=\":literal\"><semantics><mrow><mn>4.4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>127</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4.4\\times 10^{-127}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Safety</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m64\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-217}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m65\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>217</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-217}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-251}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m66\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>251</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-251}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-281}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m67\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>281</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-281}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">GPT-4</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">BLEU</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m68\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m69\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"5.5\\times 10^{-299}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m70\" intent=\":literal\"><semantics><mrow><mn>5.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>299</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5.5\\times 10^{-299}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.6\\times 10^{-226}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m71\" intent=\":literal\"><semantics><mrow><mn>3.6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>226</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.6\\times 10^{-226}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">METEOR</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.9\\times 10^{-97}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m72\" intent=\":literal\"><semantics><mrow><mn>1.9</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>97</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.9\\times 10^{-97}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.6\\times 10^{-39}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m73\" intent=\":literal\"><semantics><mrow><mn>1.6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>39</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.6\\times 10^{-39}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.5\\times 10^{-63}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m74\" intent=\":literal\"><semantics><mrow><mn>1.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>63</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.5\\times 10^{-63}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.4\\times 10^{-92}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m75\" intent=\":literal\"><semantics><mrow><mn>3.4</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>92</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.4\\times 10^{-92}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ROUGE-L</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.1\\times 10^{-53}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m76\" intent=\":literal\"><semantics><mrow><mn>1.1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>53</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.1\\times 10^{-53}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.5\\times 10^{-51}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m77\" intent=\":literal\"><semantics><mrow><mn>1.5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>51</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.5\\times 10^{-51}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"4.0\\times 10^{-34}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m78\" intent=\":literal\"><semantics><mrow><mn>4.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>34</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">4.0\\times 10^{-34}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.6\\times 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m79\" intent=\":literal\"><semantics><mrow><mn>1.6</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.6\\times 10^{-6}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">BERTScore<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">F1</span></sub>\n</th>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m81\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m82\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m83\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"1.8\\times 10^{-205}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m84\" intent=\":literal\"><semantics><mrow><mn>1.8</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>205</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1.8\\times 10^{-205}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Safety</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m85\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"&lt;10^{-207}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m86\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>207</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-207}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m87\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"&lt;10^{-300}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T8.m88\" intent=\":literal\"><semantics><mrow><mi/><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>300</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">&lt;10^{-300}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "93103793times",
            "55101155times",
            "bge",
            "181025718times",
            "against",
            "3210732times",
            "88107988times",
            "1025110251",
            "wilcoxon",
            "10791079",
            "11105311times",
            "8410984times",
            "15105115times",
            "441012744times",
            "231014423times",
            "15103315times",
            "941022094times",
            "681027668times",
            "safety",
            "1810418times",
            "1610616times",
            "551029955times",
            "361017536times",
            "1017310173",
            "65105865times",
            "7210372times",
            "generation",
            "54101054times",
            "12109012times",
            "friedman",
            "setups",
            "44103944times",
            "29103729times",
            "16103916times",
            "retrievalbased",
            "221010222times",
            "49103049times",
            "commandr",
            "1510115times",
            "significance",
            "evidence",
            "norag",
            "19109719times",
            "1020710207",
            "mistral",
            "statistical",
            "1028110281",
            "201030720times",
            "sentbert",
            "111018211times",
            "90103490times",
            "181020518times",
            "23102123times",
            "strong",
            "251017725times",
            "small",
            "methods",
            "1018510185",
            "141013214times",
            "251030725times",
            "retrieval",
            "73101873times",
            "211019421times",
            "1021710217",
            "llama",
            "1030010300",
            "40103440times",
            "model",
            "bertscoref1",
            "all",
            "1013410134",
            "29105829times",
            "affect",
            "extremely",
            "significantly",
            "54101154times",
            "meteor",
            "bonferronicorrected",
            "45103745times",
            "361022636times",
            "rougel",
            "outcomes",
            "bm25",
            "ppvalues",
            "comparing",
            "indicate",
            "tests",
            "metrics",
            "gpt4",
            "metric",
            "15106315times",
            "491024349times",
            "34109234times",
            "661012466times",
            "71101871times",
            "35103135times",
            "bleu"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Counter-speech generation is at the core of many expert activities, such as fact-checking and hate speech, to counter harmful content. Yet, existing work treats counter-speech generation as pure text generation task, mainly based on Large Language Models or NGO experts. These approaches show severe drawbacks due to the limited reliability and coherence in the generated countering text, and in scalability, respectively. To close this gap, we introduce a novel framework to model counter-speech generation as knowledge-wise text generation process. Our framework integrates advanced Retrieval-Augmented Generation (RAG) pipelines to ensure the generation of trustworthy counter-speech for 8 main target groups identified in the hate speech literature, including women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. We built a knowledge base over the United Nations Digital Library, EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792 texts. We use the MultiTarget-CONAN dataset to empirically assess the quality of the generated counter-speech, both through standard metrics (i.e., JudgeLM) and a human evaluation. Results show that our framework outperforms standard LLM baselines and competitive approach, on both assessments. The resulting framework and the knowledge base pave the way for studying trustworthy and sound counter-speech generation, in hate speech and beyond. <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">Warning<span class=\"ltx_text ltx_font_medium\">: this paper contains explicit examples some readers may find offensive.</span></span></p>\n\n",
                "matched_terms": [
                    "generation",
                    "metrics",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We tackle this challenging task by proposing a novel knowledge-grounded framework for automatic counter-speech generation that integrates advanced retrieval-augmented generation (RAG) pipelines. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows two examples of CS, with and without RAG augmentation. Our contributions are fourfold: <span class=\"ltx_text ltx_font_bold\">(1)</span> we provide a systematic comparison of multiple retrievers and LLMs for CS generation, combining three retrieval methods with four language models; <span class=\"ltx_text ltx_font_bold\">(2)</span> we enforce concise, two-sentence outputs tailored for social media deployment, ensuring responses remain natural, relatable, and effective in real-world contexts; <span class=\"ltx_text ltx_font_bold\">(3)</span> we conduct a pairwise evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, adapted to balance factual grounding with conciseness and pragmatic suitability, together with an extensive human evaluation, showing that our framework steadily outperforms standard baselines and state-of-the-art competitors; and <span class=\"ltx_text ltx_font_bold\">(4)</span> we will release all generated CS along with their top-3 retrieved evidence sentences, offering a reusable resource for the research community<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code will be made available upon acceptance.</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "against",
                    "all",
                    "generation",
                    "evidence",
                    "methods",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automatic metrics (BLEU, ROUGE, BERTScore) correlate poorly with human judgments, prompting exploration of novelty- and repetition-based scores <cite class=\"ltx_cite ltx_citemacro_cite\">Wang and Wan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib49\" title=\"\">2018</a>); Bertoldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib6\" title=\"\">2013</a>)</cite>, and LLM-based frameworks (e.g., GPT-4, PandaLM, JudgeLM, UniEval) <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib55\" title=\"\">2023</a>); Zhong et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib54\" title=\"\">2022</a>)</cite>, which show improved reliability for multi-aspect CS assessment <cite class=\"ltx_cite ltx_citemacro_cite\">Jones et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib25\" title=\"\">2024</a>); Damo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib18\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "gpt4",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">RAG addresses a key limitation of prior approaches by grounding outputs in verifiable evidence, thereby enhancing factuality and persuasiveness in countering HS.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib15\" title=\"\">2021a</a>)</cite> proposed a retrieval-augmented pipeline that first generates queries from HS using keyword extraction, then employs BM25 <cite class=\"ltx_cite ltx_citemacro_cite\">Robertson and Zaragoza (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib39\" title=\"\">2009</a>)</cite> to retrieve relevant articles from Newsroom <cite class=\"ltx_cite ltx_citemacro_cite\">Grusky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib21\" title=\"\">2018</a>)</cite> and WikiText-103 <cite class=\"ltx_cite ltx_citemacro_cite\">Merity et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib32\" title=\"\">2016</a>)</cite>. From these, the most relevant sentences are selected using the ROUGE-L metric <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib29\" title=\"\">2004</a>)</cite>, before being passed to GPT-2 <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib37\" title=\"\">2019</a>)</cite> and XNLG <cite class=\"ltx_cite ltx_citemacro_cite\">Chi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib13\" title=\"\">2020</a>)</cite>.\nSimilarly, <cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib24\" title=\"\">2023</a>)</cite> introduced RAUCG, which retrieves counter-arguments from the ChangeMyView subreddit, selecting them based on stance consistency, semantic overlap, and a custom perplexity-based fitness function. The final generation step employs energy-based decoding to preserve factual knowledge while countering HS fluently.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib23\" title=\"\">2025</a>)</cite> proposed ReZG, a retrieval-augmented zero-shot approach that integrates multi-dimensional hierarchical retrieval with constrained decoding, enabling the generation of more specific CS for unseen HS targets. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> leverage both curated background knowledge and web search to improve factuality.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> evaluated reranker-based pipelines, showing that fine-grained retrieval significantly improves factuality and relevance.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "rougel",
                    "generation",
                    "evidence",
                    "significantly",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a first step, we built a comprehensive Knowledge Base (KB) designed to ensure maximal coverage of documents addressing social groups commonly targeted by hate speech. The goal of this KB is to gather all relevant materials &#8212; such as reports, resolutions, and legal texts &#8212; that provide evidence or context regarding these topics.\nSpecifically, we focused on the following 8 target groups: women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. These categories align with the classic targets in the literature, including the ones of our baseline MultiTarget-CONAN <cite class=\"ltx_cite ltx_citemacro_cite\">Chung et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib14\" title=\"\">2019</a>)</cite>.\nWe used GPT-based prompting to generate synonyms and semantically related keywords, ensuring that queries captured diverse terminology across cultural and policy contexts (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for the prompt). To guarantee relevance, we performed a keyword-based search so that retrieved knowledge was directly aligned with the hateful messages targeting these groups. To ensure reliability in the generated CS, we relied exclusively on institutional publicly available sources, i.e., the United Nations Digital Library, EUR-Lex, and the European Union Agency for Fundamental Rights (FRA). To construct our knowledge base, we follow three main steps: document retrieval, PDF-to-text conversion, and knowledge base integration.</p>\n\n",
                "matched_terms": [
                    "all",
                    "evidence",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Document Retrieval.</span> The <span class=\"ltx_text ltx_font_bold\">United Nations Digital Library</span> serves as the central repository for official UN documents on human rights, equality, and anti-discrimination. We developed a custom crawler using <span class=\"ltx_text ltx_font_typewriter\">requests</span> and <span class=\"ltx_text ltx_font_typewriter\">BeautifulSoup</span> to systematically combine three query dimensions&#8212;target keywords, document types (e.g., resolutions, treaties, NGO statements), and years (2000&#8211;2025). The crawler paginated through results, extracted and normalized documents in English, and organized downloads by target group. A metadata file recorded <span class=\"ltx_text ltx_font_typewriter\">id, fname, target, type, year, url</span>. Error handling covered duplicate checks, retries, and skipped completed downloads. At the European level, <span class=\"ltx_text ltx_font_bold\">EUR-Lex</span> provides access to EU law, treaties, and legal acts, while the <span class=\"ltx_text ltx_font_bold\">EU Agency for Fundamental Rights (FRA)</span> publishes reports on human rights within the EU. Using the same target keywords, we retrieved documents in English (2000&#8211;2025) from both sources, following the same metadata structure as the UN corpus. These sources complement the UN materials, forming a multi-level knowledge base&#8212;global and European&#8212;that ensures comprehensive and reliable grounding for CS generation.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Knowledge Base Integration.</span> All materials from UN, EU, and FRA were standardized into JSON with metadata (target group, document type, year, URL). The KB spans the years 2000&#8211;2025 and combines factual and policy-oriented resources, organized by target group and document type. To the best of our knowledge, this is the first large-scale, authoritative knowledge base specifically constructed for counter-speech generation.</p>\n\n",
                "matched_terms": [
                    "all",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our RAG-based framework integrates three key components: (i) paragraph retrieval, (ii) paragraph summarization, and (iii) counter-speech generation. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S4.F2\" title=\"Figure 2 &#8227; 4 Pipeline &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> provides an overview of the pipeline.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a HS message <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, we aim to retrieve a small set of relevant paragraphs from the KB (i.e., the most similar to the target HS).\nFor this, we employ three complementary retrieval models: BM25, Sentence-BERT (SBERT), and BGE-M3.\nLet the KB consist of paragraphs <math alttext=\"\\mathcal{P}=\\{p_{1},p_{2},\\ldots,p_{N}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mn>1</mn></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mi>N</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{P}=\\{p_{1},p_{2},\\ldots,p_{N}\\}</annotation></semantics></math>. For each retriever\n<math alttext=\"r\\in\\{\\text{BM25},\\text{SBERT},\\text{BGE-M3}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mtext>BM25</mtext><mo>,</mo><mtext>SBERT</mtext><mo>,</mo><mtext>BGE-M3</mtext><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in\\{\\text{BM25},\\text{SBERT},\\text{BGE-M3}\\}</annotation></semantics></math>, we compute a similarity score:\n<math alttext=\"s_{r}(h,p_{i})=\\text{sim}_{r}(\\phi(h),\\phi(p_{i}))\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>s</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo>,</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mtext>sim</mtext><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>,</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">s_{r}(h,p_{i})=\\text{sim}_{r}(\\phi(h),\\phi(p_{i}))</annotation></semantics></math>\nwhere <math alttext=\"\\phi(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(\\cdot)</annotation></semantics></math> is the embedding (or term-weight) function induced by the retriever. For BM25, the similarity score is computed based on TF-IDF, and document length normalization. For SBERT and BGE-M3, similarity is measured as the cosine similarity between dense vector embeddings of the HS and candidate paragraph, capturing semantic relatedness even in the absence of exact lexical overlap.\nFor each retriever, we obtain the top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> ranked paragraphs:\n<math alttext=\"R_{r}(h)=\\{p_{r,1},p_{r,2},\\ldots,p_{r,k}\\},\\quad k=3.\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mrow><mrow><mrow><msub><mi>R</mi><mi>r</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mn>2</mn></mrow></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>p</mi><mrow><mi>r</mi><mo>,</mo><mi>k</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow></mrow><mo rspace=\"1.167em\">,</mo><mrow><mi>k</mi><mo>=</mo><mn>3</mn></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">R_{r}(h)=\\{p_{r,1},p_{r,2},\\ldots,p_{r,k}\\},\\quad k=3.</annotation></semantics></math></p>\n\n",
                "matched_terms": [
                    "small",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although retrieval provides relevant context, LLMs have limited context windows, making direct use of full paragraphs infeasible. To address this, we summarize retrieved paragraphs before passing them to the generation stage.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Summaries are stored in CSV format along with their source IDs, ensuring alignment between retrieval and generation stages. The summarization prompts are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the final stage, the summaries are used as external knowledge to generate CS. For each HS message <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>, retriever <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math>, and model <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math>, the generation function is defined as:</p>\n\n",
                "matched_terms": [
                    "generation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since we use three retrievers and four LLMs, the system produces: <math alttext=\"|C(h)|=|\\{c_{r,m}(h)\\}|=3\\times 4=12\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mo stretchy=\"false\">{</mo><mrow><msub><mi>c</mi><mrow><mi>r</mi><mo>,</mo><mi>m</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>h</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>4</mn></mrow><mo>=</mo><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">|C(h)|=|\\{c_{r,m}(h)\\}|=3\\times 4=12</annotation></semantics></math> CS outputs for each HS instance. This enables systematic comparison across retrieval methods and summarization strategies.\nTo ensure that generated CS is deployable in (online) real-world contexts, we restrict outputs to a maximum of two sentences. This reflects the communicative norms of social media platforms, where posts are typically 1&#8211;2 sentences long <cite class=\"ltx_cite ltx_citemacro_citep\">(&#350;ahinu&#231; and Toraman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib43\" title=\"\">2021</a>)</cite>. Prior work shows that overly verbose responses are less engaging and less effective in countering harmful narratives <cite class=\"ltx_cite ltx_citemacro_citep\">(Russo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib41\" title=\"\">2023</a>)</cite>. Concise and relatable CS has been repeatedly identified as key to user engagement and effectiveness <cite class=\"ltx_cite ltx_citemacro_citep\">(Bonaldi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib8\" title=\"\">2024</a>; Benesch et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib4\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BM25</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Robertson and Zaragoza (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib39\" title=\"\">2009</a>)</cite> is a sparse lexical retriever based on TF-IDF with document length normalization. It remains an effective baseline for keyword-sensitive domains where exact term overlap is important. <span class=\"ltx_text ltx_font_bold\">Sentence-BERT (SBERT)</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib38\" title=\"\">2019</a>)</cite> encodes queries and passages into dense vector embeddings optimized for semantic similarity, with ranking performed via cosine similarity.\n<span class=\"ltx_text ltx_font_bold\">BGE-M3 (BAAI General Embeddings).</span> BGE-M3 <cite class=\"ltx_cite ltx_citemacro_cite\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib12\" title=\"\">2024</a>)</cite> is a recent dense embedding model trained for multilingual and multi-task semantic retrieval. These retrievers capture complementary dimensions of relevance, from exact term matching (BM25) to semantic similarity (SBERT, BGE-M3), ensuring robust retrieval across different types of hateful content and knowledge sources.</p>\n\n",
                "matched_terms": [
                    "model",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Reference-based Metrics.</span> To evaluate alignment with human-written CS in MT-Co, we report: <span class=\"ltx_text ltx_font_bold\">BLEU-4</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib35\" title=\"\">2002</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">ROUGE-L</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib29\" title=\"\">2004</a>)</cite>, <span class=\"ltx_text ltx_font_bold\">METEOR</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Banerjee and Lavie (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib2\" title=\"\">2005</a>)</cite>, and <span class=\"ltx_text ltx_font_bold\">BERTScore</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib52\" title=\"\">2019</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "meteor",
                    "metrics",
                    "rougel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Reference-less Metrics.</span> To complement reference-based evaluation, we assess intrinsic qualities of generated CS. <span class=\"ltx_text ltx_font_bold\">Distinct-1/2</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib28\" title=\"\">2015</a>)</cite> measures the proportion of unique unigrams and bigrams, reflecting lexical diversity. <span class=\"ltx_text ltx_font_bold\">Repetition Rate (RR)</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Cettolo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib11\" title=\"\">2014</a>)</cite> represents the fraction of repeated n-grams within a generation. <span class=\"ltx_text ltx_font_bold\">Safety</span>: the OpenAI&#8217;s content moderation API scores each output across categories of potential harm (e.g., hate, sexual, violence), with higher values indicate safer counter-speech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bonaldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib8\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "metrics",
                    "indicate",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S6.T2\" title=\"Table 2 &#8227; Human evaluation metrics. &#8227; 6 Metrics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports average results, that can be interpreted along three main dimensions: content quality, diversity, and safety.\n<span class=\"ltx_text ltx_font_bold\">Content Quality:</span> scores on BLEU, METEOR, and ROUGE-L remain relatively low, as expected for open-ended generation, while BERTScore values are consistently high (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S7.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>0.85), indicating strong semantic similarity to MT-Co references. Non-RAG outputs achieve slightly higher lexical and semantic overlap, suggesting that retrieval introduces content that, while factually richer, diverges from exact reference phrasing. RAG outputs show less words overlapping,\nlikely reflecting the inclusion of factual information from the knowledge base.\n<span class=\"ltx_text ltx_font_bold\">Diversity:</span> incorporating retrieval substantially improves lexical diversity and reduces repetition. Mistral with BGE-M3 shows the largest gains, while No-RAG models, particularly Mistral, exhibit high repetition and lower diversity. <span class=\"ltx_text ltx_font_bold\">Safety:</span> No-RAG configurations achieve the highest safety, with GPT leading (0.993). RAG outputs show slightly lower safety, likely due to occasional noise introduced from the retrieval process. Nevertheless, safety remains high overall, indicating that fact-grounded augmentation does not substantially compromise non-harmfulness.\n<span class=\"ltx_text ltx_font_bold\">Model-level Trends:</span> GPT consistently delivers the safest outputs and competitive BERTScore, though with lower lexical diversity than Mistral. Mistral excels in diversity with RAG but performs poorly in No-RAG due to high repetition. CommandR and LLaMA provide stable but moderate performance across metrics. Overall, RAG with BGE-M3 achieves the best balance between quality, diversity, and informativeness. All results differ significantly (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A4\" title=\"Appendix D Statistical Significance of Retrieval Effects &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> for more details).</p>\n\n",
                "matched_terms": [
                    "commandr",
                    "safety",
                    "rougel",
                    "strong",
                    "llama",
                    "metrics",
                    "meteor",
                    "all",
                    "generation",
                    "norag",
                    "significantly",
                    "mistral",
                    "retrieval",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate CS quality using <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span>, focusing on four dimensions: factuality, number of relevant facts, relevance to the HS, and specificity. This prompt (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>) guides pairwise comparisons between model outputs, emphasizing informative and targeted CS over surface-level similarity.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T3\" title=\"Table 3 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> reports <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> results for RAG vs. No-RAG models across retrieval strategies. BGE-M3 yields the strongest results, followed by SentBERT and BM25, confirming the advantage of semantically rich retrieval. Among generators, GPT wins most comparisons, showing effective integration of retrieved content. Mistral also benefits notably from RAG, while CommandR performs less consistently.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S7.T4\" title=\"Table 4 &#8227; Interpretation. &#8227; 7 Results &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> compares the best RAG setups (LLM + BGE-M3) against human-written MT-Co CS. All RAG systems perform strongly, with GPT + BGE-M3 approaching human-level quality and Mistral + BGE-M3 remaining competitive, whereas CommandR trails slightly but still surpasses baselines.</p>\n\n",
                "matched_terms": [
                    "sentbert",
                    "commandr",
                    "setups",
                    "against",
                    "model",
                    "all",
                    "norag",
                    "mistral",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations reveal clear trends: <span class=\"ltx_text ltx_font_bold\">(1)</span> RAG outputs are less repetitive, and more diverse\nthan No-RAG, reflecting the addition of factual content. Lexical and semantic similarity remains high, ensuring CS remains aligned with the original intent.\n<span class=\"ltx_text ltx_font_bold\">(2)</span> Safety is only slightly reduced with RAG, likely due to occasional noisy content.\n<span class=\"ltx_text ltx_font_bold\">(3)</span> RAG consistently outperforms No-RAG and MT-Co CS in both automatic and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "norag",
                    "metrics",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings demonstrate that RAG &#8212;particularly with semantically rich BGE-M3 embeddings&#8212; enhances CS quality, diversity, and factual grounding, while maintaining strong safety and overall effectiveness.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "safety"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted a human evaluation with 26 participants to assess the quality and effectiveness of the generated CS<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Participants&#8217; age is between 18 and 50, there is a balance between genders, and their level of education spans high school diploma to PhD. Detailed statistics are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A5\" title=\"Appendix E Participants&#8217; Demographic Characteristics &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</span></span></span>. Our evaluation setup consisted of 10 HS examples, extracted randomly from MT-Co keeping the target distribution, with 9 CS candidates for each HS. We selected the CS from MT-Co and the CS generated by our four LLMs without the RAG pipeline, and the corresponding CS with RAG using BMG-M3 as retriever, since it has shown the strongest performance in the automatic and LLM-based metrics. Participants rated all 90 CS candidates along four dimensions, i.e., Relevance, Factuality, Cogency, and Correctness (1&#8211;3 scale), judged whether each CS was effective (Yes/No) and selected the best CS per HS. We collected a total of 2340 evaluations.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metric Scores.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S8.T5\" title=\"Table 5 &#8227; 8 Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> reports average scores for Relevance, Factuality, Cogency, and Correctness across CS methods. GPT RAG achieved the highest scores overall, particularly in Factuality (2.75) and Cogency (2.41). RAG-based methods generally outperformed their non-RAG counterparts, while MT-Co scored lowest across all metrics, indicating lower relevance, factual accuracy, and overall effectiveness.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metric",
                    "metrics",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effectiveness.</span> Out of the 2340 evaluations collected, 1312 were marked as effective, corresponding to 56% of the cases. More specifically, Llama RAG was considered effective most frequently (with 197 votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effectiveness votes (48) (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A3\" title=\"Appendix C Additional Results from the Human Evaluation &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">C</span></a> for full results). This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. The non-RAG models and MTCo were less effective overall. This indicates that the RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline, suggesting that both model choice and augmentation strategy strongly influence performance.</p>\n\n",
                "matched_terms": [
                    "llama",
                    "model",
                    "commandr",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conclusion, RAG-based methods outperform non-RAG counterparts across all the four metrics, especially in Factuality, Cogency and Effectiveness, and are preferred by annotators in best-choice votes. The baseline method MT-Co consistently scores lowest across all metrics and effectiveness. Overall, <span class=\"ltx_text ltx_font_bold\">GPT RAG</span> is the best CS method across all HS and annotators. Results from the human evaluation are aligned with both automatic metrics and <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> evaluations.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To contextualize our results, we compared our RAG-based CS with publicly available samples from two recent studies. <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> (Baseline 1) released 1,697 GPT-4o-generated CS responses from their RAG pipeline addressing MT-Co hate speech, while <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> (Baseline 2) submitted 400 CS samples to the COLING 2025 shared task using LLaMA-EUS-8B with retrieved knowledge. Since the samples were multilingual, we retained only English CS aligned with MT-Co, totaling 124 samples.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite> produced 100 English CS, 24 HS instances overlapped with MT-CONAN and were retained with multiple CS variants.</span></span></span>\nFor fair comparison, we reproduced settings closest to the original models: GPT-4o-mini + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite>, and LLaMA-8B + BGE-M3 for <cite class=\"ltx_cite ltx_citemacro_citet\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>. This alignment minimizes differences due to model size, isolating the effects of retrieval and pipeline design. Pairwise performance was then assessed with <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> (prompt available in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2\" title=\"Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>). It is fine-tuned to rate helpfulness, relevance, accuracy, and level of detail of CS responses, which already accounts for evidence and factuality in the original prompt. To additionally emphasize conciseness, and suitability for real-world deployment on social media, we limit the CS to a maximum of two sentences.</p>\n\n",
                "matched_terms": [
                    "model",
                    "evidence",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#S9.T6\" title=\"Table 6 &#8227; 9 Comparison with competitors &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> also reports the per-target breakdown of our pairwise comparison.\nAgainst Baseline 1, our system achieves overall consistent improvements. Gains are observed across all HS targets, with particularly strong performance for <span class=\"ltx_text ltx_font_italic\">DISABLED</span> (69.4%), <span class=\"ltx_text ltx_font_italic\">LGBT+</span> (64.0%), and <span class=\"ltx_text ltx_font_italic\">WOMEN</span> (61.2%). For larger target categories such as <span class=\"ltx_text ltx_font_italic\">MUSLIMS</span> and <span class=\"ltx_text ltx_font_italic\">MIGRANTS</span>, our model still maintains a clear advantage with 55.2% and 56.8% wins respectively, despite the higher difficulty and variability in these groups. The most balanced outcome is seen for <span class=\"ltx_text ltx_font_italic\">JEWS</span>, where results are split evenly (50%). Against Baseline 2, our system achieves consistently strong results across all targets, outperforming it in 93% of cases. The largest margins are observed for <span class=\"ltx_text ltx_font_italic\">MIGRANTS</span> (100% wins) and <span class=\"ltx_text ltx_font_italic\">JEWS</span> (95.7%).</p>\n\n",
                "matched_terms": [
                    "against",
                    "model",
                    "all",
                    "strong"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we propose a RAG-based framework for automatic counter-speech generation. We systematically compared three retrieval methods and four LLMs for CS generation targeting height groups (women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, other), relying on a novel and unique knowledge base, built over three institutional sources. We conducted an extensive experimental evaluation against existing state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>); Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>)</cite> using JudgeLM, and a human evaluation. Our results show that our approach outperforms competitive approaches and standard baselines on both of them. Our experiments demonstrated the versatility and soundness of our framework for counter-speech generation to fight online abusive content.</p>\n\n",
                "matched_terms": [
                    "against",
                    "generation",
                    "retrieval",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, our retrieval process operates at the paragraph level, meaning documents in the knowledge base are split into shorter segments rather than used in full. This improves efficiency but may potentially fragment contextual information, omitting relevant background or nuance. Moreover, we restrict the retrieved context to the top-<math alttext=\"k=3\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">k=3</annotation></semantics></math> most similar paragraphs to control input length and maintain conciseness in generation. Although this design balances informativeness and computational efficiency, varying <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"Sx1.p1.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> could influence the factual richness and diversity of the generated counter-speech.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, despite employing strong retrievers (BM25, SBERT, and BGE-M3), retrieval quality depends on the coverage and relevance of the knowledge base. Gaps or biases in external sources can propagate into the generated responses, particularly for emerging or culturally specific hate topics, even if we mitigated this by choosing recognized authoritative sources.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "retrieval",
                    "bm25"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study involves the use of hate speech examples for the development and evaluation of counter-speech generation systems. We acknowledge that the inclusion of HS content poses potential risks of exposure to harmful language and emotional distress for researchers and annotators. All individuals involved in data handling were informed of these risks and participated voluntarily, following institutional ethical guidelines.\nAlthough our goal is to promote positive and factual discourse, automatic CS generation can inadvertently reinforce biases, produce factually incorrect content, or convey unintended tones. To mitigate these risks, we rely on institutional sources (UN, EU, FRA) for retrieval, explicitly evaluate factuality and correctness, and use human oversight in all analyses. The system is presented for research purposes only and is not intended for unsupervised deployment.\nWe further recognize that the perceived effectiveness and appropriateness of CS depend on social and cultural context. Our methods and findings should therefore not be generalized without careful adaptation and ethical review. No personal or private data were used; all retrieved materials come from publicly available institutional sources.</p>\n\n",
                "matched_terms": [
                    "all",
                    "generation",
                    "retrieval",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Disabled</span>: Disability rights, Accessibility, Social model of disability, disability, disabled, down syndrome, autism, mental disability, physical disability, neurodiversity, ableism, inclusive design, Discrimination against people with disabilities</p>\n\n",
                "matched_terms": [
                    "against",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we list all the prompts used in our experiments. All summarization and generation experiments were conducted on an A100 GPU, with the following parameters: <span class=\"ltx_text ltx_font_typewriter\">max_new_tokens</span>=150, and <span class=\"ltx_text ltx_font_typewriter\">temperature</span>=0.5. For GPT-based models the cost was <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"A2.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math> 10$.</p>\n\n",
                "matched_terms": [
                    "all",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Step 3 of our pipeline (CS generation), we prompt all LLMs with the following:</p>\n\n",
                "matched_terms": [
                    "all",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In order to use <span class=\"ltx_text ltx_font_smallcaps\">JudgeLM</span> as a metric, we used the following prompts. The first one is used to compare RAG methods against No RAG methods, and against the MT-Co baseline. The second prompt is used to compare our RAG-based CS against those generated with competitive approaches&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wilk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib50\" title=\"\">2025</a>); Russo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#bib.bib40\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "against",
                    "metric",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.12316v1#A2.T7\" title=\"Table 7 &#8227; Appendix B Prompts used &#8227; Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the total number of times each CS method was rated effective, and how many times it has been selected as the best one across all HS and annotators. RAG-based methods are clearly preferred compared to their No RAG counter-parts. Concerning effectiveness, Llama RAG was considered effective most frequently (197 &#8220;Yes&#8221; votes), followed closely by GPT RAG (191) and CommandR RAG (172). The baseline method MTCo received the fewest effective votes (48). This indicates that RAG-based methods generally produced more effective CS than their non-RAG counterparts and the baseline. Furthermore, RAG methods are selected more often as best ones compared to their No RAG counterparts, with GPT RAG and Llama RAG achieving the best results.</p>\n\n",
                "matched_terms": [
                    "llama",
                    "all",
                    "commandr",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted non-parametric Friedman tests followed by Bonferroni-corrected Wilcoxon signed-rank tests to evaluate whether RAG retrieval strategies (BM25, Sentence-BERT, BGE) significantly affected model outputs compared to the No-RAG baseline for the per-sample automatic metrics (BLEU, METEOR, ROUGE-L, BERTScore), which allow direct pairwise comparison across systems. In contrast, diversity metrics such as Distinct-1, Distinct-2, and Repetition Rate are computed at the corpus level, producing a single value per model.\nBecause these measures do not yield per-sample scores and thus lack within-system variance, statistical testing is not applicable.\nFor all models, Friedman tests revealed significant overall effects (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), and pairwise comparisons confirmed that retrieval methods consistently induced statistically significant differences (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>) across quality and safety metrics.</p>\n\n",
                "matched_terms": [
                    "friedman",
                    "safety",
                    "rougel",
                    "tests",
                    "model",
                    "methods",
                    "metrics",
                    "wilcoxon",
                    "meteor",
                    "all",
                    "bonferronicorrected",
                    "norag",
                    "bge",
                    "significantly",
                    "statistical",
                    "retrieval",
                    "bm25",
                    "bleu"
                ]
            }
        ]
    }
}