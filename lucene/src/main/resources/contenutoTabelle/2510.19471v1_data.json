{
    "S6.T11": {
        "source_file": "Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition",
        "caption": "Table 11: Estimated average walltime of the decoding algorithms on the LibriSpeech dataset with the Whisper-large-v3 model. Note that the walltime includes the time for logging and sending the generated hypotheses to a cloud server for record, which adds to the overall time. Thus, the reported time does not reflect the performance of optimized implementations and should solely be considered as a reference.",
        "body": "Method\nWalltime (seconds)\nWER\n\n\n\n\nBeam (B=1B=1)\n0.88\n0.042\n\n\nBeam (B=5B=5)\n1.54\n0.042\n\n\nBeam (B=20B=20)\n1.56\n0.042\n\n\nMBR (N=4N=4)\n2.47\n0.035\n\n\nMBR (N=8N=8)\n3.44\n0.035\n\n\nMBR (N=16N=16)\n7.97\n0.034\n\n\nMBR (N=32N=32)\n17.89\n0.032\n\n\nMBR (N=64N=64)\n30.18\n0.033",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Walltime (seconds)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T11.m1\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.042</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T11.m2\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center\">1.54</td>\n<td class=\"ltx_td ltx_align_center\">0.042</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T11.m3\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center\">1.56</td>\n<td class=\"ltx_td ltx_align_center\">0.042</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MBR (<math alttext=\"N=4\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T11.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">N=4</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.035</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=8\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T11.m5\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">N=8</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center\">3.44</td>\n<td class=\"ltx_td ltx_align_center\">0.035</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=16\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T11.m6\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">N=16</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center\">7.97</td>\n<td class=\"ltx_td ltx_align_center\">0.034</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=32\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T11.m7\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">N=32</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center\">17.89</td>\n<td class=\"ltx_td ltx_align_center\">0.032</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T11.m8\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">30.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.033</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "b1b1",
            "wer",
            "reported",
            "cloud",
            "n16n16",
            "includes",
            "beam",
            "sending",
            "time",
            "solely",
            "reference",
            "n32n32",
            "implementations",
            "which",
            "b5b5",
            "walltime",
            "note",
            "seconds",
            "mbr",
            "adds",
            "overall",
            "optimized",
            "hypotheses",
            "method",
            "record",
            "performance",
            "n64n64",
            "estimated",
            "decoding",
            "reflect",
            "average",
            "thus",
            "n8n8",
            "dataset",
            "librispeech",
            "logging",
            "generated",
            "does",
            "b20b20",
            "server",
            "n4n4",
            "considered",
            "whisperlargev3",
            "algorithms",
            "model",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S6.T11\" title=\"Table 11 &#8227; 6 Limitations &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the average walltime on the LibriSpeech dataset with the whisper-large-v3 model.\nNote that because the experiment is not conducted to evaluate the walltime of the decoding algorithms, our codebase is not optimized to reduce the walltime.\nFor example, the reported values include the time for logging and sending the generated hypotheses to a cloud server, which adds to the overall time.\nAlso note that the walltime also depends on the choice of the utility function. Currently, computing the BLEU scores on CPU is taking the majority of the computation time. We find that using SentBERT as the utility function is much faster than using BLEU, as SentBERT runs on a GPU in parallel and does not require CPU/GPU data transfer.\nThus, the reported time does not reflect the performance of optimized implementations and should solely be considered as a reference.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent work has shown that sample-based Minimum Bayes Risk (MBR) decoding outperforms beam search in text-to-text generation tasks, such as machine translation, text summarization, and image captioning.\nOn the other hand, beam search is the current practice for speech-to-text tasks such as automatic speech recognition (ASR) and Speech Translation (ST).\nGiven that MBR decoding is effective in text-to-text generation tasks, it is reasonable to expect it to also be effective for speech-to-text tasks.\nIn this paper, we evaluate MBR decoding for ASR and ST tasks on English and Japanese using Whisper and its derivative models.\nWe observe that the accuracy of MBR decoding outperforms that of beam search in most of the experimental settings we have evaluated.\nThe results show that MBR decoding is a promising method for offline ASR and ST tasks that require high accuracy.\nThe code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/CyberAgentAILab/mbr-for-asr\" title=\"\">https://github.com/CyberAgentAILab/mbr-for-asr</a>.</p>\n\n",
                "matched_terms": [
                    "beam",
                    "method",
                    "mbr",
                    "decoding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Decoding algorithms play an important role in determining the final output quality of ASR systems. One of the common approaches, beam search, incrementally explores the most probable hypotheses to approximate the maximum-a-posteriori (MAP) solution.</p>\n\n",
                "matched_terms": [
                    "hypotheses",
                    "beam",
                    "algorithms",
                    "decoding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While effective and efficient, beam search is known to suffer from several degeneration issues in text-to-text generation tasks such as machine translation <cite class=\"ltx_cite ltx_citemacro_cite\">Holtzman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib28\" title=\"\">2020</a>); Eikema and Aziz (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib17\" title=\"\">2020</a>)</cite>.\n<span class=\"ltx_text ltx_font_bold\">M</span>inimum Bayes risk (MBR) decoding offers a promising alternative by directly optimizing for the expected utility of the output <cite class=\"ltx_cite ltx_citemacro_cite\">Goel and Byrne (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib23\" title=\"\">2000</a>); Kumar and Byrne (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib44\" title=\"\">2004</a>)</cite>. Rather than selecting the single most probable sequence, MBR considers multiple candidate hypotheses and chooses the one that minimizes the expected loss (or maximizes utility) when compared against other likely outputs <cite class=\"ltx_cite ltx_citemacro_cite\">Bickel and Doksum (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib8\" title=\"\">2015</a>)</cite>.\nThis approach has shown remarkable success in text-to-text tasks such as machine translation, summarization, and captioning <cite class=\"ltx_cite ltx_citemacro_cite\">Eikema and Aziz (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib18\" title=\"\">2022</a>); Suzgun et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib76\" title=\"\">2023</a>); Jinnai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib35\" title=\"\">2024</a>); Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib87\" title=\"\">2025</a>)</cite>, consistently outperforming beam search across diverse evaluation metrics.</p>\n\n",
                "matched_terms": [
                    "hypotheses",
                    "beam",
                    "decoding",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While MBR decoding is a well-established method in text-to-text tasks, its application to speech-to-text tasks has not been thoroughly investigated <cite class=\"ltx_cite ltx_citemacro_cite\">Prabhavalkar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib63\" title=\"\">2024</a>)</cite>.\nFor example, MBR decoding has been applied to the spoken language translation in the recent IWSLT shared tasks <cite class=\"ltx_cite ltx_citemacro_cite\">Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib2\" title=\"\">2024</a>); Abdulmumin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib1\" title=\"\">2025</a>)</cite>, but it is used for the machine translation modules rather than the ASR modules of the cascaded systems <cite class=\"ltx_cite ltx_citemacro_cite\">Yan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib91\" title=\"\">2024</a>); Ben&#160;Kheder et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib7\" title=\"\">2024</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib47\" title=\"\">2024</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib84\" title=\"\">2025</a>); Romney&#160;Robinson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib68\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "method",
                    "mbr",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given that the method is designed to improve the decoding accuracy of probabilistic models in general <cite class=\"ltx_cite ltx_citemacro_cite\">Ichihara et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib32\" title=\"\">2025a</a>)</cite>, it is reasonable to expect it to also improve the accuracy of ASR modules.\nThe absence of comprehensive studies on MBR decoding for contemporary ASR systems represents a significant gap in the literature. Given MBR&#8217;s empirical successes in text-to-text tasks and theoretical advantages, a systematic evaluation of its potential for speech recognition is valuable.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "method",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To this end, we present a comprehensive evaluation of sample-based MBR decoding for both offline ASR and Speech Translation (ST) tasks (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nOur experiments span multiple languages, with a focus on English and Japanese.\nWe use diverse datasets, multiple models based on Whisper, and with varying levels of synthesized noise added.\nMBR decoding consistently outperforms beam search across these dimensions, often by substantial margins. Remarkably, these improvements emerge with as few as 4-8 samples, suggesting that MBR can be practically implemented in scenarios where latency requirements are not stringent.</p>\n\n",
                "matched_terms": [
                    "beam",
                    "decoding",
                    "mbr",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our findings have significant implications for high-accuracy ASR applications where transcription quality takes precedence over real-time processing. While the computational overhead of MBR makes it less suitable for real-time applications, its consistent accuracy improvements make it an attractive option for offline speech-to-text systems.\nThis work thus reestablishes MBR decoding as a valuable technique in the modern neural ASR toolkit.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "thus",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first formally define the text generation problem and then describe MBR decoding.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given that <math alttext=\"\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Y}</annotation></semantics></math> is typically very large in text generation tasks, it is often infeasible to enumerate all possible output sequences in <math alttext=\"\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Y}</annotation></semantics></math>.\nThus, local optimal search methods such as beam search are used to approximate the MAP estimate as the language models are typically modeled by a autoregressive models <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib80\" title=\"\">2017</a>)</cite>.\nHowever, MAP decoding, including beam search, is known to generate undesirable outputs, such as an empty sequence, a sequence with repeated tokens, or low-quality text <cite class=\"ltx_cite ltx_citemacro_cite\">Wiseman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib85\" title=\"\">2017</a>); Holtzman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib28\" title=\"\">2020</a>); Eikema and Aziz (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib17\" title=\"\">2020</a>)</cite>.\nThus, alternative decoding algorithms have been investigated to improve the quality of the generated text.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "generated",
                    "thus",
                    "beam",
                    "algorithms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MBR decoding works by sampling multiple hypotheses from the model and selecting the one that maximizes the expected utility compared to the rest of the hypotheses <cite class=\"ltx_cite ltx_citemacro_cite\">Goel and Byrne (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib23\" title=\"\">2000</a>); Kumar and Byrne (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib44\" title=\"\">2004</a>); Eikema and Aziz (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib18\" title=\"\">2022</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "hypotheses",
                    "model",
                    "decoding",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math> is the set of hypotheses sampled from the model, <math alttext=\"N=|H|\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><mi>H</mi><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">N=|H|</annotation></semantics></math> is the number of hypotheses, and <math alttext=\"u(y,y^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>y</mi><mo>,</mo><msup><mi>y</mi><mo>&#8242;</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">u(y,y^{\\prime})</annotation></semantics></math> is a utility function that measures the quality of hypothesis <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> against reference <math alttext=\"y^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><msup><mi>y</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">y^{\\prime}</annotation></semantics></math>.\nIntuitively, MBR decoding selects the hypothesis that lies at the <span class=\"ltx_text ltx_font_italic\">center</span> of the sampled hypotheses, where the <span class=\"ltx_text ltx_font_italic\">distance</span> between two hypotheses is inversely related to their utility (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S2.F2\" title=\"Figure 2 &#8227; 2.2 Minimum Bayes Risk (MBR) Decoding &#8227; 2 Background &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>The utility functions used in MBR decoding are often not symmetric and may not satisfy the triangle inequality, so they are not proper distance functions. Nevertheless, the intuition still holds in many practical cases.</span></span></span>\nWhereas MAP decoding selects the sequence with the highest probability in the discrete hypothesis space, MBR decoding selects the one that lies near the middle of the continuous space defined by the utility function.\nThe utility function implicitly defines a continuous space over the hypotheses by quantifying their pairwise similarities.</p>\n\n",
                "matched_terms": [
                    "hypotheses",
                    "decoding",
                    "mbr",
                    "reference",
                    "model",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies have shown that the way hypotheses <math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math> are sampled is crucial for MBR decoding performance <cite class=\"ltx_cite ltx_citemacro_cite\">Eikema and Aziz (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib18\" title=\"\">2022</a>); Suzgun et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib76\" title=\"\">2023</a>); Jinnai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib35\" title=\"\">2024</a>); Ohashi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib58\" title=\"\">2024</a>)</cite>.\nOriginally, <cite class=\"ltx_cite ltx_citemacro_citet\">Goel and Byrne (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib23\" title=\"\">2000</a>)</cite> proposed MBR decoding for ASR using beam search to generate <math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math>, and this was later applied to machine translation <cite class=\"ltx_cite ltx_citemacro_cite\">Kumar and Byrne (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib44\" title=\"\">2004</a>)</cite>.\nHowever, recent work has found that using unbiased samples drawn from the model is more effective than using beam search for generating <math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_cite\">Eikema and Aziz (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib18\" title=\"\">2022</a>)</cite>.\nOther studies have also reported that probabilistic sampling methods, such as ancestral sampling, nucleus sampling <cite class=\"ltx_cite ltx_citemacro_cite\">Holtzman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib28\" title=\"\">2020</a>)</cite>, and epsilon sampling <cite class=\"ltx_cite ltx_citemacro_cite\">Hewitt et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib27\" title=\"\">2022</a>); Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib20\" title=\"\">2023</a>)</cite>, work better than beam search <cite class=\"ltx_cite ltx_citemacro_cite\">Eikema and Aziz (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib18\" title=\"\">2022</a>); Ohashi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib58\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "hypotheses",
                    "performance",
                    "decoding",
                    "reported",
                    "mbr",
                    "beam",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another advantage of MBR decoding is that it has theoretical guarantees <cite class=\"ltx_cite ltx_citemacro_cite\">Ichihara et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib32\" title=\"\">2025a</a>)</cite>. Under mild assumptions, the expected utility of the output chosen by MBR decoding improves as the number of sampled hypotheses increases, with a rate of <math alttext=\"O(\\frac{1}{\\sqrt{N}})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mfrac><mn>1</mn><msqrt><mi>N</mi></msqrt></mfrac><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(\\frac{1}{\\sqrt{N}})</annotation></semantics></math>.\nThis result is consistent with empirical findings showing that larger sample sizes lead to <cite class=\"ltx_cite ltx_citemacro_cite\">Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib20\" title=\"\">2023</a>)</cite>.\nIn contrast, beam search lacks non-vacuous theoretical guarantees regarding output quality.</p>\n\n",
                "matched_terms": [
                    "hypotheses",
                    "beam",
                    "decoding",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The drawback of MBR decoding is its computational cost.\nThe complexity is <math alttext=\"O(UN^{2}+GN)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m1\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>U</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>N</mi><mn>2</mn></msup></mrow><mo>+</mo><mrow><mi>G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(UN^{2}+GN)</annotation></semantics></math>, where <math alttext=\"U\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m2\" intent=\":literal\"><semantics><mi>U</mi><annotation encoding=\"application/x-tex\">U</annotation></semantics></math> is the cost of computing the utility function and <math alttext=\"G\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m3\" intent=\":literal\"><semantics><mi>G</mi><annotation encoding=\"application/x-tex\">G</annotation></semantics></math> is the cost of generating a hypothesis <cite class=\"ltx_cite ltx_citemacro_cite\">Eikema and Aziz (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib18\" title=\"\">2022</a>)</cite>.\nThere are faster algorithms <cite class=\"ltx_cite ltx_citemacro_cite\">Cheng and Vlachos (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib12\" title=\"\">2023</a>); Deguchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib16\" title=\"\">2024</a>); Trabelsi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib78\" title=\"\">2024</a>)</cite> that reduce the cost to <math alttext=\"O(UN\\log N+GN)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m4\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><mi>U</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi><mo lspace=\"0.167em\" rspace=\"0em\">&#8203;</mo><mrow><mi>log</mi><mo lspace=\"0.167em\">&#8289;</mo><mi>N</mi></mrow></mrow><mo>+</mo><mrow><mi>G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>N</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(UN\\log N+GN)</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_cite\">Jinnai and Ariu (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib34\" title=\"\">2024</a>)</cite>, but this is still much higher than beam search, which is <math alttext=\"O(GB)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m5\" intent=\":literal\"><semantics><mrow><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>G</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>B</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">O(GB)</annotation></semantics></math>, where <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m6\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> is the beam width.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "which",
                    "mbr",
                    "beam",
                    "algorithms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In summary, MBR decoding is a strong alternative to beam search for text generation tasks, and it consistently performs better in many settings.\nIt is not only effective in practice but also has theoretical support.\nIts main weakness is its computational cost, which makes it less suitable for real-time use.\nThere has been little evaluation of MBR decoding for speech-to-text tasks, which this paper aims to address.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "which",
                    "mbr",
                    "beam",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MBR decoding is an instance of a <span class=\"ltx_text ltx_font_bold\">reranking algorithm</span> or best-of-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> sampling that re-evaluates the candidate outputs, selecting the best output according to some criteria <cite class=\"ltx_cite ltx_citemacro_cite\">Morbini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib52\" title=\"\">2012</a>); Chiu and Chen (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib14\" title=\"\">2021</a>); Xu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib90\" title=\"\">2022</a>); Nakano et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib54\" title=\"\">2022</a>); Ichihara et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib33\" title=\"\">2025b</a>)</cite>.\nSeveral reranking methods have been proposed for speech recognition using quality estimation <cite class=\"ltx_cite ltx_citemacro_cite\">Negri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib56\" title=\"\">2014</a>); Ng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib57\" title=\"\">2015</a>); Ali and Renals (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib4\" title=\"\">2018</a>); Yuksel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib94\" title=\"\">2023</a>); Waheed et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib81\" title=\"\">2025</a>)</cite>, perplexity <cite class=\"ltx_cite ltx_citemacro_cite\">Salazar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib69\" title=\"\">2020</a>)</cite>, deliberation models <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib30\" title=\"\">2020</a>); Xu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib90\" title=\"\">2022</a>)</cite>, LLMs <cite class=\"ltx_cite ltx_citemacro_cite\">Mengxi Nie and Ming Yan and Caixia Gong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib51\" title=\"\">2022</a>); Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib31\" title=\"\">2024</a>); Tur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib79\" title=\"\">2024</a>)</cite>, and speech-text foundational models <cite class=\"ltx_cite ltx_citemacro_cite\">Shivakumar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib72\" title=\"\">2025</a>)</cite>.\nThe advantage of MBR decoding compared to these approaches is that it does not require any additional training, making it easy to apply to new systems and languages.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "does",
                    "mbr",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Model fusion is another approach to improve the accuracy of ASR and ST systems by combining multiple models <cite class=\"ltx_cite ltx_citemacro_cite\">Parikh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib61\" title=\"\">2024</a>)</cite>.\nThis approach has been shown to be effective in various settings, such as combining acoustic models and language models <cite class=\"ltx_cite ltx_citemacro_cite\">Lei et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib45\" title=\"\">2023</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib11\" title=\"\">2024</a>)</cite> and combining multiple ASR systems <cite class=\"ltx_cite ltx_citemacro_cite\">Fiscus (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib19\" title=\"\">1997</a>); Tan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib77\" title=\"\">2020</a>); Kamo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib38\" title=\"\">2025</a>)</cite>.\nMBR decoding can be seen as a form of model fusion. In fact, several studies have proposed using MBR decoding to ensemble the outputs from multiple systems <cite class=\"ltx_cite ltx_citemacro_cite\">Xu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib88\" title=\"\">2010</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib89\" title=\"\">2011</a>)</cite>.\nAt the same time, model fusion can be seen as complementary to MBR decoding, as it focuses on improving the underlying model rather than the decoding process.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "time",
                    "model",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Post-editing and error correction are alternative approaches that have been proposed to further improve the accuracy of ASR and speech translation outputs <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib49\" title=\"\">2020</a>); Kamiya et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib37\" title=\"\">2021</a>); Leng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib46\" title=\"\">2021</a>); Yang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib92\" title=\"\">2023</a>); Ma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib50\" title=\"\">2023</a>); Radhakrishnan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib65\" title=\"\">2023</a>); Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib10\" title=\"\">2023</a>)</cite>. These approaches use language models to correct errors in the initial hypotheses, generating a new hypothesis using the language model <cite class=\"ltx_cite ltx_citemacro_cite\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib26\" title=\"\">2019</a>); Hrinchuk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib29\" title=\"\">2020</a>); Radhakrishnan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib65\" title=\"\">2023</a>)</cite>.\nThis approach is orthogonal to MBR decoding, as it focuses on refining the output after generation rather than re-evaluating multiple hypotheses during decoding.</p>\n\n",
                "matched_terms": [
                    "hypotheses",
                    "model",
                    "decoding",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:382.4pt;height:242.8pt;vertical-align:-118.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Model</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">whisper-small</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">whisper-medium</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\">Metric</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.067</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.100</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.087</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.058</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.850</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.077</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.067</span>\n<span class=\"ltx_td ltx_align_center\">2.100</span>\n<span class=\"ltx_td ltx_align_center\">0.087</span>\n<span class=\"ltx_td ltx_align_center\">0.058</span>\n<span class=\"ltx_td ltx_align_center\">1.850</span>\n<span class=\"ltx_td ltx_align_center\">0.077</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.067</span>\n<span class=\"ltx_td ltx_align_center\">2.100</span>\n<span class=\"ltx_td ltx_align_center\">0.087</span>\n<span class=\"ltx_td ltx_align_center\">0.058</span>\n<span class=\"ltx_td ltx_align_center\">1.850</span>\n<span class=\"ltx_td ltx_align_center\">0.077</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MBR (<math alttext=\"N=4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">N=4</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.058</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.950</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.078</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.049</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.730</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.070</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m11\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">N=8</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.054</span>\n<span class=\"ltx_td ltx_align_center\">1.900</span>\n<span class=\"ltx_td ltx_align_center\">0.076</span>\n<span class=\"ltx_td ltx_align_center\">0.043</span>\n<span class=\"ltx_td ltx_align_center\">1.680</span>\n<span class=\"ltx_td ltx_align_center\">0.067</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=16\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m12\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">N=16</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.052</span>\n<span class=\"ltx_td ltx_align_center\">1.830</span>\n<span class=\"ltx_td ltx_align_center\">0.072</span>\n<span class=\"ltx_td ltx_align_center\">0.042</span>\n<span class=\"ltx_td ltx_align_center\">1.650</span>\n<span class=\"ltx_td ltx_align_center\">0.066</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m13\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">N=32</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.050</span></span>\n<span class=\"ltx_td ltx_align_center\">1.830</span>\n<span class=\"ltx_td ltx_align_center\">0.072</span>\n<span class=\"ltx_td ltx_align_center\">0.042</span>\n<span class=\"ltx_td ltx_align_center\">1.630</span>\n<span class=\"ltx_td ltx_align_center\">0.065</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m14\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.051</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.800</span></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.072</span></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.041</span></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.630</span></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.065</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Model</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_3\">whisper-large-v3</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_3\">distil-large-v3.5</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Metric</span>\n<span class=\"ltx_td ltx_align_center\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m16\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m17\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m18\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m19\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m20\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m21\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.042</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.750</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.060</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.048</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.930</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.056</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m22\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.042</span>\n<span class=\"ltx_td ltx_align_center\">1.750</span>\n<span class=\"ltx_td ltx_align_center\">0.060</span>\n<span class=\"ltx_td ltx_align_center\">0.048</span>\n<span class=\"ltx_td ltx_align_center\">1.930</span>\n<span class=\"ltx_td ltx_align_center\">0.056</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m23\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.042</span>\n<span class=\"ltx_td ltx_align_center\">1.750</span>\n<span class=\"ltx_td ltx_align_center\">0.060</span>\n<span class=\"ltx_td ltx_align_center\">0.048</span>\n<span class=\"ltx_td ltx_align_center\">1.930</span>\n<span class=\"ltx_td ltx_align_center\">0.056</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MBR (<math alttext=\"N=4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m24\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">N=4</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.035</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.700</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.054</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.042</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.880</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.051</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m25\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">N=8</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.035</span>\n<span class=\"ltx_td ltx_align_center\">1.680</span>\n<span class=\"ltx_td ltx_align_center\">0.055</span>\n<span class=\"ltx_td ltx_align_center\">0.040</span>\n<span class=\"ltx_td ltx_align_center\">1.830</span>\n<span class=\"ltx_td ltx_align_center\">0.049</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=16\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m26\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">N=16</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.034</span>\n<span class=\"ltx_td ltx_align_center\">1.650</span>\n<span class=\"ltx_td ltx_align_center\">0.054</span>\n<span class=\"ltx_td ltx_align_center\">0.039</span>\n<span class=\"ltx_td ltx_align_center\">1.830</span>\n<span class=\"ltx_td ltx_align_center\">0.048</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m27\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">N=32</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.032</span></span>\n<span class=\"ltx_td ltx_align_center\">1.650</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.053</span></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.038</span></span>\n<span class=\"ltx_td ltx_align_center\">1.800</span>\n<span class=\"ltx_td ltx_align_center\">0.047</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m28\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.033</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">1.650</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.053</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.038</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">1.800</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.045</span></span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "b1b1",
                    "n32n32",
                    "n64n64",
                    "b20b20",
                    "b5b5",
                    "n8n8",
                    "n16n16",
                    "mbr",
                    "n4n4",
                    "whisperlargev3",
                    "beam",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of the study is to evaluate MBR decoding for ASR and ST tasks, compared to beam search.\nWe investigate various settings, including different models, datasets, and levels of noise added to the input audio.</p>\n\n",
                "matched_terms": [
                    "beam",
                    "decoding",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments to evaluate the performance of MBR decoding and beam search on various ASR and speech translation tasks.\nFor evaluating the methods under noise, we use the free-sound subset of the MUSAN dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Snyder et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib73\" title=\"\">2015</a>)</cite> to induce background noise to the audio.\nWe sample a noise randomly from the freesound subset of the dataset and crop it to match the length of the input audio. The cropped noise audio is synthesized to the speech with the level of Signal-to-Noise Ratio (SNR) set to 0 dB, noted otherwise.\nThe same noise is used for all the decoding algorithms for fair comparison.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "decoding",
                    "mbr",
                    "beam",
                    "algorithms",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For beam search, we run with a beam width of 1, 5, and 20.\nWe generate up to 64 samples for MBR decoding as hypotheses using Epsilon sampling <cite class=\"ltx_cite ltx_citemacro_cite\">Hewitt et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib27\" title=\"\">2022</a>); Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib20\" title=\"\">2023</a>)</cite> with <math alttext=\"\\epsilon=0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.01</annotation></semantics></math> and a temperature set to <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math>.\nWe use the BLEU score <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib60\" title=\"\">2002</a>)</cite> implemented by the sacrebleu package <cite class=\"ltx_cite ltx_citemacro_cite\">Post (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib62\" title=\"\">2018</a>)</cite> as the utility function of MBR. We do not use WER (CER) as the utility function because MBR decoding is known to inflate the score used as the utility function which may not accurately reflect a model&#8217;s true capabilities <cite class=\"ltx_cite ltx_citemacro_cite\">Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib21\" title=\"\">2022</a>); Kovacs et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib42\" title=\"\">2024</a>)</cite>.\nBLEU scores are computed on the normalized texts using whisper&#8217;s normalizer for English <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib64\" title=\"\">2023</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">neologdn</span> normalizer for Japanese <cite class=\"ltx_cite ltx_citemacro_cite\">Sato et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib70\" title=\"\">2017</a>)</cite> to avoid unnecessary penalization on punctuation.\nWe use MeCab tokenizer <cite class=\"ltx_cite ltx_citemacro_cite\">Kudo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib43\" title=\"\">2005</a>)</cite> to tokenize Japanese text for computing the BLEU score.</p>\n\n",
                "matched_terms": [
                    "hypotheses",
                    "wer",
                    "decoding",
                    "reflect",
                    "which",
                    "mbr",
                    "beam",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All the code of the experiments is implemented by Python 3 using Huggingface&#8217;s transformers library <cite class=\"ltx_cite ltx_citemacro_cite\">Wolf et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib86\" title=\"\">2020</a>)</cite>.\nThe experiments are conducted on Linux Ubuntu 22.04 using NVIDIA A100 GPUs.\nWhile the codebase is not optimized for efficiency, we report the walltime with our implementation as a reference in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S6\" title=\"6 Limitations &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "optimized",
                    "reference",
                    "walltime",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the performance of MBR decoding on ASR using LibriSpeech (clean) <cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib59\" title=\"\">2015</a>)</cite>, AMI-IHM <cite class=\"ltx_cite ltx_citemacro_cite\">Carletta (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib9\" title=\"\">2007</a>)</cite>, and VoxPopuli <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib82\" title=\"\">2021a</a>)</cite> for English, ReazonSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">Yin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib93\" title=\"\">2023</a>)</cite>, Common Voice-v8 <cite class=\"ltx_cite ltx_citemacro_cite\">Ardila et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib5\" title=\"\">2020</a>)</cite>, and JSUT <cite class=\"ltx_cite ltx_citemacro_cite\">Sonobe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib74\" title=\"\">2017</a>)</cite> for Japanese.\nWe use Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib64\" title=\"\">2023</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span> for English and Kotoba-Whisper-v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0\" title=\"\">https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0</a></span></span></span> for Japanese ASR models.\nAll the audio files are resampled to 16 kHz to meet the Whisper model&#8217;s requirement.\nWe use the first 1000 samples in the test set of each dataset for the evaluation, skipping samples longer than 30 seconds so that they can be handled with the Whisper model at a single path of inference. In this way, we can evaluate the effect of MBR decoding disentangled from the effect of the long-form audio handling techniques <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib13\" title=\"\">2019</a>); Narayanan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib55\" title=\"\">2019</a>); Koluguri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib41\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "librispeech",
                    "decoding",
                    "seconds",
                    "mbr",
                    "model",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:530.6pt;height:73.6pt;vertical-align:-34.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Dataset</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">LibriSpeech</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">VoxPopuli</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">AMI-IHM</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\">Metric</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.081</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.250</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.091</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.117</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.500</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.067</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.380</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.280</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.264</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.081</span>\n<span class=\"ltx_td ltx_align_center\">2.230</span>\n<span class=\"ltx_td ltx_align_center\">0.092</span>\n<span class=\"ltx_td ltx_align_center\">0.117</span>\n<span class=\"ltx_td ltx_align_center\">1.500</span>\n<span class=\"ltx_td ltx_align_center\">0.067</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.380</span></span>\n<span class=\"ltx_td ltx_align_center\">2.280</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.264</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.082</span>\n<span class=\"ltx_td ltx_align_center\">2.200</span>\n<span class=\"ltx_td ltx_align_center\">0.090</span>\n<span class=\"ltx_td ltx_align_center\">0.117</span>\n<span class=\"ltx_td ltx_align_center\">1.500</span>\n<span class=\"ltx_td ltx_align_center\">0.067</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.380</span></span>\n<span class=\"ltx_td ltx_align_center\">2.280</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.264</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.057</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.000</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.077</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.098</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.400</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.053</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.568</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.200</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.284</span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "b1b1",
                    "n64n64",
                    "librispeech",
                    "b20b20",
                    "b5b5",
                    "mbr",
                    "beam",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use word error rate (WER) for English and character error rate (CER) for Japanese as the main evaluation metrics.\nThe same normalizers as BLEU scores are used for WER (whisper normalizer) and CER (<span class=\"ltx_text ltx_font_typewriter\">neologdn</span> normalizer).\nIn addition, SemDist <cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib39\" title=\"\">2021</a>)</cite> and MetricX (<span class=\"ltx_text ltx_font_typewriter\">metricx-23-xxl-v2p0</span>; <cite class=\"ltx_cite ltx_citemacro_citet\">Juraska et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib36\" title=\"\">2023</a></cite>) are used to evaluate the semantic similarity and overall quality of the generated outputs.\nSemDist is a metric that measures the semantic distance between the generated text and the reference text using the inner product of the embeddings of the texts, which is also known as other names such as cosine distance and contextual similarity in the NLP community <cite class=\"ltx_cite ltx_citemacro_cite\">Akula and Garibay (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib3\" title=\"\">2022</a>); Mukherjee and Shrivastava (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib53\" title=\"\">2022</a>)</cite>.\nIt is proposed to complement the problem of WER (CER), which does not capture semantic similarity well, and thus, the effectiveness of the generation in the downstream tasks is not clear by itself.\nWe use a sentence BERT model named <span class=\"ltx_text ltx_font_typewriter\">all-MiniLM-L6-v2</span> as the embedding model to compute SemDist <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib66\" title=\"\">2019</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\" title=\"\">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a></span></span></span>\nMetricX is one of the state-of-the-art metrics for machine translation that evaluates the overall quality of the generated outputs by learning human MQM evaluation results. We use it for assessing the overall quality of the generated outputs.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "generated",
                    "does",
                    "model",
                    "which",
                    "thus",
                    "reference",
                    "overall",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:521.4pt;height:73.6pt;vertical-align:-34.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Dataset</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">ReazonSpeech</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">CommonVoice</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">JSUT</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\">Metric</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">CER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">CER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">CER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m10\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.305</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.975</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.143</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.306</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.825</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.134</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.183</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.250</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.088</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m11\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.307</span>\n<span class=\"ltx_td ltx_align_center\">2.975</span>\n<span class=\"ltx_td ltx_align_center\">0.143</span>\n<span class=\"ltx_td ltx_align_center\">0.302</span>\n<span class=\"ltx_td ltx_align_center\">2.875</span>\n<span class=\"ltx_td ltx_align_center\">0.132</span>\n<span class=\"ltx_td ltx_align_center\">0.185</span>\n<span class=\"ltx_td ltx_align_center\">2.350</span>\n<span class=\"ltx_td ltx_align_center\">0.089</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m12\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.308</span>\n<span class=\"ltx_td ltx_align_center\">3.050</span>\n<span class=\"ltx_td ltx_align_center\">0.140</span>\n<span class=\"ltx_td ltx_align_center\">0.306</span>\n<span class=\"ltx_td ltx_align_center\">2.875</span>\n<span class=\"ltx_td ltx_align_center\">0.133</span>\n<span class=\"ltx_td ltx_align_center\">0.184</span>\n<span class=\"ltx_td ltx_align_center\">2.350</span>\n<span class=\"ltx_td ltx_align_center\">0.090</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m13\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.291</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.875</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.130</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.297</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.725</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.123</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.177</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.200</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.082</span></span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "b1b1",
                    "n64n64",
                    "b20b20",
                    "b5b5",
                    "mbr",
                    "beam",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use word error rate (WER) for English and character error rate (CER) for Japanese as the main evaluation metrics.\nThe same normalizers as BLEU scores are used for WER (whisper normalizer) and CER (<span class=\"ltx_text ltx_font_typewriter\">neologdn</span> normalizer).\nIn addition, SemDist <cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib39\" title=\"\">2021</a>)</cite> and MetricX (<span class=\"ltx_text ltx_font_typewriter\">metricx-23-xxl-v2p0</span>; <cite class=\"ltx_cite ltx_citemacro_citet\">Juraska et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib36\" title=\"\">2023</a></cite>) are used to evaluate the semantic similarity and overall quality of the generated outputs.\nSemDist is a metric that measures the semantic distance between the generated text and the reference text using the inner product of the embeddings of the texts, which is also known as other names such as cosine distance and contextual similarity in the NLP community <cite class=\"ltx_cite ltx_citemacro_cite\">Akula and Garibay (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib3\" title=\"\">2022</a>); Mukherjee and Shrivastava (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib53\" title=\"\">2022</a>)</cite>.\nIt is proposed to complement the problem of WER (CER), which does not capture semantic similarity well, and thus, the effectiveness of the generation in the downstream tasks is not clear by itself.\nWe use a sentence BERT model named <span class=\"ltx_text ltx_font_typewriter\">all-MiniLM-L6-v2</span> as the embedding model to compute SemDist <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib66\" title=\"\">2019</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\" title=\"\">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a></span></span></span>\nMetricX is one of the state-of-the-art metrics for machine translation that evaluates the overall quality of the generated outputs by learning human MQM evaluation results. We use it for assessing the overall quality of the generated outputs.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "generated",
                    "does",
                    "model",
                    "which",
                    "thus",
                    "reference",
                    "overall",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:429.8pt;height:61.6pt;vertical-align:-28.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">SNR (dB)</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">-20</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">-15</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">-10</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">-5</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">0</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">5</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">10</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">15</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">20</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.590</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.458</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.290</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.143</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.081</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.055</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.049</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.049</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.045</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m2\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.599</span>\n<span class=\"ltx_td ltx_align_center\">0.446</span>\n<span class=\"ltx_td ltx_align_center\">0.293</span>\n<span class=\"ltx_td ltx_align_center\">0.152</span>\n<span class=\"ltx_td ltx_align_center\">0.081</span>\n<span class=\"ltx_td ltx_align_center\">0.056</span>\n<span class=\"ltx_td ltx_align_center\">0.048</span>\n<span class=\"ltx_td ltx_align_center\">0.049</span>\n<span class=\"ltx_td ltx_align_center\">0.045</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.590</span>\n<span class=\"ltx_td ltx_align_center\">0.444</span>\n<span class=\"ltx_td ltx_align_center\">0.284</span>\n<span class=\"ltx_td ltx_align_center\">0.151</span>\n<span class=\"ltx_td ltx_align_center\">0.082</span>\n<span class=\"ltx_td ltx_align_center\">0.056</span>\n<span class=\"ltx_td ltx_align_center\">0.049</span>\n<span class=\"ltx_td ltx_align_center\">0.048</span>\n<span class=\"ltx_td ltx_align_center\">0.045</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.530</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.388</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.235</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.108</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.057</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.041</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.035</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.036</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.034</span></span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "b1b1",
                    "n64n64",
                    "b20b20",
                    "b5b5",
                    "mbr",
                    "beam"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:277.0pt;height:61.6pt;vertical-align:-28.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">SNR (dB)</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">0</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">5</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">10</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">15</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">20</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m1\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.305</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.273</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.258</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.243</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.243</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m2\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.307</span>\n<span class=\"ltx_td ltx_align_center\">0.282</span>\n<span class=\"ltx_td ltx_align_center\">0.254</span>\n<span class=\"ltx_td ltx_align_center\">0.243</span>\n<span class=\"ltx_td ltx_align_center\">0.240</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m3\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.308</span>\n<span class=\"ltx_td ltx_align_center\">0.272</span>\n<span class=\"ltx_td ltx_align_center\">0.254</span>\n<span class=\"ltx_td ltx_align_center\">0.242</span>\n<span class=\"ltx_td ltx_align_center\">0.235</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T5.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.291</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.250</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.238</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.229</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.223</span></span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "b1b1",
                    "n64n64",
                    "b20b20",
                    "b5b5",
                    "mbr",
                    "beam"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:306.5pt;height:61.6pt;vertical-align:-28.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Metric</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m4\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.042</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1.750</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.060</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m5\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>, <math alttext=\"u=\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m6\" intent=\":literal\"><semantics><mrow><mi>u</mi><mo>=</mo><mi/></mrow><annotation encoding=\"application/x-tex\">u=</annotation></semantics></math> BLEU)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.033</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.650</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.053</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m7\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>, <math alttext=\"u=\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m8\" intent=\":literal\"><semantics><mrow><mi>u</mi><mo>=</mo><mi/></mrow><annotation encoding=\"application/x-tex\">u=</annotation></semantics></math> BLEURT)</span>\n<span class=\"ltx_td ltx_align_center\">0.035</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.650</span></span>\n<span class=\"ltx_td ltx_align_center\">0.056</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m9\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>, <math alttext=\"u=\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.m10\" intent=\":literal\"><semantics><mrow><mi>u</mi><mo>=</mo><mi/></mrow><annotation encoding=\"application/x-tex\">u=</annotation></semantics></math> SentBERT)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.034</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">1.675</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.050<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">&#8727;</span></sup></span></span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "b1b1",
                    "n64n64",
                    "beam",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:277.0pt;height:61.6pt;vertical-align:-28.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Metric</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m4\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.042</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1.750</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.060</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m5\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>, <math alttext=\"\\epsilon=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m6\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.033</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.625</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.052</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m7\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>, <math alttext=\"\\epsilon=0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m8\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.01</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.033</span>\n<span class=\"ltx_td ltx_align_center\">1.650</span>\n<span class=\"ltx_td ltx_align_center\">0.053</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m9\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>, <math alttext=\"\\epsilon=0.02\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T7.m10\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0.02</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.02</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.033</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">1.650</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.052</span></span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "b1b1",
                    "n64n64",
                    "beam",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:409.1pt;height:61.6pt;vertical-align:-28.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Domain</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Arabic</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Chinese</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Hindi</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Indonesian</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Tamil</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Thai</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Vietnamese</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m1\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.305</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.231</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.180</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.090</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.278</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.366</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.193</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m2\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.305</span>\n<span class=\"ltx_td ltx_align_center\">0.231</span>\n<span class=\"ltx_td ltx_align_center\">0.180</span>\n<span class=\"ltx_td ltx_align_center\">0.090</span>\n<span class=\"ltx_td ltx_align_center\">0.278</span>\n<span class=\"ltx_td ltx_align_center\">0.366</span>\n<span class=\"ltx_td ltx_align_center\">0.193</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m3\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.305</span>\n<span class=\"ltx_td ltx_align_center\">0.231</span>\n<span class=\"ltx_td ltx_align_center\">0.180</span>\n<span class=\"ltx_td ltx_align_center\">0.090</span>\n<span class=\"ltx_td ltx_align_center\">0.278</span>\n<span class=\"ltx_td ltx_align_center\">0.366</span>\n<span class=\"ltx_td ltx_align_center\">0.193</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T8.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.259</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.205</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.142</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.069</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.260</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.354</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.180</span></span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "b1b1",
                    "n64n64",
                    "b20b20",
                    "b5b5",
                    "mbr",
                    "beam"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use whisper-small, whisper-medium, whisper-large-v3, and distil-whisper to evaluate the effect of the model size.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S3.T1\" title=\"Table 1 &#8227; 3 Related Work &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that MBR decoding outperforms beam search in all the model sizes.\nThe result shows that MBR decoding is effective regardless of the model size.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "whisperlargev3",
                    "mbr",
                    "beam",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S3.T1\" title=\"Table 1 &#8227; 3 Related Work &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows the performance of MBR decoding with different numbers of samples.\nSurprisingly, with only four to eight samples, MBR decoding outperforms beam search.\nThe result shows that MBR decoding is effective even with a small amount of additional computation, which might be admissible for real-time ASR tasks.\nStill, we observe that the accuracy of MBR decoding improves with a larger number of samples, suggesting that more computation can lead to better performance.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "decoding",
                    "which",
                    "mbr",
                    "beam"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate how much the MBR objective indicates the utility of the given hypothesis, we compute the correlation of the MBR objective with the WER.\nPearson correlation coefficient is computed for each instance of the LibriSpeech with no synthesized noise over 64 samples generated by whisper-large-v3. Then, we estimate it with the average over the 1000 instances.\nThe average value of the Pearson correlation coefficient is -0.3913, and the standard error is 0.0129, indicating that the MBR objective has a substantial negative correlation with the target objective (negative correlation because MBR objective is higher the better, and WER is lower the better). This suggests that it is a reasonable approach to use it as the reranking procedure for ASR.\n</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "wer",
                    "generated",
                    "average",
                    "mbr",
                    "whisperlargev3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T2\" title=\"Table 2 &#8227; Resources. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T3\" title=\"Table 3 &#8227; Evaluation metrics. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> show the performance of the decoding algorithms using WER (CER), SemDist, and MetricX.\nMBR decoding outperforms beam search in all the datasets except for AMI-IHM, suggesting that the advantage of MBR decoding over beam search is in a wide range of domains and on both lexical and semantic levels.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "wer",
                    "decoding",
                    "mbr",
                    "beam",
                    "algorithms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given that MBR decoding fails to improve on the AMI-IHM dataset, we conduct a post-hoc error analysis on the dataset.\nThe corpus records all utterances in the meeting recordings, including very short sentences and pseudowords. For example, some instances only have <span class=\"ltx_text ltx_font_italic\">yeah</span>, <span class=\"ltx_text ltx_font_italic\">hmm</span>, and <span class=\"ltx_text ltx_font_italic\">gosh</span> in the transcription. We hypothesize that the performance of MBR decoding is low on these instances because the BLEU score cannot evaluate the utility of these instances.\nTo this end, we compute the average WER of the beam search and MBR decoding on the AMI-IHM dataset, split according to the number of words in the reference transcription (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.F3\" title=\"Figure 3 &#8227; Datasets. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nWhile MBR decoding has comparable WER to beam search in most instances, it shows significantly higher WER on instances shorter than six words. The results suggest that beam search may be more suitable for very short sentences.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "wer",
                    "decoding",
                    "average",
                    "mbr",
                    "beam",
                    "reference",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T4\" title=\"Table 4 &#8227; Evaluation metrics. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T5\" title=\"Table 5 &#8227; Evaluation metrics. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> show the performance under different noise levels.\nThe result shows that MBR decoding is more accurate than beam search at any noise level.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "beam",
                    "mbr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance of MBR decoding is known to be dependent on the choice of the utility function <cite class=\"ltx_cite ltx_citemacro_cite\">Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib21\" title=\"\">2022</a>); Kovacs et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib42\" title=\"\">2024</a>)</cite>.\nWe evaluate MBR decoding using SentBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib66\" title=\"\">2019</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib67\" title=\"\">2020</a>)</cite> and BLEURT (<span class=\"ltx_text ltx_font_typewriter\">BLEURT-20-D12</span>) in addition to using BLEU. SentBERT is a sentence-level embedding model that captures semantic similarity between sentences computed by the cosine similarity between the two embedding vectors. Thus, the value is 1 minus the value of SemDist.\nWe use the <span class=\"ltx_text ltx_font_typewriter\">all-MiniLM-L6-v2</span> model as the embedding model to compute SentBERT.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T6\" title=\"Table 6 &#8227; Evaluation metrics. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that the differences in accuracy using these utility functions are marginal, yet they all outperform beam search.\nThe result shows that the advantage of MBR decoding over beam search is robust to the choice of the utility function.\nSentBERT achieves the best SemDist score, which is expected as it is directly optimized for the metric <cite class=\"ltx_cite ltx_citemacro_cite\">Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib21\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "decoding",
                    "which",
                    "thus",
                    "mbr",
                    "beam",
                    "optimized",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The choice of sampling algorithm is known to be crucial for the performance of MBR decoding in machine translation tasks <cite class=\"ltx_cite ltx_citemacro_cite\">Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib20\" title=\"\">2023</a>); Ohashi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib58\" title=\"\">2024</a>); Jinnai et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib35\" title=\"\">2024</a>)</cite>.\nWe evaluate epsilon sampling with varying epsilon values of <math alttext=\"0.00\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px11.p1.m1\" intent=\":literal\"><semantics><mn>0.00</mn><annotation encoding=\"application/x-tex\">0.00</annotation></semantics></math>, <math alttext=\"0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px11.p1.m2\" intent=\":literal\"><semantics><mn>0.01</mn><annotation encoding=\"application/x-tex\">0.01</annotation></semantics></math>, and <math alttext=\"0.02\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px11.p1.m3\" intent=\":literal\"><semantics><mn>0.02</mn><annotation encoding=\"application/x-tex\">0.02</annotation></semantics></math>.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T7\" title=\"Table 7 &#8227; Evaluation metrics. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the performance of MBR decoding with the different epsilon values.\nThe result shows that the performance of MBR decoding is relatively robust to the choice of epsilon values, and it outperforms beam search in all the settings.\nIt also indicates that the effective sampling strategy for ASR may be different from the effective strategy for machine translation (i.e., epsilon sampling), which may be an interesting avenue of future work.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "decoding",
                    "which",
                    "mbr",
                    "beam"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess whether the performance of MBR decoding is language-specific or generic to natural language text generation tasks, we conduct experiments on the following languages: Arabic (ar), simplified Chinese (zh-CN), Hindi (hi), Tamil (ta), Thai (th), and Vietnamese (vi).\nWe use the test split of the CommonVoice-v8 dataset and evaluate the WER (CER for Chinese). We use spaCy-Thai for segmenting words in Thai <cite class=\"ltx_cite ltx_citemacro_cite\">Zeman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib95\" title=\"\">2017</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/spacy-thai/\" title=\"\">https://pypi.org/project/spacy-thai/</a></span></span></span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T8\" title=\"Table 8 &#8227; Evaluation metrics. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the result.\nOverall, we observe MBR decoding to consistently outperform beam search in all the languages.\nThe result indicates that the method is effective across different languages.</p>\n\n",
                "matched_terms": [
                    "method",
                    "performance",
                    "wer",
                    "decoding",
                    "mbr",
                    "beam",
                    "overall",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:240.8pt;height:73.6pt;vertical-align:-34.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">LibriSpeech</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">ReazonSpeech</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m1\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.042</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.305</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">NoRefER (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.073</span>\n<span class=\"ltx_td ltx_align_center\">0.368</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ProGRes (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m3\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.043</span>\n<span class=\"ltx_td ltx_align_center\">0.358</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.033</span></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.291</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">Oracle (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m5\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.013</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.149</span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "b1b1",
                    "n64n64",
                    "librispeech",
                    "mbr",
                    "beam"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:527.1pt;height:146.8pt;vertical-align:-70.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Domain</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4\">CoVoST2 (Ja-En)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4\">FLEURS (Ja-En)</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Metric</span>\n<span class=\"ltx_td ltx_align_center\">BLEU<math alttext=\"\\uparrow^{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m1\" intent=\":literal\"><semantics><msup><mo stretchy=\"false\">&#8593;</mo><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">\\uparrow^{\\ast}</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">ROUGE-L<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">BLEURT<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">BLEU<math alttext=\"\\uparrow^{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m5\" intent=\":literal\"><semantics><msup><mo stretchy=\"false\">&#8593;</mo><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">\\uparrow^{\\ast}</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">Rouge-L<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">BLEURT<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m9\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">18.646</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">42.283</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">-0.184</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.825</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">6.218</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">30.178</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">-0.486</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">6.750</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m10\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">18.685</span>\n<span class=\"ltx_td ltx_align_center\">41.825</span>\n<span class=\"ltx_td ltx_align_center\">-0.201</span>\n<span class=\"ltx_td ltx_align_center\">2.850</span>\n<span class=\"ltx_td ltx_align_center\">6.158</span>\n<span class=\"ltx_td ltx_align_center\">29.954</span>\n<span class=\"ltx_td ltx_align_center\">-0.487</span>\n<span class=\"ltx_td ltx_align_center\">6.725</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m11\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">18.122</span>\n<span class=\"ltx_td ltx_align_center\">41.362</span>\n<span class=\"ltx_td ltx_align_center\">-0.235</span>\n<span class=\"ltx_td ltx_align_center\">2.950</span>\n<span class=\"ltx_td ltx_align_center\">6.202</span>\n<span class=\"ltx_td ltx_align_center\">29.886</span>\n<span class=\"ltx_td ltx_align_center\">-0.489</span>\n<span class=\"ltx_td ltx_align_center\">6.825</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m12\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">22.456</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">47.572</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">-0.073</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.475</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">8.078</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">34.212</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">-0.365</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">6.100</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Domain</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4\">CoVoST2 (En-Ja)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4\">FLEURS (En-Ja)</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Metric</span>\n<span class=\"ltx_td ltx_align_center\">BLEU<math alttext=\"\\uparrow^{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m13\" intent=\":literal\"><semantics><msup><mo stretchy=\"false\">&#8593;</mo><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">\\uparrow^{\\ast}</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">Rouge-L<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m14\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">BLEURT<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m16\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">BLEU<math alttext=\"\\uparrow^{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m17\" intent=\":literal\"><semantics><msup><mo stretchy=\"false\">&#8593;</mo><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">\\uparrow^{\\ast}</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">Rouge-L<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m18\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">BLEURT<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m19\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m20\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m21\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">10.395</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">34.176</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.110</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">4.975</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">8.242</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">30.771</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">-0.015</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">8.975</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m22\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">10.795</span>\n<span class=\"ltx_td ltx_align_center\">33.960</span>\n<span class=\"ltx_td ltx_align_center\">0.109</span>\n<span class=\"ltx_td ltx_align_center\">4.950</span>\n<span class=\"ltx_td ltx_align_center\">8.360</span>\n<span class=\"ltx_td ltx_align_center\">30.612</span>\n<span class=\"ltx_td ltx_align_center\">-0.019</span>\n<span class=\"ltx_td ltx_align_center\">8.950</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m23\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">10.822</span>\n<span class=\"ltx_td ltx_align_center\">34.393</span>\n<span class=\"ltx_td ltx_align_center\">0.116</span>\n<span class=\"ltx_td ltx_align_center\">4.900</span>\n<span class=\"ltx_td ltx_align_center\">8.224</span>\n<span class=\"ltx_td ltx_align_center\">30.537</span>\n<span class=\"ltx_td ltx_align_center\">-0.015</span>\n<span class=\"ltx_td ltx_align_center\">8.900</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m24\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">15.968</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">43.260</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.195</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.225</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">11.681</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">37.207</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.017</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">6.375</span></span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "b1b1",
                    "n64n64",
                    "b20b20",
                    "b5b5",
                    "mbr",
                    "beam"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to MBR decoding, we evaluate two reranking algorithms proposed recently.\nNoRefER selects the sentence with highest score according to a language model fine-tuned for the ASR reranking task <cite class=\"ltx_cite ltx_citemacro_cite\">Yuksel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib94\" title=\"\">2023</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/aixplain/NoRefER\" title=\"\">https://huggingface.co/aixplain/NoRefER</a></span></span></span>\nNoRefER does not use the audio input on reranking and relies solely on the generations.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "does",
                    "model",
                    "mbr",
                    "algorithms",
                    "solely",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ProGRes selects the hypothesis using the weighted sum of the two objectives, LLM score and ASR score <cite class=\"ltx_cite ltx_citemacro_cite\">Tur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib79\" title=\"\">2024</a>)</cite>.\nLLM score is the perplexity of the hypothesis given a prompt articulated for the reranking task as a context <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px13.p2.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>. We use the same prompt as in Section 2.1 of <cite class=\"ltx_cite ltx_citemacro_citet\">Tur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib79\" title=\"\">2024</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Tur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib79\" title=\"\">2024</a>)</cite> evaluate ProGRes using Llama-3, GPT-3.5, GPT-4 and show that GPT-4 achieves the best performance over the three. Unfortunately, the logits of GPT-3.5 and GPT-4 are no longer provided to the users, so it is not reproducible using these proprietary models. To this end, we use Llama-3 for computing the LLM score in the following experiment <cite class=\"ltx_cite ltx_citemacro_cite\">Grattafiori et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib24\" title=\"\">2024</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct</a></span></span></span>\nASR score is the loss value of the ASR model. We use cross-entropy loss, one of the standard loss functions for ASR models, as the loss function for Whisper is not disclosed <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib64\" title=\"\">2023</a>)</cite>.\nWe set the weight of the LLM score to <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px13.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math> as it performs the best in the experiments by <cite class=\"ltx_cite ltx_citemacro_citet\">Tur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib79\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T9\" title=\"Table 9 &#8227; Languages. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the comparison of the reranking algorithms on LibriSpeech and ReazonSpeech. Overall, we observe the performance of the algorithms to be suboptimal compared to MBR decoding and beam search.\nNoRefER is trained to distinguish models compressed into different sizes so that they have sufficiently different accuracy <cite class=\"ltx_cite ltx_citemacro_cite\">Yuksel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib94\" title=\"\">2023</a>)</cite>. Thus, it may be less effective for reranking samples from the same model.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "librispeech",
                    "decoding",
                    "model",
                    "thus",
                    "mbr",
                    "beam",
                    "algorithms",
                    "overall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Oracle score is the score of the hypothesis with the lowest WER (CER) to the reference in the 64 hypotheses sampled. Given that it achieves significantly better score than any of the reranking algorithms, the hypotheses set has good enough hypothesis to be selected and the reranking algorithms have room of improvement.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "hypotheses",
                    "reference",
                    "algorithms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use BLEU using sacrebleu, ROUGE-L <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib48\" title=\"\">2004</a>)</cite>, BLEURT <cite class=\"ltx_cite ltx_citemacro_cite\">Sellam et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib71\" title=\"\">2020</a>)</cite>, and MetricX as the evaluation metrics.\nThe other settings are the same as the ASR.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T10\" title=\"Table 10 &#8227; Languages. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows the results of the experiments.\nOverall, MBR decoding outperforms beam search in all the metrics in both language pairs and datasets.</p>\n\n",
                "matched_terms": [
                    "beam",
                    "overall",
                    "decoding",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that MBR decoding tends to achieve a relatively higher score than the others on the utility function used during the decoding process <cite class=\"ltx_cite ltx_citemacro_cite\">Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib21\" title=\"\">2022</a>)</cite>, which may be indicative of overfitting.\nThus, BLEU scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T10\" title=\"Table 10 &#8227; Languages. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> should be interpreted as references.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "which",
                    "thus",
                    "mbr",
                    "note"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we empirically evaluate the performance of MBR decoding for offline ASR and ST tasks.\nWe compare MBR decoding and beam search on a wide range of scenarios with various models, languages, datasets, noise levels, evaluation metrics, and hyperparameters.\nThe extensive evaluation shows that MBR decoding consistently achieves higher accuracy than beam search in both speech-to-text tasks.</p>\n\n",
                "matched_terms": [
                    "beam",
                    "decoding",
                    "mbr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The results indicate that MBR decoding has the potential to improve the state-of-the-art performance of offline speech-to-text tasks.\nUnlike other approaches that depend on heuristics, MBR decoding has a theoretical guarantee <cite class=\"ltx_cite ltx_citemacro_cite\">Ichihara et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib32\" title=\"\">2025a</a>)</cite>.\nWe believe that MBR decoding is a promising approach for a wide range of speech-to-text tasks and should be considered as one of the baseline methods to improve the system accuracy.</p>\n\n",
                "matched_terms": [
                    "considered",
                    "decoding",
                    "mbr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">One of the critical limitations of MBR decoding is the computational cost.\nFor the sake of reference, we provide the walltime of the decoding algorithms with our implementation.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "walltime",
                    "mbr",
                    "algorithms",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are several libraries dedicated to optimizing the speed of the whisper models, such as faster-whisper<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/systran/faster-whisper\" title=\"\">https://github.com/systran/faster-whisper</a></span></span></span> and whisper.cpp.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ggml-org/whisper.cpp\" title=\"\">https://github.com/ggml-org/whisper.cpp</a></span></span></span>\nThus, beam search can be made faster by using such libraries, but MBR decoding may require additional modifications to fully leverage these optimizations.\nDeveloping a fast implementation of MBR decoding is left for future work.</p>\n\n",
                "matched_terms": [
                    "beam",
                    "thus",
                    "decoding",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments are conducted using sequence-to-sequence autoregressive models <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib80\" title=\"\">2017</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib64\" title=\"\">2023</a>)</cite>.\nHowever, MBR decoding is a general decoding algorithm that can be applied to any probabilistic model.\nEvaluation of MBR decoding to other types of models (e.g., CTC-based models) is left for future work <cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib40\" title=\"\">2017</a>); Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib6\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "decoding",
                    "model",
                    "mbr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The MUSAN dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Snyder et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib73\" title=\"\">2015</a>)</cite> covers a wide range of noise types, but it may not fully represent the noise encountered in all the communities and regions. Evaluation using real-world noisy datasets for the particular communities and regions is left for future work.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments are conducted using public datasets and models that are available for research purposes.\nThe work aims to improve accuracy in speech-to-text tasks and does not pose immediately foreseeable risks of harm or misuse.\nWe encourage responsible use of ASR technologies and recommend further evaluation in real-world deployments.</p>\n\n",
                "matched_terms": [
                    "does",
                    "not"
                ]
            }
        ]
    },
    "A2.T12": {
        "source_file": "Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition",
        "caption": "Table 12: List of datasets and models used in this study. All the resources are publicly available.",
        "body": "Datasets\n\n\n\n\nLibriSpeech\n\n\nhttps://huggingface.co/datasets/openslr/librispeech_asr Panayotov etal. (2015)\n\n\n\n\nVoxPopuli\n\n\nhttps://huggingface.co/datasets/facebook/voxpopuli Wang etal. (2021a)\n\n\n\n\nAMI-IHM\n\n\nhttps://huggingface.co/datasets/edinburghcstr/ami Carletta (2007)\n\n\n\n\nReazonSpeech\n\n\nhttps://huggingface.co/datasets/japanese-asr/ja_asr.reazonspeech_test Yin etal. (2023)\n\n\n\n\nCommonVoice-v8\n\n\nhttps://huggingface.co/datasets/mozilla-foundation/common_voice_8_0 Ardila etal. (2020)\n\n\n\n\nJSUT\n\n\nhttps://huggingface.co/datasets/japanese-asr/ja_asr.jsut_basic5000 Sonobe etal. (2017)\n\n\n\n\nCoVoST2\n\n\nhttps://huggingface.co/datasets/facebook/covost2 Wang etal. (2021b)\n\n\n\n\nFLEURS\n\n\nhttps://huggingface.co/datasets/google/fleurs Conneau etal. (2023)\n\n\n\n\nModels\n\n\nwhisper-large-v3\n\n\nhttps://huggingface.co/openai/whisper-large-v3 Radford etal. (2023)\n\n\n\n\nwhisper-small\n\n\nhttps://huggingface.co/openai/whisper-small Radford etal. (2023)\n\n\n\n\nwhisper-medium\n\n\nhttps://huggingface.co/openai/whisper-medium Radford etal. (2023)\n\n\n\n\ndistil-whisper\n\n\nhttps://huggingface.co/distil-whisper/distil-large-v3.5 Gandhi etal. (2023)\n\n\n\n\nkotoba-whisper\n\n\nhttps://huggingface.co/kotoba-tech/kotoba-whisper-v2.0\n\n\n\n\nkotoba-whisper-bilingual\n\n\nhttps://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0\n\n\n\n\nOthers\n\n\nBLEURT\n\n\nhttps://huggingface.co/lucadiliello/BLEURT-20-D12 Sellam etal. (2020)\n\n\n\n\nMetricX\n\n\nhttps://huggingface.co/google/metricx-23-xxl-v2p0 Juraska etal. (2023)\n\n\n\n\nall-MiniLM-L6-v2\n\n\nhttps://huggingface.co/sentence-transformers/all-mpnet-base-v2 Reimers and Gurevych (2019, 2020)\n\n\n\n\nNoRefER\n\n\nBecause only part of the code is published (https://huggingface.co/aixplain/NoRefER), the method is implemented by us. Yuksel etal. (2023)\n\n\n\n\nProGRes\n\n\nBecause only part of the code is published (https://github.com/AdaDTur/ProGRes), the method is implemented by us. Tur etal. (2024)\n\n\n\n\nLlama-3\n\n\nhttps://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct Grattafiori etal. (2024)",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" colspan=\"2\">Datasets</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LibriSpeech</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/openslr/librispeech_asr\" title=\"\">https://huggingface.co/datasets/openslr/librispeech_asr</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib59\" title=\"\">2015</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">VoxPopuli</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/facebook/voxpopuli\" title=\"\">https://huggingface.co/datasets/facebook/voxpopuli</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib82\" title=\"\">2021a</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AMI-IHM</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/edinburghcstr/ami\" title=\"\">https://huggingface.co/datasets/edinburghcstr/ami</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Carletta (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib9\" title=\"\">2007</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ReazonSpeech</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/japanese-asr/ja_asr.reazonspeech_test\" title=\"\">https://huggingface.co/datasets/japanese-asr/ja_asr.reazonspeech_test</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Yin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib93\" title=\"\">2023</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CommonVoice-v8</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/mozilla-foundation/common_voice_8_0\" title=\"\">https://huggingface.co/datasets/mozilla-foundation/common_voice_8_0</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Ardila et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib5\" title=\"\">2020</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">JSUT</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/japanese-asr/ja_asr.jsut_basic5000\" title=\"\">https://huggingface.co/datasets/japanese-asr/ja_asr.jsut_basic5000</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Sonobe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib74\" title=\"\">2017</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CoVoST2</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/facebook/covost2\" title=\"\">https://huggingface.co/datasets/facebook/covost2</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib83\" title=\"\">2021b</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FLEURS</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/google/fleurs\" title=\"\">https://huggingface.co/datasets/google/fleurs</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib15\" title=\"\">2023</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" colspan=\"2\">Models</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">whisper-large-v3</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib64\" title=\"\">2023</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">whisper-small</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-small\" title=\"\">https://huggingface.co/openai/whisper-small</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib64\" title=\"\">2023</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">whisper-medium</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-medium\" title=\"\">https://huggingface.co/openai/whisper-medium</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib64\" title=\"\">2023</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">distil-whisper</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/distil-whisper/distil-large-v3.5\" title=\"\">https://huggingface.co/distil-whisper/distil-large-v3.5</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Gandhi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib22\" title=\"\">2023</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">kotoba-whisper</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0\" title=\"\">https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0</a></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">kotoba-whisper-bilingual</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0\" title=\"\">https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0</a></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" colspan=\"2\">Others</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">BLEURT</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/lucadiliello/BLEURT-20-D12\" title=\"\">https://huggingface.co/lucadiliello/BLEURT-20-D12</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Sellam et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib71\" title=\"\">2020</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MetricX</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/google/metricx-23-xxl-v2p0\" title=\"\">https://huggingface.co/google/metricx-23-xxl-v2p0</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Juraska et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib36\" title=\"\">2023</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">all-MiniLM-L6-v2</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/sentence-transformers/all-mpnet-base-v2\" title=\"\">https://huggingface.co/sentence-transformers/all-mpnet-base-v2</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib66\" title=\"\">2019</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib67\" title=\"\">2020</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">NoRefER</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Because only part of the code is published (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/aixplain/NoRefER\" title=\"\">https://huggingface.co/aixplain/NoRefER</a>), the method is implemented by us. <cite class=\"ltx_cite ltx_citemacro_cite\">Yuksel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib94\" title=\"\">2023</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ProGRes</th>\n<td class=\"ltx_td ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\">Because only part of the code is published (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/AdaDTur/ProGRes\" title=\"\">https://github.com/AdaDTur/ProGRes</a>), the method is implemented by us. <cite class=\"ltx_cite ltx_citemacro_cite\">Tur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib79\" title=\"\">2024</a>)</cite></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Llama-3</th>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct</a> <cite class=\"ltx_cite ltx_citemacro_cite\">Grattafiori et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib24\" title=\"\">2024</a>)</cite></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "httpshuggingfacecolucadiliellobleurt20d12",
            "commonvoicev8",
            "httpshuggingfacecoopenaiwhispersmall",
            "httpshuggingfacecometallamametallama38binstruct",
            "bleurt",
            "httpsgithubcomadadturprogres",
            "httpshuggingfacecoopenaiwhispermedium",
            "code",
            "reazonspeech",
            "study",
            "ardila",
            "datasets",
            "httpshuggingfacecoopenaiwhisperlargev3",
            "radford",
            "yuksel",
            "panayotov",
            "whisperlargev3",
            "resources",
            "gandhi",
            "httpshuggingfacecokotobatechkotobawhisperbilingualv10",
            "kotobawhisper",
            "amiihm",
            "used",
            "httpshuggingfacecodatasetsfacebookcovost2",
            "juraska",
            "norefer",
            "implemented",
            "progres",
            "httpshuggingfacecokotobatechkotobawhisperv20",
            "because",
            "gurevych",
            "whispermedium",
            "httpshuggingfacecodatasetsedinburghcstrami",
            "covost2",
            "distilwhisper",
            "httpshuggingfacecogooglemetricx23xxlv2p0",
            "whispersmall",
            "only",
            "httpshuggingfacecodatasetsgooglefleurs",
            "httpshuggingfacecodistilwhisperdistillargev35",
            "sellam",
            "part",
            "wang",
            "method",
            "tur",
            "fleurs",
            "2021a",
            "carletta",
            "httpshuggingfacecodatasetsjapaneseasrjaasrreazonspeechtest",
            "allminilml6v2",
            "grattafiori",
            "voxpopuli",
            "others",
            "httpshuggingfacecoaixplainnorefer",
            "available",
            "metricx",
            "published",
            "sonobe",
            "all",
            "librispeech",
            "llama3",
            "list",
            "models",
            "yin",
            "reimers",
            "httpshuggingfacecosentencetransformersallmpnetbasev2",
            "httpshuggingfacecodatasetsopenslrlibrispeechasr",
            "jsut",
            "conneau",
            "kotobawhisperbilingual",
            "httpshuggingfacecodatasetsjapaneseasrjaasrjsutbasic5000",
            "httpshuggingfacecodatasetsfacebookvoxpopuli",
            "2021b",
            "httpshuggingfacecodatasetsmozillafoundationcommonvoice80",
            "publicly"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The code is publicly available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/CyberAgentAILab/mbr-for-asr\" title=\"\">https://github.com/CyberAgentAILab/mbr-for-asr</a>.\nAll the experiments are conducted using publicly available language resources shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#A2.T12\" title=\"Table 12 &#8227; Appendix B Reproducibility Statement &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent work has shown that sample-based Minimum Bayes Risk (MBR) decoding outperforms beam search in text-to-text generation tasks, such as machine translation, text summarization, and image captioning.\nOn the other hand, beam search is the current practice for speech-to-text tasks such as automatic speech recognition (ASR) and Speech Translation (ST).\nGiven that MBR decoding is effective in text-to-text generation tasks, it is reasonable to expect it to also be effective for speech-to-text tasks.\nIn this paper, we evaluate MBR decoding for ASR and ST tasks on English and Japanese using Whisper and its derivative models.\nWe observe that the accuracy of MBR decoding outperforms that of beam search in most of the experimental settings we have evaluated.\nThe results show that MBR decoding is a promising method for offline ASR and ST tasks that require high accuracy.\nThe code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/CyberAgentAILab/mbr-for-asr\" title=\"\">https://github.com/CyberAgentAILab/mbr-for-asr</a>.</p>\n\n",
                "matched_terms": [
                    "code",
                    "models",
                    "method",
                    "available"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While MBR decoding is a well-established method in text-to-text tasks, its application to speech-to-text tasks has not been thoroughly investigated <cite class=\"ltx_cite ltx_citemacro_cite\">Prabhavalkar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib63\" title=\"\">2024</a>)</cite>.\nFor example, MBR decoding has been applied to the spoken language translation in the recent IWSLT shared tasks <cite class=\"ltx_cite ltx_citemacro_cite\">Ahmad et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib2\" title=\"\">2024</a>); Abdulmumin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib1\" title=\"\">2025</a>)</cite>, but it is used for the machine translation modules rather than the ASR modules of the cascaded systems <cite class=\"ltx_cite ltx_citemacro_cite\">Yan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib91\" title=\"\">2024</a>); Ben&#160;Kheder et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib7\" title=\"\">2024</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib47\" title=\"\">2024</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib84\" title=\"\">2025</a>); Romney&#160;Robinson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib68\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "used",
                    "wang",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given that the method is designed to improve the decoding accuracy of probabilistic models in general <cite class=\"ltx_cite ltx_citemacro_cite\">Ichihara et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib32\" title=\"\">2025a</a>)</cite>, it is reasonable to expect it to also improve the accuracy of ASR modules.\nThe absence of comprehensive studies on MBR decoding for contemporary ASR systems represents a significant gap in the literature. Given MBR&#8217;s empirical successes in text-to-text tasks and theoretical advantages, a systematic evaluation of its potential for speech recognition is valuable.</p>\n\n",
                "matched_terms": [
                    "models",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To this end, we present a comprehensive evaluation of sample-based MBR decoding for both offline ASR and Speech Translation (ST) tasks (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nOur experiments span multiple languages, with a focus on English and Japanese.\nWe use diverse datasets, multiple models based on Whisper, and with varying levels of synthesized noise added.\nMBR decoding consistently outperforms beam search across these dimensions, often by substantial margins. Remarkably, these improvements emerge with as few as 4-8 samples, suggesting that MBR can be practically implemented in scenarios where latency requirements are not stringent.</p>\n\n",
                "matched_terms": [
                    "models",
                    "implemented",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given that <math alttext=\"\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Y}</annotation></semantics></math> is typically very large in text generation tasks, it is often infeasible to enumerate all possible output sequences in <math alttext=\"\\mathcal{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119988;</mi><annotation encoding=\"application/x-tex\">\\mathcal{Y}</annotation></semantics></math>.\nThus, local optimal search methods such as beam search are used to approximate the MAP estimate as the language models are typically modeled by a autoregressive models <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib80\" title=\"\">2017</a>)</cite>.\nHowever, MAP decoding, including beam search, is known to generate undesirable outputs, such as an empty sequence, a sequence with repeated tokens, or low-quality text <cite class=\"ltx_cite ltx_citemacro_cite\">Wiseman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib85\" title=\"\">2017</a>); Holtzman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib28\" title=\"\">2020</a>); Eikema and Aziz (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib17\" title=\"\">2020</a>)</cite>.\nThus, alternative decoding algorithms have been investigated to improve the quality of the generated text.</p>\n\n",
                "matched_terms": [
                    "used",
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">MBR decoding is an instance of a <span class=\"ltx_text ltx_font_bold\">reranking algorithm</span> or best-of-<math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> sampling that re-evaluates the candidate outputs, selecting the best output according to some criteria <cite class=\"ltx_cite ltx_citemacro_cite\">Morbini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib52\" title=\"\">2012</a>); Chiu and Chen (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib14\" title=\"\">2021</a>); Xu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib90\" title=\"\">2022</a>); Nakano et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib54\" title=\"\">2022</a>); Ichihara et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib33\" title=\"\">2025b</a>)</cite>.\nSeveral reranking methods have been proposed for speech recognition using quality estimation <cite class=\"ltx_cite ltx_citemacro_cite\">Negri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib56\" title=\"\">2014</a>); Ng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib57\" title=\"\">2015</a>); Ali and Renals (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib4\" title=\"\">2018</a>); Yuksel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib94\" title=\"\">2023</a>); Waheed et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib81\" title=\"\">2025</a>)</cite>, perplexity <cite class=\"ltx_cite ltx_citemacro_cite\">Salazar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib69\" title=\"\">2020</a>)</cite>, deliberation models <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib30\" title=\"\">2020</a>); Xu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib90\" title=\"\">2022</a>)</cite>, LLMs <cite class=\"ltx_cite ltx_citemacro_cite\">Mengxi Nie and Ming Yan and Caixia Gong (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib51\" title=\"\">2022</a>); Hu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib31\" title=\"\">2024</a>); Tur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib79\" title=\"\">2024</a>)</cite>, and speech-text foundational models <cite class=\"ltx_cite ltx_citemacro_cite\">Shivakumar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib72\" title=\"\">2025</a>)</cite>.\nThe advantage of MBR decoding compared to these approaches is that it does not require any additional training, making it easy to apply to new systems and languages.</p>\n\n",
                "matched_terms": [
                    "yuksel",
                    "models",
                    "tur"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:382.4pt;height:242.8pt;vertical-align:-118.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Model</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">whisper-small</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">whisper-medium</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\">Metric</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.067</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.100</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.087</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.058</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.850</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.077</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.067</span>\n<span class=\"ltx_td ltx_align_center\">2.100</span>\n<span class=\"ltx_td ltx_align_center\">0.087</span>\n<span class=\"ltx_td ltx_align_center\">0.058</span>\n<span class=\"ltx_td ltx_align_center\">1.850</span>\n<span class=\"ltx_td ltx_align_center\">0.077</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.067</span>\n<span class=\"ltx_td ltx_align_center\">2.100</span>\n<span class=\"ltx_td ltx_align_center\">0.087</span>\n<span class=\"ltx_td ltx_align_center\">0.058</span>\n<span class=\"ltx_td ltx_align_center\">1.850</span>\n<span class=\"ltx_td ltx_align_center\">0.077</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MBR (<math alttext=\"N=4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">N=4</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.058</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.950</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.078</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.049</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.730</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.070</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m11\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">N=8</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.054</span>\n<span class=\"ltx_td ltx_align_center\">1.900</span>\n<span class=\"ltx_td ltx_align_center\">0.076</span>\n<span class=\"ltx_td ltx_align_center\">0.043</span>\n<span class=\"ltx_td ltx_align_center\">1.680</span>\n<span class=\"ltx_td ltx_align_center\">0.067</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=16\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m12\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">N=16</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.052</span>\n<span class=\"ltx_td ltx_align_center\">1.830</span>\n<span class=\"ltx_td ltx_align_center\">0.072</span>\n<span class=\"ltx_td ltx_align_center\">0.042</span>\n<span class=\"ltx_td ltx_align_center\">1.650</span>\n<span class=\"ltx_td ltx_align_center\">0.066</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m13\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">N=32</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.050</span></span>\n<span class=\"ltx_td ltx_align_center\">1.830</span>\n<span class=\"ltx_td ltx_align_center\">0.072</span>\n<span class=\"ltx_td ltx_align_center\">0.042</span>\n<span class=\"ltx_td ltx_align_center\">1.630</span>\n<span class=\"ltx_td ltx_align_center\">0.065</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m14\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.051</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.800</span></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.072</span></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.041</span></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">1.630</span></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.065</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Model</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_3\">whisper-large-v3</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_3\">distil-large-v3.5</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Metric</span>\n<span class=\"ltx_td ltx_align_center\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m16\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m17\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m18\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m19\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m20\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m21\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.042</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.750</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.060</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.048</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.930</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.056</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m22\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.042</span>\n<span class=\"ltx_td ltx_align_center\">1.750</span>\n<span class=\"ltx_td ltx_align_center\">0.060</span>\n<span class=\"ltx_td ltx_align_center\">0.048</span>\n<span class=\"ltx_td ltx_align_center\">1.930</span>\n<span class=\"ltx_td ltx_align_center\">0.056</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m23\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.042</span>\n<span class=\"ltx_td ltx_align_center\">1.750</span>\n<span class=\"ltx_td ltx_align_center\">0.060</span>\n<span class=\"ltx_td ltx_align_center\">0.048</span>\n<span class=\"ltx_td ltx_align_center\">1.930</span>\n<span class=\"ltx_td ltx_align_center\">0.056</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MBR (<math alttext=\"N=4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m24\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">N=4</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.035</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.700</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.054</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.042</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.880</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.051</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m25\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">N=8</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.035</span>\n<span class=\"ltx_td ltx_align_center\">1.680</span>\n<span class=\"ltx_td ltx_align_center\">0.055</span>\n<span class=\"ltx_td ltx_align_center\">0.040</span>\n<span class=\"ltx_td ltx_align_center\">1.830</span>\n<span class=\"ltx_td ltx_align_center\">0.049</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=16\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m26\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">N=16</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.034</span>\n<span class=\"ltx_td ltx_align_center\">1.650</span>\n<span class=\"ltx_td ltx_align_center\">0.054</span>\n<span class=\"ltx_td ltx_align_center\">0.039</span>\n<span class=\"ltx_td ltx_align_center\">1.830</span>\n<span class=\"ltx_td ltx_align_center\">0.048</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m27\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">N=32</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.032</span></span>\n<span class=\"ltx_td ltx_align_center\">1.650</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.053</span></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.038</span></span>\n<span class=\"ltx_td ltx_align_center\">1.800</span>\n<span class=\"ltx_td ltx_align_center\">0.047</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m28\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.033</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">1.650</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.053</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\">0.038</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">1.800</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.045</span></span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "whispersmall",
                    "whisperlargev3",
                    "whispermedium"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of the study is to evaluate MBR decoding for ASR and ST tasks, compared to beam search.\nWe investigate various settings, including different models, datasets, and levels of noise added to the input audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "study",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct experiments to evaluate the performance of MBR decoding and beam search on various ASR and speech translation tasks.\nFor evaluating the methods under noise, we use the free-sound subset of the MUSAN dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Snyder et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib73\" title=\"\">2015</a>)</cite> to induce background noise to the audio.\nWe sample a noise randomly from the freesound subset of the dataset and crop it to match the length of the input audio. The cropped noise audio is synthesized to the speech with the level of Signal-to-Noise Ratio (SNR) set to 0 dB, noted otherwise.\nThe same noise is used for all the decoding algorithms for fair comparison.</p>\n\n",
                "matched_terms": [
                    "used",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For beam search, we run with a beam width of 1, 5, and 20.\nWe generate up to 64 samples for MBR decoding as hypotheses using Epsilon sampling <cite class=\"ltx_cite ltx_citemacro_cite\">Hewitt et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib27\" title=\"\">2022</a>); Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib20\" title=\"\">2023</a>)</cite> with <math alttext=\"\\epsilon=0.01\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#1013;</mi><mo>=</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">\\epsilon=0.01</annotation></semantics></math> and a temperature set to <math alttext=\"1.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p2.m2\" intent=\":literal\"><semantics><mn>1.0</mn><annotation encoding=\"application/x-tex\">1.0</annotation></semantics></math>.\nWe use the BLEU score <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib60\" title=\"\">2002</a>)</cite> implemented by the sacrebleu package <cite class=\"ltx_cite ltx_citemacro_cite\">Post (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib62\" title=\"\">2018</a>)</cite> as the utility function of MBR. We do not use WER (CER) as the utility function because MBR decoding is known to inflate the score used as the utility function which may not accurately reflect a model&#8217;s true capabilities <cite class=\"ltx_cite ltx_citemacro_cite\">Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib21\" title=\"\">2022</a>); Kovacs et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib42\" title=\"\">2024</a>)</cite>.\nBLEU scores are computed on the normalized texts using whisper&#8217;s normalizer for English <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib64\" title=\"\">2023</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">neologdn</span> normalizer for Japanese <cite class=\"ltx_cite ltx_citemacro_cite\">Sato et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib70\" title=\"\">2017</a>)</cite> to avoid unnecessary penalization on punctuation.\nWe use MeCab tokenizer <cite class=\"ltx_cite ltx_citemacro_cite\">Kudo (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib43\" title=\"\">2005</a>)</cite> to tokenize Japanese text for computing the BLEU score.</p>\n\n",
                "matched_terms": [
                    "radford",
                    "used",
                    "because",
                    "implemented"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All the code of the experiments is implemented by Python 3 using Huggingface&#8217;s transformers library <cite class=\"ltx_cite ltx_citemacro_cite\">Wolf et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib86\" title=\"\">2020</a>)</cite>.\nThe experiments are conducted on Linux Ubuntu 22.04 using NVIDIA A100 GPUs.\nWhile the codebase is not optimized for efficiency, we report the walltime with our implementation as a reference in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S6\" title=\"6 Limitations &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "code",
                    "implemented",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate the performance of MBR decoding on ASR using LibriSpeech (clean) <cite class=\"ltx_cite ltx_citemacro_cite\">Panayotov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib59\" title=\"\">2015</a>)</cite>, AMI-IHM <cite class=\"ltx_cite ltx_citemacro_cite\">Carletta (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib9\" title=\"\">2007</a>)</cite>, and VoxPopuli <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib82\" title=\"\">2021a</a>)</cite> for English, ReazonSpeech <cite class=\"ltx_cite ltx_citemacro_cite\">Yin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib93\" title=\"\">2023</a>)</cite>, Common Voice-v8 <cite class=\"ltx_cite ltx_citemacro_cite\">Ardila et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib5\" title=\"\">2020</a>)</cite>, and JSUT <cite class=\"ltx_cite ltx_citemacro_cite\">Sonobe et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib74\" title=\"\">2017</a>)</cite> for Japanese.\nWe use Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib64\" title=\"\">2023</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span> for English and Kotoba-Whisper-v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0\" title=\"\">https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0</a></span></span></span> for Japanese ASR models.\nAll the audio files are resampled to 16 kHz to meet the Whisper model&#8217;s requirement.\nWe use the first 1000 samples in the test set of each dataset for the evaluation, skipping samples longer than 30 seconds so that they can be handled with the Whisper model at a single path of inference. In this way, we can evaluate the effect of MBR decoding disentangled from the effect of the long-form audio handling techniques <cite class=\"ltx_cite ltx_citemacro_cite\">Chiu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib13\" title=\"\">2019</a>); Narayanan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib55\" title=\"\">2019</a>); Koluguri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib41\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "sonobe",
                    "librispeech",
                    "2021a",
                    "all",
                    "yin",
                    "reazonspeech",
                    "ardila",
                    "carletta",
                    "models",
                    "radford",
                    "jsut",
                    "panayotov",
                    "amiihm",
                    "voxpopuli"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:530.6pt;height:73.6pt;vertical-align:-34.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Dataset</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">LibriSpeech</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">VoxPopuli</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">AMI-IHM</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\">Metric</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.081</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.250</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.091</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.117</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">1.500</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.067</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.380</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.280</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.264</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.081</span>\n<span class=\"ltx_td ltx_align_center\">2.230</span>\n<span class=\"ltx_td ltx_align_center\">0.092</span>\n<span class=\"ltx_td ltx_align_center\">0.117</span>\n<span class=\"ltx_td ltx_align_center\">1.500</span>\n<span class=\"ltx_td ltx_align_center\">0.067</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.380</span></span>\n<span class=\"ltx_td ltx_align_center\">2.280</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.264</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.082</span>\n<span class=\"ltx_td ltx_align_center\">2.200</span>\n<span class=\"ltx_td ltx_align_center\">0.090</span>\n<span class=\"ltx_td ltx_align_center\">0.117</span>\n<span class=\"ltx_td ltx_align_center\">1.500</span>\n<span class=\"ltx_td ltx_align_center\">0.067</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.380</span></span>\n<span class=\"ltx_td ltx_align_center\">2.280</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.264</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.057</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.000</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.077</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.098</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.400</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.053</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.568</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.200</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.284</span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "amiihm",
                    "voxpopuli",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use word error rate (WER) for English and character error rate (CER) for Japanese as the main evaluation metrics.\nThe same normalizers as BLEU scores are used for WER (whisper normalizer) and CER (<span class=\"ltx_text ltx_font_typewriter\">neologdn</span> normalizer).\nIn addition, SemDist <cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib39\" title=\"\">2021</a>)</cite> and MetricX (<span class=\"ltx_text ltx_font_typewriter\">metricx-23-xxl-v2p0</span>; <cite class=\"ltx_cite ltx_citemacro_citet\">Juraska et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib36\" title=\"\">2023</a></cite>) are used to evaluate the semantic similarity and overall quality of the generated outputs.\nSemDist is a metric that measures the semantic distance between the generated text and the reference text using the inner product of the embeddings of the texts, which is also known as other names such as cosine distance and contextual similarity in the NLP community <cite class=\"ltx_cite ltx_citemacro_cite\">Akula and Garibay (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib3\" title=\"\">2022</a>); Mukherjee and Shrivastava (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib53\" title=\"\">2022</a>)</cite>.\nIt is proposed to complement the problem of WER (CER), which does not capture semantic similarity well, and thus, the effectiveness of the generation in the downstream tasks is not clear by itself.\nWe use a sentence BERT model named <span class=\"ltx_text ltx_font_typewriter\">all-MiniLM-L6-v2</span> as the embedding model to compute SemDist <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib66\" title=\"\">2019</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\" title=\"\">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a></span></span></span>\nMetricX is one of the state-of-the-art metrics for machine translation that evaluates the overall quality of the generated outputs by learning human MQM evaluation results. We use it for assessing the overall quality of the generated outputs.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "gurevych",
                    "reimers",
                    "juraska",
                    "allminilml6v2",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:521.4pt;height:73.6pt;vertical-align:-34.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Dataset</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">ReazonSpeech</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">CommonVoice</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3\">JSUT</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\">Metric</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">CER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">CER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">CER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SemDist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m10\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.305</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.975</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.143</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.306</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.825</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.134</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.183</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.250</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.088</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m11\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.307</span>\n<span class=\"ltx_td ltx_align_center\">2.975</span>\n<span class=\"ltx_td ltx_align_center\">0.143</span>\n<span class=\"ltx_td ltx_align_center\">0.302</span>\n<span class=\"ltx_td ltx_align_center\">2.875</span>\n<span class=\"ltx_td ltx_align_center\">0.132</span>\n<span class=\"ltx_td ltx_align_center\">0.185</span>\n<span class=\"ltx_td ltx_align_center\">2.350</span>\n<span class=\"ltx_td ltx_align_center\">0.089</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m12\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.308</span>\n<span class=\"ltx_td ltx_align_center\">3.050</span>\n<span class=\"ltx_td ltx_align_center\">0.140</span>\n<span class=\"ltx_td ltx_align_center\">0.306</span>\n<span class=\"ltx_td ltx_align_center\">2.875</span>\n<span class=\"ltx_td ltx_align_center\">0.133</span>\n<span class=\"ltx_td ltx_align_center\">0.184</span>\n<span class=\"ltx_td ltx_align_center\">2.350</span>\n<span class=\"ltx_td ltx_align_center\">0.090</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m13\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.291</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.875</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.130</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.297</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.725</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.123</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.177</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.200</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.082</span></span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "jsut",
                    "reazonspeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use word error rate (WER) for English and character error rate (CER) for Japanese as the main evaluation metrics.\nThe same normalizers as BLEU scores are used for WER (whisper normalizer) and CER (<span class=\"ltx_text ltx_font_typewriter\">neologdn</span> normalizer).\nIn addition, SemDist <cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib39\" title=\"\">2021</a>)</cite> and MetricX (<span class=\"ltx_text ltx_font_typewriter\">metricx-23-xxl-v2p0</span>; <cite class=\"ltx_cite ltx_citemacro_citet\">Juraska et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib36\" title=\"\">2023</a></cite>) are used to evaluate the semantic similarity and overall quality of the generated outputs.\nSemDist is a metric that measures the semantic distance between the generated text and the reference text using the inner product of the embeddings of the texts, which is also known as other names such as cosine distance and contextual similarity in the NLP community <cite class=\"ltx_cite ltx_citemacro_cite\">Akula and Garibay (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib3\" title=\"\">2022</a>); Mukherjee and Shrivastava (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib53\" title=\"\">2022</a>)</cite>.\nIt is proposed to complement the problem of WER (CER), which does not capture semantic similarity well, and thus, the effectiveness of the generation in the downstream tasks is not clear by itself.\nWe use a sentence BERT model named <span class=\"ltx_text ltx_font_typewriter\">all-MiniLM-L6-v2</span> as the embedding model to compute SemDist <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib66\" title=\"\">2019</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\" title=\"\">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a></span></span></span>\nMetricX is one of the state-of-the-art metrics for machine translation that evaluates the overall quality of the generated outputs by learning human MQM evaluation results. We use it for assessing the overall quality of the generated outputs.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "gurevych",
                    "reimers",
                    "juraska",
                    "allminilml6v2",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use whisper-small, whisper-medium, whisper-large-v3, and distil-whisper to evaluate the effect of the model size.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S3.T1\" title=\"Table 1 &#8227; 3 Related Work &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows that MBR decoding outperforms beam search in all the model sizes.\nThe result shows that MBR decoding is effective regardless of the model size.</p>\n\n",
                "matched_terms": [
                    "all",
                    "whispermedium",
                    "distilwhisper",
                    "whispersmall",
                    "whisperlargev3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To investigate how much the MBR objective indicates the utility of the given hypothesis, we compute the correlation of the MBR objective with the WER.\nPearson correlation coefficient is computed for each instance of the LibriSpeech with no synthesized noise over 64 samples generated by whisper-large-v3. Then, we estimate it with the average over the 1000 instances.\nThe average value of the Pearson correlation coefficient is -0.3913, and the standard error is 0.0129, indicating that the MBR objective has a substantial negative correlation with the target objective (negative correlation because MBR objective is higher the better, and WER is lower the better). This suggests that it is a reasonable approach to use it as the reranking procedure for ASR.\n</p>\n\n",
                "matched_terms": [
                    "because",
                    "whisperlargev3",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T2\" title=\"Table 2 &#8227; Resources. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T3\" title=\"Table 3 &#8227; Evaluation metrics. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> show the performance of the decoding algorithms using WER (CER), SemDist, and MetricX.\nMBR decoding outperforms beam search in all the datasets except for AMI-IHM, suggesting that the advantage of MBR decoding over beam search is in a wide range of domains and on both lexical and semantic levels.</p>\n\n",
                "matched_terms": [
                    "amiihm",
                    "metricx",
                    "datasets",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given that MBR decoding fails to improve on the AMI-IHM dataset, we conduct a post-hoc error analysis on the dataset.\nThe corpus records all utterances in the meeting recordings, including very short sentences and pseudowords. For example, some instances only have <span class=\"ltx_text ltx_font_italic\">yeah</span>, <span class=\"ltx_text ltx_font_italic\">hmm</span>, and <span class=\"ltx_text ltx_font_italic\">gosh</span> in the transcription. We hypothesize that the performance of MBR decoding is low on these instances because the BLEU score cannot evaluate the utility of these instances.\nTo this end, we compute the average WER of the beam search and MBR decoding on the AMI-IHM dataset, split according to the number of words in the reference transcription (Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.F3\" title=\"Figure 3 &#8227; Datasets. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>).\nWhile MBR decoding has comparable WER to beam search in most instances, it shows significantly higher WER on instances shorter than six words. The results suggest that beam search may be more suitable for very short sentences.</p>\n\n",
                "matched_terms": [
                    "because",
                    "amiihm",
                    "only",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance of MBR decoding is known to be dependent on the choice of the utility function <cite class=\"ltx_cite ltx_citemacro_cite\">Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib21\" title=\"\">2022</a>); Kovacs et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib42\" title=\"\">2024</a>)</cite>.\nWe evaluate MBR decoding using SentBERT <cite class=\"ltx_cite ltx_citemacro_cite\">Reimers and Gurevych (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib66\" title=\"\">2019</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib67\" title=\"\">2020</a>)</cite> and BLEURT (<span class=\"ltx_text ltx_font_typewriter\">BLEURT-20-D12</span>) in addition to using BLEU. SentBERT is a sentence-level embedding model that captures semantic similarity between sentences computed by the cosine similarity between the two embedding vectors. Thus, the value is 1 minus the value of SemDist.\nWe use the <span class=\"ltx_text ltx_font_typewriter\">all-MiniLM-L6-v2</span> model as the embedding model to compute SentBERT.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T6\" title=\"Table 6 &#8227; Evaluation metrics. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that the differences in accuracy using these utility functions are marginal, yet they all outperform beam search.\nThe result shows that the advantage of MBR decoding over beam search is robust to the choice of the utility function.\nSentBERT achieves the best SemDist score, which is expected as it is directly optimized for the metric <cite class=\"ltx_cite ltx_citemacro_cite\">Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib21\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "bleurt",
                    "gurevych",
                    "reimers",
                    "allminilml6v2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess whether the performance of MBR decoding is language-specific or generic to natural language text generation tasks, we conduct experiments on the following languages: Arabic (ar), simplified Chinese (zh-CN), Hindi (hi), Tamil (ta), Thai (th), and Vietnamese (vi).\nWe use the test split of the CommonVoice-v8 dataset and evaluate the WER (CER for Chinese). We use spaCy-Thai for segmenting words in Thai <cite class=\"ltx_cite ltx_citemacro_cite\">Zeman et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib95\" title=\"\">2017</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/spacy-thai/\" title=\"\">https://pypi.org/project/spacy-thai/</a></span></span></span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T8\" title=\"Table 8 &#8227; Evaluation metrics. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> shows the result.\nOverall, we observe MBR decoding to consistently outperform beam search in all the languages.\nThe result indicates that the method is effective across different languages.</p>\n\n",
                "matched_terms": [
                    "all",
                    "commonvoicev8",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:240.8pt;height:73.6pt;vertical-align:-34.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">LibriSpeech</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">ReazonSpeech</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m1\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.042</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.305</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">NoRefER (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.073</span>\n<span class=\"ltx_td ltx_align_center\">0.368</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ProGRes (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m3\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">0.043</span>\n<span class=\"ltx_td ltx_align_center\">0.358</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m4\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.033</span></span>\n<span class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.291</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">Oracle (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T9.m5\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.013</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.149</span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "progres",
                    "reazonspeech",
                    "norefer",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:527.1pt;height:146.8pt;vertical-align:-70.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Domain</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4\">CoVoST2 (Ja-En)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4\">FLEURS (Ja-En)</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Metric</span>\n<span class=\"ltx_td ltx_align_center\">BLEU<math alttext=\"\\uparrow^{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m1\" intent=\":literal\"><semantics><msup><mo stretchy=\"false\">&#8593;</mo><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">\\uparrow^{\\ast}</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">ROUGE-L<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">BLEURT<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">BLEU<math alttext=\"\\uparrow^{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m5\" intent=\":literal\"><semantics><msup><mo stretchy=\"false\">&#8593;</mo><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">\\uparrow^{\\ast}</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">Rouge-L<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">BLEURT<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m9\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">18.646</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">42.283</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">-0.184</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">2.825</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">6.218</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">30.178</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">-0.486</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">6.750</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m10\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">18.685</span>\n<span class=\"ltx_td ltx_align_center\">41.825</span>\n<span class=\"ltx_td ltx_align_center\">-0.201</span>\n<span class=\"ltx_td ltx_align_center\">2.850</span>\n<span class=\"ltx_td ltx_align_center\">6.158</span>\n<span class=\"ltx_td ltx_align_center\">29.954</span>\n<span class=\"ltx_td ltx_align_center\">-0.487</span>\n<span class=\"ltx_td ltx_align_center\">6.725</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m11\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">18.122</span>\n<span class=\"ltx_td ltx_align_center\">41.362</span>\n<span class=\"ltx_td ltx_align_center\">-0.235</span>\n<span class=\"ltx_td ltx_align_center\">2.950</span>\n<span class=\"ltx_td ltx_align_center\">6.202</span>\n<span class=\"ltx_td ltx_align_center\">29.886</span>\n<span class=\"ltx_td ltx_align_center\">-0.489</span>\n<span class=\"ltx_td ltx_align_center\">6.825</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m12\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">22.456</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">47.572</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">-0.073</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.475</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">8.078</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">34.212</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">-0.365</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">6.100</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Domain</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4\">CoVoST2 (En-Ja)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4\">FLEURS (En-Ja)</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Metric</span>\n<span class=\"ltx_td ltx_align_center\">BLEU<math alttext=\"\\uparrow^{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m13\" intent=\":literal\"><semantics><msup><mo stretchy=\"false\">&#8593;</mo><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">\\uparrow^{\\ast}</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">Rouge-L<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m14\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">BLEURT<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m15\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m16\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">BLEU<math alttext=\"\\uparrow^{\\ast}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m17\" intent=\":literal\"><semantics><msup><mo stretchy=\"false\">&#8593;</mo><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">\\uparrow^{\\ast}</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">Rouge-L<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m18\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">BLEURT<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m19\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span>\n<span class=\"ltx_td ltx_align_center\">MetricX<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m20\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m21\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">B=1</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">10.395</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">34.176</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">0.110</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">4.975</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">8.242</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">30.771</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">-0.015</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\">8.975</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m22\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">B=5</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">10.795</span>\n<span class=\"ltx_td ltx_align_center\">33.960</span>\n<span class=\"ltx_td ltx_align_center\">0.109</span>\n<span class=\"ltx_td ltx_align_center\">4.950</span>\n<span class=\"ltx_td ltx_align_center\">8.360</span>\n<span class=\"ltx_td ltx_align_center\">30.612</span>\n<span class=\"ltx_td ltx_align_center\">-0.019</span>\n<span class=\"ltx_td ltx_align_center\">8.950</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Beam (<math alttext=\"B=20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m23\" intent=\":literal\"><semantics><mrow><mi>B</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding=\"application/x-tex\">B=20</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center\">10.822</span>\n<span class=\"ltx_td ltx_align_center\">34.393</span>\n<span class=\"ltx_td ltx_align_center\">0.116</span>\n<span class=\"ltx_td ltx_align_center\">4.900</span>\n<span class=\"ltx_td ltx_align_center\">8.224</span>\n<span class=\"ltx_td ltx_align_center\">30.537</span>\n<span class=\"ltx_td ltx_align_center\">-0.015</span>\n<span class=\"ltx_td ltx_align_center\">8.900</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">MBR (<math alttext=\"N=64\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T10.m24\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">N=64</annotation></semantics></math>)</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">15.968</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">43.260</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.195</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.225</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">11.681</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">37.207</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.017</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">6.375</span></span></span>\n</span>\n</span></span>\n</span></span></p>\n\n",
                "matched_terms": [
                    "covost2",
                    "fleurs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to MBR decoding, we evaluate two reranking algorithms proposed recently.\nNoRefER selects the sentence with highest score according to a language model fine-tuned for the ASR reranking task <cite class=\"ltx_cite ltx_citemacro_cite\">Yuksel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib94\" title=\"\">2023</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/aixplain/NoRefER\" title=\"\">https://huggingface.co/aixplain/NoRefER</a></span></span></span>\nNoRefER does not use the audio input on reranking and relies solely on the generations.</p>\n\n",
                "matched_terms": [
                    "yuksel",
                    "norefer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">ProGRes selects the hypothesis using the weighted sum of the two objectives, LLM score and ASR score <cite class=\"ltx_cite ltx_citemacro_cite\">Tur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib79\" title=\"\">2024</a>)</cite>.\nLLM score is the perplexity of the hypothesis given a prompt articulated for the reranking task as a context <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px13.p2.m1\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>. We use the same prompt as in Section 2.1 of <cite class=\"ltx_cite ltx_citemacro_citet\">Tur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib79\" title=\"\">2024</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Tur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib79\" title=\"\">2024</a>)</cite> evaluate ProGRes using Llama-3, GPT-3.5, GPT-4 and show that GPT-4 achieves the best performance over the three. Unfortunately, the logits of GPT-3.5 and GPT-4 are no longer provided to the users, so it is not reproducible using these proprietary models. To this end, we use Llama-3 for computing the LLM score in the following experiment <cite class=\"ltx_cite ltx_citemacro_cite\">Grattafiori et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib24\" title=\"\">2024</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\" title=\"\">https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct</a></span></span></span>\nASR score is the loss value of the ASR model. We use cross-entropy loss, one of the standard loss functions for ASR models, as the loss function for Whisper is not disclosed <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib64\" title=\"\">2023</a>)</cite>.\nWe set the weight of the LLM score to <math alttext=\"\\alpha=0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS0.Px13.p2.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=0.05</annotation></semantics></math> as it performs the best in the experiments by <cite class=\"ltx_cite ltx_citemacro_citet\">Tur et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib79\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "llama3",
                    "models",
                    "radford",
                    "tur",
                    "grattafiori",
                    "progres"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T9\" title=\"Table 9 &#8227; Languages. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the comparison of the reranking algorithms on LibriSpeech and ReazonSpeech. Overall, we observe the performance of the algorithms to be suboptimal compared to MBR decoding and beam search.\nNoRefER is trained to distinguish models compressed into different sizes so that they have sufficiently different accuracy <cite class=\"ltx_cite ltx_citemacro_cite\">Yuksel et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib94\" title=\"\">2023</a>)</cite>. Thus, it may be less effective for reranking samples from the same model.</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "models",
                    "reazonspeech",
                    "yuksel",
                    "norefer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the English and Japanese subsets of CoVoST2 <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib83\" title=\"\">2021b</a>)</cite> and FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib15\" title=\"\">2023</a>)</cite> datasets for speech translation.\nWe use Kotoba-Whisper-Bilingual for speech translation system.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0\" title=\"\">https://huggingface.co/kotoba-tech/kotoba-whisper-bilingual-v1.0</a></span></span></span>\nKotoba-Whisper-Bilingual is a model fine-tuned on top of the distilled Whisper model and trained on a large amount of bilingual speech translation data.\nIt is one of the state-of-the-art open-source systems for bilingual speech recognition and translation for English and Japanese.</p>\n\n",
                "matched_terms": [
                    "wang",
                    "fleurs",
                    "covost2",
                    "datasets",
                    "kotobawhisperbilingual",
                    "2021b",
                    "conneau"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use BLEU using sacrebleu, ROUGE-L <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib48\" title=\"\">2004</a>)</cite>, BLEURT <cite class=\"ltx_cite ltx_citemacro_cite\">Sellam et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib71\" title=\"\">2020</a>)</cite>, and MetricX as the evaluation metrics.\nThe other settings are the same as the ASR.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T10\" title=\"Table 10 &#8227; Languages. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> shows the results of the experiments.\nOverall, MBR decoding outperforms beam search in all the metrics in both language pairs and datasets.</p>\n\n",
                "matched_terms": [
                    "metricx",
                    "all",
                    "bleurt",
                    "datasets",
                    "sellam"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Note that MBR decoding tends to achieve a relatively higher score than the others on the utility function used during the decoding process <cite class=\"ltx_cite ltx_citemacro_cite\">Freitag et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib21\" title=\"\">2022</a>)</cite>, which may be indicative of overfitting.\nThus, BLEU scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S4.T10\" title=\"Table 10 &#8227; Languages. &#8227; 4.1 Automatic Speech Recognition (ASR) &#8227; 4 Experiments &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> should be interpreted as references.</p>\n\n",
                "matched_terms": [
                    "used",
                    "others"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we empirically evaluate the performance of MBR decoding for offline ASR and ST tasks.\nWe compare MBR decoding and beam search on a wide range of scenarios with various models, languages, datasets, noise levels, evaluation metrics, and hyperparameters.\nThe extensive evaluation shows that MBR decoding consistently achieves higher accuracy than beam search in both speech-to-text tasks.</p>\n\n",
                "matched_terms": [
                    "models",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#S6.T11\" title=\"Table 11 &#8227; 6 Limitations &#8227; Re-evaluating Minimum Bayes Risk Decoding for Automatic Speech Recognition\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the average walltime on the LibriSpeech dataset with the whisper-large-v3 model.\nNote that because the experiment is not conducted to evaluate the walltime of the decoding algorithms, our codebase is not optimized to reduce the walltime.\nFor example, the reported values include the time for logging and sending the generated hypotheses to a cloud server, which adds to the overall time.\nAlso note that the walltime also depends on the choice of the utility function. Currently, computing the BLEU scores on CPU is taking the majority of the computation time. We find that using SentBERT as the utility function is much faster than using BLEU, as SentBERT runs on a GPU in parallel and does not require CPU/GPU data transfer.\nThus, the reported time does not reflect the performance of optimized implementations and should solely be considered as a reference.</p>\n\n",
                "matched_terms": [
                    "because",
                    "whisperlargev3",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments are conducted using sequence-to-sequence autoregressive models <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib80\" title=\"\">2017</a>); Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib64\" title=\"\">2023</a>)</cite>.\nHowever, MBR decoding is a general decoding algorithm that can be applied to any probabilistic model.\nEvaluation of MBR decoding to other types of models (e.g., CTC-based models) is left for future work <cite class=\"ltx_cite ltx_citemacro_cite\">Kim et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib40\" title=\"\">2017</a>); Baevski et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib6\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "radford",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The MUSAN dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Snyder et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19471v1#bib.bib73\" title=\"\">2015</a>)</cite> covers a wide range of noise types, but it may not fully represent the noise encountered in all the communities and regions. Evaluation using real-world noisy datasets for the particular communities and regions is left for future work.</p>\n\n",
                "matched_terms": [
                    "datasets",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All experiments are conducted using public datasets and models that are available for research purposes.\nThe work aims to improve accuracy in speech-to-text tasks and does not pose immediately foreseeable risks of harm or misuse.\nWe encourage responsible use of ASR technologies and recommend further evaluation in real-world deployments.</p>\n\n",
                "matched_terms": [
                    "available",
                    "models",
                    "datasets",
                    "all"
                ]
            }
        ]
    }
}