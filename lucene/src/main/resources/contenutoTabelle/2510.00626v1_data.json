{
    "S3.T1": {
        "caption": "Table 1: Correctness change ratios across interference conditions. Results are reported on GSM8K, MMLU, and ARC-Challenge.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Cond. Pair</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GSM8K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MMLU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ARC</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni-3B</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">silence/noise</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.057</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.078</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.048</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">silence/fsd50k</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.086</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.119</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.079</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">noise/fsd50k</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.084</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.120</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.077</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4-multimodal</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">silence/noise</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.083</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.159</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.113</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">silence/fsd50k</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.112</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.174</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.120</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">noise/fsd50k</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.095</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.164</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.114</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "arcchallenge",
            "silencefsd50k",
            "mmlu",
            "phi4multimodal",
            "change",
            "reported",
            "cond",
            "gsm8k",
            "conditions",
            "across",
            "pair",
            "results",
            "silencenoise",
            "interference",
            "arc",
            "model",
            "noisefsd50k",
            "correctness",
            "qwen25omni3b",
            "ratios"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Scaling Interference Effects &#8227; 3 Analysis &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares how model predictions change under different types of irrelevant audio by reporting the correctness change ratio. Each value reflects the proportion of samples where the model&#8217;s prediction flipped from correct to incorrect, or vice versa, when moving between two interference conditions. For instance, a value of 0.057 for &#8220;silence/noise&#8221; in Qwen2.5-Omni-3B means that 5.7% of predictions changed correctness when the exact input text was paired with silence instead of Gaussian noise. For both Qwen2.5-Omni-3B and Phi-4-Multimodal, the silence&#8211;noise ratio is consistently lowest, indicating that these models treat silence and Gaussian noise as nearly equivalent. Additionally, we observe that the silence versus FSD50K comparison yields the highest ratio, with noise versus FSD50K falling in between, suggesting that FSD50K audio is perceptually closer to noise than to silence for these models.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large audio-language models (LALMs) unify speech and text processing, but their robustness in noisy real-world settings remains underexplored. We investigate how irrelevant audio, such as silence, synthetic noise, and environmental sounds, affects text reasoning tasks where audio is unnecessary. Across three text-based benchmarks, we find that even non-informative audio reduces accuracy and increases prediction volatility; the severity of interference scales with longer durations, higher amplitudes, and elevated decoding temperatures. Silence, often assumed neutral, destabilizes outputs as strongly as synthetic noise. While larger models show greater resilience, vulnerabilities persist across all evaluated systems. We further test mitigation strategies and find that prompting shows limited effectiveness, whereas self-consistency improves stability at the cost of increased computation. Our results reveal cross-modal interference as a key robustness challenge and highlight the need for efficient fusion strategies that preserve reasoning performance in the presence of irrelevant inputs. The code and data are publicly available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/lca0503/AudioInterference\" title=\"\">https://github.com/lca0503/AudioInterference</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "interference",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we ask: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">To what extent are LALMs vulnerable to irrelevant audio during text reasoning tasks?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To address this question, we systematically evaluate the effect of non-informative audio on established text-based benchmarks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib29\" title=\"\">29</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In our setup, the textual input remains fixed while the audio channel varies with non-semantic perturbations such as silence, noise, or environmental sound&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We also conduct ablations, varying decoding temperature, audio duration, and noise amplitude, to study how interference strength scales across conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "across",
                    "interference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We find that irrelevant audio lowers accuracy and alters model outputs, even when text alone is sufficient to solve the task. Surprisingly, even silence can interfere. Degradation intensifies with longer or louder noise, or with extended silence, and becomes especially pronounced at higher temperatures. Naive mitigation, such as prepending instructional phrases, proves ineffective. A simple self-consistency&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> approach offers partial mitigation but increases test-time cost&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, suggesting future work should seek more efficient solutions. These findings establish cross-modal interference as an essential evaluation axis and highlight the need for fusion strategies that preserve reasoning against irrelevant multimodal inputs.</span>\n</p>\n\n",
                "matched_terms": [
                    "interference",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the normal case, the model input is </span>\n  <math alttext=\"(\\emptyset,x_{\\text{text}}),\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8709;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">x</mi>\n            <mtext mathsize=\"0.900em\">text</mtext>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(\\emptyset,x_{\\text{text}}),</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where </span>\n  <math alttext=\"\\emptyset\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8709;</mi>\n      <annotation encoding=\"application/x-tex\">\\emptyset</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the absence of audio. We introduce audio signals that should not affect task performance to study interference. These signals may include stretches of silence, bursts of synthetic noise, or unrelated real-world sounds such as rainfall, flowing water, or animal calls. Under interference, the model input becomes </span>\n  <math alttext=\"(\\delta_{\\text{audio}},x_{\\text{text}}),\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#948;</mi>\n            <mtext mathsize=\"0.900em\">audio</mtext>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">x</mi>\n            <mtext mathsize=\"0.900em\">text</mtext>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(\\delta_{\\text{audio}},x_{\\text{text}}),</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where </span>\n  <math alttext=\"\\delta_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#948;</mi>\n        <mtext mathsize=\"0.900em\">audio</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\delta_{\\text{audio}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents non-informative audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "interference",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Benchmarks</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;For tasks that rely entirely on textual reasoning, we evaluate models on three widely used benchmarks: GSM8K&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for arithmetic reasoning, ARC-Challenge&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for science question answering, and MMLU&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for massive multi-task language understanding. To probe robustness, we introduce three types of audio interference:\n(i) five seconds of silence,\n(ii) five seconds of synthetic Gaussian noise at </span>\n  <math alttext=\"-40\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">40</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-40</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dBFS,\n(iii) real-world audio samples drawn from FSD50K&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a diverse dataset of music, environmental sounds, and sound effects. This setup allows us to study how irrelevant audio influences text-based reasoning tasks systematically.</span>\n</p>\n\n",
                "matched_terms": [
                    "mmlu",
                    "arcchallenge",
                    "interference",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Models</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We evaluate a diverse set of state-of-the-art open-source LALMs to assess robustness against irrelevant audio in text reasoning tasks. Our model set includes Qwen2.5-Omni-3B, Qwen2.5-Omni-7B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Phi-4-Multimodal-Instruct&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Voxtral-Mini-3B, Voxtral-Small-24B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and DeSTA2.5-Audio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These models differ in parameter size and architectural design, providing a representative view of current multimodal approaches. For most models, we adopt greedy decoding to isolate performance without the added variance of stochastic sampling, which makes the evaluation more stable under controlled perturbations. The only exception is the Voxtral series, where we follow the authors&#8217; recommended configuration and apply nucleus sampling with temperature set to 0.2 and top-p set to 0.95. We use vLLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib33\" title=\"\">33</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for inference, except DeSTA2.5-Audio, which uses the Transformers&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> package.</span>\n</p>\n\n",
                "matched_terms": [
                    "qwen25omni3b",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Metrics</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We adopt two evaluation metrics to assess robustness under irrelevant audio. First, we report accuracy, defined as the proportion of correctly answered samples relative to the total number of samples in the dataset: </span>\n  <math alttext=\"\\textit{Accuracy}=n_{c}/N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext class=\"ltx_mathvariant_italic\" mathsize=\"0.900em\">Accuracy</mtext>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">n</mi>\n            <mi mathsize=\"0.900em\">c</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n          <mi mathsize=\"0.900em\">N</mi>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\textit{Accuracy}=n_{c}/N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwhere </span>\n  <math alttext=\"n_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">n</mi>\n        <mi mathsize=\"0.900em\">c</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">n_{c}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the number of correct predictions and </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the total number of samples, accuracy reflects overall task performance under different interference conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "interference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Second, we use the influence rate, a notion explored in other multimodal robustness studies&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, to quantify how often irrelevant audio changes model predictions. Let </span>\n  <math alttext=\"n_{ic}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">n</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">c</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">n_{ic}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> be the number of cases where a prediction flips from incorrect to correct, and </span>\n  <math alttext=\"n_{ci}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">n</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">i</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">n_{ci}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the number of cases where it flips from correct to incorrect. We compute the influence rate as </span>\n  <math alttext=\"\\textit{Influence Rate}=(n_{ic}+n_{ci})/N.\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mtext class=\"ltx_mathvariant_italic\" mathsize=\"0.900em\">Influence Rate</mtext>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mrow>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mrow>\n                <msub>\n                  <mi mathsize=\"0.900em\">n</mi>\n                  <mrow>\n                    <mi mathsize=\"0.900em\">i</mi>\n                    <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                    <mi mathsize=\"0.900em\">c</mi>\n                  </mrow>\n                </msub>\n                <mo mathsize=\"0.900em\">+</mo>\n                <msub>\n                  <mi mathsize=\"0.900em\">n</mi>\n                  <mrow>\n                    <mi mathsize=\"0.900em\">c</mi>\n                    <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n                    <mi mathsize=\"0.900em\">i</mi>\n                  </mrow>\n                </msub>\n              </mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n            <mi mathsize=\"0.900em\">N</mi>\n          </mrow>\n        </mrow>\n        <mo lspace=\"0em\" mathsize=\"0.900em\">.</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\textit{Influence Rate}=(n_{ic}+n_{ci})/N.</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nA higher value indicates greater sensitivity to irrelevant audio, regardless of whether the change improves or harms accuracy.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "change"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.1 Problem Formulation &#8227; 2 Investigating Cross-Modal Interference &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that all evaluated LALMs are vulnerable to cross-modal interference. In the plots, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes clean inputs without interference, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">silence</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> adds silent audio, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">noise</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> corresponds to Gaussian noise, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">fsd50k</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents a real-world audio sample from FSD50K. Across GSM8K, ARC-Challenge, and MMLU, introducing irrelevant audio consistently reduces accuracy compared to the clean setting. The absolute drops remain modest, yet they appear across all models and benchmarks, showing that even non-informative audio slightly harms performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "arcchallenge",
                    "interference",
                    "gsm8k",
                    "mmlu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The influence rate provides a clearer view of instability. Under interference, predictions frequently change, leading to noticeably higher influence rates across all conditions. Surprisingly, silence, often assumed to be a &#8220;neutral input&#8221;, destabilizes outputs when persistent. Silence and Gaussian noise produce similar effects, while FSD50K sometimes increases the disruption, but not uniformly across tasks. This variation suggests that models do not react to one type of distractor consistently; instead, any non-informative audio has the potential to shift predictions. Accuracy alone fails to capture the full extent of instability, as outputs may change substantially even when task performance appears steady.</span>\n</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "across",
                    "interference",
                    "change"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Another trend visible in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.1 Problem Formulation &#8227; 2 Investigating Cross-Modal Interference &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is that the degree of interference differs across tasks. MMLU, which requires broad multi-domain reasoning, suffers from larger performance degradation and higher instability than more structured tasks like GSM8K arithmetic or ARC-Challenge science questions.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "arcchallenge",
                    "interference",
                    "gsm8k",
                    "mmlu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Duration of Audio</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.3 Main Observation &#8227; 2 Investigating Cross-Modal Interference &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates the model&#8217;s performance and influence rate across different durations of irrelevant audio. The x-axis indicates the duration of added audio, </span>\n  <math alttext=\"\\emptyset\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8709;</mi>\n      <annotation encoding=\"application/x-tex\">\\emptyset</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents the clean baseline without any interference, while the values 1, 5, 10, and 30 denote durations of silence and Gaussian noise in seconds, respectively. As duration increases, accuracy consistently drops and influence rate rises, revealing that longer non-informative segments amplify cross-modal interference. Even silence, when extended, destabilizes reasoning, suggesting that persistent irrelevant signals are not ignored but gradually entangle in the fusion process. This scaling highlights that temporal persistence of irrelevant signals is a critical factor in LALMs&#8217; robustness.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "interference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Amplitude of Noise</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.3 Main Observation &#8227; 2 Investigating Cross-Modal Interference &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> also demonstrates the effect of noise amplitude on model robustness. We injected 5-second Gaussian noise segments at three intensity levels, -60 dBFS, -40 dBFS, and -20 dBFS. The results indicate a clear trend in which model accuracy consistently declines as the amplitude increases and the influence rate rises. This pattern indicates that louder noise exacerbates cross-modal interference, making the model less stable and more prone to prediction shifts. Even when textual reasoning should dominate, high-intensity noise disrupts the fusion process, amplifying instability and eroding performance on benchmarks. In other words, stronger noise levels act as a more forceful distractor, directly undermining the reliability of LALMs in text reasoning tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "interference",
                    "model",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S3.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 3.2 Comparative Analysis of Interference Types &#8227; 3 Analysis &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows how decoding temperature interacts with irrelevant audio during GSM8K reasoning. At low temperatures, both Qwen2.5-Omni-7B and Phi-4-Multimodal retain stable accuracy and relatively low influence rates, indicating that deterministic decoding reduces the impact of cross-modal interference. As temperature rises, however, accuracy begins to drop more steeply in the presence of silence, Gaussian noise, or FSD50K audio, revealing that stochastic sampling amplifies the destabilizing effect of irrelevant signals.</span>\n</p>\n\n",
                "matched_terms": [
                    "phi4multimodal",
                    "interference",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Comparing models, Qwen2.5-Omni-7B sustains higher accuracy across all conditions and shows a slower rise in influence rate, indicating stronger resilience to interference. By contrast, Phi-4-Multimodal suffers sharper accuracy declines and a steeper increase in influence rate, suggesting greater susceptibility to stochasticity and irrelevant audio. These trends demonstrate that temperature is not a neutral hyperparameter under interference but a critical factor interacting with model design to shape robustness.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "interference",
                    "model",
                    "conditions",
                    "phi4multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate two straightforward mitigation approaches to investigate whether simple strategies can alleviate the impact of irrelevant audio. The first approach is adding a mitigation prompt. Specifically, we prepend a short instructional phrase, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">&#8220;Focus on the text or audio that contains useful information.&#8221;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> before each question. This prompt is designed to remind the model to pay explicit attention only to modalities that contribute to solving the task, thereby reducing the likelihood of being distracted by non-informative audio streams. The second approach is self-consistency. Instead of relying on a single decoding output, we generate 8 responses with sampling temperature 0.5 and aggregate the final answer by majority voting. This ensemble-style decoding reduces prediction volatility by smoothing out spurious shifts introduced by audio interference.</span>\n</p>\n\n",
                "matched_terms": [
                    "interference",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The mitigation prompt yields only limited and inconsistent improvements. In some settings, it slightly lowers the influence rate, yet in others, it reduces accuracy or even amplifies instability. The variation across benchmarks and models shows that a single explicit instruction is insufficient to counteract cross-modal interference systematically.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "interference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In contrast, self-consistency consistently improves robustness relative to its baseline without mitigation. Accuracy increases across all interference types, and the influence rate decreases substantially. This approach stabilizes predictions and delivers more reliable outputs in the presence of irrelevant audio. The results demonstrate that aggregating multiple generations and selecting the majority answer effectively counteract instability introduced by audio interference.</span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "interference",
                    "results"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Comparing with several mitigation approach under different interference conditions.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Condition</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">GSM8K</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ARC-Challenge</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Acc </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">IR </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Acc </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">IR </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2.5-Omni-3B</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">clean</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7915</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7782</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">silence</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7915</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1016</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7765</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0904</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">noise</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7862</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0963</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7713</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1007</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">fsd50k</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7794</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1001</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7645</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1143</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ Prompt</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">clean</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7779</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7722</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">silence</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7817</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1054</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7773</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0802</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">noise</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7809</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1168</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7645</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0930</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">fsd50k</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7817</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1114</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7696</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1084</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ Self-Consistency</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">clean</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8552</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8157</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">silence</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8529</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0432</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8029</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0555</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">noise</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8514</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0478</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7986</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0597</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">fsd50k</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8628</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0576</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8080</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0691</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">Phi-4-multimodal</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">clean</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8120</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7884</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">silence</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8021</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1296</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7628</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1570</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">noise</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8029</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1440</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7440</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1655</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">fsd50k</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7900</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1463</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7543</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1621</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ Prompt</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">clean</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8188</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7816</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">silence</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7900</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1440</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7619</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1101</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">noise</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7892</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1539</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7543</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1160</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">fsd50k</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7991</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1365</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7474</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1263</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ Self-Consistency</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">clean</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8825</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8370</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">silence</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8688</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0637</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7961</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1075</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">noise</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8688</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0667</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7739</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1195</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">fsd50k</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.8590</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0720</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7619</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.1280</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "arcchallenge",
            "noise",
            "phi4multimodal",
            "downarrow",
            "mitigation",
            "fsd50k",
            "several",
            "gsm8k",
            "conditions",
            "comparing",
            "condition",
            "under",
            "silence",
            "uparrow",
            "interference",
            "clean",
            "model",
            "prompt",
            "approach",
            "acc",
            "selfconsistency",
            "qwen25omni3b",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S4.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 4.1 Methodology &#8227; 4 Straightforward Mitigation Approaches &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares the performance of the two mitigation strategies, prompting and self-consistency, under silence, Gaussian noise, and FSD50K interference. The influence rate is recalculated for each method relative to its own clean baseline.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large audio-language models (LALMs) unify speech and text processing, but their robustness in noisy real-world settings remains underexplored. We investigate how irrelevant audio, such as silence, synthetic noise, and environmental sounds, affects text reasoning tasks where audio is unnecessary. Across three text-based benchmarks, we find that even non-informative audio reduces accuracy and increases prediction volatility; the severity of interference scales with longer durations, higher amplitudes, and elevated decoding temperatures. Silence, often assumed neutral, destabilizes outputs as strongly as synthetic noise. While larger models show greater resilience, vulnerabilities persist across all evaluated systems. We further test mitigation strategies and find that prompting shows limited effectiveness, whereas self-consistency improves stability at the cost of increased computation. Our results reveal cross-modal interference as a key robustness challenge and highlight the need for efficient fusion strategies that preserve reasoning performance in the presence of irrelevant inputs. The code and data are publicly available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/lca0503/AudioInterference\" title=\"\">https://github.com/lca0503/AudioInterference</a>.</span>\n</p>\n\n",
                "matched_terms": [
                    "silence",
                    "mitigation",
                    "interference",
                    "noise",
                    "selfconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Large audio-language models (LALMs)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> have shown strong performance across a variety of multimodal tasks, showing the ability to process speech and text in a unified framework&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib10\" title=\"\">10</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib16\" title=\"\">16</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, most evaluations assume clean, modality-aligned inputs. In practice, text reasoning often requires no audio, yet deployed systems still receive streams containing silence, background noise, or incidental sounds. Intuitively, we might expect such irrelevant audio to have little or no effect on the model&#8217;s text-based reasoning. Surprisingly, our study reveals that even these non-informative signals can interfere with the fusion process and degrade performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "silence",
                    "noise",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Prior work has highlighted vulnerabilities of LALMs under more adversarial conditions</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib18\" title=\"\">18</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Some studies investigate injection attacks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while others explore cross-modal conflicts where audio and text contradict each other&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Several studies have also examined how distractions affect large language models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib23\" title=\"\">23</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and vision-language models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib26\" title=\"\">26</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, showing that models often struggle when exposed to irrelevant or misleading content. However, little attention has been given to the simpler but pervasive case of irrelevant audio that does not conflict with text yet degrades performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "under",
                    "several"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we ask: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">To what extent are LALMs vulnerable to irrelevant audio during text reasoning tasks?</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> To address this question, we systematically evaluate the effect of non-informative audio on established text-based benchmarks&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib29\" title=\"\">29</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In our setup, the textual input remains fixed while the audio channel varies with non-semantic perturbations such as silence, noise, or environmental sound&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We also conduct ablations, varying decoding temperature, audio duration, and noise amplitude, to study how interference strength scales across conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "silence",
                    "interference",
                    "noise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We find that irrelevant audio lowers accuracy and alters model outputs, even when text alone is sufficient to solve the task. Surprisingly, even silence can interfere. Degradation intensifies with longer or louder noise, or with extended silence, and becomes especially pronounced at higher temperatures. Naive mitigation, such as prepending instructional phrases, proves ineffective. A simple self-consistency&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib31\" title=\"\">31</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> approach offers partial mitigation but increases test-time cost&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib32\" title=\"\">32</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, suggesting future work should seek more efficient solutions. These findings establish cross-modal interference as an essential evaluation axis and highlight the need for fusion strategies that preserve reasoning against irrelevant multimodal inputs.</span>\n</p>\n\n",
                "matched_terms": [
                    "silence",
                    "mitigation",
                    "interference",
                    "noise",
                    "model",
                    "approach",
                    "selfconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We analyze how large audio-language models (LALMs) handle tasks that rely only on text when the audio channel introduces irrelevant or distracting content. Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates our problem setup, a text-only reasoning task with irrelevant audio signals such as silence, synthetic noise, or environmental sounds. Formally, the model generates predictions according to </span>\n  <math alttext=\"\\hat{y}=f_{\\theta}(x_{\\text{audio}},x_{\\text{text}}),\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mover accent=\"true\">\n            <mi mathsize=\"0.900em\">y</mi>\n            <mo mathsize=\"0.900em\">^</mo>\n          </mover>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">f</mi>\n              <mi mathsize=\"0.900em\">&#952;</mi>\n            </msub>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <msub>\n                <mi mathsize=\"0.900em\">x</mi>\n                <mtext mathsize=\"0.900em\">audio</mtext>\n              </msub>\n              <mo mathsize=\"0.900em\">,</mo>\n              <msub>\n                <mi mathsize=\"0.900em\">x</mi>\n                <mtext mathsize=\"0.900em\">text</mtext>\n              </msub>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </mrow>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\hat{y}=f_{\\theta}(x_{\\text{audio}},x_{\\text{text}}),</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where </span>\n  <math alttext=\"f_{\\theta}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">f_{\\theta}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the modeling process, </span>\n  <math alttext=\"x_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">x</mi>\n        <mtext mathsize=\"0.900em\">audio</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">x_{\\text{audio}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the audio input, </span>\n  <math alttext=\"x_{\\text{text}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">x</mi>\n        <mtext mathsize=\"0.900em\">text</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">x_{\\text{text}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the text input, and </span>\n  <math alttext=\"\\hat{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mover accent=\"true\">\n        <mi mathsize=\"0.900em\">y</mi>\n        <mo mathsize=\"0.900em\">^</mo>\n      </mover>\n      <annotation encoding=\"application/x-tex\">\\hat{y}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the model output.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "silence",
                    "noise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the normal case, the model input is </span>\n  <math alttext=\"(\\emptyset,x_{\\text{text}}),\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8709;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">x</mi>\n            <mtext mathsize=\"0.900em\">text</mtext>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(\\emptyset,x_{\\text{text}}),</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where </span>\n  <math alttext=\"\\emptyset\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8709;</mi>\n      <annotation encoding=\"application/x-tex\">\\emptyset</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the absence of audio. We introduce audio signals that should not affect task performance to study interference. These signals may include stretches of silence, bursts of synthetic noise, or unrelated real-world sounds such as rainfall, flowing water, or animal calls. Under interference, the model input becomes </span>\n  <math alttext=\"(\\delta_{\\text{audio}},x_{\\text{text}}),\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#948;</mi>\n            <mtext mathsize=\"0.900em\">audio</mtext>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">x</mi>\n            <mtext mathsize=\"0.900em\">text</mtext>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n        <mo mathsize=\"0.900em\">,</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(\\delta_{\\text{audio}},x_{\\text{text}}),</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> where </span>\n  <math alttext=\"\\delta_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#948;</mi>\n        <mtext mathsize=\"0.900em\">audio</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\delta_{\\text{audio}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents non-informative audio.</span>\n</p>\n\n",
                "matched_terms": [
                    "silence",
                    "interference",
                    "noise",
                    "model",
                    "under"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Benchmarks</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;For tasks that rely entirely on textual reasoning, we evaluate models on three widely used benchmarks: GSM8K&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for arithmetic reasoning, ARC-Challenge&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib28\" title=\"\">28</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for science question answering, and MMLU&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib29\" title=\"\">29</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for massive multi-task language understanding. To probe robustness, we introduce three types of audio interference:\n(i) five seconds of silence,\n(ii) five seconds of synthetic Gaussian noise at </span>\n  <math alttext=\"-40\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">40</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">-40</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dBFS,\n(iii) real-world audio samples drawn from FSD50K&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib30\" title=\"\">30</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a diverse dataset of music, environmental sounds, and sound effects. This setup allows us to study how irrelevant audio influences text-based reasoning tasks systematically.</span>\n</p>\n\n",
                "matched_terms": [
                    "silence",
                    "arcchallenge",
                    "interference",
                    "noise",
                    "fsd50k",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Models</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We evaluate a diverse set of state-of-the-art open-source LALMs to assess robustness against irrelevant audio in text reasoning tasks. Our model set includes Qwen2.5-Omni-3B, Qwen2.5-Omni-7B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Phi-4-Multimodal-Instruct&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, Voxtral-Mini-3B, Voxtral-Small-24B&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and DeSTA2.5-Audio&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. These models differ in parameter size and architectural design, providing a representative view of current multimodal approaches. For most models, we adopt greedy decoding to isolate performance without the added variance of stochastic sampling, which makes the evaluation more stable under controlled perturbations. The only exception is the Voxtral series, where we follow the authors&#8217; recommended configuration and apply nucleus sampling with temperature set to 0.2 and top-p set to 0.95. We use vLLM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib33\" title=\"\">33</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for inference, except DeSTA2.5-Audio, which uses the Transformers&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#bib.bib34\" title=\"\">34</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> package.</span>\n</p>\n\n",
                "matched_terms": [
                    "qwen25omni3b",
                    "under",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Metrics</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;We adopt two evaluation metrics to assess robustness under irrelevant audio. First, we report accuracy, defined as the proportion of correctly answered samples relative to the total number of samples in the dataset: </span>\n  <math alttext=\"\\textit{Accuracy}=n_{c}/N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mtext class=\"ltx_mathvariant_italic\" mathsize=\"0.900em\">Accuracy</mtext>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <msub>\n            <mi mathsize=\"0.900em\">n</mi>\n            <mi mathsize=\"0.900em\">c</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n          <mi mathsize=\"0.900em\">N</mi>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\textit{Accuracy}=n_{c}/N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwhere </span>\n  <math alttext=\"n_{c}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">n</mi>\n        <mi mathsize=\"0.900em\">c</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">n_{c}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the number of correct predictions and </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the total number of samples, accuracy reflects overall task performance under different interference conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "interference",
                    "under",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.1 Problem Formulation &#8227; 2 Investigating Cross-Modal Interference &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that all evaluated LALMs are vulnerable to cross-modal interference. In the plots, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">clean</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes clean inputs without interference, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">silence</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> adds silent audio, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">noise</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> corresponds to Gaussian noise, and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">fsd50k</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents a real-world audio sample from FSD50K. Across GSM8K, ARC-Challenge, and MMLU, introducing irrelevant audio consistently reduces accuracy compared to the clean setting. The absolute drops remain modest, yet they appear across all models and benchmarks, showing that even non-informative audio slightly harms performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "silence",
                    "arcchallenge",
                    "interference",
                    "noise",
                    "fsd50k",
                    "clean",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The influence rate provides a clearer view of instability. Under interference, predictions frequently change, leading to noticeably higher influence rates across all conditions. Surprisingly, silence, often assumed to be a &#8220;neutral input&#8221;, destabilizes outputs when persistent. Silence and Gaussian noise produce similar effects, while FSD50K sometimes increases the disruption, but not uniformly across tasks. This variation suggests that models do not react to one type of distractor consistently; instead, any non-informative audio has the potential to shift predictions. Accuracy alone fails to capture the full extent of instability, as outputs may change substantially even when task performance appears steady.</span>\n</p>\n\n",
                "matched_terms": [
                    "silence",
                    "interference",
                    "noise",
                    "fsd50k",
                    "under",
                    "conditions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In addition, we observe a scaling effect when comparing models with the same architecture but different parameter sizes; larger models generally achieve better performance and exhibit reduced sensitivity to interference. In other words, larger LALMs are more robust against irrelevant audio, showing minor accuracy drops and lower influence rates under the same perturbations. Beyond scaling, we also note that some models, such as Qwen2.5-Omni, display comparatively lower volatility than others of similar size. This suggests that factors beyond parameter count, such as training data and optimization design, may influence robustness, highlighting that resilience is shaped by multiple dimensions rather than scaling alone.</span>\n</p>\n\n",
                "matched_terms": [
                    "interference",
                    "under",
                    "comparing",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Another trend visible in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S2.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 2.1 Problem Formulation &#8227; 2 Investigating Cross-Modal Interference &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is that the degree of interference differs across tasks. MMLU, which requires broad multi-domain reasoning, suffers from larger performance degradation and higher instability than more structured tasks like GSM8K arithmetic or ARC-Challenge science questions.</span>\n</p>\n\n",
                "matched_terms": [
                    "arcchallenge",
                    "interference",
                    "gsm8k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Duration of Audio</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.3 Main Observation &#8227; 2 Investigating Cross-Modal Interference &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> illustrates the model&#8217;s performance and influence rate across different durations of irrelevant audio. The x-axis indicates the duration of added audio, </span>\n  <math alttext=\"\\emptyset\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8709;</mi>\n      <annotation encoding=\"application/x-tex\">\\emptyset</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents the clean baseline without any interference, while the values 1, 5, 10, and 30 denote durations of silence and Gaussian noise in seconds, respectively. As duration increases, accuracy consistently drops and influence rate rises, revealing that longer non-informative segments amplify cross-modal interference. Even silence, when extended, destabilizes reasoning, suggesting that persistent irrelevant signals are not ignored but gradually entangle in the fusion process. This scaling highlights that temporal persistence of irrelevant signals is a critical factor in LALMs&#8217; robustness.</span>\n</p>\n\n",
                "matched_terms": [
                    "silence",
                    "interference",
                    "noise",
                    "clean",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Amplitude of Noise</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8196;&#8202;Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.3 Main Observation &#8227; 2 Investigating Cross-Modal Interference &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> also demonstrates the effect of noise amplitude on model robustness. We injected 5-second Gaussian noise segments at three intensity levels, -60 dBFS, -40 dBFS, and -20 dBFS. The results indicate a clear trend in which model accuracy consistently declines as the amplitude increases and the influence rate rises. This pattern indicates that louder noise exacerbates cross-modal interference, making the model less stable and more prone to prediction shifts. Even when textual reasoning should dominate, high-intensity noise disrupts the fusion process, amplifying instability and eroding performance on benchmarks. In other words, stronger noise levels act as a more forceful distractor, directly undermining the reliability of LALMs in text reasoning tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "interference",
                    "noise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Scaling Interference Effects &#8227; 3 Analysis &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares how model predictions change under different types of irrelevant audio by reporting the correctness change ratio. Each value reflects the proportion of samples where the model&#8217;s prediction flipped from correct to incorrect, or vice versa, when moving between two interference conditions. For instance, a value of 0.057 for &#8220;silence/noise&#8221; in Qwen2.5-Omni-3B means that 5.7% of predictions changed correctness when the exact input text was paired with silence instead of Gaussian noise. For both Qwen2.5-Omni-3B and Phi-4-Multimodal, the silence&#8211;noise ratio is consistently lowest, indicating that these models treat silence and Gaussian noise as nearly equivalent. Additionally, we observe that the silence versus FSD50K comparison yields the highest ratio, with noise versus FSD50K falling in between, suggesting that FSD50K audio is perceptually closer to noise than to silence for these models.</span>\n</p>\n\n",
                "matched_terms": [
                    "silence",
                    "interference",
                    "noise",
                    "fsd50k",
                    "model",
                    "qwen25omni3b",
                    "under",
                    "conditions",
                    "phi4multimodal",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.00626v1#S3.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 3.2 Comparative Analysis of Interference Types &#8227; 3 Analysis &#8227; When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows how decoding temperature interacts with irrelevant audio during GSM8K reasoning. At low temperatures, both Qwen2.5-Omni-7B and Phi-4-Multimodal retain stable accuracy and relatively low influence rates, indicating that deterministic decoding reduces the impact of cross-modal interference. As temperature rises, however, accuracy begins to drop more steeply in the presence of silence, Gaussian noise, or FSD50K audio, revealing that stochastic sampling amplifies the destabilizing effect of irrelevant signals.</span>\n</p>\n\n",
                "matched_terms": [
                    "silence",
                    "interference",
                    "noise",
                    "fsd50k",
                    "gsm8k",
                    "phi4multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The influence rate highlights this compounding effect more clearly than accuracy alone. For both models, the influence rate is relatively low near greedy decoding but escalates sharply as temperature increases. This means that irrelevant audio reduces accuracy and actively makes predictions more volatile, flipping outcomes more frequently under higher sampling variance. The effect is particularly evident with real-world FSD50K inputs, where instability intensifies beyond what is observed with silence or synthetic noise.</span>\n</p>\n\n",
                "matched_terms": [
                    "silence",
                    "under",
                    "noise",
                    "fsd50k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Comparing models, Qwen2.5-Omni-7B sustains higher accuracy across all conditions and shows a slower rise in influence rate, indicating stronger resilience to interference. By contrast, Phi-4-Multimodal suffers sharper accuracy declines and a steeper increase in influence rate, suggesting greater susceptibility to stochasticity and irrelevant audio. These trends demonstrate that temperature is not a neutral hyperparameter under interference but a critical factor interacting with model design to shape robustness.</span>\n</p>\n\n",
                "matched_terms": [
                    "comparing",
                    "interference",
                    "model",
                    "under",
                    "conditions",
                    "phi4multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate two straightforward mitigation approaches to investigate whether simple strategies can alleviate the impact of irrelevant audio. The first approach is adding a mitigation prompt. Specifically, we prepend a short instructional phrase, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">&#8220;Focus on the text or audio that contains useful information.&#8221;</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> before each question. This prompt is designed to remind the model to pay explicit attention only to modalities that contribute to solving the task, thereby reducing the likelihood of being distracted by non-informative audio streams. The second approach is self-consistency. Instead of relying on a single decoding output, we generate 8 responses with sampling temperature 0.5 and aggregate the final answer by majority voting. This ensemble-style decoding reduces prediction volatility by smoothing out spurious shifts introduced by audio interference.</span>\n</p>\n\n",
                "matched_terms": [
                    "mitigation",
                    "interference",
                    "prompt",
                    "model",
                    "approach",
                    "selfconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The mitigation prompt yields only limited and inconsistent improvements. In some settings, it slightly lowers the influence rate, yet in others, it reduces accuracy or even amplifies instability. The variation across benchmarks and models shows that a single explicit instruction is insufficient to counteract cross-modal interference systematically.</span>\n</p>\n\n",
                "matched_terms": [
                    "mitigation",
                    "interference",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In contrast, self-consistency consistently improves robustness relative to its baseline without mitigation. Accuracy increases across all interference types, and the influence rate decreases substantially. This approach stabilizes predictions and delivers more reliable outputs in the presence of irrelevant audio. The results demonstrate that aggregating multiple generations and selecting the majority answer effectively counteract instability introduced by audio interference.</span>\n</p>\n\n",
                "matched_terms": [
                    "approach",
                    "mitigation",
                    "interference",
                    "selfconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In summary, mitigation prompts alone are insufficient to protect models against irrelevant audio, often yielding inconsistent or minor effects compared with their baseline. Self-consistency, on the other hand, reliably enhances both accuracy and robustness relative to its baseline, though at the cost of considerable computational overhead since multiple generations must be produced for each input.</span>\n</p>\n\n",
                "matched_terms": [
                    "mitigation",
                    "selfconsistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Our study shows that irrelevant audio can interfere with how large audio-language models reason over text. Silence, noise, and environmental sounds disrupted performance, and the impact grew with longer duration, louder volume, and higher decoding temperatures. Even silence, often assumed neutral, proved disruptive, destabilizing outputs as much as synthetic noise. Larger models and specific architectures showed greater resilience, but none were fully robust. Mitigation experiments revealed that prompting was ineffective, whereas self-consistency improved stability but introduced substantial test-time compute overhead. These findings establish cross-modal interference as a key robustness challenge and call for more efficient fusion strategies to preserve reasoning quality in realistic multimodal settings.</span>\n</p>\n\n",
                "matched_terms": [
                    "silence",
                    "mitigation",
                    "interference",
                    "noise",
                    "selfconsistency"
                ]
            }
        ]
    }
}